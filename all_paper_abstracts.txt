7E0F0E4A	Graphical models provide a powerful general framework for encoding the structure of large-scale estimation problems. However, the graphs describing typical real-world phenomena contain many cycles, making direct estimation procedures prohibitively costly. In this paper, we develop an iterative inference algorithm for general Gaussian graphical models. It operates by exactly solving a series of modified estimation problems on spanning trees embedded within the original cyclic graph. When these subproblems are suitably chosen, the algorithm converges to the correct conditional means. Moreover, and in contrast to many other iterative methods, the tree-based procedures we propose can also be used to calculate exact error variances. Although the conditional mean iteration is effective for quite densely connected graphical models, the error variance computation is most efficient for sparser graphs. In this context, we present a modeling example suggesting that very sparsely connected graphs with cycles may provide significant advantages relative to their tree-structured counterparts, thanks both to the expressive power of these models and to the efficient inference algorithms developed herein. The convergence properties of the proposed tree-based iterations are characterized both analytically and experimentally. In addition, by using the basic tree-based iteration to precondition the conjugate gradient method, we develop an alternative, accelerated iteration that is finitely convergent. Simulation results are presented that demonstrate this algorithm effectiveness on several inference problems, including a prototype distributed sensing application.
7F9FC5C6	Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse.
6049E046	A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.
7E144371	In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.
7FF9B70B	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion. 
81431585	Traditional binary hypothesis testing relies on the precise knowledge of the probability density of an observed random vector conditioned on each hypothesis. However, for many applications, these densities can only be approximated due to limited training data or dynamic changes affecting the observed signal. A classical approach to handle such scenarios of imprecise knowledge is via minimax robust hypothesis testing (RHT), where a test is designed to minimize the worst case performance for all models in the vicinity of the approximated imprecise density. Despite the promise of RHT for robust classification problems, its applications have remained rather limited because RHT in its native form does not scale gracefully with the dimension of the observed random vector. In this paper, we use approximations via probabilistic graphical models, in particular block-tree graphs, to enable computationally tractable algorithms for realizing RHT on high-dimensional data. We quantify the reductions in computational complexity. Experimental results on simulated data and a target recognition problem show minimal loss over a true RHT.
7CF1D0CA	The use of Bayesian networks for classification problems has received a significant amount of recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal of classification (label prediction accuracy). Recent approaches to optimizing classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present boosted Bayesian network classifiers, a framework to combine discriminative data-weighting with generative training of intermediate models. We show that boosted Bayesian network classifiers encompass the basic generative models in isolation, but improve their classification performance when the model structure is suboptimal. We also demonstrate that structure learning is beneficial in the construction of boosted Bayesian network classifiers. On a large suite of benchmark data-sets, this approach outperforms generative graphical models such as naive Bayes and TAN in classification accuracy. Boosted Bayesian network classifiers have comparable or better performance in comparison to other discriminatively trained graphical models including ELR and BNC. Furthermore, boosted Bayesian networks require significantly less training time than the ELR and BNC algorithms.
7EBA7FB7	The literature review presented discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The article avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples.
7A89250C	Graphical Markov models use undirected graphs (UDGs), acyclic directed graphs (ADGs), or (mixed) chain graphs to represent possible dependencies among random variables in a multivariate distribution. Whereas a UDG is uniquely determined by its associated Markov model, this is not true for ADGs or for general chain graphs (which include both UDGs and ADGs as special cases). This paper addresses three questions regarding the equivalence of graphical Markov models: when is a given chain graph Markov equivalent (1) to some UDG? (2) to some (at least one) ADG? (3) to some decomposable UDG? The answers are obtained by means of an extension of Frydenberg (1990) elegant graph-theoretic characterization of the Markov equivalence of chain graphs.
7E62FD1B	In this paper, we consider the problem of learning Gaussian multiresolution (MR) models in which data are only available at the finest scale, and the coarser, hidden variables serve to capture long-distance dependencies. Tree-structured MR models have limited modeling capabilities, as variables at one scale are forced to be uncorrelated with each other conditioned on other scales. We propose a new class of Gaussian MR models in which variables at each scale have sparse conditional covariance structure conditioned on other scales. Our goal is to learn a tree-structured graphical model connecting variables across scales (which translates into sparsity in inverse covariance), while at the same time learning sparse structure for the conditional covariance (not its inverse) within each scale conditioned on other scales. This model leads to an efficient, new inference algorithm that is similar to multipole methods in computational physics. We demonstrate the modeling and inference advantages of our approach over methods that use MR tree models and single-scale approximation methods that do not use hidden variables.
7A730712	Markov networks are a common class of graphical models used in machine learning. Such models use an undirected graph to capture dependency information among random variables in a joint probability distribution. Once one has chosen to use a Markov network model, one aims to choose the model that best explains the data that has been observed this model can then be used to make predictions about future data. We show that the problem of learning a maximum likelihood Markov network given certain observed data can be reduced to the problem of identifying a maximum weight low-treewidth graph under a given input weight function. We give the first constant factor approximation algorithm for this problem. More precisely, for any fixed treewidth objective k, we find a treewidth-k graph with an f(k) fraction of the maximum possible weight of any treewidth-k graph.
7E25BD71	Sparse graphical models have proven to be a flexible class of multivariate probability models for approximating high-dimensional distributions. In this paper, we propose techniques to exploit this modeling ability for binary classification by discriminatively learning such models from labeled training data, i.e., using both positive and negative samples to optimize for the structures of the two models. We motivate why it is difficult to adapt existing generative methods, and propose an alternative method consisting of two parts. First, we develop a novel method to learn tree-structured graphical models which optimizes an approximation of the log-likelihood ratio. We also formulate a joint objective to learn a nested sequence of optimal forests-structured models. Second, we construct a classifier by using ideas from boosting to learn a set of discriminative trees. The final classifier can interpreted as a likelihood ratio test between two models with a larger set of pairwise features. We use cross-validation to determine the optimal number of edges in the final model. The algorithm presented in this paper also provides a method to identify a subset of the edges that are most salient for discrimination. Experiments show that the proposed procedure outperforms generative methods such as Tree Augmented Naive Bayes and Chow-Liu as well as their boosted counterparts.
638196DE	This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks frorn data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented.The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.
8000315F	Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.
7F3655D3	We show that the class of strongly connected graphical models with treewidth at most k can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for k > 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time.
79B9CC7E	This paper deals with chain graphs under the Andersson Madigan Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score + search learning algorithms under assumptions weaker than faithfulness. We also study the problem of how to represent the result of marginalizing out some nodes in an AMP CG. We introduce a new family of graphical models that solves this problem partially. We name this new family maximal covariance concentration graphs because it includes both covariance and concentration graphs as subfamilies.
797F1FB0	We define and investigate classes of statistical models for the analysis of associations between variables, some of which are qualitative and some quantitative. In the cases where only one kind of variables is present, the models are well-known models for either contingency tables or covariance structures. We characterize the subclass of decomposable models where the statistical theory is especially simple. All models can be represented by a graph with one vertex for each variable. The vertices are possibly connected with arrows or lines corresponding to directional or symmetric associations being present. Pairs of vertices that are not connected are conditionally independent given some of the remaining variables according to specific rules.
7C7FFC7E	We study the problem of projecting a distribution  onto  (or  finding  a  maximum  likelihood distribution  among)  Markov networks  of  bounded tree-width. By casting it as the combinatorial optimization problem of finding a maximum weight hypertree, we prove that it is NP-hard to solve exactly and provide an approximation algorithm with a provable performance guarantee.
805ED9C0	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on $\ell_1$-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an $\ell_1$-constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes $p$ and maximum neighborhood sizes $d$ are allowed to grow as a function of the number of observations $n$. Our main results provide sufficient conditions on the triple $(n, p, d)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as $\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions are imposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$ samples are sufficient.
7BDE06A5	Advances in acoustic sensing have enabled the simultaneous acquisition of multiple measurements of the same physical event via co-located acoustic sensors. We exploit the inherent correlation among such multiple measurements for acoustic signal classification,to identify the launch/impact of munition (i.e., rockets, mortars).Specifically, we propose a prob- abilistic  graphical  model framework that can explicitly learn the class conditional correlations between the cepstral features extracted from these different measurements.Additionally, we employ symbolic dynamic filtering-based features, which offer improvements over the traditional cepstral features in terms of robustness to signal distortions.Experiments on real acoustic data sets show that our proposed algorithm outperforms conventional classifiers as well as the recently proposed joint sparsity models for multisensor acoustic classification.Additionally our proposed algorithm is less sensitive to insufficiency in training samples compared to competing approaches.
803EA8B7	We propose a new iterative, distributed approach for linear minimum mean-square-error (LMMSE) estimation in graphical models with cycles. The embedded subgraphs algorithm (ESA) decomposes a loopy graphical model into a number of linked embedded subgraphs and applies the classical parallel block Jacobi iteration comprising local LMMSE estimation in each subgraph (involving inversion of a small matrix) followed by an information exchange between neighboring nodes and subgraphs. Our primary application is sensor networks, where the model encodes the correlation structure of the sensor measurements, which are assumed to be Gaussian. The resulting LMMSE estimation problem involves a large matrix inverse, which must be solved in-network with distributed computation and minimal intersensor communication. By invoking the theory of asynchronous iterations, we prove that ESA is robust to temporary communication faults such as failing links and sleeping nodes, and enjoys guaranteed convergence under relatively mild conditions. Simulation studies demonstrate that ESA compares favorably with other recently proposed algorithms for distributed estimation. Simulations also indicate that energy consumption for iterative estimation increases substantially as more links fail or nodes sleep. Thus, somewhat surprisingly, sensor network energy conservation strategies such as low-powered transmission and aggressive sleep schedules could actually prove counterproductive.
80D0DD2E	We consider a problem encountered when trying to estimate a Gaussian random field using a distributed estimation approach based on Gaussian graphical models. Because of constraints imposed by estimation tools used in Gaussian graphical models, the a priori covariance of the random field is constrained to embed conditional independence constraints among a significant number of variables. The problem is, then: given the (unconstrained) a priori covariance of the random field, and the conditional independence constraints, how should one select the constrained covariance, optimally representing the (given) a priori covariance, but also satisfying the constraints? In 1972, Dempster provided a solution, optimal in the maximum likelihood sense, to the above problem. Since then, many works have used Dempster's optimal covariance, but none has addressed the issue of suitability of this covariance for Bayesian estimation problems. We prove that Dempster's covariance is not optimal in most minimum mean squared error (MMSE) estimation problems. We also propose a method for finding the MMSE optimal covariance, and study its properties. We then illustrate the analytical results via a numerical example, that demonstrates the estimation performance advantage gained by using the optimal covariance vs Dempster's covariance. The numerical example also shows that, for the particular estimation scenario examined, Dempster's covariance violates the necessary conditions for optimality.
7C1A282A	Gaussian Markov random fields (GMRFs) or Gaussian graphical models have been widely used in many applications. Efficiently drawing samples from GMRFs has been an important research problem. In this paper, we introduce the subgraph perturbation sampling algorithm, which makes use of any pre-existing tractable inference algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. We study the stationary version where a single fixed subgraph is used in all iterations, as well as the non-stationary version where tractable subgraphs are adaptively selected. The subgraphs used can have any structure for which efficient inference algorithms exist: for example, tree-structured, low tree-width, or having a small feedback vertex set. We present new theoretical results that give convergence guarantees for both stationary and non-stationary graphical splittings. Our experiments using both simulated models and large-scale real models demonstrate that this subgraph perturbation algorithm efficiently yields accurate samples for many graph topologies.
7EF55ED4	Graphical models provide a powerful formalism for statistical signal processing. Due to their sophisticated modeling capabilities,  they have found applications in a variety of fields such as computer vision,image processing, and distributed sensor networks. In this paper, we present a general class of algorithms for estimation in Gaussian graphical models with arbitrary struc- ture. These algorithms involve a sequence of inference problems on tractable subgraphs over subsets of variables.This framework includes parallel iterations such as embedded trees,serial  iterations such as block Gauss Seidel, and hybrid versions of these iterations. We also discuss a method that uses local memory at each node to overcome temporary communication failures that may arise in distributed sensor network applications.We analyze these algorithms based on the recently developed walk-sum interpretation of Gaussian inference.We describe the walks computed by the algorithms using walk-sum diagrams ,and show that for iterations based on a very large and flexible set of sequences of subgraphs, convergence is guaranteed in walk-sum- mable models. Consequently, we are free to choose spanning trees and subsets of variables adaptively at each iteration. This leads to efficient methods for optimizing the next iteration step to achieve maximum reduction in error. Simulation results demonstrate that these nonstationary algorithms provide a significant speedup in convergence over traditional one-tree and two-tree iterations
7F86EE0A	Reviews a significant component of the rich field of statistical multiresolution (MR) modeling and processing. These MR methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. A second goal is to describe how this topic fits into the even larger field of MR methods and concepts-in particular, making ties to topics such as wavelets and multigrid methods. A third goal is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. The principle focus of our presentation is the class of MR Markov processes defined on pyramidally organized trees. The attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability. We show how a variety of methods and models relate to this framework including models for self-similar and 1/f processes. We also illustrate how these methods have been used in practice
008901FD	Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks. This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models. This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
776AFD98	Scalability is another major issue for recommender systems except data sparsity and prediction quality. However, it has still not been well solved while many social recommendation models have been propose to improve the latter two problems. In this paper, we propose a scalable collaborative filtering algorithm based matrix factorization that introduce two common context factors: category and keyword besides social information. In the proposed model, we make prediction together using two preference matrices:user-category and user-keyword instead of only using the user item rating matrix. This has the advantage that for new items, our model can make use of the two factors to make prediction, although they do not exist in the rating matrix. Experimental results on real dataset show that our model has a good scalability for new items, while still performing better than other state of art models.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
5D4FE30E	Given the increasing volume and complexity of network traffic nowadays, network operators often leverage application-layer protocols to differentiate network traffic, so as to improve quality-of-service control, security protection, and resource profiling. We present ProGraph, a tool that accurately infers protocol message formats at both byte-level and bit-level granularities. Unlike existing approaches that mainly exploit statistical features across packets, ProGraph exploits intra-packet dependency among the values of different portions of a packet payload. It systematically constructs a graphical model that captures intra-packet dependency, using various techniques in graph theory and information theory. It also achieves several important design properties for real deployment, including finegrained inference, protocol independence, simple parameterization, robustness to noisy training sets, and fast execution. We show via trace-driven evaluations that ProGraph achieves more accurate inference than existing approaches. We further show how ProGraph can be used for classifying traffic.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
08A0B727	The enormous number of questions needed to acquire a full preference model when the size of the outcome space is large forces us to work with partial models that approximate the users preferences. In this way we must devise elicitation strategies that focus on the most important questions and at the same time do not need to enumerate the outcome space. In this paper we focus on adaptive elicitation of GAI-decomposable preferences for top-k recommendation tasks in large combinatorial domains. We propose a method that interleaves the generation of top-k solutions with a heuristic selection of questions for refining the user preference model. Empirical results for a large combinatorial problem are given.
782AF565	In collaborative environments, members may try to acquire similar information on the Web in order to gain knowledge in one domain. For example, in a company several departments may successively need to buy business intelligence software and employees from these departments may have studied online about different business intelligence tools and their features independently. It will be productive to get them connected and share learned knowledge. We investigate fine-grained knowledge sharing in collaborative environments. We propose to analyze members Web surfing data to summarize the fine-grained knowledge acquired by them. A two-step framework is proposed for mining fine-grained knowledge:Web surfing data is clustered into tasks by a nonparametric generative model;a novel discriminative infinite Hidden Markov Model is developed to mine fine-grained aspects in each task. Finally, the classic expert search method is applied to the mined results to find proper members for knowledge sharing. Experiments on Web surfing data collected from our lab at UCSB and IBM show that the finegrained aspect mining framework works as expected and outperforms baselines. When it is integrated with expert search, the search accuracy improves significantly, in comparison with applying the classic expert search method directly on Web surfing data.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
5B4F822C	Although information extraction and coreference resolution appear together in many applications, most current systems perform them as independent steps. This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models. We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning. On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields.
7A3B2F5E	As social media sites grow in popularity, tagging has naturally emerged as a method of searching, categorizing and filtering online information, especially multimedia content. The unrestricted vocabulary users choose from to annotate content however, has often lead to an explosion of the size of space in which search is performed. This paper is concerned with investigating generative models of social annotations, and testing their efficiency with respect to two information consumption oriented tasks. One task considers recommending new tags (similarly new resources) for new, previously unknown users. We use perplexity as a standard measure for estimating the generalization performance of a probabilistic model. The second task is aimed at recommending new users to connect with. In this task, we examine which users activity is most discriminative in predicting social ties: annotation (i.e. tags), resource usage (i.e. artists), or collective annotation of resources altogether. For the second task, we propose a framework to integrate the modeling of social annotations with network proximity. The proposed approach consists of two steps: (1) discovering salient topics that characterize users, resources and annotations; and (2) enhancing the recommendation power of such models by incorporating social clues from the immediate neighborhood of users. In particular, we propose four classification schemes for social link recommendation, which we evaluate on a real world dataset from Last.fm. Our results demonstrate significant improvements over traditional approaches.
83EF0AC0	The increasing popularity of mobile devices has brought severe challenges to device usability and big data analysis. In this paper we investigate the intellectual recommender system on cell phones by incorporating mobile data analysis. Nowadays with the development of smart phones, more and more applications have emerged on various areas, such as entertainment, education and health care. While these applications have brought great convenience to peoples daily life, they also provide tremendous opportunities for analyzing users interests. In this work we develop an Android background service to collect the user behaviors and analyze their preferences based on their Android application usage. As one of the most intuitive media for visual representation, videos with various types of contents are recommended to users based on a proposed graphical model. The proposed model jointly utilizes the textual descriptions of Android applications and videos, as well as the extracted video content based features. Besides, by analyzing the user habit of application usage we seamlessly integrate the users personal interests during the recommendation. The extensive comparisons to multiple baselines reveal the superiority of the proposed model on the recommendation quality. Furthermore, we conduct experiments on personalized recommendation to demonstrate the capacity of the proposed model in effectively analyzing the users personal interests.
5A7B1DEB	This paper presents a generative graphical model (VC-Aspect) for filtering visual documents such as images. The proposed VC-Aspect extends the well-known Aspect model and combines both content based and collaborative filtering approaches in a unified framework. Instead of considering item indices in the model such as model-based collaborative filtering techniques, we use visual features in describing visual documents. This allows the model to predict ratings for new visual documents with the same set of parameters. Experimental results show the usefulness of such an approach in a real life application such as the content based image retrieval.
7ED83CC3	A fuzzy clustering algorithm is used for the image tree structure vector quantization (TSVQ). First, a digital image is divided into subblocks of fixed size, which consists of 4/spl times/4 blocks of pixels. By performing a 2-D discrete cosine transform (DCT), we select six DCT coefficients to form the feature vector, and use the fuzzy c-means algorithm in constructing the TSVQ codebook. By doing so, the algorithm can preserve the edge of image, make good image quality, and reduce the processing time while constructing the tree structured codebook, and reduce coding and decoding time.
8054BEFC	A convolution-based algorithm for computing the discrete cosine transform (DCT) (with power of two length) that is based on some theorems of number theory is proposed: It computes a length-N DCT (with N a power of two) using only N multiplications.
7B613B01	Video codecs operating at integral multiples of 64 kbps are well-known in visual communications technology as p * 64 systems (p equals 1 to 24). Originally developed as a class of ITU standards, these codecs have served as core technology for videoconferencing, and they have also influenced the MPEG standards for addressable video. Video compression in the above systems is provided by motion compensation followed by discrete cosine transform -- quantization of the residual signal. Notwithstanding the promise of higher bit rates in emerging generations of networks and storage devices, there is a continuing need for facile audiovisual communications over voice band and wireless modems. Consequently, video compression at bit rates lower than 64 kbps is a widely-sought capability. In particular, video codecs operating at rates in the neighborhood of 64, 32, 16, and 8 kbps seem to have great practical value, being matched respectively to the transmission capacities of basic rate ISDN (64 kbps), and voiceband modems that represent high (32 kbps), medium (16 kbps) and low- end (8 kbps) grades in current modem technology. The purpose of this talk is to describe the state of video technology at these transmission rates, without getting too literal about the specific speeds mentioned above. In other words, we expect codecs designed for non- submultiples of 64 kbps, such as 56 kbps or 19.2 kbps, as well as for sub-multiples of 64 kbps, depending on varying constraints on modem rate and the transmission rate needed for the voice-coding part of the audiovisual communications link. The MPEG-4 video standards process is a natural platform on which to examine current capabilities in sub-ISDN rate video coding, and we shall draw appropriately from this process in describing video codec performance. Inherent in this summary is a reinforcement of motion compensation and DCT as viable building blocks of video compression systems, although there is a need for improving signal quality even in the very best of these systems. In a related part of our talk, we discuss the role of preprocessing and postprocessing subsystems which serve to enhance the performance of an otherwise standard codec. Examples of these (sometimes proprietary) subsystems are automatic face-tracking prior to the coding of a head-and-shoulders scene, and adaptive postfiltering after conventional decoding, to reduce generic classes of artifacts in low bit rate video. The talk concludes with a summary of technology targets and research directions. We discuss targets in terms of four fundamental parameters of coder performance: quality, bit rate, delay and complexity; and we emphasize the need for measuring and maximizing the composite quality of the audiovisual signal. In discussing research directions, we examine progress and opportunities in two fundamental approaches for bit rate reduction: removal of statistical redundancy and reduction of perceptual irrelevancy; we speculate on the value of techniques such as analysis-by-synthesis that have proved to be quite valuable in speech coding, and we examine the prospect of integrating speech and image processing for developing next-generation technology for audiovisual communications
7591F8D7	This paper presents an efficient fast algorithm to reduce redundant discrete cosine transform (DCT) and quantization computations for H.264/AVC encoding optimization. A mathematic model is established based on analyzing residual coefficients distribution and considering the properties of DCT coefficientsenergy distribution. The experimental results demonstrate that the proposed approach can achieve the best performance in reducing the DCT and quantization(Q) computations and obtain almost the same video quality as the original encoder in H.264/AVC
7D506DE7	The MPEG committee has recently completed development of a new audio coding standard MPEG-4 Advanced Audio Coding-Enhanced Low Delay (AAC-ELD). AAC-ELD is targeted towards high-quality, full-duplex communication applications such as audio and video conferencing. AAC-ELD uses low delay spectral band replication (LD-SBR) technology together with a low delay AAC core encoder to achieve high coding efficiency and low algorithmic delays. In this paper, we present fast algorithms for computing LD-SBR filterbanks in AAC-ELD. The proposed algorithms map complex exponential modulation portion of the filterbanks to discrete cosine transforms of types IV and II. Our proposed mapping also allows to merge some multiplications with the windowing stage that precedes or succeeds the modulation step. This further reduces computational complexity. Our presentation includes detailed explanation and flow-graphs of the algorithms, complexity analysis, and comparisons with alternative implementations.
7F506E8E	Compressing an image is significantly different than compressing raw binary data. Evidently, general purpose compression algorithms can be used to compress images, but the result is less than optimal. Discrete Cosine Transform (DCT) has been widely used in signal processing of image. Joint Photographic Experts Group (JPEG) is a commonly used standard technique of compression for photographic images and in turn utilizes DCT. Apart from DCT, their also exist a decomposition algorithm well known as Singular Value Decomposition (SVD). The proposed schemes investigate the performance evaluation of variable quantization DCT and variable rank of image matrix SVD based image compression. The numerical analysis of such algorithms is carried out by measuring Peak Signal to Noise Ratio (PSNR), Compression Ratio (CR).
218C8616	 	Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use. 
7B8FFF7E	 2D fast cosine and sine transforms with regular structure are developed for 2n X 2n data points. These algorithms are extended versions of the 1D fast regular algorithms introduced in our recent paper. The rationale for these 2D algorithms for sine/cosine transforms in a 2D decomposition of data sequences into 2D subblocks with reduced dimension, rather than 1D, separable treatments for the columns and rows of the data sets. As a result the number of multiplications is 25 percent less than in row- column approach. Numerous algorithms of these type were proposed previously for discrete Fourier transform (DFT) and discrete cosine transform of type 2 (DCT-II). In DCT-II case the algorithms do not have a regular structure as is the case in DFT algorithms and motivation of this work is to derive 2D algorithms for discrete sine and cosine transforms with regular constant geometry structures. Extension to 2n X 2m data points is straightforward.
7D26715E	The authors present the design and performance evaluation of a robust, DCT-based (discrete-cosine-transform-based) variable-bit-rate (VBR) compression algorithm for use on B-ISDN/ATM networks. The algorithm class under consideration is based on a recent proposal by F. Kishino et al. (1989), intended to provide robust delivery of video under relatively high ATM cell loss conditions. The robust VBR codec is based on separation of subjectively important low-frequency DCT coefficients (for high-priority transport) from the less important high-frequency coefficients (which are sent at a lower priority level). Temporal propagation of error after loss of low-priority ATM cells is avoided by limiting interframe prediction to low-frequency information transmitted in high-priority cells. Several key questions that arise in the design of such an ATM codec are considered, including: (a) the trade-off between total bit-rate and robustness; (b) the influence of the high/low priority boundary parameter on the high-priority and low-priority bit-rates; and (c) performance at the decoder in the presence of ATM channel loss
813D8ACA	Discrete Tchebichef transform (DTT), derived from a discrete class of the popular Chebyshev polynomials, is a novel orthogonal transform that has high energy compaction and de-correlation properties. Therefore, in this paper, DTT is examined and treated for transform coding applications. A framework is laid to derive an approximation-free integer representation of DTT to meet the current application requirements. A fast algorithm is further proposed for multiplier-free computation of DTT. The image compression performance of the 4-point DTT is found to be superior to that of the 4-point discrete cosine transform (DCT) and integer cosine transform (ICT), the integer approximation of DCT. It is shown that the fast DTT is easily derived, has low complexity, does not involve approximations and can be carried out within the same dynamic range. Hence, DTT can be used for image and data compression applications. Since the image compression performance and computational simplicity of DTT are found to be significantly better than that of ICT, the use of DTT in place of ICT for transform coding in the H.264/AVC looks promising.
80D06662	Research of robust and invisible double digital watermark is one of the hot fields currently, and it has received considerable attention. To change the situation that many methods about watermarking are based on embedding one single watermark, a new double digital watermark algorithm on the basis of discrete cosine transformation and image blocks is presented. The algorithm embeds both robust watermark and fragile watermark to one video sequence by using DCT and multiple embedded methods. The later embedded fragile watermark is served for the early robust watermark. The experiment results verify the algorithm achieves better robustness and imperceptibility.
7686BE02	On the basis of the Mobius function, a two-stage algorithm for the discrete cosine transform (DCT) and the inverse DCT (IDCT) is proposed. In this approach, the DCT matrix is factorized into the preprocessing and postprocessing matrices. The preprocessing matrix has elements of values 1 and -1, and the postprocessing matrix is a circular convolution/correlation matrix.
7BCFEA89	In fractal image compression the encoding step is computationally expensive, because every range block must be compared to all domain blocks in the codebook to find the best-matched one during the coding procedure. In this paper, a fast classification algorithm using DCT coefficients is proposed. Simulation results show that the runtime of the proposed algorithm is reduced greatly compared to the existing methods. At the same time, the new algorithm also achieved high PSNR values
6F004731	Applications of bounded error parameter estimation in the field of image compression are described. A bounded error parameter estimator is shown to improve the performance of an adaptive predictive compression scheme. The quantization of discrete cosine transform coefficients is viewed from a parameter bounding perspective and bounds for the coefficient quantization error are derived. They can be used to keep the reconstructed image sample quantization error within bounds.
7D7F7051	We implement a two-dimensional 8/spl times/8 fast discrete cosine transform and its inverse by the Feig's algorithm which is recognized the fastest method so far and this algorithm potentially has very wide applications. All the equations are derived in detail. The verification and evaluation are proved by computer simulation. The result of real application in MPEG1 is also presented.
79046CE5	In this paper, a replacement algorithm for Linear Prediction Coefficients (LPC) along with Hamming Correction Code based Compressor (HCDC) algorithms are investigated for speech compression. We started with an CELP system with order 12 and with Discrete Cosine Transform (DCT) based residual excitation. Forty coefficients with transmission rate of 5.14 kbps were first used. For each frame of the testing signals we applied a multistage HCDC, we tested the compression performance for parities from 2 to 7, we were able to achieve compression only at parity 4. This rate reduction was made with no compromise in the original CELP signal quality since compression is lossless. The compression approach is based on constructing dynamic reflection coefficients codebook, this codebook is constructed and used simultaneously using a certain store/retrieve threshold. The initial linear prediction codec we used is excited by a discrete cosine transform (DCT) residual, the results were tested using the MOS and SSNR, we had acceptable ranges for the MOS (average 3.6), and small variations of the SSNR.
7B8ECF46	In this paper,a novel grayscale watermarking algorithm based on Two-Level Discrete Cosine Transform (DCT) and Singular Value Decomposition (SVD) is proposed. The watermark signal is bit gray image.First,The original image is divided into blocks according to the size of the watermark;each block corresponds to each pixel value of watermark.Second,the DCT is applied in each block twice and form new blocks.Then,perform SVD on each new block to get matrices U,S and V for each block.The pixel value of watermark is embedded into the largest singular value of S matrix of each new block.And the watermark can be detected with the original image.The experimental results show that the algorithm can satisfy the transparence and robustness of the watermarking system very well. Experimental evaluation demonstrates that the proposed scheme is able to withstand a variety of attacks.
5CFE86E1	The DFT is an important tool in digital signal processing. An effective index mapping is introduce by which the odd-length sinusoid-class orthogonal transforms, such as the generalised discrete Fourier transforms (GDFTs), generalised discrete Hartley transforms (GDHTs), discrete cosine transforms (DCTs) and discrete sine transforms (DSTs), can be converted to DFTs, real-valued DFTs (RDFTs) or DHTs of the same length, using primarily permutations and sign changes only. The algorithm proposed is more efficient than others previously published.
7B2F9E2B	this research presents a new algorithm for an image compression consist of three phases; the first phase is using "Discrete Wavelet Transformation (DWT)", to produce low-frequency and high-frequencies sub-bands. The high-frequencies sub-bands are ignored (i.e. not used in this research), in the second phase used "Discrete Cosine Transformation (DCT)" applied on each "2x2" block from "LL" sub-band, then each block stored as a one- dimensional array in the new matrix called "Multi-Array- Matrix (MA-Matrix)". The third phase; MA-Matrix separated into "DC-Column" and "MA2-Matrix", and then applied Minimize-Matrix-Size algorithm on the "MA2- Matrix", to be as a one-dimensional array. Our decompression algorithm phase starts from "Sequential Search Algorithm (SS-Algorithm)" to find the estimated values for the "MA2-Matrix". The SS-Algorithm depends on the three pointers, for decompress MA2-Matrix, and then combined it with DC-Column for reconstructs MA-Matrix. Finally the inverse DCT and the inverse DWT are used for reconstructs approximately original image. Our approach compared with JPEG and JPEG2000 by using PSNR. 
674BBF3E	An effective index mapping is introduced by which the odd-length sinusoid-class orthogonal transforms, such as GDFTs,GDHTs,DCTs and DSTs, can be converted to DFTs,real-valued DFTs (RDFTs) or DHTs of the same length,using primarily permutations and sign changes only.The algorithm proposed here is more efficient than others previously published. 
7EDB4A91	Fast recursive algorithms for the computation of the discrete cosine and sine transforms are developed. An N-point discrete cosine transform (DCT) or discrete sine transform (DST) can be computed from two N/2-point DCTs or DSTs. Compared to the existing algorithms the algorithms have less multiplications by two, and add operations are better positioned, giving rise to faster computation and easier VLSI implementation.
80E6A82C	An efficient direct method for the computation of a length-N discrete cosine transform (DCT) given two adjacent length-(N/2) DCT coefficients, is presented. The computational complexity of the proposed method is lower than the traditional approach for lengths N>8. Savings of N memory locations and 2N data transfers are also achieved.
7A61FB18	In this paper we present a new global method to derive invertible integer-to-integer mappings from given linear mappings . If is given by and Hn is an invertible matrix, then one can always find a suitable factor such that the condition is satisfied. An invertible mapping can now simply be defined by , and obviously, this nonlinear integer mapping is close to the linear mapping . We apply this idea in order to derive a new invertible integer DCT-II transform of radix-2 length and new integer wavelet algorithms. It turns out, that the expansion factors can be chosen very small.
76D04F48	The discrete cosine transform (DCT) is widely applied in various fields, including image data compression, because it operates like the Karhunen-Loeve transform for stationary random data. This paper presents a recursive algorithm for DCT with a structure that allows the generation of the next higher order DCT from two identical lower order DCT's. As a result, the method for implementing this recursive DCT requires fewer multipliers and adders than other DCT algorithms.
7FC2B1C0	Recently, many applications for three-dimensional (3-D) image and video compression have been proposed using 3-D discrete cosine transforms (3-D DCTs). Among different types of DCTs, the type-II DCT (DCT-II) is the most used. In order to use the 3-D DCTs in practical applications, fast 3-D algorithms are essential. Therefore, in this paper, the 3-D vector-radix decimation-in-frequency (3-D VR DIF) algorithm that calculates the 3-D DCT-II directly is introduced. The mathematical analysis and the implementation of the developed algorithm are presented, showing that this algorithm possesses a regular structure, can be implemented in-place for efficient use of memory, and is faster than the conventional row-column-frame (RCF) approach. Furthermore, an application of 3-D video compression-based 3-D DCT-II is implemented using the 3-D new algorithm. This has led to a substantial speed improvement for 3-D DCT-II-based compression systems and proved the validity of the developed algorithm.
781655CB	In this paper, 3-D discrete Hartley transform is applied for the compression of two medical modalities, namely, magnetic resonance images and X-ray angiograms and the performance results are compared with those of 3-D discrete cosine and Fourier transforms using the parameters such as PSNR and bit rate. It is shown that the 3-D discrete Hartley transform is better than the other two transforms for magnetic resonance brain images whereas for the X-ray angiograms, the 3-D discrete cosine transform is found to be superior.
7FE3364C	Recently, a fast radix-q algorithm for an efficient computation of the type-IV discrete cosine transform (DCT-IV) has been proposed in , where q is an odd positive integer. In particular, based on the proposed fast algorithm, optimized efficient 3-, 5-, and 9-point scaled DCT-IV (SDCT-IV) modules have been derived in . As a response, an improved efficient optimized 9-point scaled DCT-IV (SDCT-IV) module in terms of the arithmetic complexity is presented. The improved optimized efficient 9-point SDCT-IV module requires 17 multiplications, 53 additions, and three shifts. Consequently, the arithmetic complexity of extended fast mixed-radix DCT-IV algorithm for composite lengths is also significantly improved.
7FD3F0EB	An index permutation-based fast two-dimensional discrete cosine transform (2-D DCT) algorithm is presented. It is shown that the N/spl times/N 2-D DCT, where N=2/sup m/, can be computed using only N 1-D DCTs and some post additions.
7E68A6E2	The discrete cosine transform (DCT) is often computed from a discrete Fourier transform (DFT) of twice or four times the DCT length. DCT algorithms based on identical-length DFT algorithms generally require additional arithmetic operations to shift the phase of the DCT coefficients. It is shown that a DCT of odd length can be computed by an identical-length DFT algorithm, by simply permuting the input and output sequences. Using this relation, odd-length DCT modules for a prime factor DCT are derived from corresponding DFT modules. The multiplicative complexity of the DCT is then derived in terms of DFT complexities.
77C93D55	The modified discrete cosine transform (MDCT) and modified discrete sine transform (MDST) are employed in subband/transform coding schemes as the analysis/synthesis filter banks based on the concept of time domain aliasing cancellation (TDAC). Princen, Bradley and Johnson defined two types of the MDCT, specifically, for an evenly stacked and oddly stacked analysis/synthesis systems. The MDCT is the basic processing component in the international audio coding standards and commercial products for high-quality audio compression. Almost all existing audio coding systems have used the complex-valued or real-valued FFT algorithms, and the DCT/DST of type IV (DCT-IV/DST-IV) for the fast MDCT computation. New fast and efficient algorithm for a unified forward and inverse MDCT/MDST computation in the oddly stacked system is proposed. It is based on the DCT/DST of types II and III (DCT-II/DST-II, DCT-III/DST-III), and the real arithmetic is used only. Corresponding generalized signal flow graph is regular, structurally simple and enables to compute MDCT/MDST and their inverses in general for any N divisible by 4 (N being length of a data sequence). Consequently, the new fast algorithm can be adopted for the MDCT computation in the current audio coding standards such as MPEG family (MPEG-1, MPEG-2, MPEG-2 Advanced Audio Coding and MPEG-4 audio), and in commercial products (proprietary audio coding algorithms) such as Sony MiniDisc/ATRAC/ATRAC2/SDDS digital audio coding systems, the AT&T Perceptual Audio Coder (PAC) or Lucent Technologies PAC/Enhanced PAC/Multichannel PAC, and Dolby Labs AC-3 digital audio compression algorithm. Besides the new fast algorithm has some interesting properties, it provides an efficient implementation of the forward and inverse MDCT computation for layer III in MPEG audio coding, where the length of data blocks. Especially, for the AC-3 algorithm, it is shown how both the proposed new MDCT/MDST algorithm and existing fast algorithms/computational architectures for the discrete sinusoidal transforms computation of real data sequences such as the DCT-IV/DST-IV, generalized discrete Fourier transform of type IV (DFT-IV) and generalized discrete Hartley transform of type IV (DHT-IV) can be used for the fast alternate or simultaneous (on-line) MDCT/MDST computation by simple pre- and post-processing of data sequences.
78363BC0	Integer DCTs have a wide range of applications in lossless coding, especially in image compression. An integer-to-integer DCT of radix-2-length n is a nonlinear, left-invertible mapping, which acts on /spl Zopf//sup n/ and approximates the classical discrete cosine transform (DCT) of length n. All known integer-to-integer DCT-algorithms of length 8 are based on factorizations of the cosine matrix C/sub 8//sup II/ into a product of sparse matrices and work with lifting steps and rounding off. For fast implementation one replaces floating point numbers by appropriate dyadic rationals. Both rounding and approximation leads to truncation errors. In this paper, we consider an integer-to-integer transform for (2/spl times/2) rotation matrices and give estimates of the truncation errors for arbitrary approximating dyadic rationals. Further, using two known integer-to-integer DCT-algorithms, we show examplarily how to estimate the worst-case truncation error of lifting based integer-to-integer algorithms in fixed-point arithmetic, whose factorizations are based on (2/spl times/2) rotation matrices.
7D10C675	In this paper, we first propose an efficient algorithm for computing one-dimensional (1-D) discrete cosine transform (DCT) for a signal block, given its two adjacent subblocks in the DCT domain and then introduce several algorithms for the fast computation of multidimensional (m-D) DCT with size N/sub 1//spl times/N/sub 2//spl times/.../spl times/N/sub m/ given 2/sup m/ subblocks of DCT coefficients with size N/sub 1//2/spl times/N/sub 2//2/spl times/.../spl times/N/sub m//2, where N/sub i/(i=1,2,...,m) are powers of 2. Obviously, the row-column method, which employs the most efficient algorithms along each dimension, reduces the computational complexity considerably, compared with the traditional method, which employs only the one-dimensional (1-D) fast DCT and inverse DCT (IDCT) algorithms. However, when m/spl ges/2, the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms, has lower computational complexity than the row-column method. Besides, we propose a direct method by dividing the data into 2/sup m/ parts for independent fast computation, in which only two steps of r-dimensional (r=1,2,...,m) IDCT and additional multiplications and additions are required. If all the dimensional sizes are the same, the number of multiplications required for the direct method is only (2/sup m/-1)/m2/sup m-1/ times of that required for the row-column method, and if N/spl ges/2/sup 2m-1/, the computational efficiency of the direct method is surely superior to that of the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms.
7EBEC735	An efficient method for computing the discrete cosine transform (DCT) is proposed. Based on direct decomposition of the DCT, the recursive properties of the DCT for an even length input sequence is derived, which is a generalization of the radix 2 DCT algorithm. Based on the recursive property, a new DCT algorithm for an even length sequence is obtained. The proposed algorithm is very structural and requires fewer computations when compared with others. The regular structure of the proposed algorithm is suitable for fast parallel algorithm and VLSI implementation.
80D1B10B	We present a dynamic graphical model (DGM) for automated multi-instrument musical transcription. By multi-instrument transcription, we mean a system capable of listening to a recording in which two or more instruments are playing, and identifying both the notes that were played and the instruments that played them. Our transcription system models two musical instruments, each capable of playing at most one note at a time. We present results for two-instrument transcription on piano and violin sounds.
7F77E0BA	The discrete cosine transform (DCT) and the discrete sine transform (DST) have found wide applications in speech and image processing, as well as telecommunication signal processing for the purpose of data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new recursive algorithms for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design recursive algorithms for the 2-D DCT and the 2-D DST. These new structured recursive algorithms are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other recursive algorithms. Finally, we propose two parallel algorithms for accelerating the computation.
781849EC	Integer DCTs have important applications in lossless coding. In this paper, an integer DCT of radix-2 length n is understood to be a nonlinear, (left-)invertible mapping which acts on and approximates the classical discrete cosine transform (DCT) of length n. In image compression, the DCT of type II (DCT-II) is of special interest. In this paper we present a new approach to invertible integer DCT-II and integer DCT-IV. Our method is based on a factorization of the cosine matrices of types II and IV into products of sparse, orthogonal matrices. Up to some permutations, each matrix factor is a block-diagonal matrix with blocks being orthogonal matrices of order 2. Hence one has to construct only integer transforms of length 2. We factorize an orthogonal matrix of order 2 into three lifting matrices and work with lifting steps and rounding-off. This allows the construction of new integer DCT algorithms. We give uniform bounds for the worst case difference between the results of exact DCT and the corresponding integer DCT. Finally, we present some numerical experiments for the integer DCT-II of length 8 and for the 2-dimensional integer DCT-II of size 8 8.
7F3438A9	This tutorial paper describes the methods for constructing fast algorithms for the computation of the discrete Fourier transform (DFT) of a real-valued series. The application of these ideas to all the major fast Fourier transform (FFT) algorithms is discussed, and the various algorithms are compared. We present a new implementation of the real-valued split-radix FFT, an algorithm that uses fewer operations than any other real-valued power-of-2-length FFT. We also compare the performance of inherently real-valued transform algorithms such as the fast Hartley transform (FHT) and the fast cosine transform (FCT) to real-valued FFT algorithms for the computation of power spectra and cyclic convolutions. Comparisons of these techniques reveal that the alternative techniques always require more additions than a method based on a real-valued FFT algorithm and result in computer code of equal or greater length and complexity.
7D05A275	As the circuit complexity is increasing in demand for the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera is one of them where realtime image capturing, compression and storage of compressed image data is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image.
7C5BD5F9	A systematic method of sparse matrix factorization is developed for all four versions of the discrete W transform, the discrete cosine transform, and the discrete sine transform, as well as for the discrete Fourier transform. The factorization leads to fast algorithms in which only real arithmetic is involved. A scheme for reducing multiplications and a convenient index system are introduced. This makes new algorithms more efficient than conventional algorithms for the discrete Fourier transform, the discrete cosine transform, and the discrete sine transform.
79411F44	The fast Hartley transform (FHT) is similar to the Cooley-Tukey fast Fourier transform (FFT) but performs much faster because it requires only real arithmetic computations compared to the complex arithmetic computations required by the FFT. Through use of the FHT, discrete cosine transforms (DCT) and discrete Fourier transforms (DFT) can be obtained. The recursive nature of the FHT algorithm derived in this paper enables us to generate the next higher order FHT from two identical lower order FHT's. In practice, this recursive relationship offers flexibility in programming different sizes of transforms, while the orderly structure of its signal flow-graphs indicates an ease of implementation in VLSI.
7576CA59	A new VLSI algorithm and its associated systolic array architecture for a prime length type IV discrete cosine transform is presented. They represent the basis of an efficient design approach for deriving a linear systolic array architecture for type IV DCT. The proposed algorithm uses a regular computational structure called pseudoband correlation structure that is appropriate for a VLSI implementation. The proposed algorithm is then mapped onto a linear systolic array with a small number of I/O channels and low I/O bandwidth. The proposed architecture can be unified with that obtained for type IV DST due to a similar kernel. A highly efficient VLSI chip can be thus obtained with good performance in the architectural topology, computing parallelism, processing speed, hardware complexity and I/O costs similar to those obtained for circular correlation and cyclic convolution computational structures.
7EDD5239	This correspondence presents an odd-factor algorithm for the type-III discrete cosine transform (DCT) for uniform or mixed radix decomposition. By jointly using the old-factor and the existing radix-2 algorithms, a general decomposition method for arbitrarily composite sequence length is developed. A reduction of computational complexity can be achieved compared with that needed by other reported algorithms for M=2/sup m/. The decomposition approach has a simple computational structure and supports a wider range of choices for different sequence lengths.
0BD946B6	Building accurate models from a small amount of available training data can sometimes prove to be a great challenge. Expert domain knowledge can often be used to alleviate this burden. Parameter Sharing is one such important form of domain knowledge. Graphical models like HMMs, DBNs and Module Networks use different forms of Parameter Sharing to reduce the variance in the parameter estimates. The goal of this paper is to present a theoretical approach for learning in presence of several other types of Parameter Related Domain Knowledge that go beyond the ones in the above models. First, we introduce a General Parameter Sharing Framework that describes the models just mentioned, but allows for much finer grained parameter sharing assumptions. In this framework, we present sound procedures for parameter learning from both a Frequentist and a Bayesian point of view, from both complete and incomplete data, in the case where a domain expert specifies in advance the structure of the graphical model, and the subsets of parameters to be shared. Second, we describe a hierarchical extension of this framework based on Parameter Sharing Trees. Finally we present algorithms for using domain knowledge that specifies that certain groups of parameters share certain properties. In particular, we consider two kinds of constraints: first kind states certain groups of parameters share the same aggregate probability mass and second kind states the ratio of the parameters is preserved (shared) in several groups. As an example, we derive a novel form of parameter sharing for Bayesian Multinetworks.
7EBEC735	An efficient method for computing the discrete cosine transform (DCT) is proposed. Based on direct decomposition of the DCT, the recursive properties of the DCT for an even length input sequence is derived, which is a generalization of the radix 2 DCT algorithm. Based on the recursive property, a new DCT algorithm for an even length sequence is obtained. The proposed algorithm is very structural and requires fewer computations when compared with others. The regular structure of the proposed algorithm is suitable for fast parallel algorithm and VLSI implementation.
59E5C607	In this paper I explore the psychology of ritual performance and present a simple graphical model that clarifies several issues in William Irons’s theory of religion as a “hard-to-fake” sign of commitment. Irons posits that religious behaviors or rituals serve as costly signals of an individual’s commitment to a religious group. Increased commitment among members of a religious group may facilitate intra-group cooperation, which is argued to be the primary adaptive benefit of religion. Here I propose a proximate explanation for how individuals are able to pay the short-term costs of ritual performance to achieve the long-term fitness benefits offered by religious groups. The model addresses three significant problems raised by Irons’s theory. First, the model explains why potential free-riders do not join religious groups even when there are significant net benefits that members of religious groups can achieve. Second, the model clarifies how costly a ritual must be to achieve stability and prevent potential free-riders from joining the religious group. Third, the model suggests why religious groups may require adherents to perform private rituals that are not observed by others. Several hypotheses generated from the model are also discussed.
7B13384D	 This paper reviews some of the key statistical ideas that are encountered when trying to find empirical support to causal interpretations and conclusions, by applying statistical methods on experimental or observational longitudinal data. In such data, typically a collection of individuals are followed over time, then each one has registered a sequence of covariate measurements along with values of control variables that in the analysis are to be interpreted as causes, and finally the individual outcomes or responses are reported. Particular attention is given to the potentially important problem of confounding. We provide conditions under which, at least in principle, unconfounded estimation of the causal effects can be accomplished. Our approach for dealing with causal problems is entirely probabilistic, and we apply Bayesian ideas and techniques to deal with the corresponding statistical inference. In particular, we use the general framework of marked point processes for setting up the probability models, and consider posterior predictive distributions as providing the natural summary measures for assessing the causal effects. We also draw connections to relevant recent work in this area, notably to Judea Pearl's formulations based on graphical models and his calculus of so-called do-probabilities. Two examples illustrating different aspects of causal reasoning are discussed in detail.
7E19344D	A novel algorithm to convert the discrete cosine transform (DCT) to skew-circular convolutions is presented. The motivation for developing such an algorithm is the fact that VLSI implementation of distributed arithmetic is very efficient for computing convolutions. It is also shown that the inverse DCT (IDCT) can be computed using the same building blocks which are used for computing the DCT. A DCT/IDCT processor can be designed to compute either the DCT or the IDCT depending on a 1-b control signal.
7CE629CE	Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.
772CE3A1	Most interactive graphical applications that use direct manip- ulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high de- sign, development and maintenance costs and encourages the development of stereotyped applications based on buttons, menus and dialogue boxes instead of direct manipulation of the applications objects. There have been several attempts to provide high level tools for building such applications, including popular toolkits such as Garnet [26], Unidraw [33], Fresco [21, 32] and Open- Inventor [28]. Unfortunately, these tools are not adapted to the development of sophisticated graphical editors because of their lack of extensibility: In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction. We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi- Layer Model and we show how it can take advantage of soft- ware and hardware graphic extensions to provide good per- formance. We also show how it supports multiple input de- vices and simplifies the description of a wide variety of in- teraction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional an- imation system. 
7FA005E7	This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.
77C93D55	The modified discrete cosine transform (MDCT) and modified discrete sine transform (MDST) are employed in subband/transform coding schemes as the analysis/synthesis filter banks based on the concept of time domain aliasing cancellation (TDAC). Princen, Bradley and Johnson defined two types of the MDCT, specifically, for an evenly stacked and oddly stacked analysis/synthesis systems. The MDCT is the basic processing component in the international audio coding standards and commercial products for high-quality audio compression. Almost all existing audio coding systems have used the complex-valued or real-valued FFT algorithms, and the DCT/DST of type IV (DCT-IV/DST-IV) for the fast MDCT computation. New fast and efficient algorithm for a unified forward and inverse MDCT/MDST computation in the oddly stacked system is proposed. It is based on the DCT/DST of types II and III (DCT-II/DST-II, DCT-III/DST-III), and the real arithmetic is used only. Corresponding generalized signal flow graph is regular, structurally simple and enables to compute MDCT/MDST and their inverses in general for any N divisible by 4 (N being length of a data sequence). Consequently, the new fast algorithm can be adopted for the MDCT computation in the current audio coding standards such as MPEG family (MPEG-1, MPEG-2, MPEG-2 Advanced Audio Coding and MPEG-4 audio), and in commercial products (proprietary audio coding algorithms) such as Sony MiniDisc/ATRAC/ATRAC2/SDDS digital audio coding systems, the AT&T Perceptual Audio Coder (PAC) or Lucent Technologies PAC/Enhanced PAC/Multichannel PAC, and Dolby Labs AC-3 digital audio compression algorithm. Besides the new fast algorithm has some interesting properties, it provides an efficient implementation of the forward and inverse MDCT computation for layer III in MPEG audio coding, where the length of data blocks. Especially, for the AC-3 algorithm, it is shown how both the proposed new MDCT/MDST algorithm and existing fast algorithms/computational architectures for the discrete sinusoidal transforms computation of real data sequences such as the DCT-IV/DST-IV, generalized discrete Fourier transform of type IV (DFT-IV) and generalized discrete Hartley transform of type IV (DHT-IV) can be used for the fast alternate or simultaneous (on-line) MDCT/MDST computation by simple pre- and post-processing of data sequences.
80E6A82C	An efficient direct method for the computation of a length-N discrete cosine transform (DCT) given two adjacent length-(N/2) DCT coefficients, is presented. The computational complexity of the proposed method is lower than the traditional approach for lengths N>8. Savings of N memory locations and 2N data transfers are also achieved.
7F9A8948	Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
7E68A6E2	The discrete cosine transform (DCT) is often computed from a discrete Fourier transform (DFT) of twice or four times the DCT length. DCT algorithms based on identical-length DFT algorithms generally require additional arithmetic operations to shift the phase of the DCT coefficients. It is shown that a DCT of odd length can be computed by an identical-length DFT algorithm, by simply permuting the input and output sequences. Using this relation, odd-length DCT modules for a prime factor DCT are derived from corresponding DFT modules. The multiplicative complexity of the DCT is then derived in terms of DFT complexities.
7F77E0BA	The discrete cosine transform (DCT) and the discrete sine transform (DST) have found wide applications in speech and image processing, as well as telecommunication signal processing for the purpose of data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new recursive algorithms for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design recursive algorithms for the 2-D DCT and the 2-D DST. These new structured recursive algorithms are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other recursive algorithms. Finally, we propose two parallel algorithms for accelerating the computation.
7D10C675	In this paper, we first propose an efficient algorithm for computing one-dimensional (1-D) discrete cosine transform (DCT) for a signal block, given its two adjacent subblocks in the DCT domain and then introduce several algorithms for the fast computation of multidimensional (m-D) DCT with size N/sub 1//spl times/N/sub 2//spl times/.../spl times/N/sub m/ given 2/sup m/ subblocks of DCT coefficients with size N/sub 1//2/spl times/N/sub 2//2/spl times/.../spl times/N/sub m//2, where N/sub i/(i=1,2,...,m) are powers of 2. Obviously, the row-column method, which employs the most efficient algorithms along each dimension, reduces the computational complexity considerably, compared with the traditional method, which employs only the one-dimensional (1-D) fast DCT and inverse DCT (IDCT) algorithms. However, when m/spl ges/2, the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms, has lower computational complexity than the row-column method. Besides, we propose a direct method by dividing the data into 2/sup m/ parts for independent fast computation, in which only two steps of r-dimensional (r=1,2,...,m) IDCT and additional multiplications and additions are required. If all the dimensional sizes are the same, the number of multiplications required for the direct method is only (2/sup m/-1)/m2/sup m-1/ times of that required for the row-column method, and if N/spl ges/2/sup 2m-1/, the computational efficiency of the direct method is surely superior to that of the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms.
7FE3364C	Recently, a fast radix-q algorithm for an efficient computation of the type-IV discrete cosine transform (DCT-IV) has been proposed in , where q is an odd positive integer. In particular, based on the proposed fast algorithm, optimized efficient 3-, 5-, and 9-point scaled DCT-IV (SDCT-IV) modules have been derived in . As a response, an improved efficient optimized 9-point scaled DCT-IV (SDCT-IV) module in terms of the arithmetic complexity is presented. The improved optimized efficient 9-point SDCT-IV module requires 17 multiplications, 53 additions, and three shifts. Consequently, the arithmetic complexity of extended fast mixed-radix DCT-IV algorithm for composite lengths is also significantly improved.
7C5AAA1F	Therapy processes are complex dynamical systems where several variables are constantly interacting with each other. In general, the underlying mechanisms are difficult to assess. Our approach is to identify the dependency structure of relevant variables within the therapy process using interaction graphs. These are instruments for multivariate time series which are based on the analysis of partial spectral coherences. We used interaction graphs in order to investigate the therapy process of a multimodal therapy concept for fibromyalgia patients. Our main hypothesis was that self-efficacy plays a central role in the therapy process. Methods: Patients kept an electronic diary for 13 weeks. Pain intensity, depression, sleep quality, anxiety and self-efficacy were assessed via visual analogue scales. The resulting multivariate time series were aggregated over individuals, and partial spectral coherences between each pair of the variables were calculated. From the partial coherences, interaction graphs were plotted. Results: Within the resulting graphical model, self-efficacy was strongly related to pain intensity, depression and sleep quality. All other relations were substantially weaker. There was no direct relationship between pain intensity and sleep quality. Conclusions: The relations between two variables within the therapy process are mainly induced by self-efficacy. Interaction graphs can be used to pool time series data of several patients and thus to assess the common underlying dependency structure of a group of patients. The graphical representation is easily comprehensible and allows to distinguish between direct and indirect relationships.
80D10329	Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.
7D843341	A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.
7963E765	The forensic investigation of the origin and cause of a fire incident is a particularly demanding area of expertise. As the available evidence is often incomplete or vague, uncertainty is a key element. The present study is an attempt to approach this through the use of Bayesian networks, which have been found useful in assisting human reasoning in a variety of disciplines in which uncertainty plays a central role. The present paper describes the construction of a Bayesian network (BN) and its use for drawing inferences about propositions of interest, based upon a single, possibly non replicable item of evidence: detected residual quantities of a flammable liquid in fire debris.
7A1904AD	A new recursive algorithm and two types of circuit architectures are presented for the computation of the two-dimensional discrete cosine transform (2D DCT). The new algorithm permits to compute the 2D DCT by a simple procedure of the 1D recursive calculations involving only cosine coefficients. The recursive kernel for the proposed algorithm contains a small number of operations. Also, it requires a smaller number of pre-computed data compared with many of existing algorithms in the same category. The kernel can be easily implemented in a simple circuit block with a short critical delay path. In order to evaluate the performance improvement resulting from the new algorithm, an architecture for the 2D DCT designed by direct mapping from the computation structure of the proposed algorithm has been implemented in an FPGA board. The results show that the reduction of the hardware consumption can easily reach 25 and the clock frequency can increase 17 compared with a system implementing a recently reported 2D DCT recursive algorithm. For a further reduction of the hardware, another architecture has been proposed for the same 2D DCT computation. Using one recursive computation block to perform different functions, this architecture needs only approximately one-half of the hardware that is required in the first architecture, which has been confirmed by an FPGA implementation.
7B64D19B	The discrete cosine transform of type IV (DCT-IV) and corresponding discrete sine transform of type IV (DST-IV) have played key role in the efficient implementation of orthogonal lapped transforms and perfect reconstruction cosine-modulated filter banks such as the oddly stacked modified discrete cosine transform (MDCT) or equivalently, the modulated lapped transform (MLT). However, the DCT-IV and DST-IV of double sizes are related to two variants of filter banks defined by Dolby Labs AC-3 digital audio compression algorithm. Since these two variants of filter banks are efficiently computed by recently proposed new fast algorithm for the oddly stacked MDCT (Signal Processing 82 (2002) 433), it is shown that the efficient DCT-IV and DST-IV computation can be realized via the MDCT of double size. The careful analysis of regular structure of the new fast MDCT algorithm allows to extract a new DCT-IV/DST-IV computational structure and to suggest a new sparse matrix factorization of the DCT-IV matrix. Finally, the new DCT-IV/DST-IV computational structure provides an alternative efficient implementation of the forward and inverse MDCT in layer III of MPEG (MP3) audio coding.
7A8EB39B	Recommender system provides users with personalized suggestions of product or information. Typically, recommender systems rely on a bipartite graph model to capture user interest. As an extension, some boosted methods analyze content information to further improve the quality of personalized recommendation. However, due to the prevalence of short and sparse messages in online social media, traditional content-boosted methods do not guarantee to capture user preference accurately especially for web contents. In this paper, we propose a novel graphical model to extract hidden topics from web contents, cluster web contents, and detect users' interests on each cluster. In addition, we introduce two reranking models which utilize the detected user interest to further boost the quality of personalized recommendation. Experiment results on a public dataset demonstrated the limitation of a traditional content-boosted approach, and also showed the validity of our proposed techniques.
77988EDA	Two types of efficient algorithms for fast implementation of the 2-D discrete cosine transform (2-D DCT) are developed. One involves recursive structure which implies that the algorithm for (M/2 X N/2) block be extended to (M X N/2) (M/2 X M) and (M X N) blocks (M and N are integer powers of two). The second algorithm is nonrecursive and therefore it has to be tailored for each block size. Both algorithms involve real arithmetic and they reduce the number of multiplications significantly compared to the fast algorithm developed by Chen et al. [8], while the number of additions remain unchanged.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7E271971	We propose a Bayesian Network (BN) model to integrate multiple contextual information and the image measurements for image segmentation. The BN model systematically encodes the contextual relationships between regions, edges and vertices, as well as their image measurements with uncertainties. It allows a principled probabilistic inference to be performed so that image segmentation can be achieved through a most probable explanation (MPE) inference in the BN model. We have achieved encouraging results on the horse images from the Weizmann dataset. We have also demonstrated the possible ways to extend the BN model so as to incorporate other contextual information such as the global object shape and human intervention for improving image segmentation. Human intervention is encoded as new evidence in the BN model. Its impact is propagated through belief propagation to update the states of the whole model. From the updated BN model, new image segmentation is produced.
7D05A275	As the circuit complexity is increasing in demand for the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera is one of them where realtime image capturing, compression and storage of compressed image data is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image.
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
5E453122	In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al.
7F4AA6EA	Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
7B368C93	Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in more highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties. 
81262D6D	A Complex Adaptive System (CAS) is a network of self-organizing, intelligent agents that share knowledge and adapt their operations in order to achieve overall system goals. Three things are needed to understand, design, and evaluate CAS. First, a mathematical model or way-of-thinking about CAS, called Context-Sensitive Systems (CSS) theory, is required to provide a solid foundation upon which to represent and describe the kinds of interactions that occur among the CAS agents during system operation. Second, a graphical modeling language is required that implements CSS theory in a way that enhances visualization and understanding of CAS. Third, a systems design and evaluation tool is required that makes it easy to apply CSS theory, expressed using a graphical modeling language, to understand, design, and evaluate CAS. As an example, an OpEMCSS model of two intelligent agents is discussed that learn rules and maximize their average reward in the prisoner's dilemma game.
80D1B10B	We present a dynamic graphical model (DGM) for automated multi-instrument musical transcription. By multi-instrument transcription, we mean a system capable of listening to a recording in which two or more instruments are playing, and identifying both the notes that were played and the instruments that played them. Our transcription system models two musical instruments, each capable of playing at most one note at a time. We present results for two-instrument transcription on piano and violin sounds.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
761D5749	We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
7576CA59	A new VLSI algorithm and its associated systolic array architecture for a prime length type IV discrete cosine transform is presented. They represent the basis of an efficient design approach for deriving a linear systolic array architecture for type IV DCT. The proposed algorithm uses a regular computational structure called pseudoband correlation structure that is appropriate for a VLSI implementation. The proposed algorithm is then mapped onto a linear systolic array with a small number of I/O channels and low I/O bandwidth. The proposed architecture can be unified with that obtained for type IV DST due to a similar kernel. A highly efficient VLSI chip can be thus obtained with good performance in the architectural topology, computing parallelism, processing speed, hardware complexity and I/O costs similar to those obtained for circular correlation and cyclic convolution computational structures.
7D3C2818	AnN-point discrete Fourier transform (DFT) algorithm can be used to evaluate a discrete cosine transform by a simple rearrangement of the input data. This method is about two times faster compared to the conventional method which uses a2N-point DFT.
5937B17A	By combining the polynomial transform and radix-q decomposition, the paper presents a new algorithm for the type-III r-dimensional discrete Cosine transform (rD-DCT-III) with size ql1 ql2  ...  qlr, where q is an odd prime number. The number of multiplications for computing an rD-DCT-III is approximately 1/r times that needed by the row-column method while the number of additions increase slightly. The total number of operations (additions plus multiplications) is also reduced. The proposed algorithm has a simple computational structure because it needs only 1D-DCT-III and the polynomial transform.
75538016	Discrete cosine transforms (DCT) are essential tools in numerical analysis and digital signal processing. Processors in digital signal processing often use fixed point arithmetic. In this paper, we consider the numerical stability of fast DCT algorithms in fixed point arithmetic. The fast DCT algorithms are based on known factorizations of the corresponding cosine matrices into products of sparse, orthogonal matrices of simple structure. These algorithms are completely recursive, are easy to implement and use only permutations, scaling, butterfly operations, and plane rotations/rotation-reflections. In comparison with other fast DCT algorithms, these algorithms have low arithmetic costs. Using von Neumann Goldstine model of fixed point arithmetic, we present a detailed roundoff error analysis for fast DCT algorithms in fixed point arithmetic. Numerical tests demonstrate the performance of our results.
81736770	A new algorithm for the type-II multidimensional discrete cosine transform (MD-DCT) is proposed. Based on the polynomial transform, the rD-DCT with size N/sub 1//spl times/N/sub 2//spl times//spl middot//spl middot//spl middot//spl times/N/sub r/, where N/sub i/ is a power of 2, can be converted into a series of one dimensional (1-D) discrete cosine transforms (DCTs). The algorithm achieves considerable savings on the number of operations compared with the row-column method. For example, the number of multiplications for computing an r-dimensional DCT is only 1/r times that needed by the row-column method, and the number of additions is also reduced. Compared with other known polynomial transform algorithms for MD-DCT and the most recently presented algorithm for MD-DCT, the proposed one uses about the same number of operations. However, advantages such as better computational structure and flexibility in the choice of dimensional sizes can be achieved.
7D843341	A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.
7B64D19B	The discrete cosine transform of type IV (DCT-IV) and corresponding discrete sine transform of type IV (DST-IV) have played key role in the efficient implementation of orthogonal lapped transforms and perfect reconstruction cosine-modulated filter banks such as the oddly stacked modified discrete cosine transform (MDCT) or equivalently, the modulated lapped transform (MLT). However, the DCT-IV and DST-IV of double sizes are related to two variants of filter banks defined by Dolby Labs AC-3 digital audio compression algorithm. Since these two variants of filter banks are efficiently computed by recently proposed new fast algorithm for the oddly stacked MDCT (Signal Processing 82 (2002) 433), it is shown that the efficient DCT-IV and DST-IV computation can be realized via the MDCT of double size. The careful analysis of regular structure of the new fast MDCT algorithm allows to extract a new DCT-IV/DST-IV computational structure and to suggest a new sparse matrix factorization of the DCT-IV matrix. Finally, the new DCT-IV/DST-IV computational structure provides an alternative efficient implementation of the forward and inverse MDCT in layer III of MPEG (MP3) audio coding.
7A1904AD	A new recursive algorithm and two types of circuit architectures are presented for the computation of the two-dimensional discrete cosine transform (2D DCT). The new algorithm permits to compute the 2D DCT by a simple procedure of the 1D recursive calculations involving only cosine coefficients. The recursive kernel for the proposed algorithm contains a small number of operations. Also, it requires a smaller number of pre-computed data compared with many of existing algorithms in the same category. The kernel can be easily implemented in a simple circuit block with a short critical delay path. In order to evaluate the performance improvement resulting from the new algorithm, an architecture for the 2D DCT designed by direct mapping from the computation structure of the proposed algorithm has been implemented in an FPGA board. The results show that the reduction of the hardware consumption can easily reach 25 and the clock frequency can increase 17 compared with a system implementing a recently reported 2D DCT recursive algorithm. For a further reduction of the hardware, another architecture has been proposed for the same 2D DCT computation. Using one recursive computation block to perform different functions, this architecture needs only approximately one-half of the hardware that is required in the first architecture, which has been confirmed by an FPGA implementation.
7FA005E7	This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.
801D5F6A	A method is proposed to factor the type-II discrete cosine transform (DCT-II) into lifting steps and additions. After approximating the lifting matrices, we get a new type-II integer discrete cosine transform (IntDCT-II) that is float-point multiplication free. Based on the relationships among the various types of DCTs, we can generally factor any DCTs into lifting steps and additions and then get four types of integer DCTs, which need no float-point multiplications. By combining the polynomial transform and the one-dimensional (1-D) integer cosine transform, a two-dimensional (2-D) integer discrete cosine transform is proposed. The proposed transform needs only integer operations and shifts. Furthermore, it is nonseparable and requires a far fewer number of operations than that used by the corresponding row-column 2-D integer discrete cosine transform.
7E83E2D3	In this paper, a generalized fast computational algorithm for the n-dimensional discrete cosine transform (DCT) of length N=2/sup m/ (m/spl ges/2) is presented. The developed algorithm is proved and its efficiency is evaluated theoretically. The theoretical results show that compared with the conventional method of computing the one-dimensional along n directions, the number of multiplications needed by our algorithm is only 1/n of that required by the conventional method; for the total number of additions, the latter is a bit more when N/spl les/8 and much fewer when N/spl ges/16 than the former. To validate the proposed algorithm, we take the case when n=3 as an example and apply it to motion-picture coding. The results show that our method is superior to MPEG-2 in speed and coding performance. The algorithm is clearly described and it is easy to make a computer program for implementation.
7D419CF0	New algorithms are proposed for the type-III multidimensional discrete cosine transform (MD-DCT-III). The polynomial transform is used to convert the type-III MD-DCT into a series of one-dimensional type-III discrete cosine transforms (1-D-DCT-III). The algorithms achieve considerable savings on the number of operations compared to the row column method. For computing an r-dimensional DCT-III, the number of multiplications required by the proposed algorithm is only 1//spl tau/ times that needed by the row-column method, and the number of additions is also reduced. Compared to other known fast algorithms for two-dimensional- and MD-DCTs, the proposed method uses about the same number of operations. However, advantages such as better computational structure and flexibility on the choice of dimensional sizes can be achieved.
77E8F1CA	Due to technology scaling trends, the accurate and efficient calculations of the temperature distribution corresponding to a specific circuit layout and power density distribution will become indispensable in the design of high-performance very large scale integrated circuits. In this paper, we present three highly efficient thermal simulation algorithms for calculating the on-chip temperature distribution in a multilayered substrate structure. All three algorithms are based on the concept of the Green function and utilize the technique of discrete cosine transform. However, the application areas of the algorithms are different. The first algorithm is suitable for localized analysis in thermal problems, whereas the second algorithm targets full-chip temperature profiling. The third algorithm, which combines the advantages of the first two algorithms, can be used to perform thermal simulations where the accuracy requirement differs from place to place over the same chip. Experimental results show that all three algorithms can achieve relative errors of around 1% compared with that of a commercial computational fluid dynamic software package for thermal analysis, whereas their efficiencies are orders of magnitude higher than that of the direct application of the Green function method.
7D9FC910	Advances in wavelet transforms and quantization methods have produced algorithms capable of surpassing the existing image compression standards like the Joint Photographic Experts Group (JPEG) algorithm. The existing compression methods for JPEG standards are using DCT with arithmetic coding and DWT with Huffman coding. The DCT uses a single kernel where as wavelet offers more number of filters depends on the applications. The wavelet based Set Partitioning In Hierarchical Trees (SPIHT) algorithm gives better compression. For best performance in image compression, wavelet transforms require filters that combine a number of desirable properties, such as orthogonality and symmetry, but they cannot simultaneously possess all of these properties. The relatively new field of multiwavelets offer more design options and can combine all desirable transform features. But there are some limitations in using the SPIHT algorithm for multiwavelet coefficients. This paper presents a new method for encoding the multiwavelet decomposed images by defining coefficients suitable for SPIHT algorithm which gives better compression performance over the existing methods in many cases.
7E0F3E15	The Discrete Tchebichef Transform (DTT) which based on discrete orthogonal Tchebichef polynomials can be an alternative to the Discrete Cosine Transform (DCT) for image processing such as image compression and image recognition as the properties of the DTT are similar to that of the DCT. The DTT not only has higher energy compactness than the DCT in images that have high illumination value variations such as artificial diagrams but also has the advantage of easily computation using a set of recurrence relations. In this paper, the DTT will be introduced with explanation of its similarity to DCT, also a new fast and efficient 4x4 algorithm for computing the DTT coefficients which can be used in image compression will be proposed. This algorithm reduces computational complexity as measured in terms of the number of arithmetic operations while keeping the accuracy of the reconstructed images. 
8112BD3C	Recently,  with  the  advances  in  digital  signal  processing,  compression  of  biomedical  signals  has  received  great attention for telemedicine applications. In this paper, an adaptive transform  coding-based  method  for  compression  of  respiratory and  swallowing  sounds  is  proposed.  Using  special  characteristics  of  respiratory  sounds,  the  recorded  signals  are  divided  into stationary and nonstationary portions, and two different bit allocation methods (BAMs) are designed for each portion. The method was applied to the data of 12 subjects and its performance in terms of overall signal-to-noise ratio (SNR) values was calculated at different bit rates. The performance of different quantizers was also considered  and  the  sensitivity  of  the  quantizers  to  initial  conditions has been alleviated. In addition, the fuzzy clustering method was examined for classifying the signal into different numbers of clusters and investigating the performance of the adaptive BAM with increasing the number of classes. Furthermore, the effects of assigning  different  numbers  of  bits  for  encoding  stationary  and nonstationary  portions  of  the  signal  were  studied.  The  adaptive BAM with variable number of bits was found to improve the SNR values of the fixed BAM by 5 dB. Last, the possibility of removing the  training  part  for  finding  the  parameters  of  adaptive  BAMs for  each individual was  investigated. The results indicate that it is  possible  to  use  a  predefined  set  of  BAMs  for  all  subjects  and remove the training part completely. Moreover, the method is fast enough to be implemented for real-time application.
76FFC627	In this paper introduce new idea for image compression based on the two levels DWT. The low-frequency sub-band is minimized by using DCT with the Minimize-Matrix-Size-Algorithm, which is converting the AC-coefficients into array contains stream of real values, and then store the DC-coefficients are stored in a column called DC-Column. DC-Column is transformed by one-dimensional DCT to be converted into T-Matrix, then T-Matrix compressed by RLE and arithmetic coding. While the high frequency sub-bands are compressed by the technique; Eliminate Zeros and Store Data (EZSD). This technique eliminates each 8 × 8 sub-matrix contains zeros from the high frequencies sub-bands, in another hands store nonzero  data in an array.The results of our compression algorithm compared with JPEG2000 by using four different gray level images. 
80A84DE7	This paper presents a robust image watermarking method in discrete cosine transform (DCT) domain based on chaotic sequences encryption. Exploiting some characteristics of Human Visual System (HVS) and DC(Direct Current) components having much larger perceptual capacity than any AC (Alternating Current)components, watermark is embedded into the DC components of the host image. First, we scramble the watermark image to avoid the block effect. Then we split the host image and the scrambled watermark image into 8×8block respectively, and the scrambled watermark is embedded into the DC components of the host image. The experimental results show that the embedded watermark is invisible and robust against noise and commonly used image processing methods such as Gaussian, JPEG compression, Median filtering etc
792E12D0	A modified discrete Fourier-cosine transform (DFCT) algorithm and its VLSI implementation on a high speed VLSI chip are presented. The proposed DFCT algorithm achieves a considerably higher throughput rate when compared to other implementations by exploiting the inherent parallelism of the new flowgraph, proposed for the DFCT algorithm, to the full. DFCTs of greater length and two dimensional DFCTs can be performed by a set of such chips, at board level. 1.2 DLM CMOS technology was used for the implementation of the chip and the chip die size is 112.54 mm2. The throughput rate for the DFCT is 750 Mbitss.
5BE8D5F7	Digital image and video in their raw form require an enormous amount of storage capacity. Considering the important role played by digital imaging and video in medical and health science, it is necessary to develop a system that produces high degree of compression while preserving critical image/video information. In this paper, we present a hybrid algorithm that performs the discrete cosine transform on the discrete wavelet transform coefficients. Simulation has been carried out on several medical and endoscopic images and videos. The results show that the proposed hybrid algorithm performs much better in term of peak-signal-to-noise-ratio with a higher compression ratio compared to standalone DCT and DWT algorithms. The scheme is intended to be used as the image/video compressor engine in medical imaging and video applications, such as, telemedicine and wireless capsule endoscopy
60C2371C	Small length Discrete Cosine Transforms (DCT's) are used for image data compression. In that case, length 8 or 16 DCT's are needed to be performed at video rate. We propose two new implementation of DCT's which have several interesting features, as far as VLSI implementation is concerned. A first one, using modulo-arithmetic, needs only one multiplication per input point, so that a single multiplier is needed on-chip. A second one, based on a decomposition of the DCT into polynomial products, and evaluation of these polynomial products by distributed arithmetic, results in a very small chip, with a great regularity and testability. Furthermore, the same structure can be used for FFT computation by changing only the ROM-part of the chip. Both new architectures are mainly based on a new formulation of a length-2nDCT as a cyclic convolution, which is explained in the first section of the paper.
7E6FD3E0	An adaptive DCT-based image compression algorithm for radar images is proposed, tested and compared to JPEG and to classical coding algorithms for remote sensing imagery. The Modified Adaptive Discrete Cosine Transform (MADCT) scheme is proposed, which allows one to classify each image block by means of a threshold criterion based on AC and DC activity. The strategy of transmission of the DCT coefficients, the recovering process of blocks incorrectly discarded, and the bit-allocation phase have been properly designed to provide high compression of two classes of images: X-band real-aperture radar images for ship traffic control, and SAR images for browsing applications. The experimental results, in terms of PSNR and compression ratio, prove the superiority of the novel scheme with respect to standard coding techniques.
7A6D7FA4	Digital images in their uncompressed form require an enormous amount of storage capacity. Such uncompressed data needs large transmission bandwidth for the transmission over the network. Discrete Cosine Transform (DCT) is one of the widely used image compression method and the Discrete Wavelet Transform (DWT) provides substantial improvements in the quality of picture because of multi resolution nature. Image compression reduces the storage space of image and also maintains the quality information of the image. In this research study the performance of three most widely used techniques namely DCT, DWT and Hybrid DCT-DWT are discussed for image compression and their performance is evaluated in terms of Peak Signal to Noise Ratio (PSNR), Mean Square Error (MSE) and Compression Ratio (CR). The experimental results obtained from the study shows that the Hybrid DCT- DWT technique for image compression has in general a better performance than individual DCT or DWT. 
816929C1	A novel block matching algorithm for motion estimation in a video frame sequence, well suited for a high performance FPGA implementation is presented in this paper. The algorithm is up to 40% faster when compared to one of the fastest existing algorithms, viz., one-at-a-time step search algorithm without compromising either in the image quality or in the compression effected. The speed advantage is preserved even in the event of a sudden scene change in a video sequence. The proposed algorithm is also capable of dynamically detecting the direction of motion of image blocks. The FPGA implementation of the algorithm is capable of processing color pictures of sizes up to 1024x768 pixels at the real time video rate of 25 frames/second and conforms to MPEG-2 standards.
76022FC0	This paper assesses the arithmetic benefits provided by the Residue Number System (RNS) for building Digital Signal Processing (DSP) systems with Field-Programmable Logic (FPL) technology. The quantifiable benefits of this approach are studied in the context of a new Fast Cosine Transform (FCT) architecture enhanced by using the Quadratic Residue Number System (QRNS). The system reduces the number of adders and multipliers required for the N-point Discrete Cosine Transform (DCT) and provides high throughput. For an FPL-based implementation, the proposed design gets significant improvements over an equivalent 2C structure. By using up to 6-bit moduli, an overall increase in the system performance of about 140% is achieved. If this speed increase is considered along with the penalty in device resources, the presented QRNS-based FCT system provides an improvement in the area-delay figure factor of about 20%. Finally, the conversion overhead was carefully studied and it was found that the quantifiable benefits of the proposed design are not affected when converters are included 
80FFB575	In this paper, we propose a system-level error tolerance scheme for systems where a linear transform is combined with quantization. These are key components in multimedia compression systems, e.g., video and image codecs. Using the concept of acceptable degradation, our scheme classifies hardware faults into acceptable and unacceptable faults. We propose analysis techniques that allow us to estimate the faults' impact on compression performance, and in particular on the quality of decoded images/video. We consider as an example the discrete cosine transform (DCT), which is part of a large number of existing image and video compression systems. We propose methods to establish thresholds of acceptable degradation and corresponding testing algorithms for DCT-based systems. Our results for a JPEG encoder using a typical DCT architecture show that over 50% of single stuck-at interconnection faults in one of its 1D DCT modules lead to imperceptible quality degradation in the decoded images, over the complete range of compression rates at which JPEG can operate.
7F066D3F	Frequency -warped signal processing techniques are attractive to many wideband speech and audio applications since they have a clear connection to the frequency resolution of human hearing. A warped version of the linear predictive coding (LPC) for speech compression is implemented in this paper and an analysis of the application of Set Partitioning In Hierarchical Trees (SPIHT) algorithm to the compression of speech signals is performed. It has been shown that the proposed scheme i.e. Warped LPC with MLT_SPIHT algorithm produces an enhancement in speech quality. The proposed scheme is based on the combination of the Modulated Lapped Transform(MLT) and SPIHT. Comparisons are made with Plain LPC Coder, Voice Excited LPC Coder with the coding of the residual signal with DCT, Voice Excited LPC Coder with the coding of the residual signal with MLT and SPIHT. The performance of the coders described has been assessed by computer simulation in terms of a) Signal -to -noise ratio (SNR) b) Compression ratio c) Informal subjective listening test.
80CB08BD	This paper presents a new adaptive algorithm for speech compression using cosine packet transform. The proposed algorithm uses packet decomposition, which reduces a computational complexity of a system. This paper compare the compression ratio of methods using wavelet transform, cosine transform, wavelet packet transform and proposed adaptive algorithm using cosine packet transform for different speech signal samples. The mean compression ratio is calculated for all the methods and compared. The implemented results show that the proposed compression algorithm gives the better performance for speech signals.
7996F7EA	Unlike classical wired networks and wireless sensor networks, WMSN differs from their predecessor’s scalar network basically in the following points; nature and size of data being transmitted, important memory resources, as well as, power consumed per each node for processing and transmission. The most effective solution to overcome those problems is image compression. As the image contains massive amount of redundancies resulting from high correlation between pixels, many compression algorithms have been developed. The main objective of this survey was to study and analyze relevant research directions and the most recent algorithms of image compression over WMSN. This survey characterizes the benefits and shortcomings of recent efforts of such algorithms. Moreover, it provides an open research issue for each compression method; and its potentials to WMSN. Reducing consumed power thus granting long life time is considered the main performance metric and will be the main target in the investigated solution
77ABB4F5	As data compression plays now an important role in the development of medical PACS, a technique has been developed for medical image sequences storage and transmission in order to obtain very high compression ratio: in dynamic nuclear medicine studies it can achieve a compression ratio as high as 100:1 without significant degradation. The implemented technique combines two methods which multiply their effects. In a first step, a principal component analysis (PCA) of the image series is performed. It extracts a limited number of principal components and their associated images. For data compression it is not necessary to perform an oblique factor analysis to estimate the so-called ‘physiological functions’ and their spatial distributions as in factor analysis of dynamic structures (FADS). In a second step, the principal images are compressed by means of a transform coding procedure: an adaptive block-quantization technique using the 2D discrete cosine transform (DCT) is implemented, followed by a statistical quantization method to encode the DCT coefficients. To reconstruct the principal images, an inverse DCT is applied. Then the original series is computed from the reconstructed images combined with the principal components which have been stored without any modification. The reconstructed series is compared to the original series, as well as the time activity curves generated on different regions of interest (ROI) and the factor estimates obtained using FADS performed on the two series. Method and evaluation are illustrated on an example of first pass radionuclide angiocardiography.
80BAFEB7	This paper presents the results of a questionnaire on user behaviour during searching the Web as part of information gathering tasks. In this study, users' Web activities related to finding information, comparing information, managing information, and re-finding information, using different Web search and navigation tools, used for information gathering tasks are explored. The results indicate that current Web tools lack important functionalities for supporting how users find, re-find, and manage information during Web information gathering tasks. Furthermore, the results indicate that visual characteristics of search results, re-finding tools that bookmark complete and partial search sessions with user annotation, and integrated information management features may be useful in improving how users gather information on the Web.
7C5CBF21	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.
7BC471DD	Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination. We focus on two techniques, inverted index compression and in- dex  caching,  which  play  a  crucial  rule  in  web  search  engines  as well as other high-performance information retrieval systems.  We perform a comparison and evaluation of several inverted list compression algorithms, including new variants of existing algorithms that have not been studied before.  We then evaluate different inverted list caching policies on large query traces, and finally study the possible performance benefits of combining compression and caching. The overall goal of this paper is to provide an updated discussion and evaluation of these two techniques, and to show how to select the best set of approaches and settings depending on parameter such as disk speed and main memory cache size.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
5CFC04D2	Commercial web search engines adopt parallel and replicated architecture in order to support high query throughput. In this paper, we investigate the effect of caching on the throughput in such a setting. A simple scheme, called uniform caching, would replicate the cache content to all servers. Unfortunately, it does not exploit the variations among queries, thus wasting memory space on caching the same cache content redundantly on multiple servers. To tackle this limitation, we propose a diversified caching problem, which aims to diversify the types of queries served by different servers, and maximize the sharing of terms among queries assigned to the same server. We show that it is NP-hard to find the optimal diversified caching scheme, and identify intuitive properties to seek good solutions. Then we present a framework with a suite of techniques and heuristics for diversified caching. Finally, we evaluate the proposed solution with competitors by using a real dataset and a real query log.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7EE3F39D	Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines. We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information. With this aim, we conducted a survey with 236 experienced web users. The results showed that this group has frequently used key strategies (e.g., using several browser windows in parallel) that they find important, whereas some of the strategies that have been suggested in previous studies are clearly less important for them (e.g., including URLs on a webpage). In some aspects, such as query formulation, this group resembles less experienced web users. For instance, we found that most of the respondents had misconceptions about how their search engine handles queries, as well as other problems with information search and re-access. In addition to presenting the prevalence of the strategies and rationales for their use, we present concrete designs solutions and ideas for making the key strategies also available to less experienced users.
78A4DC3B	The phenomenon of sponsored search advertising â where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results â is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
7644AE7A	We study the problem of caching query result pages in Web search engines. Popular search engines receive millions of queries per day, and for each query, return a result page to the user who submitted the query. The user may request additional result pages for the same query, submit a new query, or quit searching altogether. An efficient scheme for caching query result pages may enable search engines to lower their response time and reduce their hardware requirements. This work studies query result caching within the framework of the competitive analysis of algorithms. We define a discrete time stochastic model for the manner in which queries are submitted to search engines by multiple user sessions. We then present an adaptation of a known online paging scheme to this model. The expected number of cache misses of the resulting algorithm is no greater than 4 times the expected number of misses that any online caching algorithm will experience under our specific model of query generation.
7E418727	Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.
75F59C14	To inform the design of next-generation Web search tools, researchers must better understand how users find, manage, and refind online information. Synthesizing results from one of their studies with related work, the authors propose a search engine use model based on prior task frequency and familiarity.
7B829984	In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.
7D837E78	When a person issues a query, that person has expectations about the search results that will be returned. These expectations can be based on the current information need, but are also influenced by how the searcher believes the search engine works, where relevant results are expected to be ranked, and any previous searches the individual has run on the topic. This paper looks in depth at how the expectations people develop about search result lists during an initial query affect their perceptions of and interactions with future repeat search result lists. Three studies are presented that give insight into how people recall, recognize, and reuse results. The first study (a study of recall) explores what people recall about previously viewed search result lists. The second study (a study of recognition) builds on the first to reveal that people often recognize a result list as one they have seen before even when it is quite different. As long as those aspects that the searcher remembers about the initial list remain the same, other aspects can change significantly. This is advantageous because, as the third study (a study of reuse) shows, when a result list appears to have changed, people have trouble re-using the previously viewed content in the list. They are less likely to find what they are looking for, less happy with the result quality, more likely to find the task hard, and more likely to take a long time searching. Although apparent consistency is important for reuse, people's inability to recognize change makes consistency without stagnation possible. New relevant results can be presented where old results have been forgotten, making both old and new content easy to find.
766A0371	Assistance technology is undoubtedly one of the important elements in the commercial search engines, and routing the user towards the right direction throughout the search sessions is of great importance for providing a good search experience. Most search assistance methods in the literature that involve query generation, query expansion and other techniques consider each suggestion candidate individually, which implies an independence assumption. We challenge this independence assumption and give a method to maximize the utility of a given set of suggestions. For this, we will define a measure of conditional utility for query pairs using query-URL bipartite graphs based on the session logs (clicked and viewed URLs). Afterwards, we remove the redundant queries from the suggestion set using a greedy algorithm to be able to replace them with more useful ones. Both offline (based on user studies and session log analysis) and online (based on millions of user interactions) evaluations show that modeling the conditional utility and maximizing the utility of the set of queries (by eliminating redundant ones) significantly increases the effectiveness of the search assistance both for the presubmit and postsubmit modes.
80026F92	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
7DCF9C8B	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.
5995BFC1	We study the process in which search engines with segmented indices serve queries. In particular, we investigate the number of result pages which search engines should prepare during the query processing phase.Search engine users have been observed to browse through very few pages of results for queries which they submit. This behavior of users suggests that prefetching many results upon processing an initial query is not efficient, since most of the prefetched results will not be requested by the user who initiated the search. However, a policy which abandons result prefetching in favor of retrieving just the first page of search results might not make optimal use of system resources as well. We argue that for a certain behavior of users, engines should prefetch a constant number of result pages per query. We define a concrete query processing model for search engines with segmented indices, and analyze the cost of such prefetching policies. Based on these costs, we show how to determine the constant which optimizes the prefetching policy. Our results are mostly applicable to local index partitions of the inverted files, but are also applicable to processing of short queries in global index architectures.
8035BDA2	A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, “Can link information result in better rankings?”, “Do longer queries result in better answers?”, and, “Do TREC systems work well on Web data?” The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art.
804E7F29	A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel, otherwise it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th-percentile), which we call extreme tail latency.We propose a prediction framework to reduce the extreme tail latency of search servers. The framework has a unique set of characteristics to predict long-running queries with high recall and improved precision. Specifically, prediction is delayed by a short duration to allow many short-running queries to complete without parallelization, and to allow the predictor to collect a set of dynamic features using runtime information. These features estimate query execution time with high accuracy. We also use them to estimate the prediction errors to override an uncertain prediction by selectively accelerating the query for a higher recall.We evaluate the proposed prediction framework to improve search engine performance in two scenarios using a simulation study: (1) query parallelization on a multicore processor, and (2) query scheduling on a heterogeneous processor. The results show that, for both scenarios, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision.
808285A5	Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called "information need". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
7D08AE3B	Long web search result lists can be hard to browse. We demonstrated experimentally, in a previous study, the usefulness of a categorization algorithm and filtering interface. However, the nature of interaction in real settings is not known from an experiment in laboratory settings. To address this problem, we provided our categorizing web search user interface to 16 users for a two month period. The interactions with the system were logged and the users' opinions were elicited with two questionnaires. The results show that categories are successfully used as part of users' search habits. They are helpful when the result ranking of the search engine fails. In those cases, the users are able to access results that locate far in the rank order list with the categories. Users can also formulate simpler queries and find needed results with the help of the categories. In addition, the categories are beneficial when more than one result is needed like in an exploratory or undirected search task.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
7AB88A91	Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14\% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing algorithms even on HDD-based architecture.
7CF71B8F	The Web holds a great quantity of material that can be used to enhance classroom instruction. However, it is not easy to retrieve this material with the search engines currently available. This study produced a specialized search assistant based on Google that significantly increases the number of instances in which teachers find the desired learning objects as compared to using this popular public search engine directly. Success in finding learning objects by study participants went from 80&percnt; using Google alone to 96&percnt; when using our search assistant in one scenario and, in another scenario, from a 40&percnt; success rate with Google alone to 66&percnt; with our assistant. This specialized search assistant implements features such as bilingual search and term suggestion which were requested by teacher participants to help improve their searches. Study participants evaluated the specialized search assistant and found it significantly easier to use and more useful than the popular search engine for the purpose of finding learning objects. 
7F6257E8	Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.
78E21799	Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!
7A6F5F82	Finding the right information in the World Wide Web is becoming a fundamental problem, since the amount of global information that the WWW contains is growing at an incredible rate. In this paper, we present a novel method to extract from a web object its “hyper” informative content, in contrast with current search engines, which only deal with the “textual” informative content. This method is not only valuable per se, but it is shown to be able to considerably increase the precision of current search engines, Moreover, it integrates smoothly with existing search engines technology since it can be implemented on top of every search engine, acting as a post-processor, thus automatically transforming a search engine into its corresponding “hyper” version. We also show how, interestingly, the hyper information can be usefully employed to face the search engines persuasion problem.
7BB23ED5	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time
816FE39A	This paper presents a modified diary study that investigated how people performed personally motivated searches in their email, in their files, and on the Web. Although earlier studies of directed search focused on keyword search, most of the search behavior we observed did not involve keyword search. Instead of jumping directly to their information target using keywords, our participants navigated to their target with small, local steps using their contextual knowledge as a guide, even when they knew exactly what they were looking for in advance. This stepping behavior was especially common for participants with unstructured information organization. The observed advantages of searching by taking small steps include that it allowed users to specify less of their information need and provided a context in which to understand their results. We discuss the implications of such advantages for the design of personal information management tools.
7F79648F	The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.
7B2E5EC6	In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want
7BC0845F	We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We propose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strategies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Yahoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
77445FD3	Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query "breast cancer treatments", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten.
777B5147	For many search settings, distributed/replicated search engines deploy a large number of machines to ensure efficient retrieval. This paper investigates how the power consumption of a replicated search engine can be automatically reduced when the system has low contention, without compromising its efficiency. We propose a novel self-adapting model to analyse the trade-off between latency and power consumption for distributed search engines. When query volumes are high and there is contention for the resources, the model automatically increases the necessary number of active machines in the system to maintain acceptable query response times. On the other hand, when the load of the system is low and the queries can be served easily, the model is able to reduce the number of active machines, leading to power savings. The model bases its decisions on examining the current and historical query loads of the search engine. Our proposal is formulated as a general dynamic decision problem, which can be quickly solved by dynamic programming in response to changing query loads. Thorough experiments are conducted to validate the usefulness of the proposed adaptive model using historical Web search traffic submitted to a commercial search engine. Our results show that our proposed self-adapting model can achieve an energy saving of 33% while only degrading mean query completion time by 10 ms compared to a baseline that provisions replicas based on a previous day's traffic.
7F48E77D	With the proliferation of social media, consumers’ cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers’ search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
80FCD18B	Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
7DB5FDAD	This paper reports our efforts to address the grand challenge of the Digital Earth vision in terms of intelligent data discovery from vast quantities of geo-referenced data. We propose an algorithm combining LSA and a Two-Tier Ranking (LSATTR) algorithm based on revised cosine similarity to build a more efficient search engine – Semantic Indexing and Ranking (SIR) – for a semantic-enabled, more effective data discovery. In addition to its ability to handle subject-based search, we propose a mechanism to combine geospatial taxonomy and Yahoo! GeoPlanet for automatic identification of location information from a spatial query and automatic filtering of datasets that are not spatially related. The metadata set, in the format of ISO19115, from NASA's SEDAC (Socio-Economic Data Application Center) is used as the corpus of SIR. Results show that our semantic search engine SIR built on LSATTR methods outperforms existing keyword-matching techniques, such as Lucene, in terms of both recall and precision. Moreover, the semantic associations among all existing words in the corpus are discovered. These associations provide substantial support for automating the population of spatial ontologies. We expect this work to support the operationalization of the Digital Earth vision by advancing the semantic-based geospatial data discovery.
777B5147	For many search settings, distributed/replicated search engines deploy a large number of machines to ensure efficient retrieval. This paper investigates how the power consumption of a replicated search engine can be automatically reduced when the system has low contention, without compromising its efficiency. We propose a novel self-adapting model to analyse the trade-off between latency and power consumption for distributed search engines. When query volumes are high and there is contention for the resources, the model automatically increases the necessary number of active machines in the system to maintain acceptable query response times. On the other hand, when the load of the system is low and the queries can be served easily, the model is able to reduce the number of active machines, leading to power savings. The model bases its decisions on examining the current and historical query loads of the search engine. Our proposal is formulated as a general dynamic decision problem, which can be quickly solved by dynamic programming in response to changing query loads. Thorough experiments are conducted to validate the usefulness of the proposed adaptive model using historical Web search traffic submitted to a commercial search engine. Our results show that our proposed self-adapting model can achieve an energy saving of 33% while only degrading mean query completion time by 10 ms compared to a baseline that provisions replicas based on a previous day's traffic.
7CF24ED6	We investigate how people interact with Web search engine result pages using eye-tracking. While previous research has focused on the visual attention devoted to the 10 organic search results, this paper examines other components of contemporary search engines, such as ads and related searches. We systematically varied the type of task (informational or navigational), the quality of the ads (relevant or irrelevant to the query), and the sequence in which ads of different quality were presented. We measured the effects of these variables on the distribution of visual attention and on task performance. Our results show significant effects of each variable. The amount of visual attention that people devote to organic results depends on both task type and ad quality. The amount of visual attention that people devote to ads depends on their quality, but not the type of task. Interestingly, the sequence and predictability of ad quality is also an important factor in determining how much people attend to ads. When the quality of ads varied randomly from task to task, people paid little attention to the ads, even when they were good. These results further our understanding of how attention devoted to search results is influenced by other page elements, and how previous search experiences influence how people attend to the current page.
7DF6AF1C	In modern commercial search engines, the pay-per-click (PPC) advertising model is widely used in sponsored search. The search engines try to deliver ads which can produce greater click yields (the total number of clicks for the list of ads per impression). Therefore, predicting user clicks plays a critical role in sponsored search. The current ad-delivery strategy is a two-step approach which first predicts individual ad CTR for the given query and then selects the ads with higher predicted CTR. However, this strategy is naturally suboptimal and correlation between ads is often ignored under this strategy. The learning problem is focused on predicting individual performance rather than group performance which is the more important measurement.In this paper, we study click yield measurement in sponsored search and focus on the problem---predicting group performance (click yields) in sponsored search. To tackle all challenges in this problem---depth effects, interactive influence, cold start and sparseness of ad textual information---we first investigate several effects and propose a novel framework that could directly predict group performance for lists of ads. Our extensive experiments on a large-scale real-world dataset from a commercial search engine show that we achieve significant improvement by solving the sponsored search problem from the new perspective. Our methods noticeably outperform existing state-of-the-art approaches.
80026F92	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
80650A24	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
7D1F7697	As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.
7FA14777	FPGAs (field programmable gate arrays) have appealing features such as customizable internal and external bandwidth and the ability to exploit vast amounts of fine-grain parallelism. In this paper, we explore the applicability of these features in using FPGAs as smart memory engines for search and reorganization computations over spatial pointer-based data structures. The experimental results in this paper suggests that reconfigurable logic, when combined with the data reorganization, can lead to dramatic performance improvements of up to 20x over traditional computer architectures for pointer-based computations, traditionally not viewed as a good match for reconfigurable technologies.
7340259A	In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset. 
80601F25	We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a "Learning to Rank" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
77D497A2	The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user. In this paper we describe an interface which enables users to have more control over the optimization function used to present the results. We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory. We use an algorithm based on bounded greedy selection [5] to construct the result set based on parameters specified by the user.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7FEBCDC1	This paper analyzes the computational complexity of finding relevant documents on the Web. Given a search query that has n significant terms, relevant documents retrieved by search engines will contain at least a number k of the significant terms. The threshold k chosen will depend on the collection of documents and is determined experimentally upon formation of the collection. Algorithms are then provided to compute a similarity ranking. The fundamental analysis is based on combinatorial theory and theorems providing bounds on the runtime complexity of the algorithms are proven.
77B1FCBA	Providing an effective mobile search service is a difficult task given the unique characteristics of the mobile space. Small-screen devices with limited input and interaction capabilities do not make ideal search devices. In addition, mobile content, by its concise nature, offers limited indexing opportunities, which makes it difficult to build high-quality mobile search engines and indexes. In this paper we consider the issue of limited page content by evaluating a heuristic content enrichment framework that uses standard Web resources as a source of additional indexing knowledge. We present an evaluation using a mobile news service that demonstrates significant improvements in search performance compared to a benchmark mobile search engine.
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
02426620	Code Search Engines (CSE) can serve as powerful resources of open source code, as they can search in billions of lines of open source code available on the web. The strength of CSEs can be used for several tasks like searching relevant code samples, identifying hotspots, and finding bugs. However, the major limitations in using CSEs for these tasks are that the returned samples are too many and they are often partial. Our framework addresses the preceding limitations and thereby helps in using CSEs for these tasks. We showed the effectiveness of our framework with two tools developedbased on our framework.
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
7F48E77D	With the proliferation of social media, consumers’ cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers’ search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
58FB967D	A part of a larger user study conducted within the scope of the EU project IRIS was to investigate the preferences of people with disabilities in regard to interface design of Web applications. User requirements of online help and search engines were also in the focus of this study, since the target user group depends heavily upon powerful tools to support them in the process of seeking information. The results showed that user preferences for information presentation vary a great deal, which could be solved by comprehensive user profiling. However, the functionalities of online help and search functions, as presented in the Internet today, need to be enhanced so they can be used more flexibly and be adjusted to the users’ mental representation of problems and information needs to help them fulfill their tasks.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
7BB9B487	The world wide web of today publishes a great number of real-time content, causing the increasing need for a differentiated way of searching. In this paper, three issues related to retrieving real-time content are presented, and their applications are proposed. First, the characteristics of real-time content, as well as the concept of real-time search are introduced. Second, the real-time technologies that enable real-time search are described. Finally, a platform for application services utilizing real-time search is proposed.
75B65C35	We investigate search engines' mechanism for allocating impressions generated from different search terms. This mechanism is equivalent to running an independent GSP auction for each search term only when the number of search terms is small. In practice, the number of search terms is so large that an advertiser cannot possibly communicate to the search engine all the GSP auctions that he wishes to participate in. For example, a travel agency is interested in all search terms pertaining to flight, including "flight to boston", "ticket to SFO", "cheap airfare", etc. Therefore, the search engine introduces broad match keywords as a bidding language that allows an advertiser to submit a bid for multiple GSP auctions at once. However, with broad match keywords, the GSP auctions are no longer independent, i.e. an advertiser's bid in one auction may depend on his bid in another auction.We propose the broad match mechanism as a model that captures this aspect of the multi-keyword sponsored search mechanism. We study the performance of this mechanism under the price of anarchy (POA) framework. We identify two properties of broad match keywords, namely expressiveness and homogeneity, that characterize the POA and we prove almost tight bounds on the POA. The bounds allow us to explore trade-offs between the two properties. We introduce the exact-match-only mechanism whose performance, when compared to that of broad match mechanisms, gives us an insight into the net benefit of broad match keywords. The broad match mechanism can also be viewed as a mechanism that copes with severe communication constraint i.e. the valuation of an advertiser is described by many more numbers than the search engine can solicit.
7FBD8242	The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.
80BE578A	Structured documents (predominantly encoded in XML) utilize markup dialects for several purposes, such as conveying logical structure, or providing rendering instructions. XML structure can also help users to navigate within documents to satisfy their information needs. However, including the user's structural preferences in the ranking of retrieved elements remains a key challenge in XML retrieval. In this paper, we propose an approach for including structural preferences in the ranking of XML elements by improving the structural relevance (SR) of results. SR is an evaluation measure which relies on graphical navigation models to capture the structural preferences of users. We propose several algorithms to post-process search engine output to improve the SR of the output. Experimental results (using data, assessments, and search engines from INEX 2007 and 2008) demonstrate the effect of different combinations of post-processing algorithms and navigation models on the effectiveness of systems.
7B60E99F	Understanding the functional mechanisms of the complex biological system as a whole is drawing more and more attention in global health care management. Traditional Chinese Medicine (TCM), essentially different from Western Medicine (WM), is gaining increasing attention due to its emphasis on individual wellness and natural herbal medicine, which satisfies the goal of integrative medicine. However, with the explosive growth of biomedical data on the Web, biomedical researchers are now confronted with the problem of large-scale data analysis and data query. Besides that, biomedical data also has a wide coverage which usually comes from multiple heterogeneous data sources and has different taxonomies, making it hard to integrate and query the big biomedical data. Embedded with domain knowledge from different disciplines all regarding human biological systems, the heterogeneous data repositories are implicitly connected by human expert knowledge. Traditional search engines cannot provide accurate and comprehensive search results for the semantically associated knowledge since they only support keywords-based searches. In this paper, we present BioTCM-SE, a semantic search engine for the information retrieval of modern biology and TCM, which provides biologists with a comprehensive and accurate associated knowledge query platform to greatly facilitate the implicit knowledge discovery between WM and TCM.
7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
75CC9391	With much information available from open sources on the internet, computer generated databases have become commonplace. The question whether computer generated databases can be protected under the sui generis database right has hitherto received little attention. This article investigates this question and finds that the substantiality of investment, the definition of the rights holder and the interpretation of exclusive rights raise fundamental issues.
7F2BF256	Today, Web browsers can interpret an enormous amount of different file types, including time-continuous data. By consuming an audio or video, however, the hyperlinking functionality of the Web is "left behind" since these files are typically unsearchable, thus not indexed by common text-based search engines. Our XML-based CMML annotation format and the Annodex file format presented in this paper are designed to solve this problem of "dark matter" on the Internet: Continuous media files are annotated and indexed (i.e., Annodexed), enabling hyperlinks to and from the media. Furthermore, the hyperlinks do not typically point to an entire media file, but to and from arbitrary fragments or intervals. The standards proposed in to create the Continuous Media Web have been submitted to the IETF for review.
753A28D6	Document similarity search is to find documents similar to a given query document and return a ranked list of similar documents to users, which is widely used in many text and web systems, such as digital library, search engine, etc. Traditional retrieval models, including the Okapi's BM25 model and the Smart's vector space model with length normalization, could handle this problem to some extent by taking the query document as a long query. In practice, the Cosine measure is considered as the best model for document similarity search because of its good ability to measure similarity between two documents. In this paper, the quantitative performances of the above models are compared using experiments. Because the Cosine measure is not able to reflect the structural similarity between documents, a new retrieval model based on TextTiling is proposed in the paper. The proposed model takes into account the subtopic structures of documents. It first splits the documents into text segments with TextTiling and calculates the similarities for different pairs of text segments in the documents. Lastly the overall similarity between the documents is returned by combining the similarities of different pairs of text segments with optimal matching method. Experiments are performed and results show: 1) the popular retrieval models (the Okapi's BM25 model and the Smart's vector space model with length normalization) do not perform well for document similarity search; 2) the proposed model based on TextTiling is effective and outperforms other models, including the Cosine measure; 3) the methods for the three components in the proposed model are validated to be appropriately employed.
7F561450	The market for Internet search is not only economically and socially important, it is also highly concentrated. Is this a problem? We study the question whether "competition is only a free click away". We argue that the market for Internet search is characterized by indirect network externalities and construct a simple model of search engine competition, which produces a market share development that fits the empirically observed development since 2003 well. We find that there is a strong tendency towards market tipping and, subsequently, monopolization, with negative consequences on economic welfare. Therefore, we propose to require search engines to share their data on previous searches. We compare the resulting "competitive oligopoly" market structure with the less competitive current situation and show that our proposal would spur innovation, search quality, consumer surplus, and total welfare. We also discuss the practical feasibility of our policy proposal and sketch the legal issues involved.
75F71181	One of the enabling technologies of the World Wide Web, along with browsers, domain name servers, and hypertext markup language, is the search engine. Although the Web contains over 100 million pages of information, those millions of pages are useless if you cannot find the pages you need. All major Web search engines operate the same way: a gathering program explores the hyperlinked documents of the Web, foraging for Web pages to index. These pages are stockpiled by storing them in some kind of database or repository. Finally, a retrieval program takes a user query and creates a list of links to Web documents matching the words, phrases, or concepts in the query. Although the retrieval program itself is correctly called a search engine, by popular usage the term now means a database combined with a retrieval program. For example, the Lycos search engine comprises the Lycos Catalog of the Internet and the Pursuit retrieval program. This paper describes the Lycos system for collecting, storing, and retrieving information about pages on the Web. After outlining the history and precursors of the Lycos system, the paper discusses some of the design choices made in building this Web indexer and touches briefly on the economic issues involved in working with very large retrieval systems.
7D600956	Web search engines are powerful tools used to satisfy specific information needs on the web. Their purpose is to maximize user satisfaction when performing this task. Although there are other sources of evidence, besides text, to characterize document relevance for a specific need, especially for HTML documents, current search engines do not allow users to explore these features when posing a query. Search engine queries are based almost exclusively on keywords. We believe that it is possible to improve user satisfaction if HTML tags and document metadata are available to users at query time. In this paper we present Xearch, a meta-search system that wraps public search engines in a framework that improves both the expressiveness of the language available for the user to specify information needs and the control over the answer format. Xearch converts HTML pages to a specific XML Schema, covering text and metadata derived from HTML. User queries are then submitted on this schema and can be specified through keywords but also explore documents’ HTML tags and metadata. Results from our experimental evaluation confirm that it is possible to improve the answer quality with this framework.
77C4F21A	In  this  work,  we  investigate  how  to  propagate  annotated labels for a given single image from the image-level to their corresponding semantic regions, namely Label-to-Region  (L2R),  by  utilizing  the  auxiliary  knowledge  from Internet image search with the annotated image labels as queries.  A nonparametric solution is proposed to perform L2R for single image with complete labels.  First, each la- bel of the image is used as query for online image search engines to obtain a set of semantically related and visually similar images, which along with the input image are encoded as Bags-of-Hierarchical-Patches.   Then,  an efficient  two-stage feature  mining procedure is  presented  to discover  those  input-image  specific,  salient  and  descriptive features for each label from the proposed Interpolation SIFT (iSIFT) feature pool. These features consequently constitute a patch-level representation, and the continuity biased sparse coding is proposed to select few patches from the online images with preference to larger patches to reconstruct a candidate region, which randomly merges the spatially connected patches of the input image.  Such candidate regions are further ranked according to the reconstruction errors, and the top regions are used to derive the label confidence vector for each patch of the input image. Finally, a patch clustering procedure is performed as postprocessing to finalize L2R for the input image.  Extensive experiments on three public databases demonstrate the encouraging performance of the proposed nonparametric L2R solution.
7E48A49D	People often want to know expected future events related to given real world entities. For supporting users in the process of future scenario analysis, we propose several methods that enable to retrieve and analyze future-related opinions from large text collections. In particular, we focus on time-unreferenced predictions, which do not contain any explicit future time reference and hence are more difficult to be retrieved. As a second contribution, we propose estimating validity of predictions by automatically searching for real world events corresponding to the predictions. This kind of analysis aims to help detect predictions that are no longer valid as well as help estimating prediction accuracy of information sources.
5F6157DD	The ability to rapidly locate useful on-line services (e.g. software applications, software components, process models, or service organizations), as opposed to simply useful documents, is becoming increasingly critical in many domains. Current service retrieval technology is, however, notoriously prone to low precision. This paper describes a novel service retrieval approached based on the sophisticated use of process ontologies. Our preliminary evaluations suggest that this approach offers qualitatively higher retrieval precision than existing (keyword and table-based) approaches without sacrificing recall and computational tractability/scalability.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
00B17298	he search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
5A9D2B23	We present an approach to increasing the effectiveness of ranked-output retrieval systems that relies on graphical display and user manipulation of “views” of retrieval results, where a view is the subset of retrieved documents that contain a specified subset of query terms. This approach has been implemented in a system named VIEWER (VIEwing WEb Results), acting as an interface to available search engines. An experimental evaluation of the performance of VIEWER in contrast to AltaVista is the major focus of the paper. We first report the results of an experiment on single, short query searches where VIEWER, used as an interactive ranking system, markedly outperformed AltaVista. We then concentrate on a more realistic searching scenario, involving free query formulation, unconstrained selection of retrieval results, and possibility of query reformulation. We report the results of an experiment where the use of VIEWER, compared to AltaVista, seemed to shift the user effort from inspection to evaluation of results, increasing retrieval effectiveness, and user satisfaction. In particular, we found that the VIEWER users retrieved half as many nonrelevant documents as the AltaVista users while retrieving a comparable number of relevant documents.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
80BF1EF4	The information explosion on the Internet makes it hard for users to obtain required information from the Web searched results in a more personalized way. For the same input word, most search engines return the same result to each user without taking into consideration user preference. For many users, it is no longer sufficient to get non-customized results. It is crucial to analyze users' search and browsing behaviors based on searching keywords entered by users, the clicking rate of each link in the result and the time they spend on each site. To this end, we have proposed a method to derive user searching profiles. We have also proposed a mechanism to derive document profiles, based on similarity score of documents. In this paper, we discuss how to use our model to combine the user searching profiles and the document profile, with a view to presenting customized search results to the users.
76244FC6	We conduct a broad survey of query-adaptive search strategies in a variety of application domains, where the internal retrieval mechanisms used for search are adapted in response to the anticipated needs for each individual query experienced by the system. While these query-adaptive approaches can range from meta-search over text collections to multimodal search over video databases, we propose that all such systems can be framed and discussed in the context of a single, unified framework. In our paper, we keep an eye towards the domain of video search, where search cues are available from a rich set of modalities, including textual speech transcripts, low-level visual features, and high-level semantic concept detectors. The relative efficacy of each of the modalities is highly variant between many types of queries. We observe that the state of the art in query-adaptive retrieval frameworks for video collections is highly dependent upon the definition of classes of queries, which are groups of queries that share similar optimal search strategies, while many applications in text and Web retrieval have included many advanced strategies, such as direct prediction of search method performance and inclusion of contextual cues from the searcher. We conclude that such advanced strategies previously developed for text retrieval have a broad range of possible applications in future research in multimodal video search.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
803D2737	In the current information age, the dominant method for information search is by providing few keywords to a search engine. Keyword search is currently one of the most important operations in search engines and numerous other applications. In this paper we propose a new text indexing technique for improving the performance of keyword search. Our proposed technique not only speeds up searching operations but also the operations for inserting and for deleting keywords, which are particularly important for the ever increasing and dynamic changing databases such as that for search engines. We propose to partition all keywords into search trees based on the first character and the length of the keywords. Our partitioning scheme creates a much more even distribution of keywords and results in a 32% speedup in the worst cases and a 1% speedup in the average cases in comparing to one of the leading text indexing techniques called burst tries. In addition, our proposed technique stores document indexes only at the leaf nodes of the search trees and results in efficient algorithms for searching, insertion, and deletion of keywords. We successfully integrated the technique into our Information Classification and Search Engine system and showed its potential and feasibility.
6E411207	This study examines existent and new methods for evaluating the success of information retrieval systems. The theory underlying current methods is not robust enough to allow testing retrieval using different meta-tagging schemas. Traditional measures rely on judgments of whether a document is relevant to a particular question. A good system returns all the relevant documents and no extraneous documents. There is a rich literature questioning the efficacy of relevance judgments. Such questions as, Relevant to whom? When? and To what purpose? are not well-answered in traditional theory. In this study, two new measures (Spink's Information Need and Cooper's Utility) are used in evaluating two search tools (tag-based and text-based), comparing these new measures with traditional measures and each other. The open-source Swish text-based search engine and a self-constructed tag-based search tool were used. Thirty-four educators searched for information using both search engines and evaluated the information retrieved by each. Construct measures, derived by multiplying each of the three measures (traditional, information need, and utility) by a rating of satisfaction were compared using two way analysis of variance. This study specifically analyzes small information systems. The design concepts would be untenable for large systems. Results indicated that there was a significant correlation between the three measures, indicating that the new measures provide an equivalent method of evaluating systems and have some significant advantages, which include not requiring relevance judgments and the ability to use the measures in situ
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7EE419B7	Information Retrieval (IR) approaches for semantic web search engines have become very populars in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
78A4DC3B	The phenomenon of sponsored search advertising where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7DEA9D12	An information retrieval (IR) process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In IR a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.An object is an entity which keeps or stores information in a database. User queries are matched to objects stored in the database. Depending on the application the data objects may be, for example, text documents, images or videos. The documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates.Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.In this paper we try to explain IR methods and asses them from two view points and finally propose a simple method for ranking terms and documents on IR and implement the method and check the result.
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
72030340	Users often express confusion and frustration in trying to locate the full-text availability of a journal and having to check multiple resources and interfaces on a library's Web site. The challenge was to seek an interim practical solution which would address this need effectively. The Social Science Journals Database and Search Engine (Soc-dbase) project demonstrates a low-cost one-stop search solution that can be easily and quickly adopted and implemented. The project involves creating a single Web interface to search a database of selected social science and sociology journal titles that include full-text online availability information. This paper presents the design and creation of the social sciences journals database that can be searched to find a journal's full-text availability at the Rutgers University Libraries or on the Web.
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the document side(type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
01D30712	Personalized search engines must be able to cope with various user preferences, retrieving the best matches to a query. For SQL and XML applications new methods for such preference-based searches have been implemented recently. Here we adopt this approach to keyword search in full-text search engines. We propose to augment the vector space model (VSM) by preference constructors having an intuitive partial order semantics: Pareto-accumulation and prioritization. We show that prioritization can be interpreted as subspace preference in the VSM. Using a preprocessor approach we succeed to map prioritization onto the VSM. A first query benchmark, using the standard Time-collection, revealed promising results. The retrieval quality, measured by average expected search length, could be slightly improved. Using the proposed meta engine approach, this gain in retrieval quality is accompanied by a substantial speed up of query runtimes. Thus subspace preferences can be integrated efficiently into existing full-text search engines. 
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.
7A6F5F82	Finding the right information in the World Wide Web is becoming a fundamental problem, since the amount of global information that the WWW contains is growing at an incredible rate. In this paper, we present a novel method to extract from a web object its “hyper” informative content, in contrast with current search engines, which only deal with the “textual” informative content. This method is not only valuable per se, but it is shown to be able to considerably increase the precision of current search engines, Moreover, it integrates smoothly with existing search engines technology since it can be implemented on top of every search engine, acting as a post-processor, thus automatically transforming a search engine into its corresponding “hyper” version. We also show how, interestingly, the hyper information can be usefully employed to face the search engines persuasion problem.
7DCF9C8B	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.
808285A5	Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called "information need". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
7E418727	Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7D2009E3	Search advertising is typical heterogeneous multi-dimensional data sets whose click-through rate depends on query words, terms and other factors. Traditional data mining methods, limited to homogenous data source, represent search ads as the vector space model, so they fail to sufficiently consider the search advertisements’ characteristics of heterogeneous data. This paper presents consistent bipartite graph model to describe ads, adopting spectral co-clustering method in data mining. In order to solve the balance partition of the map in clustering, heuristic algorithm is introduced into consistent bipartite graph's co-partition; a more effective subgraph redistribution algorithm is established. Experiments on real ads dataset shows that our approach worked effectively and efficiently in both clustering and prediction.
7A81F96C	Today’s knowledge workers rely increasingly on information to get their job done, and the availability of search engines to locate relevant information is thus essential. Understanding how users interact with search engines is a prerequisite for the successful design of useful systems and a body of knowledge has in recent years begun to compile. However, all previous studies have focused on the public web, not acknowledging the fact that much business-related information seeking occur on corporate internal networks. In this exploratory study, we have collected and analysed intranet search engine log files from three different years – 2000, 2002, and 2004 – enabling us to detect shifting trends in intranet search behaviour. Comparing our data to what has been reported from the public web we conclude that intranet searchers are both similar to and different from searchers on the public web. In sum, it appears that intranet users are more extreme in their behaviour and that qualitative studies are needed to understand the motives and rationales governing their actions.
79A2A305	The Internet is a widely used source of information for patients searching for medical/health care information. While many studies have assessed existing medical/health care information on the Internet, relatively few have examined methods for design and delivery of such websites, particularly those aimed at the general public.This study describes a method of evaluating material for new medical/health care websites, or for assessing those already in existence, which is correlated with higher rankings on Google's Search Engine Results Pages (SERPs).A website quality assessment (WQA) tool was developed using criteria related to the quality of the information to be contained in the website in addition to an assessment of the readability of the text. This was retrospectively applied to assess existing websites that provide information about generic medicines. The reproducibility of the WQA tool and its predictive validity were assessed in this studyThe WQA tool demonstrated very high reproducibility (intraclass correlation coefficient=0.95) between 2 independent users. A moderate to strong correlation was found between WQA scores and rankings on Google SERPs. Analogous correlations were seen between rankings and readability of websites as determined by Flesch Reading Ease and Flesch-Kincaid Grade Level scores.The use of the WQA tool developed in this study is recommended as part of the design phase of a medical or health care information provision website, along with assessment of readability of the material to be used. This may ensure that the website performs better on Google searches. The tool can also be used retrospectively to make improvements to existing websites, thus, potentially enabling better Google search result positions without incurring the costs associated with Search Engine Optimization (SEO) professionals or paid promotion.
048F1814	Recent advances in click modeling have established it as an attractive approach to interpret search click data. These advances characterize users’ search behavior either in advertisement blocks, or within an organic search block through probabilistic models. Yet, when searching for information on a search result page, one is often interacting with the search engine via an entire page instead of a single block. Consequently, previous works that exclusively modeled user behavior in a single block may sacrifice much useful user behavior information embedded in other blocks.To solve this problem, in this paper, we put forward a novel Whole Page Click (WPC) Model to characterize user behavior in multiple blocks. Specifically, WPC uses a Markov chain to learn the user transition probabilities among different blocks in the whole page. To compare our model with the best alternatives in the Web-Search literature, we run a large-scale experiment on a real dataset and demonstrate the advantage of the WPC model in terms of both the whole page and each block in the page. Especially, we find that WPC can achieve significant gain in interpreting the advertisement data, despite of the sparsity of the advertisement click data.
7AB88A91	Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14\% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing algorithms even on HDD-based architecture.
77EB1685	Current automatic wrappers using DOM tree and visual properties of data records to extract the required information from the search engine results pages generally have limitations such as the inability to check the similarity of tree structures accurately. Our study on the properties of data records shows that these data records located in search engine results pages are not only having similar visual properties and tree structures, but they are also related semantically in their contents. In this context, we propose an ontological technique using existing lexical database for English (WordNet) for the extraction of data records. We find that wrappers designed based on ontological technique are able to reduce the number of potential data regions to be extracted, thus they are able to improve the data extraction accuracy. We then use visual cue from the browser rendering engine to locate and extract the relevant data region from the web page by measuring the size of text and image of data records. Experimental results indicate that our technique is robust and performs better than the existing state of the art visual based wrappers.
7F9F2D76	This study measures the frequency with which search engines update their indices. Therefore, 38 websites that are updated on a daily basis were analysed within a time-span of six weeks. The analysed search engines were Google, Yahoo and MSN. We find that Google performs best overall with the most pages updated on a daily basis, but only MSN is able to update all pages within a time-span of less than 20 days. Both other engines have outliers that are older. In terms of indexing patterns, we find different approaches at the different engines. While MSN shows clear update patterns, Google shows some outliers and the update process of the Yahoo index seems to be quite chaotic. Implications are that the quality of different search engine indices varies and more than one engine should be used when searching for current content.
7BC0845F	We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We propose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strategies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Yahoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
809F7B04	To confront with an ever increasing number of published scientific articles, an effective, efficient, and easy-to-use tool is required to support biomedical scientists, while entering a new scientific field and encountering clinical decision, to organize a vast amount of PubMed abstracts into the panorama of specific topics according to their relevance. In brief, the set of associations among frequently co-occurring terms in given a set of PubMed documents forms naturally a simplicial complex. Afterwards each connected component of this simplicial complex represents a concept in the collection. Based on these concepts, documents can be clustered into meaningful classes. This paper presents an alternative search engine that applies a combinatorial topological method to automatically extract semantic clusters from the PubMed database of biomedical literature. We use several qualitative parameters to perform the user study that shows users are able to reduce search time.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
7B829984	In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.
7ECEAEEC	With web image search engines, we face a situation where the results are very noisy, and when we ask for a specific object, we are not ensured that this object is contained in all the images returned by the search engines: about 50% of the images returned are off-topic. In this paper, we explain how knowing the color of an object can help locating the object in images, and we also propose methods to automatically find the color of an object, so that the whole process can be fully automatic. Results reveal that this method allows us to reduce the noise in returned images while providing automatic segmentation so that it can be used for clustering or object learning.
5CFC04D2	Commercial web search engines adopt parallel and replicated architecture in order to support high query throughput. In this paper, we investigate the effect of caching on the throughput in such a setting. A simple scheme, called uniform caching, would replicate the cache content to all servers. Unfortunately, it does not exploit the variations among queries, thus wasting memory space on caching the same cache content redundantly on multiple servers. To tackle this limitation, we propose a diversified caching problem, which aims to diversify the types of queries served by different servers, and maximize the sharing of terms among queries assigned to the same server. We show that it is NP-hard to find the optimal diversified caching scheme, and identify intuitive properties to seek good solutions. Then we present a framework with a suite of techniques and heuristics for diversified caching. Finally, we evaluate the proposed solution with competitors by using a real dataset and a real query log.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
7F6257E8	Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
78A4DC3B	The phenomenon of sponsored search advertising where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7BB23ED5	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time
7F427012	We examine sponsored search auctions run by Overture (now part of Yahoo!) and Google and present evidence of strategic bidder behavior in these auctions. Between June 15, 2002, and June 14, 2003, we estimate that Overture's revenue might have been higher if it had been able to prevent this strategic behavior. We present an alternative mechanism that could reduce the amount of strategizing by bidders, raise search engines' revenues, and increase the efficiency of the market. We conclude by showing that strategic behavior has not disappeared over time; it remains present on both search engines.
80359F5F	Among the challenges of searching the vast information source the Web has become, improving Web search efficiency by different strategies using semantics and the user generated data from Web 2.0 applications remains a promising and interesting approach. In this paper, we present the Personal Social Dataset and Ontology-guided Input strategies and couple them together, providing a proofof- concept implementation.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7698D6DC	A constrained L1 minimization method is proposed for estimating a sparse inverse covariance matrix based on a sample of n iid p-variate random variables. The resulting estimator is shown to enjoy a number of desirable properties. In particular, it is shown that the rate of convergence between the estimator and the true s-sparse precision matrix under the spectral norm is slogp/n−−−−−−√ when the population distribution has either exponential-type tails or polynomial-type tails. Convergence rates under the elementwise L∞ norm and Frobenius norm are also presented. In addition, graphical model selection is considered. The procedure is easily implementable by linear programming. Numerical performance of the estimator is investigated using both simulated and real data. In particular, the procedure is applied to analyze a breast cancer dataset. The procedure performs favorably in comparison to existing methods.
797F1FB0	We define and investigate classes of statistical models for the analysis of associations between variables, some of which are qualitative and some quantitative. In the cases where only one kind of variables is present, the models are well-known models for either contingency tables or covariance structures. We characterize the subclass of decomposable models where the statistical theory is especially simple. All models can be represented by a graph with one vertex for each variable. The vertices are possibly connected with arrows or lines corresponding to directional or symmetric associations being present. Pairs of vertices that are not connected are conditionally independent given some of the remaining variables according to specific rules.
0BD946B6	Building accurate models from a small amount of available training data can sometimes prove to be a great challenge. Expert domain knowledge can often be used to alleviate this burden. Parameter Sharing is one such important form of domain knowledge. Graphical models like HMMs, DBNs and Module Networks use different forms of Parameter Sharing to reduce the variance in the parameter estimates. The goal of this paper is to present a theoretical approach for learning in presence of several other types of Parameter Related Domain Knowledge that go beyond the ones in the above models. First, we introduce a General Parameter Sharing Framework that describes the models just mentioned, but allows for much finer grained parameter sharing assumptions. In this framework, we present sound procedures for parameter learning from both a Frequentist and a Bayesian point of view, from both complete and incomplete data, in the case where a domain expert specifies in advance the structure of the graphical model, and the subsets of parameters to be shared. Second, we describe a hierarchical extension of this framework based on Parameter Sharing Trees. Finally we present algorithms for using domain knowledge that specifies that certain groups of parameters share certain properties. In particular, we consider two kinds of constraints: first kind states certain groups of parameters share the same aggregate probability mass and second kind states the ratio of the parameters is preserved (shared) in several groups. As an example, we derive a novel form of parameter sharing for Bayesian Multinetworks.
59E5C607	In this paper I explore the psychology of ritual performance and present a simple graphical model that clarifies several issues in William Irons’s theory of religion as a “hard-to-fake” sign of commitment. Irons posits that religious behaviors or rituals serve as costly signals of an individual’s commitment to a religious group. Increased commitment among members of a religious group may facilitate intra-group cooperation, which is argued to be the primary adaptive benefit of religion. Here I propose a proximate explanation for how individuals are able to pay the short-term costs of ritual performance to achieve the long-term fitness benefits offered by religious groups. The model addresses three significant problems raised by Irons’s theory. First, the model explains why potential free-riders do not join religious groups even when there are significant net benefits that members of religious groups can achieve. Second, the model clarifies how costly a ritual must be to achieve stability and prevent potential free-riders from joining the religious group. Third, the model suggests why religious groups may require adherents to perform private rituals that are not observed by others. Several hypotheses generated from the model are also discussed.
7B13384D	This paper reviews some of the key statistical ideas that are encountered when trying to find empirical support to causal interpretations and conclusions, by applying statistical methods on experimental or observational longitudinal data. In such data, typically a collection of individuals are followed over time, then each one has registered a sequence of covariate measurements along with values of control variables that in the analysis are to be interpreted as causes, and finally the individual outcomes or responses are reported. Particular attention is given to the potentially important problem of confounding. We provide conditions under which, at least in principle, unconfounded estimation of the causal effects can be accomplished. Our approach for dealing with causal problems is entirely probabilistic, and we apply Bayesian ideas and techniques to deal with the corresponding statistical inference. In particular, we use the general framework of marked point processes for setting up the probability models, and consider posterior predictive distributions as providing the natural summary measures for assessing the causal effects. We also draw connections to relevant recent work in this area, notably to Judea Pearl's formulations based on graphical models and his calculus of so-called do-probabilities. Two examples illustrating different aspects of causal reasoning are discussed in detail.
7BCF362F	Multivariate Gaussian graphical models are defined in terms of Markov properties, i.e., conditional independences, corresponding to missing edges in the graph. Thus model selection can be accomplished by testing these independences, which are equivalent to zero values of corresponding partial correlation coefficients. For concentration graphs, acyclic directed graphs, and chain graphs (both LWF and AMP classes), we apply Fisher's z-transform, Šidák's correlation inequality, and Holm's step-down procedure to simultaneously test the multiple hypotheses specified by these zero values. This simple method for model selection controls the overall error rate for incorrect edge inclusion. Prior information about the presence and/or absence of particular edges can be readily incorporated.
772CE3A1	Most interactive graphical applications that use direct manipulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high design, development and maintenance costs and encourages the development of stereotyped  applications based  on buttons,menus and dialogue boxes instead of direct manipulation of the applications objects. In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction . We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi-Layer Model and we show how it can take advantage of software and hardware graphic extensions to provide good performance.  We also show how it supports multiple input devices and simplifies the description of a wide variety of interaction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional animation system.
096AEEA7	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion. 
7F3655D3	We show that the class of strongly connected graphical models with tree-width at most <i>k</i> can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Hoffgen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for <i>k</i> > 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time.
7BDE06A5	Advances in acoustic sensing have enabled the simultaneous acquisition of multiple measurements of the same physical event via co-located acoustic sensors. We exploit the inherent correlation among such multiple measurements for acoustic signal classification, to identify the launch/impact of munition (i.e., rockets, mortars). Specifically, we propose a probabilistic graphical model framework that can explicitly learn the class conditional correlations between the cepstral features extracted from these different measurements. Additionally, we employ symbolic dynamic filtering-based features, which offer improvements over the traditional cepstral features in terms of robustness to signal distortions. Experiments on real acoustic data sets show that our proposed algorithm outperforms conventional classifiers as well as the recently proposed joint sparsity models for multisensor acoustic classification. Additionally our proposed algorithm is less sensitive to insufficiency in training samples compared to competing approaches.
7F9A8948	Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
80379CA8	We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.
008901FD	Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks. This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models. This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.
76E3283E	We consider the problem of estimating a sparse precision matrix of a multivariate Gaussian distribution, where the dimension may be large. Gaussian graphical models provide an important tool in describing conditional independence through presence or absence of edges in the underlying graph. A popular non-Bayesian method of estimating a graphical structure is given by the graphical lasso. In this paper, we consider a Bayesian approach to the problem. We use priors which put a mixture of a point mass at zero and certain absolutely continuous distribution on off-diagonal elements of the precision matrix. Hence the resulting posterior distribution can be used for graphical structure learning. The posterior convergence rate of the precision matrix is obtained and is shown to match the oracle rate. The posterior distribution on the model space is extremely cumbersome to compute using the commonly used reversible jump Markov chain Monte Carlo methods. However, the posterior mode in each graph can be easily identified as the graphical lasso restricted to each model. We propose a fast computational method for approximating the posterior probabilities of various graphs using the Laplace approximation approach by expanding the posterior density around the posterior mode. We also provide estimates of the accuracy in the approximation.
5A0EE170	Multiscale (multiresolution) graphical models have gained widespread popularity in recent years, since they enjoy rich modeling power as well as efficient inference procedures. Existing approaches to learning multiscale graphical models often leverage the framework of penalized likelihood, and therefore suffer from the issue of regularization selection. In this paper, we propose a novel method to learn multiscale graphical models from the Bayesian perspective. More specifically, the regularization parameters are treated as random variables that follow Gamma distributions. We then derive an efficient variational Bayes algorithm to learn the model, and further demonstrate the advantages of the proposed method through numerical experiments.
761D5749	We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.
7E144371	In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.
7C7FFC7E	We study the problem of projecting a distribution  onto  (or  finding  a  maximum  likelihood distribution  among)  Markov networks  of  bounded tree-width. By casting it as the combinatorial optimization problem of finding a maximum weight hypertree, we prove that it is NP-hard to solve exactly and provide an approximation algorithm with a provable performance guarantee.
79AF77CD	We consider acyclic directed mixed graphs, in which directed edges (x→y) and bi-directed edges (x↔y) may occur. A simple extension of Pearl's d-separation criterion, called m-separation, is applied to these graphs. We introduce a local Markov property which is equivalent to the global property resulting from the m-separation criterion for arbitrary distributions.
7A730712	Markov networks are a common class of graphical models used in machine learning. Such models use an undirected graph to capture dependency information among random variables in a joint probability distribution. Once one has chosen to use a Markov network model, one aims to choose the model that best explains the data that has been observed this model can then be used to make predictions about future data. We show that the problem of learning a maximum likelihood Markov network given certain observed data can be reduced to the problem of identifying a maximum weight low-treewidth graph under a given input weight function. We give the first constant factor approximation algorithm for this problem. More precisely, for any fixed treewidth objective k, we find a treewidth-k graph with an f(k) fraction of the maximum possible weight of any treewidth-k graph.
7E25BD71	Sparse graphical models have proven to be a flexible class of multivariate probability models for approximating high-dimensional distributions. In this paper, we propose techniques to exploit this modeling ability for binary classification by discriminatively learning such models from labeled training data, i.e., using both positive and negative samples to optimize for the structures of the two models. We motivate why it is difficult to adapt existing generative methods, and propose an alternative method consisting of two parts. First, we develop a novel method to learn tree-structured graphical models which optimizes an approximation of the log-likelihood ratio. We also formulate a joint objective to learn a nested sequence of optimal forests-structured models. Second, we construct a classifier by using ideas from boosting to learn a set of discriminative trees. The final classifier can interpreted as a likelihood ratio test between two models with a larger set of pairwise features. We use cross-validation to determine the optimal number of edges in the final model. The algorithm presented in this paper also provides a method to identify a subset of the edges that are most salient for discrimination. Experiments show that the proposed procedure outperforms generative methods such as Tree Augmented Naive Bayes and Chow-Liu as well as their boosted counterparts.
805ED9C0	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on $\ell_1$-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an $\ell_1$-constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes $p$ and maximum neighborhood sizes $d$ are allowed to grow as a function of the number of observations $n$. Our main results provide sufficient conditions on the triple $(n, p, d)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as $\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions are imposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$ samples are sufficient.
7F6DA698	Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula---or "nonparanormal"---for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples. 
7C5AAA1F	Therapy processes are complex dynamical systems where several variables are constantly interacting with each other. In general, the underlying mechanisms are difficult to assess. Our approach is to identify the dependency structure of relevant variables within the therapy process using interaction graphs. These are instruments for multivariate time series which are based on the analysis of partial spectral coherences. We used interaction graphs in order to investigate the therapy process of a multimodal therapy concept for fibromyalgia patients. Our main hypothesis was that self-efficacy plays a central role in the therapy process. Methods: Patients kept an electronic diary for 13 weeks. Pain intensity, depression, sleep quality, anxiety and self-efficacy were assessed via visual analogue scales. The resulting multivariate time series were aggregated over individuals, and partial spectral coherences between each pair of the variables were calculated. From the partial coherences, interaction graphs were plotted. Results: Within the resulting graphical model, self-efficacy was strongly related to pain intensity, depression and sleep quality. All other relations were substantially weaker. There was no direct relationship between pain intensity and sleep quality. Conclusions: The relations between two variables within the therapy process are mainly induced by self-efficacy. Interaction graphs can be used to pool time series data of several patients and thus to assess the common underlying dependency structure of a group of patients. The graphical representation is easily comprehensible and allows to distinguish between direct and indirect relationships.
80D10329	Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.
7963E765	The forensic investigation of the origin and cause of a fire incident is a particularly demanding area of expertise. As the available evidence is often incomplete or vague, uncertainty is a key element. The present study is an attempt to approach this through the use of Bayesian networks, which have been found useful in assisting human reasoning in a variety of disciplines in which uncertainty plays a central role. The present paper describes the construction of a Bayesian network (BN) and its use for drawing inferences about propositions of interest, based upon a single, possibly non replicable item of evidence: detected residual quantities of a flammable liquid in fire debris.
77757110	Motivated by the analysis of genetical genomic data, we consider the problem of estimating high-dimensional sparse precision matrix adjusting for possibly a large number of covariates, where the covariates can affect the mean value of the random vector. We develop a two-stage estimation procedure to first identify the relevant covariates that affect the means by a joint ℓ1 penalization. The estimated regression coefficients are then used to estimate the mean values in a multivariate sub-Gaussian model in order to estimate the sparse precision matrix through a ℓ1-penalized log-determinant Bregman divergence. Under the multivariate normal assumption, the precision matrix has the interpretation of a conditional Gaussian graphical model. We show that under some regularity conditions, the estimates of the regression coefficients are consistent in element-wise ℓ∞ norm, Frobenius norm and also spectral norm even when p≫n and q≫n. We also show that with probability converging to one, the estimate of the precision matrix correctly specifies the zero pattern of the true precision matrix. We illustrate our theoretical results via simulations and demonstrate that the method can lead to improved estimate of the precision matrix. We apply the method to an analysis of a yeast genetical genomic data.
801D527C	Using domain/expert knowledge when learning Bayesian networks from data has been considered a promising idea since the very beginning of the field. However, in most of the previously proposed approaches, human experts do not play an active role in the learning process. Once their knowledge is elicited, they do not participate any more. The interactive approach for integrating domain/expert knowledge we propose in this work aims to be more efficient and effective. In contrast to previous approaches, our method performs an active interaction with the expert in order to guide the search based learning process. This method relies on identifying the edges of the graph structure which are more unreliable considering the information present in the learning data. Another contribution of our approach is the integration of domain/expert knowledge at different stages of the learning process of a Bayesian network: while learning the skeleton and when directing the edges of the directed acyclic graph structure.
7ED9E3E7	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on ℓ1-regularized logistic regression,in which the neighborhood of any given node is estimated by performing logistic regression subject to an ℓ1-constraint. Our framework applies to the high-dimensional setting,in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n= Ω(d3logp), with the error decaying as O(exp(−Cn/d3)) for some constant C. If these same assumptions are imposed directly on the sample matrices, we show that n= Ω(d2logp) samples are sufficient.
756B4845	Automatically learning the graph structure of a single Bayesian network (BN) which accurately represents the underlying multivariate probability distribution of a collection of random variables is a challenging task. But obtaining a Bayesian solution to this problem based on computing the posterior probability of the presence of any edge or any directed path between two variables or any other structural feature is a much more involved problem, since it requires averaging over all the possible graph structures. For the former problem, recent advances have shown that search + score approaches find much more accurate structures if the search is constrained by a previously inferred skeleton (i.e. a relaxed structure with undirected edges which can be inferred using local search based methods). Based on similar ideas, we propose two novel skeleton-based approaches to approximate a Bayesian solution to the BN learning problem: a new stochastic search which tries to find directed acyclic graph (DAG) structures with a non-negligible score; and a new Markov chain Monte Carlo method over the DAG space. These two approaches are based on the same idea. In a first step, both employ a previously given skeleton and build a Bayesian solution constrained by this skeleton. In a second step, using the preliminary solution, they try to obtain a new Bayesian approximation but this time in an unconstrained graph space, which is the final outcome of the methods. As shown in the experimental evaluation, this new approach strongly boosts the performance of these two standard techniques proving that the idea of employing a skeleton to constrain the model space is also a successful strategy for performing Bayesian structure learning of BNs.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
80A025FA	Many statistical methods gain robustness and flexibility by sacrificing convenient computational structures. In this article, we illustrate this fundamental tradeoff by studying a semiparametric graph estimation problem in high dimensions. We explain how novel computational techniques help to solve this type of problem. In particular, we propose a nonparanormal neighborhood pursuit algorithm to estimate high-dimensional semiparametric graphical models with theoretical guarantees. Moreover, we provide an alternative view to analyze the tradeoff between computational efficiency and statistical error under a smoothing optimization framework. Though this article focuses on the problem of graph estimation, the proposed methodology is widely applicable to other problems with similar structures. We also report thorough experimental results on text, stock, and genomic datasets.	
79B9CC7E	This paper deals with chain graphs under the Andersson–Madigan–Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score + search learning algorithms under assumptions weaker than faithfulness.We also study the problem of how to represent the result of marginalizing out some nodes in an AMP CG. We introduce a new family of graphical models that solves this problem partially. We name this new family maximal covariance–concentration graphs because it includes both covariance and concentration graphs as subfamilies.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7E271971	We propose a Bayesian Network (BN) model to integrate multiple contextual information and the image measurements for image segmentation. The BN model systematically encodes the contextual relationships between regions, edges and vertices, as well as their image measurements with uncertainties. It allows a principled probabilistic inference to be performed so that image segmentation can be achieved through a most probable explanation (MPE) inference in the BN model. We have achieved encouraging results on the horse images from the Weizmann dataset. We have also demonstrated the possible ways to extend the BN model so as to incorporate other contextual information such as the global object shape and human intervention for improving image segmentation. Human intervention is encoded as new evidence in the BN model. Its impact is propagated through belief propagation to update the states of the whole model. From the updated BN model, new image segmentation is produced.
766DF9F3	G protein coupled receptors (GPCRs) are seven helical transmembrane proteins that function as signal transducers. They bind ligands in their extracellular and transmembrane regions and activate cognate G proteins at their intracellular surface at the other side of the membrane. The relay of allosteric communication between the ligand binding site and the distant G protein binding site is poorly understood. In this study, GREMLIN 1, a recently developed method that identifies networks of co-evolving residues from multiple sequence alignments, was used to identify those that may be involved in communicating the activation signal across the membrane. The GREMLIN-predicted long-range interactions between amino acids were analyzed with respect to the seven GPCR structures that have been crystallized at the time this study was undertaken. GREMLIN significantly enriches the edges containing residues that are part of the ligand binding pocket, when compared to a control distribution of edges drawn from a random graph. An analysis of these edges reveals a minimal GPCR binding pocket containing four residues (T1183.33, M2075.42, Y2686.51 and A2927.39). Additionally, of the ten residues predicted to have the most long-range interactions (A1173.32, A2726.55, E1133.28, H2115.46, S186EC2, A2927.39, E1223.37, G902.57, G1143.29 and M2075.42), nine are part of the ligand binding pocket. We demonstrate the use of GREMLIN to reveal a network of statistically correlated and functionally important residues in class A GPCRs. GREMLIN identified that ligand binding pocket residues are extensively correlated with distal residues. An analysis of the GREMLIN edges across multiple structures suggests that there may be a minimal binding pocket common to the seven known GPCRs. Further, the activation of rhodopsin involves these long-range interactions between extracellular and intracellular domain residues mediated by the retinal domain. A minimal ligand binding pocket within a network of correlated mutations identified by multiple sequence and structural analysis of G protein coupled receptors.
80DC1B7C	Pearl's d-separation concept and the ensuing Markov property is applied to graphs which may have, between each two different vertices i and j, any subset of {i←j, i→j, i↔j} as edges. The class of graphs so obtained is closed under marginalization. Furthermore, the approach permits a direct proof of this theorem: “The distribution of a multivariate normal random vector satisfying a system of linear simultaneous equations is Markov w.r.t. the path diagram of the linear system.
7E66E464	Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using l1-penalization methods. We propose and study the following method. We combine a multiple regression approach with ideas of thresholding and refitting: first we infer a sparse undirected graphical model structure via thresholding of each among many l1-norm penalized regression functions; we then estimate the covariance matrix and its inverse using the maximum likelihood estimator. We show that under suitable conditions, this approach yields consistent estimation in terms of graphical structure and fast convergence rates with respect to the operator and Frobenius norm for the covariance matrix and its inverse. We also derive an explicit bound for the Kullback Leibler divergence. 
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
5E453122	In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al.
7F4AA6EA	Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.
7A8EB39B	Recommender system provides users with personalized suggestions of product or information. Typically, recommender systems rely on a bipartite graph model to capture user interest. As an extension, some boosted methods analyze content information to further improve the quality of personalized recommendation. However, due to the prevalence of short and sparse messages in online social media, traditional content-boosted methods do not guarantee to capture user preference accurately especially for web contents. In this paper, we propose a novel graphical model to extract hidden topics from web contents, cluster web contents, and detect users' interests on each cluster. In addition, we introduce two reranking models which utilize the detected user interest to further boost the quality of personalized recommendation. Experiment results on a public dataset demonstrated the limitation of a traditional content-boosted approach, and also showed the validity of our proposed techniques.
7CE629CE	Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
7FF9B70B	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.
7CF1D0CA	The use of Bayesian networks for classification problems has received a significant amount of recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal of classification (label prediction accuracy). Recent approaches to optimizing classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present boosted Bayesian network classifiers, a framework to combine discriminative data-weighting with generative training of intermediate models. We show that boosted Bayesian network classifiers encompass the basic generative models in isolation, but improve their classification performance when the model structure is suboptimal. We also demonstrate that structure learning is beneficial in the construction of boosted Bayesian network classifiers. On a large suite of benchmark data-sets, this approach outperforms generative graphical models such as naive Bayes and TAN in classification accuracy. Boosted Bayesian network classifiers have comparable or better performance in comparison to other discriminatively trained graphical models including ELR and BNC. Furthermore, boosted Bayesian networks require significantly less training time than the ELR and BNC algorithms.
7CA63E85	Undernutrition is one of the most important health problems in developing countries. Examining its determinants implies the investigation of a complex association structure including a large number of potential influence variables and different types of influences. A recently developed statistical technique to cope with such situations are graphical chain models. In this paper, this approach is used to investigate the determinants of undernutrition in Benin (West Africa). Since this method also reveals indirect influences, interesting insight is gained into the association structure of all variables incorporated. The analysis identifies mother's education, socioeconomic status, and religion as three variables with particularly strong direct and indirect linkages to undernutrition.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
7B368C93	Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in more highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties. 
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
814F9EFD	We introduce a new approach to learning statistical models from multiple sequence alignments (MSA) of proteins. Our method, called GREMLIN (Generative REgularized ModeLs of proteINs), learns an undirected probabilistic graphical model of the amino acid composition within the MSA. The resulting model encodes both the position-specific conservation statistics and the correlated mutation statistics between sequential and long-range pairs of residues. Existing techniques for learning graphical models from MSA either make strong, and often inappropriate assumptions about the conditional independencies within the MSA (e.g., Hidden Markov Models), or else use suboptimal algorithms to learn the parameters of the model. In contrast, GREMLIN makes no a priori assumptions about the conditional independencies within the MSA. We formulate and solve a convex optimization problem, thus guaranteeing that we find a globally optimal model at convergence. The resulting model is also generative, allowing for the design of new protein sequences that have the same statistical properties as those in the MSA. We perform a detailed analysis of covariation statistics on the extensively studied WW and PDZ domains and show that our method out-performs an existing algorithm for learning undirected probabilistic graphical models from MSA. We then apply our approach to 71 additional families from the PFAM database and demonstrate that the resulting models significantly out-perform Hidden Markov Models in terms of predictive accuracy. 
80F77A3D	A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.
7F9FC5C6	Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse.
0A7B0D45	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning, in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is founded on an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a typical Smart Environment – the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
75C9738A	The binding of regulatory proteins to their specific DNA targets determines the accurate expression of the neighboring genes. The in silico prediction of new binding sites in completely sequenced genomes is a key aspect in the deeper understanding of gene regulatory networks. Several algorithms have been described to discriminate against false-positives in the prediction of new binding targets; however none of them has been implemented so far to assist the detection of binding sites at the genomic scale.FITBAR (Fast Investigation Tool for Bacterial and Archaeal Regulons) is a web service designed to identify new protein binding sites on fully sequenced prokaryotic genomes. This tool consists in a workbench where the significance of the predictions can be compared using different statistical methods, a feature not found in existing resources. The Local Markov Model and the Compound Importance Sampling algorithms have been implemented to compute the P-value of newly discovered binding sites. In addition, FITBAR provides two optimized genomic scanning algorithms using either log-odds or entropy-weighted position-specific scoring matrices. Other significant features include the production of a detailed genomic context map for each detected binding site and the export of the search results in spreadsheet and portable document formats. FITBAR discovery of a high affinity Escherichia coli NagC binding site was validated experimentally in vitro as well as in vivo and publishedFITBAR was developed in order to allow fast, accurate and statistically robust predictions of prokaryotic regulons. This feature constitutes the main advantage of this web tool over other matrix search programs and does not impair its performance.
7E8549A3	Markov models are often employed to represent stochastic processes, that is, random processes that evolve over time. In a healthcare context, Markov models are particularly suited to modelling chronic disease. In this article, we describe the use of Markov models for economic evaluation of healthcare interventions. The intuitive way in which Markov models can handle both costs and outcomes make them a powerful tool for economic evaluation modelling. The time component of Markov models can offer advantages of standard decision tree models, particularly with respect to discounting. This paper gives a comprehensive description of Markov modelling for economic evaluation, including a discussion of the assumptions on which the type of model is based, most notably the memoryless quality of Markov models often termed the 'Markovian assumption'. A hypothetical example of a drug intervention to slow the progression of a chronic disease is employed to demonstrate the modelling technique and the possible methods of analysing Markov models are explored. Analysts should be aware of the limitations of Markov models, particularly the Markovian assumption, although the adept modeller will often find ways around this problem.
7D5544BA	An intelligent home is likely in the near future. A n important ingredient in an intelligent environment such as a home is prediction - of the next action, the next l ocation, and the next task that an inhabitant is likely to p erform. In this paper we describe our approach to solving the problem of predicting inhabitant behavior in a smart home. We model the inhabitant actions as states in a simple Markov model, then improve the model by supplying it with data from discovered high-level inhabitant tasks. For si mulated data we achieved good accuracy, whereas on real dat a we had marginal performance. We also investigate clust ering of actions and subsequently predict the next action an d the task with hidden Markov models created using the clusters.
5C2535CD	In this paper, we present a new search algorithm for sequential labeling tasks based on the conditional Markov models (CMMs) frameworks. Unlike conventional beam search, our method traverses all possible incoming arcs and also considers the “local best” so-far of each previous node. Furthermore, we propose two heuristics to fit the efficiency requirement. To demonstrate the effect of our method, six variant and large-scale sequential labeling tasks were conducted in the experiment. In addition, we compare our method to Viterbi and Beam search approaches. The experimental results show that our method yields not only substantial improvement in runtime efficiency, but also slightly better accuracy. In short, our method achieves 94.49 F(β) rate in the well-known CoNLL-2000 chunking task.
7D89C765	Web based learning systems provides huge volume of educational content to learners. However, a single learner might not be interested in learning all the contents delivered. To encourage learners of varying skill sets and to develop learning interests web recommendation system is needed for web based learning. This paper focuses on providing recommendations to learners as well as web masters to improve overall effectiveness of web based teaching and learning. This work deals with analysis of web log data and development of recommendation framework using web usage mining techniques like upper approximation based rough set clustering using k nearest neighbors, dynamic support pruned all k-th order Markov model and all k-th order association rule mining by dynamic frequent (k+1) item set generation using Apriori. The goal of this integrated approach is to make accurate recommendations for learning management systems with reduced state space complexity.
07027B17	Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.
7E5073A1	Efficient intra prediction is an important aspect of video coding with high compression efficiency. H.264/AVC applies directional prediction from neighboring pixels on an adjustable block size for local decorrelation. In this paper, we present an extended prediction scheme in the context of H.264/AVC that comprises two additional prediction methods exploiting self-similar properties of the encoded texture. A new macroblock type is implemented, allowing for flexible selection of the available prediction methods for sub-partitions of the macroblock. Depending on the content of the encoded video sequence, substantial gains in rate-distortion performance are achieved. The results may indicate directions towards an enhanced intra coding scheme with improved rate-distortion performance.
7E714964	In this paper, we present a stream-based mining algorithm for online anomaly prediction. Many real-world applications such as data stream analysis requires continuous cluster operation. Unfortunately, today's large-scale cluster systems are still vulnerable to various software and hardware problems. System administrators are often overwhelmed by the tasks of correcting various system anomalies such as processing bottlenecks (i.e., full stream buffers), resource hot spots, and service level objective (SLO) violations. Our anomaly prediction scheme raises early alerts for impending system anomalies and suggests possible anomaly causes. Specifically, we employ Bayesian classification methods to capture different anomaly symptoms and infer anomaly causes. Markov models are introduced to capture the changing patterns of different measurement metrics. More importantly, our scheme combines Markov models and Bayesian classification methods to predict when a system anomaly will appear in the foreseeable future and what are the possible anomaly causes. To the best of our knowledge, our work provides the first stream-based mining algorithm for predicting system anomalies. We have implemented our approach within the IBM System S distributed stream processing cluster, and conducted case study experiments using fully implemented distributed data analysis applications processing real application workloads. Our experiments show that our approach efficiently predicts and diagnoses several bottleneck anomalies with high accuracy while imposing low overhead to the cluster system.
769C66A6	Understanding and modeling user online behavior, as well as predicting future requests remain an open challenge for researchers, analysts and marketers. In this paper, we propose an efficient prediction schema based on the extraction of sequential navigation patterns from server log files, combined with web site topology. Traversed paths are monitored, internally recorded and cleaned before being completed with cashed page views. After session and episode identification follows the construction of n-grams. Prediction is based upon a 5 + n-gram schema with all lower level n-grams participating, a procedure that resembles the construction of an All 5th-order Markov Model. The schema achieves full coverage while maintaining competitive prediction precision.
7E17B5B5	Most epidemiological studies of major depression report period prevalence estimates. These are of limited utility in characterizing the longitudinal epidemiology of this condition. Markov models provide a methodological framework for increasing the utility of epidemiological data. Markov models relating incidence and recovery to major depression prevalence have been described in a series of prior papers. In this paper, the models are extended to describe the longitudinal course of the disorder. Data from three national surveys conducted by the Canadian national statistical agency (Statistics Canada) were used in this analysis. These data were integrated using a Markov model. Incidence, recurrence and recovery were represented as weekly transition probabilities. Model parameters were calibrated to the survey estimates. The population was divided into three categories: low, moderate and high recurrence groups. The size of each category was approximated using lifetime data from a study using the WHO Mental Health Composite International Diagnostic Interview (WMH-CIDI). Consistent with previous work, transition probabilities reflecting recovery were high in the initial weeks of the episodes, and declined by a fixed proportion with each passing week. Markov models provide a framework for integrating psychiatric epidemiological data. Previous studies have illustrated the utility of Markov models for decomposing prevalence into its various determinants: incidence, recovery and mortality. This study extends the Markov approach by distinguishing several recurrence categories.
80846A31	Conditional Random Fields (CRFs) are undirected probabilistic graphical models that were introduced for solving sequence labeling and segmenting problems. CRFs have several advantages compared to other well understood and widely used techniques such as Hidden Markov Models (HMMs) or Maximum Entropy Markov Models (MEMMs). Being a conditional model, it does not explicitly model the input data sequences but uses feature functions (features) to incorporate the arbitrary interactions and inter-dependencies that exist in the observation sequences. The number of all possible features is extremely large, up to millions, and is usually specified and designed in advance or according to a feature-generating scheme based on domain knowledge. This paper introduces a feature subset selection method for CRFs based on genetic algorithms, in which a population of candidate feature function subsets is evolved to achieve a maximal CRF performance. The method was experimentally validated on the well known bioinformatics problem of protein phosphorylation site prediction, phosphorylation being one of the most important protein modification mechanisms.
5D7D351E	Intuitively, any ‘bag of words’ approach in IR should benefit from taking term dependencies into account. Unfortunately, for years the results of exploiting such dependencies have been mixed or inconclusive. To improve the situation, this paper shows how the natural language properties of the target documents can be used to transform and enrich the term dependencies to more useful statistics. This is done in three steps. The term co-occurrence statistics of queries and documents are each represented by a Markov chain. The paper proves that such a chain is ergodic, and therefore its asymptotic behavior is unique, stationary, and independent of the initial state. Next, the stationary distribution is taken to model queries and documents, rather than their initial distributions. Finally, ranking is achieved following the customary language modeling paradigm. The main contribution of this paper is to argue why the asymptotic behavior of the document model is a better representation then just the document’s initial distribution. A secondary contribution is to investigate the practical application of this representation in case the queries become increasingly verbose. In the experiments (based on Lemur’s search engine substrate) the default query model was replaced by the stable distribution of the query. Just modeling the query this way already resulted in significant improvements over a standard language model baseline. The results were on a par or better than more sophisticated algorithms that use fine-tuned parameters or extensive training. Moreover, the more verbose the query, the more effective the approach seems to become.
772EDB8B	A probabilistic graphical model is proposed in order to detect the coevolution between different sites in biological sequences. The model extends the continuous-time Markov process of sequence substitution for single nucleic or amino acids and imposes general constraints regarding simultaneous changes on the substitution rate matrix. Given a multiple sequence alignment for each molecule of interest and a phylogenetic tree, the model can predict potential interactions within or between nucleic acids and proteins. Initial validation of the model is carried out using tRNA and 16S rRNA sequence data. The model accurately identifies the secondary interactions of tRNA as well as several known tertiary interactions. In addition, results on 16S rRNA data indicate this general and simple coevolutionary model outperforms several other parametric and nonparametric methods in predicting secondary interactions. Furthermore, the majority of the putative predictions exhibit either direct contact or proximity of the nucleotide pairs in the 3-dimensional structure of the Thermus thermophilus ribosomal small subunit. The results on RNA data suggest a general model of coevolution might be applied to other types of interactions between protein, DNA, and RNA molecules.
7F8D3B50	In recent years there has been an increased interest in the modelling and recognition of human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approach is presented for automatically acquiring stochastic models of the high-level structure of an activity without the assumption of any prior knowledge. The process involves temporal segmentation intoplausible atomic behaviour com-ponents and the use of variable length Markov models for the efficient rep-resentation of behaviours. Experimental results are presented which demon-strate the generation of realistic sample behaviours and evaluate the perfor-mance of models for long-term temporal prediction.
5B14A4B4	Proactive User Interfaces (PUIs) aim at facili- tating the interaction with a user interface, e.g., by highlighting fields or adapting the interface. For that purpose, they need to be able to pre- dict the next user action from the interaction his- tory. In this paper, we give an overview of se- quence prediction algorithms (SPAs) that are ap- plied in this domain, and build upon them to de- velop two new algorithms that base on combin- ing different order Markov models. We iden- tify the special requirements that PUIs pose on these algorithms, and evaluate the performance of the SPAs in this regard. For that purpose, we use three datasets with real usage-data and syn- thesize further data with specific characteristics. Our relatively simple yet efficient algorithm FxL performs extremely well in the domain of SPAs which make it a prime candidate for integration in a PUI. To facilitate further research in this field, we provide a Perl library that contains all presented algorithms and tools for the evaluation.
7FDEABB6	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.
00BA8AB5	MUSART is a research project developing and studying new techniques for music information retrieval.The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling,melodic contour, and phonetic streams for music retrieval.To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music.Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.
78B9FFFA	Various software packages are commonly used for the implementation and calculation of decision-analytic models for health economic evaluations. However, comparison of these programs with regard to ease of implementing a model is lacking.(i) to compare the assets and drawbacks of three commonly used software packages for Markov models with regard to ease of implementation; and (ii) to investigate how a technical model validation can be conducted by comparing the results of the three implementations.A Markov model on chronic obstructive pulmonary disease was implemented in TreeAge, Microsoft Excel and Arena with the same assumptions on model structure, transition probabilities and costs. A hypothetical smoking cessation programme for patients in stage 1 was evaluated against usual care. The packages were compared with respect to time and effort for implementation, run-time, features for the presentation of results, and flexibility. Agreement between the packages on average costs and life-years gained and on the incremental cost-effectiveness ratio was considered for technical validation in the form of expected values (between TreeAge and Excel only) and Monte Carlo simulations.Ease of implementation was best in TreeAge, whereas Arena offered the highest flexibility. Deterministic results were in agreement between TreeAge and Excel, as were simulated values between all three packages.Excel offers an intuitive spreadsheet interface, but the acquisition of and the training in TreeAge or Arena is worthwhile for more complex models. Double implementation is a practicable validation technique that should be conducted to ensure correct model implementation.
7E61B32A	Depression is among the major contributors to worldwide disease burden and adequate modelling requires a framework designed to depict real world disease progression as well as its economic implications as closely as possible.In light of the specific characteristics associated with depression (multiple episodes at varying intervals, impact of disease history on course of illness, sociodemographic factors), our aim was to clarify to what extent "Discrete Event Simulation" (DES) models provide methodological benefits in depicting disease evolution.We conducted a comprehensive review of published Markov models in depression and identified potential limits to their methodology. A model based on DES principles was developed to investigate the benefits and drawbacks of this simulation method compared with Markov modelling techniques.The major drawback to Markov models is that they may not be suitable to tracking patients' disease history properly, unless the analyst defines multiple health states, which may lead to intractable situations. They are also too rigid to take into consideration multiple patient-specific sociodemographic characteristics in a single model. To do so would also require defining multiple health states which would render the analysis entirely too complex. We show that DES resolve these weaknesses and that its flexibility allow patients with differing attributes to move from one event to another in sequential order while simultaneously taking into account important risk factors such as age, gender, disease history and patients attitude towards treatment, together with any disease-related events (adverse events, suicide attempt etc.).DES modelling appears to be an accurate, flexible and comprehensive means of depicting disease progression compared with conventional simulation methodologies. Its use in analysing recurrent and chronic diseases appears particularly useful compared with Markov processes.
7DC8E808	A research area that has become increasingly important in recent years is that of on-board mobile communication, where users on a vehicle are connected to a local network that attaches to the Internet via a mobile router and a wireless link. In this architecture, link disruptions (e.g., due to signal degradation) may have an immediate impact on a potentially large number of connections. We argue that the advance knowledge of public transport routes, and their repetitive nature, allows a certain degree of prediction of impending link disruptions, which can be used to offset their catastrophic impact. Focusing on the transmission control protocol (TCP) and its extension known as Freeze-TCP, we present a detailed analysis of the performance improvement of TCP connections in the presence of disruption prediction. In particular, we propose a Markov model of Freeze-TCP that captures both the TCP behavior and the prediction+"freezing" feature and, using simulations, show that it accurately predicts the performance of the protocol. Our results demonstrate the significant throughput improvement that can be gained by disruption prediction, even with random packet losses or imperfect timing of the predicted disruptions
7EAB152E	Tropical Pacific sea surface temperatures (SSTs) and the accompanying El Niño–Southern Oscillation phenomenon are recognized as significant components of climate behavior. The atmospheric and oceanic processes involved display highly complicated variability over both space and time. Researchers have applied both physically derived modeling and statistical approaches to develop long-lead predictions of tropical Pacific SSTs. The comparative successes of these two approaches are a subject of substantial inquiry and some controversy. Presented in this article is a new procedure for long-lead forecasting of tropical Pacific SST fields that expresses qualitative aspects of scientific paradigms for SST dynamics in a statistical manner. Through this combining of substantial physical understanding and statistical modeling and learning, this procedure acquires considerable predictive skill. Specifically, a Markov model, applied to a low-order (empirical orthogonal function–based) dynamical system of tropical Pacific SST, with stochastic regime transition, is considered. The approach accounts explicitly for uncertainty in the formulation of the model, which leads to realistic error bounds on forecasts. The methodology that makes this possible is hierarchical Bayesian dynamical modeling.
7A42DD4C	Recently, attention has been focused on spatial databases which combine conventional and spatial data. The need for spatial query languages has been identified in several different application domain as Geographic Information Systems (GISs) and Image Databases. Several extensions to the relational database query language SQL have been proposed to serve as a spatial query language. The large availability on the market place of the relational database technology is the major reason why an SQL-based spatial query language is welcome both from the GIS vendors and the GIS user community. Recent SQL extensions convinced us that it is the right time for working on the standardization of SQL-based spatial query languages. This paper sets a kernel of basic features for the creation of a standard and compares a large number of SQL spatial extensions according to such features.
7F5443CD	In this paper, we study the correlation properties of the fading mobile radio channel. Based on these studies, we model the channel as a one-step Markov process whose transition probabilities are a function of the channel characteristics. Then we present the throughput performance of the Go-Back-N and selective-repeat automatic repeat request (ARQ) protocols with timer control, using the Markov model for both forward and feedback channels. This approximation is found to be very good, as confirmed by simulation results.
7E4DC75A	We propose a new and effective method of predicting tracking failures and apply it to the robust analysis of gait and human motion. We define a tracking failure as an event and describe its temporal characteristics using a hidden Markov model (HMM). We represent the human body using a three-dimensional, multicomponent structural model, where each component is designed to independently allow the extraction of certain gait variables. To enable a fault-tolerant tracking and feature extraction system, we introduce a single HMM for each element of the structural model, trained on previous examples of tracking failures. The algorithm derives vector observations for each Markov model using the time-varying noise covariance matrices of the structural model parameters. When transformed with a logarithmic function, the conditional output probability of each HMM is shown to have a causal relationship with imminent tracking failures. We demonstrate the effectiveness of the proposed approach on a variety of multiview video sequences of complex human motion.
7E33249D	Smoking cessation is the only strategy that has shown a lasting reduction in the decline of lung function in patients with chronic obstructive pulmonary disease. This study aims to evaluate the cost-effectiveness of smoking cessation interventions in patients with chronic obstructive pulmonary disease, to assess the quality of the Markov models and to estimate the consequences of model structure and input data on cost–effectiveness. A systematic literature search was conducted in PubMed, Embase, BusinessSourceComplete and Econlit on June 11, 2014. Data were extracted, and costs were inflated. Model quality was evaluated by a quality appraisal, and results were interpreted. Ten studies met the inclusion criteria. The results varied widely from cost savings to additional costs of €17,004 per quality adjusted life year. The models scored best in the category structure, followed by data and consistency. The quality of the models seems to rise over time, and regarding the results there is no economic reason to refuse the reimbursement of any smoking cessation intervention 
7F603AAF	A method of dynamically constructing Markov chain models that describe the characteristics of binary messages is developed. Such models can be used to predict future message characters and can therefore be used as a basis for data compression. To this end, the Markov modelling technique is combined with Guazzo's arithmetic coding scheme to produce a powerful method of data compression. The method has the advantage of being adaptive: messages may be encoded or decoded with just a single pass through the data. Experimental results reported here indicate that.the Markov modelling approach generally achieves much better data compression than that observed with competing methods on typical computer data.
7F5E056C	Motivated by applications such as automated visual surveillance and video monitoring and annotation, there has been a lot of interest in constructing cognitive vision systems capable of interpreting the high level semantics of dynamic scenes. In this paper we present a novel approach for automatically inferring models of object interactions that can be used to interpret observed behaviour within a scene. A real-time low-level computer vision system, together with an attentional control mechanism, are used to identify incidents or events that occur in the scene. A data driven approach has been taken in order to automatically infer discrete and abstract representations (symbols) of primitive object interactions; effectively the system learns a set of qualitative spatial relations relevant to the dynamic behaviour of the domain. These symbols then form the alphabet of a VLMM which automatically infers the high level structure of typical interactive behaviour. The learnt behaviour model has generative capabilities and is also capable of recognizing typical or atypical activities within a scene. Experiments have been performed within the traffic monitoring domain; however the proposed method is applicable to the general automatic surveillance task since it does not assume a priori knowledge of a specific domain.
7E030AE4	A foremost objective in wireless networks is to facilitate the communication of mobile users and the widespread tracking and prediction of their mobility regardless of their point of attachment to the network. In indoor environments the effective users' motion prediction system and wireless localization technology play an important role in all aspects of people's daily lives, including e.g. living assistant, navigation, emergency detection, surveillance/tracking of target-of-interest, evacuation purposes, and many other location-based services. Prediction techniques that are currently used do not consider the motivation behind the movement of mobile nodes and incur huge overheads to manage and manipulate the information required to make predictions. In this paper we propose an activity-based continuous-time Markov model to define and predict the human movement patterns. Then we demonstrate the utility of Nonparametric Belief Propagation (NBP) technique in particle filtering, for both estimating the node locations and representing location uncertainties, and for prediction of the areas that would be visited and those that would not in the future. NBP method admits a wide variety of statistical models, and can represent multi-modal uncertainty. This prediction system may be used as an additional input into intelligent building automation systems
7ABB00FC	Markov modelling offers the possibility of extracting long-term results from dynamic simulations with a significantly reduced execution time over that which would be necessary with an equivalent time-series simulation. The discretization of the problem variables which is necessary for Markov modelling introduces an inaccuracy into the simulation process which means that a balance must be struck between the accuracy required and the time reduction that is possible. The paper describes an approach to the quantification of the errors introduced by discretization of the simulation variables. Two error expressions were tested by running a representative building/HVAC system dynamic model in both time-series and Markov modes with varying degrees of discretization of the variables. A simplified error bound was found useful in identifying near-optimal discretization schemes: further refinement of this approach led to an expression that was found to give a good prediction of the error in Markov simulations of this type of system.
81507EC1	In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.
802AF8D6	We investigate the behavior of block errors which arise in data transmission on fading channels. Our approach takes into account the details of the specific coding/modulation scheme and tracks the fading process symbol by symbol. It is shown that a Markov approximation for the block error process (possibly degenerating into an identically distributed (i.i.d.) process for sufficiently fast fading) is a good model for a broad range of parameters. Also, it is observed that the relationship between the marginal error rate and the transition probability is largely insensitive to parameters such as block length, degree of forward error correction and modulation format, and depends essentially on an appropriately normalized version of the Doppler frequency. This relationship can therefore be computed in the simple case of a threshold model and then used more generally as an accurate approximation. This observation leads to a unified approach for the channel modeling, and to a simplified performance analysis of upper layer protocols.
78744637	In this paper, we introduce a 3-D human-body tracker capable of handling fast and complex motions in real-time. The parameter space, augmented with first order derivatives, is automatically partitioned into Gaussian clusters each representing an elementary motion: hypothesis propagation inside each cluster is therefore accurate and efficient.  The transitions between clusters use the predictions of a Variable Length Markov Model which can explain high level behaviours over a long history. Using Monte-Carlo methods, evaluation of model candidates is critical for both speed and robustness.  We present a new evaluation scheme based on volumetric reconstruction and blobs-fitting, where appearance models and image evidences are represented by Gaussian mixtures.  We demonstrate the application of our tracker to long video sequences exhibiting rapid and diverse movements.
80105513	With the introduction of dynamic image processing, such as in image analysis, the computational complexity has become data dependent and memory usage irregular. Therefore, the possibility of runtime estimation of resource usage would be highly attractive and would enable Quality-of-Service (QoS) control for dynamic image-processing applications with shared resources. A possible solution to this problem is to characterize the application execution using model descriptions of the resource usage. In this paper, we attempt to predict resource usage for groups of dynamic image-processing tasks based on Markov-chain modeling. As a typical application, we explore a medical imaging application to enhance a wire mesh tube (stent) under X-ray fluoroscopy imaging during angioplasty. Simulations show that Markov modeling can be successfully applied to describe the resource usage function even if the flow graph dynamically switches between groups of tasks. For the evaluated sequences, an average prediction accuracy of 97% is reached with sporadic excursions of the prediction error up to 20–30%.
793EC21A	We consider the prediction of new observations in a general Gauss–Markov model. We state the fundamental equations of the best linear unbiased prediction, BLUP, and consider some properties of the BLUP. Particularly, we focus on such linear statistics, which preserve enough information for obtaining the BLUP of new observations as a linear function of them. We call such statistics linearly prediction sufficient for new observations, and introduce some equivalent characterizations for this new concept.
80739C5A	Spectrum sensing is one of the key functionalities in cognitive radios which enables opportunistic spectrum access. In a cognitive radio system, secondary users need to detect the emergence of primary users as soon as possible to avoid harmful interference. In particular, sensing performance can be evaluated by detection delay and sensing overhead. Sequential detection techniques such as quickest detection can achieve minimum detection delay, while MAC layer sensing scheduling of periodic energy detection has demonstrated its high sensing efficiency. These motivate us to propose a joint PHY-MAC spectrum sensing algorithm in this letter, which employs sequential probability ratio test in the PHY layer and a probability-based sensing scheduling mechanism in the MAC layer. This algorithm can minimize detection delay with limited sensing overhead. Simulation results reveal that it has remarkable performance improvement compared with periodic energy detection.
80B4014C	The SAM-T04 method for predicting protein structures uses a single protocol across the entire range of targets, from comparative modeling to new folds. This protocol is similar to the SAM-T02 protocol used in CASP5, but has improvements in the iterative search for similar sequences in finding and aligning templates, in creating fragment libraries, in generating protein conformations, and in scoring the conformations. The automatic procedure made some improvements over simply selecting an alignment to the highest-scoring template, and human intervention made substantial improvements over the automatic procedure. The main improvements made by human intervention were from adding constraints to build (or retain) beta-sheets and from splitting multidomain proteins into separate domains. The uniform protocol was moderately successful across the entire range of target difficulty, but was somewhat less successful than other approaches in CASP6 on the comparative modeling targets.
7D9B3729	Background. Serial period prevalence estimates for recurrent diseases such as major depression are available more frequently than fully detailed longitudinal data, but it is difficult to estimate incidence and episode duration from such data. Incidence and episode duration are critical decision modeling parameters for recurrent diseases. Objectives. To reduce bias that would otherwise occur in national incidence and duration-of-episode estimates for major depressive episodes deriving from studies using serial period prevalence data and to illustrate amethodological approach for the estimation of incidence from such studies. Methods. Monte Carlo simulation was applied to a Markov process describingincidence and recovery from major depressive episodes. Results. The annual incidence and episode duration were found to be 3.1% and 17.1 weeks, respectively. These estimates are expected to be less subject to bias than those generated without modeling. Conclusions. These results highlight the usefulness of Markov models for analysis of longitudinal data. The methods described here may be useful for decision modeling andmay be generalizable to other chronic diseases.
810600AC	Markov models are useful when a decision problem involves risk that is continuous over time, when the timing of events is important, and when important events may happen more than once. Representing such clinical settings with conventional decision trees is difficult and may require unrealistic simplifying assumptions. Markov models assume that a patient is always in one of a finite number of discrete health states, called Markov states. All events are represented as transitions from one state to another. A Markov model may be evaluated by matrix algebra, as a cohort simulation, or as a Monte Carlo simulation. A newer representation of Markov models, the Markov-cycle tree, uses a tree representation of clinical events and may be evaluated either as a cohort simulation or as a Monte Carlo simulation. The ability of the Markov model to represent repetitive events and the time dependence of both probabilities and utilities allows for more accurate representation of clinical settings that involve these issues.
7CBB23DF	Chronic obstructive pulmonary disease (COPD) is currently the fourth leading cause of death worldwide. It has serious health effects and causes substantial costs for society.The aim of the present paper was to develop a state-of-the-art decision-analytic model of COPD whereby the cost effectiveness of interventions in Germany can be estimated. To demonstrate the applicability of the model, a smoking cessation programme was evaluated against usual care. A seven-stage Markov model (disease stages I to IV according to the GOLD [Global Initiative for Chronic Obstructive Lung Disease] classification, states after lung-volume reduction surgery and lung transplantation, death) was developed to conduct a cost-utility analysis from the societal perspective over a time horizon of 10, 40 and 60 years. Patients entered the cohort model at the age of 45 with mild COPD. Exacerbations were classified into three levels: mild, moderate and severe. Estimation of stage-specific probabilities (for smokers and quitters), utilities and costs was based on German data where possible. Data on effectiveness of the intervention was retrieved from the literature. A discount rate of 3% was applied to costs and effects. Probabilistic sensitivity analysis was used to assess the robustness of the results.The smoking cessation programme was the dominant strategy compared with usual care, and the intervention resulted in an increase in health effects of 0.54 QALYs and a cost reduction of €1115 per patient (year 2007 prices) after 60 years. In the probabilistic analysis, the intervention dominated in about 95% of the simulations. Sensitivity analyses showed that uncertainty primarily originated from data on disease progression and treatment cost in the early stages of disease.e model developed allows the long-term cost effectiveness of interventions to be estimated, and has been adapted to Germany. The model suggests that the smoking cessation programme evaluated was more effective than usual care as well as being cost-saving. Most patients had mild or moderate COPD, stages for which parameter uncertainty was found to be high. This raises the need to improve data on the early stages of COPD.
80D7D606	The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
78AD17F6	Using a finite state Markov channel model, we develop an analytical method for evaluation of the packet error structure in multiple-input multiple-output (MIMO) systems based on singular value decomposition (SVD). We consider dual-branch MIMO systems, with either two transmit and arbitrary number of receive antennas, or arbitrary number of transmit and two receive antennas. The corresponding Markov model parameters are obtained using a novel closed-form expressions for probability density function and level crossing rate of the signal-to-noise ratio at the output of eigenchannels in a MIMO system, derived for a case of Rayleigh propagation, imperfect channel state information and any fixed power allocation. The exact bit error rate for the transmission of quadrature amplitude modulated (QAM) symbols through the eigenchannels is derived in polynomial closed form. Furthermore, by using the developed Markov model, the packet error statistics in the corresponding eigenchannels are determined, and the closed-form analytical expression for the system throughput is derived when ‘go-back-N’ automatic repeat request procedure is applied in time-varying eigenchannels. The analytical results are validated by using Monte Carlo simulations.
7EA1B380	A formula for the go-back-N ARQ (automatic repeat request) scheme applicable to Markov error patterns is derived. It is a generalization of the well-known efficiency formula p/(p+m(1-p)) (where m is the round trip delay in number of block durations and p is the block transmission success probability), and it has been successfully validated against simulation measurements. It is found that for a given error rate, error patterns having zero correlation between successive transmission generally fare better than those with negative correlation, and that error patterns with positive correlation fare better still. It is shown that the present analysis can be extended in a straightforward manner to cope with error patterns of a more complex nature. Simple procedures for numerical evaluation of efficiency under quite general error structures are presented.
81018323	We use several approaches to demonstrate that neural networks can detect precursors to failure. That is, they can detect subtle changes in the process signals. In some cases these subtle changes are early warnings that a subsystem failure is imminent. The results on detection of precursors and faults with various types of time-delay neural networks are discussed. We also measure the noise inherent in our database and place bounds on neural network prediction in the presence of noise. We observe that the noise level can be as high as 40% for detection of failures and can be at 30% to still detect precursors to failure. We note that although self-organizing networks for classification of faults seems like a good idea, in fact they do not perform well in the presence of noise. Lastly, we show that neural networks can induce, or self-build, Markov models from process data and these models can be used to predict system state to a significant distance in the future (e.g., 100 wafers).
7D1020E8	An automatic-repeat-request (ARQ) Go-Back-N (GBN) protocol with unreliable feedback and time-out mechanism is studied, using renewal theory. Transmissions on both the forward and the reverse channels are assumed to experience Markovian errors. The exact throughput of the protocol is evaluated, and simulation results, that confirm the analysis, are presented. A detailed comparison of the proposed method and the commonly used transfer function method reveals that the proposed approach is simple and potentially more powerful.
80567ABE	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning,in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events  that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is  founded  on  an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a  typical Smart Environment–the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of  this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
762DB3E0	The measurement of individual single-channel events arising from the gating of ion channels provides a detailed data set from which the kinetic mechanism of a channel can be deduced. In many cases, the pattern of dwells in the open and closed states is very complex, and the kinetic mechanism and parameters are not easily determined. Assuming a Markov model for channel kinetics, the probability density function for open and closed time dwells should consist of a sum of decaying exponentials. One method of approaching the kinetic analysis of such a system is to determine the number of exponentials and the corresponding parameters which comprise the open and closed dwell time distributions. These can then be compared to the relaxations predicted from the kinetic model to determine, where possible, the kinetic constants. We report here the use of a linear technique, linear prediction/singular value decomposition, to determine the number of exponentials and the exponential parameters. Using simulated distributions and comparing with standard maximum-likelihood analysis, the singular value decomposition techniques provide advantages in some situations and are a useful adjunct to other single-channel analysis techniques.
7FD11145	Block-error processes in transmissions over slow-fading channels can be accurately modeled by a two-state Markov chain [the block Markov model (BMM)]. Another line of research has focused on the use of a channel-state Markov model (CSMM) to analyze block transmissions. Although both techniques provide results that agree well with observations, the relationship between both Markov models has not been recognized in the previous literature. In this letter, we show that the BMM for slow-fading channels can be directly derived from the CSMM. In addition, we introduce a greatly simplified channel-modeling methodology. In the new methodology, the BMM is the primary channel characterization tool, and the CSMM becomes essentially an estimation technique that provides parameters for the BMM. Results of packet transmissions in slow-fading channels show that our approach provides significant improvements in both accuracy and simplicity over previously proposed techniques.
799E3C70	This paper presents a software simulator applicable to multipath fading channels in urban environments of mobile communication networks. The simulator is constructed by a two-state Markov model and several statistical models for simulating the characterizations of different environments. A core idea of the simulator is to construct a Rice distribution-based multipath fading module produced by a modified Gans Doppler power spectrum, and in combination with a Markov model to predict the time-dependent characteristics of packet in different radio circumstances. It can simply predict the packet performance of the future channel and evaluate the relations between the radio channel and the modulation schemes, error control protocols and channel coding. Simulation results demonstrate that it is a reliable and efficient method.
7852BB29	A novel multi-layer perceptrons (MLP)-based speech recognition method is proposed in this study. In this method, the dynamic time warping capability of hidden Markov models (HMM) is directly combined with the discriminant based learning of MLP for the sake of employing a sequence of MLPs (SMLP) as a word recognizer. Each MLP is regarded as a state recognizer to distinguish an acoustic event. Next, the word recognizer is formed by serially cascading all state recognizers. Advantages of both HMM and MLP methods are attained in this system through training the SMLP with an algorithm which combines a dynamic programming (DP) procedure with a generalized probabilistic descent (GPD) algorithm. Additionally, two sub-syllable SMLP-based schemes are studied through application of this method toward the recognition of isolated Mandarin digits. Simulation results confirm that the performance of the method is comparable to a well modeled continuous Gaussian mixture density HMM trained with the minimum error criterion. Not only does the SMLP require less trainable parameters than the HMM system, but the former is more convenient for analysing internal features. With the aid of internal feature selection, discarding the least useful parameters of SMLP without affecting its performance is relatively easy.
7559C9A4	This paper proposes an algorithm, called sequence prediction via enhanced episode discovery (SPEED), to predict inhabitant activity in smart homes. SPEED is a variant of the sequence prediction algorithm. It works with the episodes of smart home events that have been extracted based on the on -off states of home appliances. An episode is a set of sequential user activities that periodically occur in smart homes. The extracted episodes are processed and arranged in a finite-order Markov model. A method based on prediction by partial matching (PPM) algorithm is applied to predict the next activity from the previous history. The result shows that SPEED achieves an 88.3% prediction accuracy, which is better than LeZi Update, Active LeZi, IPAM, and C4.5.
7C7C2B02	To date, decision trees and Markov models have been the most common methods used in pharmacoeconomic evaluations. Both of these techniques lack the flexibility required to appropriately represent clinical reality. In this paper an alternative, more natural, way to model clinical reality — discrete event simulation — is presented and its application is illustrated with a real world example.A discrete event simulation represents the course of disease very naturally, with few restrictions. Neither mutually exclusive branches nor states are required, nor is a fixed cycle. All relevant aspects can be incorporated explicitly and efficiently. Flexibility in handling perspectives and carrying out sensitivity analyses, including structural variations, is incorporated and the entire model can be presented very transparently. The main limitations are imposed by lack of data to fit realistic models.Discrete event simulation, though rarely employed in pharmacoeconomics today, should be strongly considered when carrying out economic evaluations, particularly those aimed at informing policy makers and at estimating the budget impact of a pharmaceutical intervention.
7F79AB85	This paper investigates the properties of a method for obtaining Markov models of unspecified order to be applied to narrow-band fading channels with additive white Gaussian noise. The models are obtained by applying the context tree pruning algorithm to experimental or simulated sequences. Fading environments are identified in which the extension from first-order to higher order models is justified. The paper presents, as examples, the evaluation of the covariance function and the packet error distribution.
5BD5188C	This paper develops a service for ontology evolution based on crowdsourcing. The approach is demonstrated using OntoAssist, a specially designed semantic search service that is capable of capturing and disambiguating user’s search intent as well as automatically enabling ontology evolution. Successful and consistent ontology evolution often requires large amount of input data to specify new terms or changes in relationships. These inputs typically come mainly from domain experts or ontology professionals, which makes it hard to keep up with the change of open, dynamic World Wide Web environment. By integrating OntoAssist with an existing search engine, we show that users’ search intent can be disambiguated and aggregated to help to evolve underlying ontology. The disambiguation feature helps the users to find desirable search results. OntoAssist has been implemented and tested by Turkers from Amazon Mechanical Turk in a live demonstration site. Promising results and analysis are reported.
0319453A	Search advertising is typical heterogeneous multi-dimensional data sets whose click-through rate depends on query words, terms and other factors. Traditional data mining methods, limited to homogenous data source, represent search ads as the vector space model, so they fail to sufficiently consider the search advertisements’ characteristics of heterogeneous data. This paper presents consistent bipartite graph model to describe ads, adopting spectral co-clustering method in data mining. In order to solve the balance partition of the map in clustering, heuristic algorithm is introduced into consistent bipartite graph's co-partition; a more effective subgraph redistribution algorithm is established. Experiments on real ads dataset shows that our approach worked effectively and efficiently in both clustering and prediction.
8035BDA2	A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, “Can link information result in better rankings?”, “Do longer queries result in better answers?”, and, “Do TREC systems work well on Web data?” The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art.
79E123BB	This paper investigates the composition of search engine results pages. We define what elements the most popular web search engines use on their results pages (e.g., organic results, advertisements, shortcuts) and to which degree they are used for popular vs. rare queries. Therefore, we send 500 queries of both types to the major search engines Google, Yahoo, Live.com and Ask. We count how often the different elements are used by the individual engines. In total, our study is based on 42,758 elements. Findings include that search engines use quite different approaches to results pages composition and therefore, the user gets to see quite different results sets depending on the search engine and search query used. Organic results still play the major role in the results pages, but different shortcuts are of some importance, too. Regarding the frequency of certain host within the results sets, we find that all search engines show Wikipedia results quite often, while other hosts shown depend on the search engine used. Both Google and Yahoo prefer results from their own offerings (such as YouTube or Yahoo Answers). Since we used the .com interfaces of the search engines, results may not be valid for other country-specific interfaces.
7918FE86	Influenza epidemics detection is critically important in recent years because there is a significant economic and public health impact associated with the influenza epidemic. Influenza epidemics detection attracts much attention from governments, organizations, and research institutes, and recently, a novel method using search engine query data to detect influenza activities was presented by Google. In this paper, a data mining based framework using web data is introduced for influenza epidemics detection. Under the framework, a neural network based approach using search engine query data is developed to detect influenza activities. In the proposed method, an automated feature selection model is firstly constructed to reduce the dimension of the query data. Secondly, various neural networks are employed to model the relationship between influenza-like illness data and query data. Thirdly, an optimal neural network is selected as the detector by using the cross-validation method. Finally, the selective neural network detector with the best feature subset is used to detect influenza activities. Experimental results show that the proposed method can outperform traditional statistical models and other models used in the experiments in terms of accuracy. These findings imply that data mining, such as neural network method, can be used as a promising tool to detect influenza activities.
78D19500	Internet search engines and comparison shopping have recently begun implementing a paid placement strategy, where some content providers are given prominent positioning in return for a placement fee. This bias generates placement revenues but creates a disutility to users, thus reducing user-based revenues. We formulate the search engine design problem as a tradeoff between these two types of revenues. We demonstrate that the optimal placement strategy depends on the relative benefits (to providers) and disutilities (to users) of paid placement. We compute the optimal placement fee, characterize the optimal bias level, and analyze sensitivity of the placement strategy to various factors. In the optimal paid placement strategy, the placement revenues are set below the monopoly level due to its negative impact on advertising revenues. An increase in the search engine's quality of service allows it to improve profits from paid placement, moving it closer to the ideal. However, an increase in the value-per-user motivates the gatekeeper to increase market share by reducing further its reliance on paid placement and fraction of paying providers.
815859E9	Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.
80E58704	This paper deals with one aspect of the index quality of search engines: index freshness. The purpose is to analyse the update strategies of the major web search engines Google, Yahoo, and MSN/Live.com. We conducted a test of the updates of 40 daily updated pages and 30 irregularly updated pages. We used data from a time span of six weeks in the years 2005, 2006 and 2007. We found that the best search engine in terms of up-to-dateness changes over the years and that none of the engines has an ideal solution for index freshness. Indexing patterns are often irregular, and there seems to be no clear policy regarding when to revisit Web pages. A major problem identified in our research is the delay in making crawled pages available for searching, which differs from one engine to another.
84D0666F	In recent years, the search engine results pages (SERP's) have been augmented with new markup elements that introduce seamlessly additional semantic information. Examples of such elements are the aggregated results disseminated by vertical portals, and the enriched snippets that display meta-information from the landing pages. In this paper, we investigate the gaze behaviour of web users who inter- act with SERP's that contain plain and rich snippets, and observe the impact of both types of snippets on the web search experience. For our study, we consider a wide range of snippet types, such as multimedia elements (Google Images, Google Videos), recommendation snippets (Author, Google Plus, Reviews, Google Shopping Product), and geo-location snippets (Google Places). We conduct two controlled user studies that employ eye tracking and mouse tracking, and analyse the search interactions of 213 participants, focusing on three factors: noticeability, interest, and conversion. Our findings indicate that ranking remains the most critical factor in relevance perception, although in certain cases the richness of snippets can capture user attention.
7CE1777E	HyPursuit is a new hierarchical network search engine that clusters hypertext documents to structure a given information space for browsing and search act ivities. Our content-link clustering algorithm is based on the semantic information embedded in hyperlink structures and document contents. HyPursuit admits multiple, coexisting cluster hierarchies based on different prin- ciples for grouping documents, such as the Library of Congress catalog scheme and automatically created hy- pertext clusters. HyPursuit’s abstraction functions summarize cluster con- tents to support scalable query processing. The abstrac- tion functions satisfy system resource limitations with controlled information 10SS. The result of query pro- cessing operations on a cluster summary approximates the result of performing the operations on the entire in- formation space. We constructed a prototype system comprising 100 leaf World- Wide Web sites and a hier- archy of 42 servers that route queries to the leaf sites. Experience with our system suggests that abstraction functions based on hypertext clustering can be used to construct meaningful and scalable cluster hierarchies. We are also encouraged by preliminary results on clus- tering based on both document contents and hyperlink structures
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
7CA05884	Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.
7C5CBF21	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.
7E6A3532	Understanding how people interact with search engines is important in improving search quality. Web search engines typically analyze queries and clicked results, but these actions provide limited signals regarding search interaction. Laboratory studies often use richer methods such as gaze tracking, but this is impractical at Web scale. In this paper, we examine mouse cursor behavior on search engine results pages (SERPs), including not only clicks but also cursor movements and hovers over different page regions. We: (i) report an eye-tracking study showing that cursor position is closely related to eye gaze, especially on SERPs; (ii) present a scalable approach to capture cursor movements, and an analysis of search result examination behavior evident in these large-scale cursor data; and (iii) describe two applications (estimating search result relevance and distinguishing good from bad abandonment) that demonstrate the value of capturing cursor data. Our findings help us better understand how searchers use cursors on SERPs and can help design more effective search systems. Our scalable cursor tracking method may also be useful in non-search settings.
7E0964F5	PageRank algorithm is used to re-rank the search results according to relations of Web links and to capture the relative Web pages dependent on particular search query terms. According to the features of web links of structural model, a new adaptive T-Rank algorithm for specific topics was proposed in order to yield more accurate results, deal with a problem of topic-drift exists in links among Web pages and to decrease the re-ranked time after new pages being crawled. Analysis and simulation results show that the improved T-Rank algorithm can efficiently save CPU resources, reduce the calculating time, and better resolve the problem of topic-drift.
06B1E08B	Search engines have changed the way we see the Internet. The ability to find the information by just typing in keywords was a big contribution to the overall web experience. While the conventional search engine methodology worked well for textual documents, locating scientific data remains a problem since they are stored in databases not readily accessible by search engine bots. Considering different temporal, spatial and thematic coverage of different databases, especially for interdisciplinary research it is typically necessary to work with multiple data sources. These sources can be federal agencies which generally offer national coverage or regional sources which cover a smaller area with higher detail. However for a given geographic area of interest there often exists more than one database with relevant data. Thus being able to query multiple databases simultaneously is a desirable feature that would be tremendously useful for scientists. Development of such a search engine requires dealing with various heterogeneity issues. In scientific databases, systems often impose controlled vocabularies which ensure that they are generally homogeneous within themselves but are semantically heterogeneous when moving between different databases. This defines the boundaries of possible semantic related problems making it easier to solve than with the conventional search engines that deal with free text. We have developed a search engine that enables querying multiple data sources simultaneously and returns data in a standardized output despite the aforementioned heterogeneity issues between the underlying systems. This application relies mainly on metadata catalogs or indexing databases, ontologies and webservices with virtual globe and AJAX technologies for the graphical user interface. Users can trigger a search of dozens of different parameters over hundreds of thousands of stations from multiple agencies by providing a keyword, a spatial extent, i.e. a bounding box, and a temporal bracket. As part of this development we have also added an environment that allows users to do some of the semantic tagging, i.e. the linkage of a variable name (which can be anything they desire) to defined concepts in the ontology structure which in turn provides the backbone of the search engine. 
815D5340	Modern computer vision research consumes labelled data in quantity, and building datasets has become an important activity. The Internet has become a tremendous resource for computer vision researchers. By seeing the Internet as a vast, slightly disorganized collection of visual data, we can build datasets. The key point is that visual data are surrounded by contextual information like text and HTML tags, which is a strong, if noisy, cue to what the visual data means. In a series of case studies, we illustrate how useful this contextual information is. It can be used to build a large and challenging labelled face dataset with no manual intervention. With very small amounts of manual labor, contextual data can be used together with image data to identify pictures of animals. In fact, these contextual data are sufficiently reliable that a very large pool of noisily tagged images can be used as a resource to build image features, which reliably improve on conventional visual features. By seeing the Internet as a marketplace that can connect sellers of annotation services to researchers, we can obtain accurately annotated datasets quickly and cheaply. We describe methods to prepare data, check quality, and set prices for work for this annotation process. The problems posed by attempting to collect very big research datasets are fertile for researchers because collecting datasets requires us to focus on two important questions: What makes a good picture? What is the meaning of a picture?
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
804E7F29	A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel, otherwise it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th-percentile), which we call extreme tail latency.We propose a prediction framework to reduce the extreme tail latency of search servers. The framework has a unique set of characteristics to predict long-running queries with high recall and improved precision. Specifically, prediction is delayed by a short duration to allow many short-running queries to complete without parallelization, and to allow the predictor to collect a set of dynamic features using runtime information. These features estimate query execution time with high accuracy. We also use them to estimate the prediction errors to override an uncertain prediction by selectively accelerating the query for a higher recall.We evaluate the proposed prediction framework to improve search engine performance in two scenarios using a simulation study: (1) query parallelization on a multicore processor, and (2) query scheduling on a heterogeneous processor. The results show that, for both scenarios, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision.
78E21799	Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
7D2BE772	The overall goal of an information retrieval process is to retrieve the information relevant to the given request. The information retrieval techniques commonly used are based on keywords. These techniques use keyword listed to describe the content of information, but one problem with such list is that they do not say anything about the semantic relationships between keywords, nor do they take into account the meaning of words and phrases. To overcome these limitations, from the recent literature it is identified that it is necessary to analyze and determine the semantic features of both the content in document and query. Hence in this paper it is proposed to develop ontology and a comparison is made between the normal search and ontology based semantic search. Various experimental results are carried on, which shows the increase in document retrieval recall and precision rates, thereby demonstrating the effectiveness of the model.
8117FEA8	Web search behaviour studies, including eye-tracking studies of search result examination, have resulted in numerous insights to improve search result quality and presentation. Yet, eye tracking studies have been restricted in scale, due to the expense and the effort required. Furthermore, as the reach of the Web expands, it becomes increasingly important to understand how searchers around the world see and interact with the search results. To address both challenges, we introduce ViewSer, a novel methodology for performing web search examination studies remotely, at scale, and without requiring eye-tracking equipment. ViewSer operates by automatically modifying the appearance of a search engine result page, to clearly show one search result at a time as if through a "viewport", while partially blurring the rest and allowing the participant to move the viewport naturally with a computer mouse or trackpad. Remarkably, the resulting result viewing and clickthrough patterns agree closely with unrestricted viewing of results, as measured by eye-tracking equipment, validated by a study with over 100 participants. We also explore applications of ViewSer to practical search tasks, such as analyzing the search result summary (snip- pet) attractiveness, result re-ranking, and evaluating snippet quality. These experiments could have only be done previously by tracking the eye movements for a small number of subjects in the lab. In contrast, our study was performed with over 100 participants, allowing us to reproduce and extend previous findings, establishing ViewSer as a valuable tool for large-scale search behavior experiments.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
76974C30	Web search engines rank potentially relevant pages/sites for a user query. Ranking documents for user queries has also been at the heart of the Text REtrieval Conference (TREC in short) under the label ad-hoc retrieval. The TREC community has developed document ranking algorithms that are known to be the best for searching the document collections used in TREC, which are mainly comprised of newswire text. However, the web search community has developed its own methods to rank web pages/sites, many of which use link structure on the web, and are quite different from the algorithms developed at TREC. This study evaluates the performance of a state-of-the-art keyword-based document ranking algorithm (coming out of TREC) on a popular web search task: finding the web page/site of an entity, e.g. companies, universities, organizations, individuals, etc. This form of querying is quite prevalent on the web. The results from the TREC algorithms are compared to four commercial web search engines. Results show that for finding the web page/site of an entity, commercial web search engines are notably better than a state-of-the-art TREC algorithm. These results are in sharp contrast to results from several previous studies. 
812F4683	Nowadays machine learning algorithms are intensively used to improve the relevance of search engines by training on Internet search data, while their software implementations on commodity computers are not efficient (in terms of computation speed, power consumption, etc). Therefore FPGA-based accelerators have been proposed to process these large scale data. Data compression/decompression technology plays an important role for it could significantly increase those accelerators’ performance. Based on the statistics on datasets and the analysis of conventional data compression algorithms, we propose a bit mapping compression/decompression method to provide non-blocking streaming data to accelerators. Experiments indicate that the performance of hardware accelerator is increased up to 140% after adding the bit mapping modules. This method could also be extended to other data-intensive hardware accelerators.
80FCD18B	Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
80103D90	Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
7B2E5EC6	In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want
80846772	We propose a model that leverages the millions of clicks received by web search engines to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgments between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.
5E342F1A	Search sessions consist of a person presenting a query to a search engine, followed by that person examining the search results, selecting some of those search results for further review, possibly following some series of hyperlinks, and perhaps backtracking to previously viewed pages in the session. The series of pages selected for viewing in a search session, sometimes called the click data, is intuitively a source of relevance feedback information to the search engine. We are interested in how that relevance feedback can be used to improve the search results quality for all users, not just the current user. For example, the search engine could learn which documents are frequently visited when certain search queries are given. In this article, we address three issues related to using click data as implicit relevance feedback: (1) How click data beyond the search results page might be more reliable than just the clicks from the search results page; (2) Whether we can further subselect from this click data to get even more reliable relevance feedback; and (3) How the reliability of click data for relevance feedback changes when the goal becomes finding one document for the user that completely meets their information needs (if possible). We refer to these documents as the ones that are strictly relevant to the query. Our conclusions are based on empirical data from a live website with manual assessment of relevance. We found that considering all of the click data in a search session as relevance feedback has the potential to increase both precision and recall of the feedback data. We further found that, when the goal is identifying strictly relevant documents, that it could be useful to focus on last visited documents rather than all documents visited in a search session. 
7AEA750B	Unemployment rate prediction has become critically important, because it can help government to make decision and design policies. In recent years, forecast of unemployment rate attracts much attention from governments, organizations, and research institutes, and researchers. Recently, a novel method using search engine query data to forecast unemployment was proposed by scholars. In this paper, a data mining based framework using web information is introduced for unemployment rate prediction. Under the framework, a neural network method, as one of the most effective data mining tools, is developed to forecast unemployment trend using search engine query data. In the proposed method, search engine query data related with employment activities is firstly found. Secondly, feature selection models including correlation coefficient method and genetic algorithm are constructed to reduce the dimension of the query data. Thirdly, various neural networks are employed to model the relationship between unemployment rate data and query data. Fourthly, an optimal neural network is selected as the selective predictor by using the cross-validation method. Finally, the selective neural network predictor with the best feature subset is used to forecast unemployment trend. The empirical results show that the proposed method clearly outperforms the classical forecasting approaches for the unemployment rate prediction. These findings imply that data mining method, such as neural networks, together with web information, can be used as an alternative tool to forecast social/economic hotspot.
7918FE86	Influenza epidemics detection is critically important in recent years because there is a significant economic and public health impact associated with the influenza epidemic. Influenza epidemics detection attracts much attention from governments, organizations, and research institutes, and recently, a novel method using search engine query data to detect influenza activities was presented by Google. In this paper, a data mining based framework using web data is introduced for influenza epidemics detection. Under the framework, a neural network based approach using search engine query data is developed to detect influenza activities. In the proposed method, an automated feature selection model is firstly constructed to reduce the dimension of the query data. Secondly, various neural networks are employed to model the relationship between influenza-like illness data and query data. Thirdly, an optimal neural network is selected as the detector by using the cross-validation method. Finally, the selective neural network detector with the best feature subset is used to detect influenza activities. Experimental results show that the proposed method can outperform traditional statistical models and other models used in the experiments in terms of accuracy. These findings imply that data mining, such as neural network method, can be used as a promising tool to detect influenza activities.
815D5340	Modern computer vision research consumes labelled data in quantity, and building datasets has become an important activity. The Internet has become a tremendous resource for computer vision researchers. By seeing the Internet as a vast, slightly disorganized collection of visual data, we can build datasets. The key point is that visual data are surrounded by contextual information like text and HTML tags, which is a strong, if noisy, cue to what the visual data means. In a series of case studies, we illustrate how useful this contextual information is. It can be used to build a large and challenging labelled face dataset with no manual intervention. With very small amounts of manual labor, contextual data can be used together with image data to identify pictures of animals. In fact, these contextual data are sufficiently reliable that a very large pool of noisily tagged images can be used as a resource to build image features, which reliably improve on conventional visual features. By seeing the Internet as a marketplace that can connect sellers of annotation services to researchers, we can obtain accurately annotated datasets quickly and cheaply. We describe methods to prepare data, check quality, and set prices for work for this annotation process. The problems posed by attempting to collect very big research datasets are fertile for researchers because collecting datasets requires us to focus on two important questions: What makes a good picture? What is the meaning of a picture?
776647D6	   The  World Wide Web provides an immense source of information.Accessing information of interest presents a challenge to scientists and analysts, particularly if the desired information is structural in nature. Our goal is to design a structural search engine that uses the hyperlink structure of the Web, in addition to textual information, to search for sites of interest.Our structural search engine, called WebSUBDUE, searches not only for particular words or topics but also for a desired hyperlink structure.Enhanced by WordNet text functions, our search engine retrieves sites corresponding to structures formed by graph-based userqueries. We hypothesize that this system can form the heart  of a structural query engine,and demonstrate the approach on a number of structural web queries.
06B1E08B	Search engines have changed the way we see the Internet. The ability to find the information by just typing in keywords was a big contribution to the overall web experience. While the conventional search engine methodology worked well for textual documents, locating scientific data remains a problem since they are stored in databases not readily accessible by search engine bots. Considering different temporal, spatial and thematic coverage of different databases, especially for interdisciplinary research it is typically necessary to work with multiple data sources. These sources can be federal agencies which generally offer national coverage or regional sources which cover a smaller area with higher detail. However for a given geographic area of interest there often exists more than one database with relevant data. Thus being able to query multiple databases simultaneously is a desirable feature that would be tremendously useful for scientists. Development of such a search engine requires dealing with various heterogeneity issues. In scientific databases, systems often impose controlled vocabularies which ensure that they are generally homogeneous within themselves but are semantically heterogeneous when moving between different databases. This defines the boundaries of possible semantic related problems making it easier to solve than with the conventional search engines that deal with free text. We have developed a search engine that enables querying multiple data sources simultaneously and returns data in a standardized output despite the aforementioned heterogeneity issues between the underlying systems. This application relies mainly on metadata catalogs or indexing databases, ontologies and webservices with virtual globe and AJAX technologies for the graphical user interface. Users can trigger a search of dozens of different parameters over hundreds of thousands of stations from multiple agencies by providing a keyword, a spatial extent, i.e. a bounding box, and a temporal bracket. As part of this development we have also added an environment that allows users to do some of the semantic tagging, i.e. the linkage of a variable name (which can be anything they desire) to defined concepts in the ontology structure which in turn provides the backbone of the search engine. 
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
77EB1685	Current automatic wrappers using DOM tree and visual properties of data records to extract the required information from the search engine results pages generally have limitations such as the inability to check the similarity of tree structures accurately. Our study on the properties of data records shows that these data records located in search engine results pages are not only having similar visual properties and tree structures, but they are also related semantically in their contents. In this context, we propose an ontological technique using existing lexical database for English (WordNet) for the extraction of data records. We find that wrappers designed based on ontological technique are able to reduce the number of potential data regions to be extracted, thus they are able to improve the data extraction accuracy. We then use visual cue from the browser rendering engine to locate and extract the relevant data region from the web page by measuring the size of text and image of data records. Experimental results indicate that our technique is robust and performs better than the existing state of the art visual based wrappers.
7FA14777	FPGAs (field programmable gate arrays) have appealing features such as customizable internal and external bandwidth and the ability to exploit vast amounts of fine-grain parallelism. In this paper, we explore the applicability of these features in using FPGAs as smart memory engines for search and reorganization computations over spatial pointer-based data structures. The experimental results in this paper suggests that reconfigurable logic, when combined with the data reorganization, can lead to dramatic performance improvements of up to 20x over traditional computer architectures for pointer-based computations, traditionally not viewed as a good match for reconfigurable technologies.
75D95FB3	Unlabeled document collections are becoming increasingly common and mining such databases becomes a major challenge. It is a major issue to retrieve good websites from the larger collections of websites. As the number of available Web pages grows, it is become more difficult for users finding documents relevant to their interests. Clustering is the classification of a data set into subsets (clusters), so that the data in each subset share some common trait - often proximity according to some defined distance measure. By clustering we improve the quality of websites by grouping similar websites in groups. This paper addresses the applications of data mining tool Weka by applying k means clustering to find clusters from huge data sets and find the attributes that govern optimization of search engines.
7BD8C7FA	In an web based application, different users may have different search goals when they submit it to a search engine. For a broad-topic and ambiguous query it is difficult. Here we Propose a novel approach to infer user search goals by analyzing search engine query logs. A major deficiency of generic search engines is that they follow the “one size fits all” model and are not adaptable to individual users. This is typically shown in cases such as these: Different users have different backgrounds and interests. However, effective personalization cannot be achieved without accurate user profiles. We address the problem of learning the user profile within the user's ongoing behaviors by using the user search. We propose a framework that enables large-scale evaluation of personalized search. User interest is employed in the clustering process to achieve personalization effect. The goal of personalized IR (information retrieval) is to return search results that better match the user intent. First, we propose a framework to discover different user search goals for a query by clustering the proposed feedback sessions. Feedback sessions are get constructed from user click-through logs and can efficiently reflect the information needs of users. Second, we propose an approach to generate pseudo-documents to better represent the feedback sessions for clustering. Most document-based methods focus on analyzing users' clicking and browsing behaviors recorded at the users' clickthrough data. In the Web search engines, clickthrough data are important implicit feedback mechanism from users. An example of clickthrough data for the query "apple," which contains a list of ranked search results presented to the user, which contains identification on the results that was previously clicked by the user. The bolded documents that have been clicked by the user have been ranked. Several personalized systems that employ clickthrough data to capture users' interest have been proposed.
802A5C28	Web page classification is a major issue for categorising web documents to facilitate indexing, search and retrieval of web pages for search engine. Different crawling techniques have been utilised to accumulate web pages of different domains under separate databases depending on practical scenario. Downloaded web pages are being parsed for further processing. A classifier is designed dynamically using single cycle multiple attractor cellular automata for mapping downloaded web pages of different domains into specific structure. This paper proposes alternate technique for automatic categorisation of web pages into different domains. Retrieved web pages have been ranked automatically at the time of classifier formation. Typically, our system consists of crawling, ranking and storage parts created in a different way. Hierarchical concept has been used over parallel crawler. GF(2P) concept is introduced in ranking. The concept of SMACA has been utilised in indexing storage. Overall, a search engine module has been created using agent-based method.
58B0DC1D	Open-ended questions can be a nightmare for statistical processing. Any mistake in spelling can result in a mismatch during merging, or multiple counting of the same object. For example, the answers to the "place-of-birth" question might be "Chicago" and "San Francisco", but in practice they are often "Chicaga" and "SanFrancisko". Manual correction of hundreds of answers is tedious, and becomes infeasible with a larger dataset. For a long time, algorithms like SOUNDEX remained the only alternative for researchers. A new Stata command allows taking advantage of Internet search engines, like Google or Yahoo to find proper substitutes for an unclear word or multiple words. The distinctive feature of the search engines is that they rely not only on the spelling similarity, but are also context driven: other words may affect the suggestion, such as including "city" into the query. This will hint to the search engine to give more priority to the names of cities. This presentation will demonstrate this new command and explain the main steps necessary to programmatically acquire information available on the Internet and convert it into Stata-usable format.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you’ll see a vision for Web search’s future that's more sophisticated, individual, and portable.
7E0964F5	PageRank algorithm is used to re-rank the search results according to relations of Web links and to capture the relative Web pages dependent on particular search query terms. According to the features of web links of structural model, a new adaptive T-Rank algorithm for specific topics was proposed in order to yield more accurate results, deal with a problem of topic-drift exists in links among Web pages and to decrease the re-ranked time after new pages being crawled. Analysis and simulation results show that the improved T-Rank algorithm can efficiently save CPU resources, reduce the calculating time, and better resolve the problem of topic-drift.
7D2BE772	The overall goal of an information retrieval process is to retrieve the information relevant to the given request. The information retrieval techniques commonly used are based on keywords. These techniques use keyword listed to describe the content of information, but one problem with such list is that they do not say anything about the semantic relationships between keywords, nor do they take into account the meaning of words and phrases. To overcome these limitations, from the recent literature it is identified that it is necessary to analyze and determine the semantic features of both the content in document and query. Hence in this paper it is proposed to develop ontology and a comparison is made between the normal search and ontology based semantic search. Various experimental results are carried on, which shows the increase in document retrieval recall and precision rates, thereby demonstrating the effectiveness of the model.
7A81F96C	Today’s knowledge workers rely increasingly on information to get their job done, and the availability of search engines to locate relevant information is thus essential. Understanding how users interact with search engines is a prerequisite for the successful design of useful systems and a body of knowledge has in recent years begun to compile. However, all previous studies have focused on the public web, not acknowledging the fact that much business-related information seeking occur on corporate internal networks. In this exploratory study, we have collected and analysed intranet search engine log files from three different years – 2000, 2002, and 2004 – enabling us to detect shifting trends in intranet search behaviour. Comparing our data to what has been reported from the public web we conclude that intranet searchers are both similar to and different from searchers on the public web. In sum, it appears that intranet users are more extreme in their behaviour and that qualitative studies are needed to understand the motives and rationales governing their actions.
812F4683	Nowadays machine learning algorithms are intensively used to improve the relevance of search engines by training on Internet search data, while their software implementations on commodity computers are not efficient (in terms of computation speed, power consumption, etc). Therefore FPGA-based accelerators have been proposed to process these large scale data. Data compression/decompression technology plays an important role for it could significantly increase those accelerators’ performance. Based on the statistics on datasets and the analysis of conventional data compression algorithms, we propose a bit mapping compression/decompression method to provide non-blocking streaming data to accelerators. Experiments indicate that the performance of hardware accelerator is increased up to 140% after adding the bit mapping modules. This method could also be extended to other data-intensive hardware accelerators.
809F7B04	To confront with an ever increasing number of published scientific articles, an effective, efficient, and easy-to-use tool is required to support biomedical scientists, while entering a new scientific field and encountering clinical decision, to organize a vast amount of PubMed abstracts into the panorama of specific topics according to their relevance. In brief, the set of associations among frequently co-occurring terms in given a set of PubMed documents forms naturally a simplicial complex. Afterwards each connected component of this simplicial complex represents a concept in the collection. Based on these concepts, documents can be clustered into meaningful classes. This paper presents an alternative search engine that applies a combinatorial topological method to automatically extract semantic clusters from the PubMed database of biomedical literature. We use several qualitative parameters to perform the user study that shows users are able to reduce search time. This clustering search engine is publicly available at http://ginni. bme.ntu. edu. tw/
7DB5FDAD	This paper reports our efforts to address the grand challenge of the Digital Earth vision in terms of intelligent data discovery from vast quantities of geo-referenced data. We propose an algorithm combining LSA and a Two-Tier Ranking (LSATTR) algorithm based on revised cosine similarity to build a more efficient search engine – Semantic Indexing and Ranking (SIR) – for a semantic-enabled, more effective data discovery. In addition to its ability to handle subject-based search, we propose a mechanism to combine geospatial taxonomy and Yahoo! GeoPlanet for automatic identification of location information from a spatial query and automatic filtering of datasets that are not spatially related. The metadata set, in the format of ISO19115, from NASA's SEDAC (Socio-Economic Data Application Center) is used as the corpus of SIR. Results show that our semantic search engine SIR built on LSATTR methods outperforms existing keyword-matching techniques, such as Lucene, in terms of both recall and precision. Moreover, the semantic associations among all existing words in the corpus are discovered. These associations provide substantial support for automating the population of spatial ontologies. We expect this work to support the operationalization of the Digital Earth vision by advancing the semantic-based geospatial data discovery.
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
5BD5188C	This paper develops a service for ontology evolution based on crowdsourcing. The approach is demonstrated using OntoAssist, a specially designed semantic search service that is capable of capturing and disambiguating user’s search intent as well as automatically enabling ontology evolution. Successful and consistent ontology evolution often requires large amount of input data to specify new terms or changes in relationships. These inputs typically come mainly from domain experts or ontology professionals, which makes it hard to keep up with the change of open, dynamic World Wide Web environment. By integrating OntoAssist with an existing search engine, we show that users’ search intent can be disambiguated and aggregated to help to evolve underlying ontology. The disambiguation feature helps the users to find desirable search results. OntoAssist has been implemented and tested by Turkers from Amazon Mechanical Turk in a live demonstration site. Promising results and analysis are reported.
7E054376	We describe an ensemble approach to learning salient spatial regions from arbitrarily partitioned simulation data. Ensemble approaches for anomaly detection are also explored. The partitioning comes from the distributed processing requirements of large-scale simulations. The volume of the data is such that classifiers can train only on data local to a given partition. Since the data partition reflects the needs of the simulation, the class statistics can vary from partition to partition.Some classes will likely be missing from some or even most partitions. We combine a fast ensemble learning algorithm with scaled probabilistic majority voting in order to learn an accurate classifier from such data. Since some simulations are difficult to model without a considerable number of false positive errors, and since we are essentially building a search engine for simulation data, we order predicted regions to increase the likelihood that most of the top-ranked predictions are correct(salient). Results from simulation runs of a canister being torn and from a casing being dropped show that regions of interest are successfully identified in spite of the class imbalance in the individual training sets. Lift curve analysis shows that the use of data driven ordering methods provides a statistically significant improvement over the use of the default, natural time step ordering. Significant time is saved for the end user by allowing an improved focus on areas of interest without the need to conventionally search all of the data. We have also found that using random forests weighted and distance-based outlier ensemble methods for supervised learning of anomaly detection provide significant accuracy improvements when compared to existing methods on the same dataset. Further, distance-based outlier and local outlier factor ensemble methods for unsupervised learning of anomaly detection also compare favorably to existing methods.
7D4DA857	This work describes a Web search approach taking into account the semantic content of Web pages. Eliminating irrelevant Web pages, the time-consuming task of revise the obtained results from actual search engines is reduced. The proposed approach is focused on Web pages that are not defined with semantic Web structure (most of the actual Web pages are in this format). The challenge is extract the semantic content from heterogeneous and human oriented Web pages. The approach integrates structures of ontologies, WordNet, and a hierarchical similarity measure to determine the relevance of a Web page.
7F83374A	Local business voice search is a popular application for mobile phones, where hands-free interaction and speed are critical to users. However, speech recognition accuracy is still not satisfactory when the number of businesses and locations is extended nationwide. For mobile users, searching a local business directory is often related to the fulfillment of specific tasks "on-the-move", such as finding a restaurant, a movie theater, or a retailer chain. Restricting the local search to specific domains improves the quality of search results. In this paper, we present a new approach to data selection for bootstrapping and optimizing language models for vertical business sectors by exploiting semantic knowledge encoded in the business database and in the business category taxonomy. We demonstrate that, in the case of queries in the restaurant domain and without collecting new data, speech recognition word accuracy improves by 9.5% relative when compared with a generic local business language model.
80359F5F	Among the challenges of searching the vast information source the Web has become, improving Web search efficiency by different strategies using semantics and the user generated data from Web 2.0 applications remains a promising and interesting approach. In this paper, we present the Personal Social Dataset and Ontology-guided Input strategies and couple them together, providing a proofof- concept implementation.
7D6DFDA0	Contextual retrieval is a critical technique for todaypsilas search engines in terms of facilitating queries and returning relevant information. This paper reports on the development and evaluation of a system designed to tackle some of the challenges associated with contextual information retrieval from the World Wide Web (WWW). The developed system has been designed with a view to capturing both implicit and explicit user data which is used to develop a personal contextual profile. Such profiles can be shared across multiple users to create a shared contextual knowledge base. These are used to refine search queries and improve both the search results for a user as well as their search experience. An empirical study has been undertaken to evaluate the system against a number of hypotheses. In this paper, results related to one are presented that support the claim that users can find information more readily using the contextual search system	
5D1A2FAD	The main purpose of analyzing the social network data is to observe the behaviors and trends that are followed by people. How people interact with each other, what they usually share, what are their interests on social networks, so that analysts can focus new trends for the provision of those things which are of great interest for people so in this paper an easy approach of gathering and analyzing data through keyword based search in social networks is examined using NodeXL and data is gathered from twitter in which political trends have been analyzed. As a result it will be analyzed that, what people are focusing most in politics.
79A2A305	The Internet is a widely used source of information for patients searching for medical/health care information. While many studies have assessed existing medical/health care information on the Internet, relatively few have examined methods for design and delivery of such websites, particularly those aimed at the general public.This study describes a method of evaluating material for new medical/health care websites, or for assessing those already in existence, which is correlated with higher rankings on Google's Search Engine Results Pages (SERPs).A website quality assessment (WQA) tool was developed using criteria related to the quality of the information to be contained in the website in addition to an assessment of the readability of the text. This was retrospectively applied to assess existing websites that provide information about generic medicines. The reproducibility of the WQA tool and its predictive validity were assessed in this studyThe WQA tool demonstrated very high reproducibility (intraclass correlation coefficient=0.95) between 2 independent users. A moderate to strong correlation was found between WQA scores and rankings on Google SERPs. Analogous correlations were seen between rankings and readability of websites as determined by Flesch Reading Ease and Flesch-Kincaid Grade Level scores.The use of the WQA tool developed in this study is recommended as part of the design phase of a medical or health care information provision website, along with assessment of readability of the material to be used. This may ensure that the website performs better on Google searches. The tool can also be used retrospectively to make improvements to existing websites, thus, potentially enabling better Google search result positions without incurring the costs associated with Search Engine Optimization (SEO) professionals or paid promotion.
7ECEAEEC	With web image search engines, we face a situation where the results are very noisy, and when we ask for a specific object, we are not ensured that this object is contained in all the images returned by the search engines: about 50% of the images returned are off-topic. In this paper, we explain how knowing the color of an object can help locating the object in images, and we also propose methods to automatically find the color of an object, so that the whole process can be fully automatic. Results reveal that this method allows us to reduce the noise in returned images while providing automatic segmentation so that it can be used for clustering or object learning.
7340259A	In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset. 
7D55F348	The size of the publicly indexable World Wide Web (WWW) has probably surpassed 14.3 billion documents and as yet growth shows no sign of leveling off. Search engines encounter the problem of ambiguity in words; therefore, search engines use ontology to find pages with words that are syntactically different but semantically similar. The knowledge provided by ontology is extremely useful in defining the structure and scope for mining web content. Context-ontology is a shared vocabulary to share context information in a pervasive computing domain and include machine-interpretable definitions of basic concepts in the domain and relations among them. This paper proposes an architecture for relevant searching of web documents using data mining techniques such as clustering and association rules. These techniques together with context and ontology extract potentially useful documents from the database. Also, an algorithm has been devised which shows the working in sequence of steps. Finally, the results are compared with the prevailing approaches and with the help of an example it has been seen that CODT is better in context of relevancy.
7FC2A67C	The Semantic Web, an extension of the current web,is an emerging field with the aim of building infrastructure,wherein software agents and people can work in cooperation by sharing knowledge.The emerging Semantic Web community has proposed ontology’s to express knowledge in a machine understandable way. Using this ontology, the search engine is proposed which combines content based search techniques with spread  activation techniques applied to a semantic model of a given domain. It is based on a hybrid spread activation algorithm applied to the concept instances graph to find the related concepts in the ontology based on the given query.
78B9FFFA	Various software packages are commonly used for the implementation and calculation of decision-analytic models for health economic evaluations. However, comparison of these programs with regard to ease of implementing a model is lacking.(i) to compare the assets and drawbacks of three commonly used software packages for Markov models with regard to ease of implementation; and (ii) to investigate how a technical model validation can be conducted by comparing the results of the three implementations.A Markov model on chronic obstructive pulmonary disease was implemented in TreeAge, Microsoft Excel and Arena with the same assumptions on model structure, transition probabilities and costs. A hypothetical smoking cessation programme for patients in stage 1 was evaluated against usual care. The packages were compared with respect to time and effort for implementation, run-time, features for the presentation of results, and flexibility. Agreement between the packages on average costs and life-years gained and on the incremental cost-effectiveness ratio was considered for technical validation in the form of expected values (between TreeAge and Excel only) and Monte Carlo simulations.Ease of implementation was best in TreeAge, whereas Arena offered the highest flexibility. Deterministic results were in agreement between TreeAge and Excel, as were simulated values between all three packages.Excel offers an intuitive spreadsheet interface, but the acquisition of and the training in TreeAge or Arena is worthwhile for more complex models. Double implementation is a practicable validation technique that should be conducted to ensure correct model implementation.
7F5443CD	In this paper, we study the correlation properties of the fading mobile radio channel. Based on these studies, we model the channel as a one-step Markov process whose transition probabilities are a function of the channel characteristics. Then we present the throughput performance of the Go-Back-N and selective-repeat automatic repeat request (ARQ) protocols with timer control, using the Markov model for both forward and feedback channels. This approximation is found to be very good, as confirmed by simulation results.
78744637	In this paper, we introduce a 3-D human-body tracker capable of handling fast and complex motions in real-time. The parameter space, augmented with first order derivatives, is automatically partitioned into Gaussian clusters each representing an elementary motion: hypothesis propagation inside each cluster is therefore accurate and efficient.  The transitions between clusters use the predictions of a Variable Length Markov Model which can explain high level behaviours over a long history. Using Monte-Carlo methods, evaluation of model candidates is critical for both speed and robustness.  We present a new evaluation scheme based on volumetric reconstruction and blobs-fitting, where appearance models and image evidences are represented by Gaussian mixtures.  We demonstrate the application of our tracker to long video sequences exhibiting rapid and diverse movements.
7D1020E8	An automatic-repeat-request (ARQ) Go-Back-N (GBN) protocol with unreliable feedback and time-out mechanism is studied, using renewal theory. Transmissions on both the forward and the reverse channels are assumed to experience Markovian errors. The exact throughput of the protocol is evaluated, and simulation results, that confirm the analysis, are presented. A detailed comparison of the proposed method and the commonly used transfer function method reveals that the proposed approach is simple and potentially more powerful.
80D7D606	The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
78AD17F6	Using a finite state Markov channel model, we develop an analytical method for evaluation of the packet error structure in multiple-input multiple-output (MIMO) systems based on singular value decomposition (SVD). We consider dual-branch MIMO systems, with either two transmit and arbitrary number of receive antennas, or arbitrary number of transmit and two receive antennas. The corresponding Markov model parameters are obtained using a novel closed-form expressions for probability density function and level crossing rate of the signal-to-noise ratio at the output of eigenchannels in a MIMO system, derived for a case of Rayleigh propagation, imperfect channel state information and any fixed power allocation. The exact bit error rate for the transmission of quadrature amplitude modulated (QAM) symbols through the eigenchannels is derived in polynomial closed form. Furthermore, by using the developed Markov model, the packet error statistics in the corresponding eigenchannels are determined, and the closed-form analytical expression for the system throughput is derived when ‘go-back-N’ automatic repeat request procedure is applied in time-varying eigenchannels. The analytical results are validated by using Monte Carlo simulations.
7D9B3729	Background. Serial period prevalence estimates for recurrent diseases such as major depression are available more frequently than fully detailed longitudinal data, but it is difficult to estimate incidence and episode duration from such data. Incidence and episode duration are critical decision modeling parameters for recurrent diseases. Objectives. To reduce bias that would otherwise occur in national incidence and duration-of-episode estimates for major depressive episodes deriving from studies using serial period prevalence data and to illustrate amethodological approach for the estimation of incidence from such studies. Methods. Monte Carlo simulation was applied to a Markov process describingincidence and recovery from major depressive episodes. Results. The annual incidence and episode duration were found to be 3.1% and 17.1 weeks, respectively. These estimates are expected to be less subject to bias than those generated without modeling. Conclusions. These results highlight the usefulness of Markov models for analysis of longitudinal data. The methods described here may be useful for decision modeling andmay be generalizable to other chronic diseases.
7E17B5B5	Most epidemiological studies of major depression report period prevalence estimates. These are of limited utility in characterizing the longitudinal epidemiology of this condition. Markov models provide a methodological framework for increasing the utility of epidemiological data. Markov models relating incidence and recovery to major depression prevalence have been described in a series of prior papers. In this paper, the models are extended to describe the longitudinal course of the disorder. Data from three national surveys conducted by the Canadian national statistical agency (Statistics Canada) were used in this analysis. These data were integrated using a Markov model. Incidence, recurrence and recovery were represented as weekly transition probabilities. Model parameters were calibrated to the survey estimates. The population was divided into three categories: low, moderate and high recurrence groups. The size of each category was approximated using lifetime data from a study using the WHO Mental Health Composite International Diagnostic Interview (WMH-CIDI). Consistent with previous work, transition probabilities reflecting recovery were high in the initial weeks of the episodes, and declined by a fixed proportion with each passing week. Markov models provide a framework for integrating psychiatric epidemiological data. Previous studies have illustrated the utility of Markov models for decomposing prevalence into its various determinants: incidence, recovery and mortality. This study extends the Markov approach by distinguishing several recurrence categories. 
7E61B32A	Depression is among the major contributors to worldwide disease burden and adequate modelling requires a framework designed to depict real world disease progression as well as its economic implications as closely as possible.In light of the specific characteristics associated with depression (multiple episodes at varying intervals, impact of disease history on course of illness, sociodemographic factors), our aim was to clarify to what extent "Discrete Event Simulation" (DES) models provide methodological benefits in depicting disease evolution.We conducted a comprehensive review of published Markov models in depression and identified potential limits to their methodology. A model based on DES principles was developed to investigate the benefits and drawbacks of this simulation method compared with Markov modelling techniques.The major drawback to Markov models is that they may not be suitable to tracking patients' disease history properly, unless the analyst defines multiple health states, which may lead to intractable situations. They are also too rigid to take into consideration multiple patient-specific sociodemographic characteristics in a single model. To do so would also require defining multiple health states which would render the analysis entirely too complex. We show that DES resolve these weaknesses and that its flexibility allow patients with differing attributes to move from one event to another in sequential order while simultaneously taking into account important risk factors such as age, gender, disease history and patients attitude towards treatment, together with any disease-related events (adverse events, suicide attempt etc.).DES modelling appears to be an accurate, flexible and comprehensive means of depicting disease progression compared with conventional simulation methodologies. Its use in analysing recurrent and chronic diseases appears particularly useful compared with Markov processes.
802AF8D6	We investigate the behavior of block errors which arise in data transmission on fading channels. Our approach takes into account the details of the specific coding/modulation scheme and tracks the fading process symbol by symbol. It is shown that a Markov approximation for the block error process (possibly degenerating into an identically distributed (i.i.d.) process for sufficiently fast fading) is a good model for a broad range of parameters. Also, it is observed that the relationship between the marginal error rate and the transition probability is largely insensitive to parameters such as block length, degree of forward error correction and modulation format, and depends essentially on an appropriately normalized version of the Doppler frequency. This relationship can therefore be computed in the simple case of a threshold model and then used more generally as an accurate approximation. This observation leads to a unified approach for the channel modeling, and to a simplified performance analysis of upper layer protocols.
7FD11145	Block-error processes in transmissions over slow-fading channels can be accurately modeled by a two-state Markov chain [the block Markov model (BMM)]. Another line of research has focused on the use of a channel-state Markov model (CSMM) to analyze block transmissions. Although both techniques provide results that agree well with observations, the relationship between both Markov models has not been recognized in the previous literature. In this letter, we show that the BMM for slow-fading channels can be directly derived from the CSMM. In addition, we introduce a greatly simplified channel-modeling methodology. In the new methodology, the BMM is the primary channel characterization tool, and the CSMM becomes essentially an estimation technique that provides parameters for the BMM. Results of packet transmissions in slow-fading channels show that our approach provides significant improvements in both accuracy and simplicity over previously proposed techniques.
7CBB23DF	Chronic obstructive pulmonary disease (COPD) is currently the fourth leading cause of death worldwide. It has serious health effects and causes substantial costs for society.The aim of the present paper was to develop a state-of-the-art decision-analytic model of COPD whereby the cost effectiveness of interventions in Germany can be estimated. To demonstrate the applicability of the model, a smoking cessation programme was evaluated against usual care. A seven-stage Markov model (disease stages I to IV according to the GOLD [Global Initiative for Chronic Obstructive Lung Disease] classification, states after lung-volume reduction surgery and lung transplantation, death) was developed to conduct a cost-utility analysis from the societal perspective over a time horizon of 10, 40 and 60 years. Patients entered the cohort model at the age of 45 with mild COPD. Exacerbations were classified into three levels: mild, moderate and severe. Estimation of stage-specific probabilities (for smokers and quitters), utilities and costs was based on German data where possible. Data on effectiveness of the intervention was retrieved from the literature. A discount rate of 3% was applied to costs and effects. Probabilistic sensitivity analysis was used to assess the robustness of the results.The smoking cessation programme was the dominant strategy compared with usual care, and the intervention resulted in an increase in health effects of 0.54 QALYs and a cost reduction of €1115 per patient (year 2007 prices) after 60 years. In the probabilistic analysis, the intervention dominated in about 95% of the simulations. Sensitivity analyses showed that uncertainty primarily originated from data on disease progression and treatment cost in the early stages of disease.e model developed allows the long-term cost effectiveness of interventions to be estimated, and has been adapted to Germany. The model suggests that the smoking cessation programme evaluated was more effective than usual care as well as being cost-saving. Most patients had mild or moderate COPD, stages for which parameter uncertainty was found to be high. This raises the need to improve data on the early stages of COPD.
7F603AAF	A method of dynamically constructing Markov chain models that describe the characteristics of binary messages is developed. Such models can be used to predict future message characters and can therefore be used as a basis for data compression. To this end, the Markov modelling technique is combined with Guazzo's arithmetic coding scheme to produce a powerful method of data compression. The method has the advantage of being adaptive: messages may be encoded or decoded with just a single pass through the data. Experimental results reported here indicate that.the Markov modelling approach generally achieves much better data compression than that observed with competing methods on typical computer data.
7F5E056C	Motivated by applications such as automated visual surveillance and video monitoring and annotation, there has been a lot of interest in constructing cognitive vision systems capable of interpreting the high level semantics of dynamic scenes. In this paper we present a novel approach for automatically inferring models of object interactions that can be used to interpret observed behaviour within a scene. A real-time low-level computer vision system, together with an attentional control mechanism, are used to identify incidents or events that occur in the scene. A data driven approach has been taken in order to automatically infer discrete and abstract representations (symbols) of primitive object interactions; effectively the system learns a set of qualitative spatial relations relevant to the dynamic behaviour of the domain. These symbols then form the alphabet of a VLMM which automatically infers the high level structure of typical interactive behaviour. The learnt behaviour model has generative capabilities and is also capable of recognizing typical or atypical activities within a scene. Experiments have been performed within the traffic monitoring domain; however the proposed method is applicable to the general automatic surveillance task since it does not assume a priori knowledge of a specific domain.
810600AC	Markov models are useful when a decision problem involves risk that is continuous over time, when the timing of events is important, and when important events may happen more than once. Representing such clinical settings with conventional decision trees is difficult and may require unrealistic simplifying assumptions. Markov models assume that a patient is always in one of a finite number of discrete health states, called Markov states. All events are represented as transitions from one state to another. A Markov model may be evaluated by matrix algebra, as a cohort simulation, or as a Monte Carlo simulation. A newer representation of Markov models, the Markov-cycle tree, uses a tree representation of clinical events and may be evaluated either as a cohort simulation or as a Monte Carlo simulation. The ability of the Markov model to represent repetitive events and the time dependence of both probabilities and utilities allows for more accurate representation of clinical settings that involve these issues. 
7E33249D	Smoking cessation is the only strategy that has shown a lasting reduction in the decline of lung function in patients with chronic obstructive pulmonary disease. This study aims to evaluate the cost-effectiveness of smoking cessation interventions in patients with chronic obstructive pulmonary disease, to assess the quality of the Markov models and to estimate the consequences of model structure and input data on cost–effectiveness. A systematic literature search was conducted in PubMed, Embase, BusinessSourceComplete and Econlit on June 11, 2014. Data were extracted, and costs were inflated. Model quality was evaluated by a quality appraisal, and results were interpreted. Ten studies met the inclusion criteria. The results varied widely from cost savings to additional costs of €17,004 per quality adjusted life year. The models scored best in the category structure, followed by data and consistency. The quality of the models seems to rise over time, and regarding the results there is no economic reason to refuse the reimbursement of any smoking cessation intervention 
81507EC1	In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.
804AD709	Most epidemiological studies of major depression report estimates of period prevalence. Such estimates are useful for public health applications, but are not very helpful for informing clinical practice. Period prevalence is determined predominantly by incidence and episode duration, but it is difficult to connect these epidemiological concepts to clinical issues such as risk and prognosis. Incidence is important for primary and secondary prevention, and prognostic information is useful for clinical decision-making. The objective of this study was to decompose period prevalence data for major depression into its constituent elements, thereby enhancing the value of these estimates for clinical practice. Data from a series of population-based Canadian studies were used in the analysis. Markov models depicting incidence, prevalence and recovery from major depressive episodes were developed. Monte Carlo simulation was used to constrain model parameters to the epidemiological data.The association of sex with major depression was found to be due to a higher incidence in women. In distinction, the higher prevalence in unmarried subjects was mostly due to a different prognosis. Age-related changes in prevalence were influenced by both factors. Education, which was not found to be associated with major depression in the survey data, had no impact either on risk or prognosis.The period prevalence of major depression is influenced both by incidence (risk) and episode duration (prognosis). Mathematical modeling of the underlying epidemiological relationships can make such data more readily interpretable in relation to clinical practice.
7EA1B380	A formula for the go-back-N ARQ (automatic repeat request) scheme applicable to Markov error patterns is derived. It is a generalization of the well-known efficiency formula p/(p+m(1-p)) (where m is the round trip delay in number of block durations and p is the block transmission success probability), and it has been successfully validated against simulation measurements. It is found that for a given error rate, error patterns having zero correlation between successive transmission generally fare better than those with negative correlation, and that error patterns with positive correlation fare better still. It is shown that the present analysis can be extended in a straightforward manner to cope with error patterns of a more complex nature. Simple procedures for numerical evaluation of efficiency under quite general error structures are presented.
7E8549A3	Markov models are often employed to represent stochastic processes, that is, random processes that evolve over time. In a healthcare context, Markov models are particularly suited to modelling chronic disease. In this article, we describe the use of Markov models for economic evaluation of healthcare interventions. The intuitive way in which Markov models can handle both costs and outcomes make them a powerful tool for economic evaluation modelling. The time component of Markov models can offer advantages of standard decision tree models, particularly with respect to discounting. This paper gives a comprehensive description of Markov modelling for economic evaluation, including a discussion of the assumptions on which the type of model is based, most notably the memoryless quality of Markov models often termed the 'Markovian assumption'. A hypothetical example of a drug intervention to slow the progression of a chronic disease is employed to demonstrate the modelling technique and the possible methods of analysing Markov models are explored. Analysts should be aware of the limitations of Markov models, particularly the Markovian assumption, although the adept modeller will often find ways around this problem.
7F8D3B50	In recent years there has been an increased interest in the modelling and recognition of human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approach is presented for automatically acquiring stochastic models of the high-level structure of an activity without the assumption of any prior knowledge. The process involves temporal segmentation intoplausible atomic behaviour com-ponents and the use of variable length Markov models for the efficient rep-resentation of behaviours. Experimental results are presented which demon-strate the generation of realistic sample behaviours and evaluate the perfor-mance of models for long-term temporal prediction.
7F79AB85	This paper investigates the properties of a method for obtaining Markov models of unspecified order to be applied to narrow-band fading channels with additive white Gaussian noise. The models are obtained by applying the context tree pruning algorithm to experimental or simulated sequences. Fading environments are identified in which the extension from first-order to higher order models is justified. The paper presents, as examples, the evaluation of the covariance function and the packet error distribution.
7C7C2B02	To date, decision trees and Markov models have been the most common methods used in pharmacoeconomic evaluations. Both of these techniques lack the flexibility required to appropriately represent clinical reality. In this paper an alternative, more natural, way to model clinical reality — discrete event simulation — is presented and its application is illustrated with a real world example.A discrete event simulation represents the course of disease very naturally, with few restrictions. Neither mutually exclusive branches nor states are required, nor is a fixed cycle. All relevant aspects can be incorporated explicitly and efficiently. Flexibility in handling perspectives and carrying out sensitivity analyses, including structural variations, is incorporated and the entire model can be presented very transparently. The main limitations are imposed by lack of data to fit realistic models.Discrete event simulation, though rarely employed in pharmacoeconomics today, should be strongly considered when carrying out economic evaluations, particularly those aimed at informing policy makers and at estimating the budget impact of a pharmaceutical intervention.
0A7B0D45	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning, in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is founded on an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a typical Smart Environment – the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
5D7D351E	Intuitively, any ‘bag of words’ approach in IR should benefit from taking term dependencies into account. Unfortunately, for years the results of exploiting such dependencies have been mixed or inconclusive. To improve the situation, this paper shows how the natural language properties of the target documents can be used to transform and enrich the term dependencies to more useful statistics. This is done in three steps. The term co-occurrence statistics of queries and documents are each represented by a Markov chain. The paper proves that such a chain is ergodic, and therefore its asymptotic behavior is unique, stationary, and independent of the initial state. Next, the stationary distribution is taken to model queries and documents, rather than their initial distributions. Finally, ranking is achieved following the customary language modeling paradigm. The main contribution of this paper is to argue why the asymptotic behavior of the document model is a better representation then just the document’s initial distribution. A secondary contribution is to investigate the practical application of this representation in case the queries become increasingly verbose. In the experiments (based on Lemur’s search engine substrate) the default query model was replaced by the stable distribution of the query. Just modeling the query this way already resulted in significant improvements over a standard language model baseline. The results were on a par or better than more sophisticated algorithms that use fine-tuned parameters or extensive training. Moreover, the more verbose the query, the more effective the approach seems to become.
7A42DD4C	Recently, attention has been focused on spatial databases which combine conventional and spatial data. The need for spatial query languages has been identified in several different application domain as Geographic Information Systems (GISs) and Image Databases. Several extensions to the relational database query language SQL have been proposed to serve as a spatial query language. The large availability on the market place of the relational database technology is the major reason why an SQL-based spatial query language is welcome both from the GIS vendors and the GIS user community. Recent SQL extensions convinced us that it is the right time for working on the standardization of SQL-based spatial query languages. This paper sets a kernel of basic features for the creation of a standard and compares a large number of SQL spatial extensions according to such features.
7559C9A4	This paper proposes an algorithm, called sequence prediction via enhanced episode discovery (SPEED), to predict inhabitant activity in smart homes. SPEED is a variant of the sequence prediction algorithm. It works with the episodes of smart home events that have been extracted based on the on -off states of home appliances. An episode is a set of sequential user activities that periodically occur in smart homes. The extracted episodes are processed and arranged in a finite-order Markov model. A method based on prediction by partial matching (PPM) algorithm is applied to predict the next activity from the previous history. The result shows that SPEED achieves an 88.3% prediction accuracy, which is better than LeZi Update, Active LeZi, IPAM, and C4.5.
7FDEABB6	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.
7D5544BA	An intelligent home is likely in the near future. A n important ingredient in an intelligent environment such as a home is prediction - of the next action, the next l ocation, and the next task that an inhabitant is likely to p erform. In this paper we describe our approach to solving the problem of predicting inhabitant behavior in a smart home. We model the inhabitant actions as states in a simple Markov model, then improve the model by supplying it with data from discovered high-level inhabitant tasks. For si mulated data we achieved good accuracy, whereas on real dat a we had marginal performance. We also investigate clust ering of actions and subsequently predict the next action an d the task with hidden Markov models created using the clusters.
00BA8AB5	MUSART is a research project developing and studying new techniques for music information retrieval.The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling,melodic contour, and phonetic streams for music retrieval.To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music.Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.
797DF42C	This paper proposes a new approach to combined spatial (Intra) prediction and adaptive transform coding in block-based video and image compression. Context-adaptive spatial prediction from available, previously decoded boundaries of the block, is followed by optimal transform coding of the prediction residual. The derivation of both the prediction and the adaptive transform for the prediction error, assumes a separable first-order Gauss-Markov model for the image signal. The resulting optimal transform is shown to be a close relative of the sine transform with phase and frequencies such that basis vectors tend to vanish at known boundaries and maximize energy at unknown boundaries. The overall scheme switches between the above sine-like transform and discrete cosine transform (per direction, horizontal or vertical) depending on the prediction and boundary information. It is implemented within the H.264/AVC intra mode, is shown in experiments to significantly outperform the standard intra mode, and achieve significant reduction of the blocking effect.
799E3C70	This paper presents a software simulator applicable to multipath fading channels in urban environments of mobile communication networks. The simulator is constructed by a two-state Markov model and several statistical models for simulating the characterizations of different environments. A core idea of the simulator is to construct a Rice distribution-based multipath fading module produced by a modified Gans Doppler power spectrum, and in combination with a Markov model to predict the time-dependent characteristics of packet in different radio circumstances. It can simply predict the packet performance of the future channel and evaluate the relations between the radio channel and the modulation schemes, error control protocols and channel coding. Simulation results demonstrate that it is a reliable and efficient method.
80567ABE	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning,in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events  that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is  founded  on  an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a  typical Smart Environment–the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of  this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.  
762DB3E0	The measurement of individual single-channel events arising from the gating of ion channels provides a detailed data set from which the kinetic mechanism of a channel can be deduced. In many cases, the pattern of dwells in the open and closed states is very complex, and the kinetic mechanism and parameters are not easily determined. Assuming a Markov model for channel kinetics, the probability density function for open and closed time dwells should consist of a sum of decaying exponentials. One method of approaching the kinetic analysis of such a system is to determine the number of exponentials and the corresponding parameters which comprise the open and closed dwell time distributions. These can then be compared to the relaxations predicted from the kinetic model to determine, where possible, the kinetic constants. We report here the use of a linear technique, linear prediction/singular value decomposition, to determine the number of exponentials and the exponential parameters. Using simulated distributions and comparing with standard maximum-likelihood analysis, the singular value decomposition techniques provide advantages in some situations and are a useful adjunct to other single-channel analysis techniques.
75C9738A	The binding of regulatory proteins to their specific DNA targets determines the accurate expression of the neighboring genes. The in silico prediction of new binding sites in completely sequenced genomes is a key aspect in the deeper understanding of gene regulatory networks. Several algorithms have been described to discriminate against false-positives in the prediction of new binding targets; however none of them has been implemented so far to assist the detection of binding sites at the genomic scale.FITBAR (Fast Investigation Tool for Bacterial and Archaeal Regulons) is a web service designed to identify new protein binding sites on fully sequenced prokaryotic genomes. This tool consists in a workbench where the significance of the predictions can be compared using different statistical methods, a feature not found in existing resources. The Local Markov Model and the Compound Importance Sampling algorithms have been implemented to compute the P-value of newly discovered binding sites. In addition, FITBAR provides two optimized genomic scanning algorithms using either log-odds or entropy-weighted position-specific scoring matrices. Other significant features include the production of a detailed genomic context map for each detected binding site and the export of the search results in spreadsheet and portable document formats. FITBAR discovery of a high affinity Escherichia coli NagC binding site was validated experimentally in vitro as well as in vivo and publishedFITBAR was developed in order to allow fast, accurate and statistically robust predictions of prokaryotic regulons. This feature constitutes the main advantage of this web tool over other matrix search programs and does not impair its performance.
5B14A4B4	Proactive User Interfaces (PUIs) aim at facili- tating the interaction with a user interface, e.g., by highlighting fields or adapting the interface. For that purpose, they need to be able to pre- dict the next user action from the interaction his- tory. In this paper, we give an overview of se- quence prediction algorithms (SPAs) that are ap- plied in this domain, and build upon them to de- velop two new algorithms that base on combin- ing different order Markov models. We iden- tify the special requirements that PUIs pose on these algorithms, and evaluate the performance of the SPAs in this regard. For that purpose, we use three datasets with real usage-data and syn- thesize further data with specific characteristics. Our relatively simple yet efficient algorithm FxL performs extremely well in the domain of SPAs which make it a prime candidate for integration in a PUI. To facilitate further research in this field, we provide a Perl library that contains all presented algorithms and tools for the evaluation. 
80B4014C	The SAM-T04 method for predicting protein structures uses a single protocol across the entire range of targets, from comparative modeling to new folds. This protocol is similar to the SAM-T02 protocol used in CASP5, but has improvements in the iterative search for similar sequences in finding and aligning templates, in creating fragment libraries, in generating protein conformations, and in scoring the conformations. The automatic procedure made some improvements over simply selecting an alignment to the highest-scoring template, and human intervention made substantial improvements over the automatic procedure. The main improvements made by human intervention were from adding constraints to build (or retain) beta-sheets and from splitting multidomain proteins into separate domains. The uniform protocol was moderately successful across the entire range of target difficulty, but was somewhat less successful than other approaches in CASP6 on the comparative modeling targets.
7BCB90E0	Augmenting accurate prediction of channel attenuations can be of immense value in improving the quality of signals at high frequency for satellite communication networks. Such prediction of weather related attenuation factors for the impending weather conditions based on the weather data and the Markovian theory are the main object of this paper. The paper also describes an intelligent weather aware control system (IWACS) that is used to employ the predictions made from Markov model to maintain the quality of service (QoS) in channels that are impacted by rain, gaseous, cloud, fog, and scintillation attenuations. Based on that, a three dimensional relationship is proposed among estimated atmospheric attenuations, propagation angle, and predicted rainfall rate at a given location and operational frequency. This novel method of predicting weather characteristics supplies valuable data for mitigation planning, and subsequently for developing an algorithm to iteratively tune the IWACS by adaptively selecting appropriate channel frequency, modulation, coding, propagation angle, transmission power level, and data transmission rate to improve the satellite's system performance. Some simulation results are presented to show the effectiveness of the proposed scheme. 
7E5073A1	Efficient intra prediction is an important aspect of video coding with high compression efficiency. H.264/AVC applies directional prediction from neighboring pixels on an adjustable block size for local decorrelation. In this paper, we present an extended prediction scheme in the context of H.264/AVC that comprises two additional prediction methods exploiting self-similar properties of the encoded texture. A new macroblock type is implemented, allowing for flexible selection of the available prediction methods for sub-partitions of the macroblock. Depending on the content of the encoded video sequence, substantial gains in rate-distortion performance are achieved. The results may indicate directions towards an enhanced intra coding scheme with improved rate-distortion performance.
7EAB152E	Tropical Pacific sea surface temperatures (SSTs) and the accompanying El Niño–Southern Oscillation phenomenon are recognized as significant components of climate behavior. The atmospheric and oceanic processes involved display highly complicated variability over both space and time. Researchers have applied both physically derived modeling and statistical approaches to develop long-lead predictions of tropical Pacific SSTs. The comparative successes of these two approaches are a subject of substantial inquiry and some controversy. Presented in this article is a new procedure for long-lead forecasting of tropical Pacific SST fields that expresses qualitative aspects of scientific paradigms for SST dynamics in a statistical manner. Through this combining of substantial physical understanding and statistical modeling and learning, this procedure acquires considerable predictive skill. Specifically, a Markov model, applied to a low-order (empirical orthogonal function–based) dynamical system of tropical Pacific SST, with stochastic regime transition, is considered. The approach accounts explicitly for uncertainty in the formulation of the model, which leads to realistic error bounds on forecasts. The methodology that makes this possible is hierarchical Bayesian dynamical modeling.
7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
7F48E77D	With the proliferation of social media, consumers  cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers  search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
00B17298	The search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
595640C9	In order to retrieve an item of information on the Web, many search engines have been proposed. They are rarely efficient at the first attempt: the display of results “forces” the user to navigate. In parallel, Web query languages have been developed to avoid these two sequential phases: research then navigation. In this context, the QIRI@D experimental platform, based on the functional programming language SgmlQL, enables both information retrieval and manipulation of distributed semi-structured documents published on a sub-network made up of the sites where QIRI@D is running. It is possible in an unique query to specify criteria to find a document, to filter it, to extract parts of it for building the result. An automatical enrichment of any published document is used to improve the search efficiency.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the documentside (type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.
58CF4DA7	Clustering of data has numerous applications and has been studied extensively. Though most of the algorithms in the literature are sequential, many parallel algorithms have also been designed. In this paper, we present parallel algorithms with better performance than known algorithms. We consider algorithms that work well in the worst case as well as algorithms with good expected performance.
6FA7B132	Clustering of data has numerous applications and has been studied extensively. It is very important in Bioinformatics and data mining. Though many parallel algorithms have been designed, most of algorithms use the CRCW-PRAM or CREW-PRAM models of computing. This paper proposed a parallel EREW deterministic algorithm for hierarchical clustering. Based on algorithms of complete graph and Euclidean minimum spanning tree, the proposed algorithms can cluster n objects with O(p) processors in O(n2/p) time where 1≤ p ≤ nlogn. Performance comparisons show that our algorithm is the first algorithm that is both without memory conflicts and adaptive.
7CE77E1C	Hierarchical agglomerative clustering (HAC) is a clustering method widely used in various disciplines from astronomy to zoology. HAC is useful for discovering hierarchical structure embedded in input data. The cost of executing HAC on large data is typically high, due to the need for maintaining global inter-cluster distance information throughout the execution. To address this issue, we propose a new parallelization scheme for multi-threaded shared-memory machines based on the concept of nearest-neighbor (NN) chains. The proposed multi-threaded algorithm allocates available threads into two groups, one for managing NN chains and the other for updating distance information. In-depth analysis of our approach gives insight into the ideal configuration of threads and theoretical performance bounds. We evaluate our proposed method by testing it with multiple public datasets and comparing its performance with that of several alternatives. In our test, the proposed method completes hierarchical clustering 3.09-51.79 times faster than the alternatives. Our test results also reveal the effects of performance-limiting factors such as starvation in chain growing, overhead incurred from using synchronization locks, and hardware aspects including memory-bandwidth saturation. According to our evaluation, the proposed scheme is effective in improving the HAC algorithm, achieving significant gains over the alternatives in terms of runtime and scalability.
79AFD2A7	We introduce information retrieval strategies which are based on automatic hierarchic clustering of documents. We discuss the evaluation of retrieval strategies and show, using a subset of the Cranfield Aeronautics document collection, that cluster-based retrieval strategies can be devised which are as effective as linear associative retrieval strategies and much more efficient. Finally, we outline how cluster-based retrieval may be extended to large growing document collections and indicate some ways in which the effectiveness of cluster-based retrieval strategies may be improved.
7E45BA76	Two years after the first edition, a new Fingerprint Verification Competition (FVC2002) was organized by the authors, with the aim of determining the state-of-the-art in this challenging pattern recognition application. The experience and the feedback received from FVC2000 allowed the authors to improve the organization of FVC2002 and to capture the attention of a significantly higher number of academic and commercial organizations (33 algorithms were submitted). This paper discusses the FVC2002 database, the test protocol and the main differences between FVC2000 and FVC2002. The algorithm performance evaluation will be presented at the 16/sup th/ ICPR.
7FA45194	In style-constrained classification often there are onlya few samples of each style and class, and the correspondencesbetween styles in the training set and the test setare unknown. To avoid gross misestimates of the classifierparameters it is therefore important to model the patterndistributions accurately. We offer empirical evidence for intuitivelyappealing assumptions, in feature spaces appropriatefor symbolic patterns, for (1) tetrahedral configurationsof class means that suggests linear style-adaptive classification,(2) improved estimates of classification boundariesby taking into account the asymmetric configuration of thepatterns with respect to the directions toward other classes,and (3) pattern-correlated style variability.
759199FF	Fingerprint enhancement is a key issue in fingerprint minutiae extraction due to various image qualities. Most of exist fingerprint enhancement algorithms need to estimate the ridge orient and ridge distance, which are used to design the enhancement filter. However ridge distance estimation is a rather dffIcult task due to the fingerprint image quality and singularity, therefbre, the ridge distance estimate usually fails in those case and leads to enhancement algorithm failed. In this paper, we propose an AM-FM basedfingerprint image enhancement algorithm, which uses a novel Dominate Component Analysis (DGA,) technique to estimate the dominate component, and uses band pass filter to enhance those component rather than directly estimate the ridge distance, ridge orientation. Experiment results show that our enhancement algorithm leads to signflcant image quality improvement as well as the system efficient. 
7E7B912E	Fingerprint identification is based on two basic premises: (1) persistence and (2) individuality. We address the problem of fingerprint individuality by quantifying the amount of information available in minutiae features to establish a correspondence between two fingerprint images. We derive an expression which estimates the probability of a false correspondence between minutiae-based representations from two arbitrary fingerprints belonging to different fingers. Our results show that (1) contrary to the popular belief, fingerprint matching is not infallible and leads to some false associations, (2) while there is an overwhelming amount of discriminatory information present in the fingerprints, the strength of the evidence degrades drastically with noise in the sensed fingerprint images, (3) the performance of the state-of-the-art automatic fingerprint matchers is not even close to the theoretical limit, and (4) because automatic fingerprint verification systems based on minutia use only a part of the discriminatory information present in the fingerprints, it may be desirable to explore additional complementary representations of fingerprints for automatic matching.
80EC24E2	As a global feature of fingerprints, the orientation field is very important for automatic fingerprint recognition. Many algorithms have been proposed for orientation field estimation, but their results are unsatisfactory, especially for poor quality fingerprint images. In this paper, a model-based method for the computation of orientation field is proposed. First a combination model is established for the representation of the orientation field by considering its smoothness except for several singular points, in which a polynomial model is used to describe the orientation field globally and a point-charge model is taken to improve the accuracy locally at each singular point. When the coarse field is computed by using the gradient-based algorithm, a further result can be gained by using the model for a weighted approximation. Due to the global approximation, this model-based orientation field estimation algorithm has a robust performance on different fingerprint images. A further experiment shows that the performance of a whole fingerprint recognition system can be improved by applying this algorithm instead of previous orientation estimation methods.
77610135	Algorithms are identified which are best suited for an automatic fingerprint recognition system operating on low quality images. New preprocessing algorithms for noise removal and binarization are described. Three approaches to classification are investigated: a correlation classifier, and two feature-based classification schemes. The best results on a database of 80 fingerprints are obtained with spatial-frequency features. Three classifiers (neural net, linear classifier and nearest neighbour) using these features are successful in identifying an independent test set. Details of the results are shown. In conclusion suggestions are made concerning the most suitable algorithms in each of the processing steps.
77FB0890	Fingerprint matching is the most important part in the field of fingerprint recognition. In this paper, a novel fingerprint matching algorithm based on the probabilistic graphical model and 3-tree model is proposed. First, minutiae matching problems are considered as a special point-set matching. Fingerprint minutiae are viewed as random variables. Each minutia pairs have some probability to be matched. Second, an algorithm is proposed to generate the graphical model and choose "signal points", which dynamically have corresponding points in other point set. We choose three base minutiae pairs as signal pairs. Third, the model is converted into a Junction Tree. A 3-tree model is built and the potentials of other minutiae pairs are calculated through Junction Tree (J.T.) algorithm. Then we translate the matching problem into the best matching problem of a weighted bipartite graph. Finally, the number of common matching pairs can be got through maximum flow algorithm. The similarity of two fingerprints is evaluated using the number of common matching pairs and the maximal posteriori probability. In order to deal with part-matching problems, we use the smallest convex hull which contains all the matched minutiae. Experiments evaluated on FVC 2004 show both effectiveness and efficiency of our methods.
855DC306	Fingerprint recognition has found a reliable application for verification or identification of people in biometrics. Globally, fingerprints can be viewed as valuable traits due to several perceptions observed by the experts; such as the distinctiveness and the permanence on humans and the performance in real applications. Among the main stages of fingerprint recognition, the automated matching phase has received much attention from the early years up to nowadays. This paper is devoted to review and categorize the vast number of fingerprint matching methods proposed in the specialized literature. In particular, we focus on local minutiae-based matching algorithms, which provide good performance with an excellent trade-off between efficacy and efficiency. We identify the main properties and differences of existing methods. Then, we include an experimental evaluation involving the most representative local minutiae-based matching models in both verification and evaluation tasks. The results obtained will be discussed in detail, supporting the description of future directions.
811D3336	Since there are many possible types of nonlinear charge-flux constitutive relations of ideal memristors, such elements can manifest various behavior, and the identification of typical memristor fingerprints from the measured data or simulator outputs can be difficult in some cases. The aim of this paper is to reveal several fingerprints of ideal memristors, which extend the repertoire of hitherto published and currently well-known memristor fingerprints. These results can be useful for a clear and fast identification of a system behavior that violates the principles of the operation of ideal memristors.
7E655DA1	In automatic fingerprint identification systems poor quality impressions and latents often require expert interpretation if usable encodings are to be obtained. When this is necessary there are basically two different approaches taken for data entry "direct" encoding of the minutiae and "tracing" the ridges. The minutiae identified using these two methods on a test set of 49 latents were compared. The results indicate that these two methods are not just alternative ways of entering information but are radically different approaches which often lead to substantially different interpretations of the impression. The results of running searches on the two sets of minutiae using the Home Office matching algorithm do not indicate that either method is to be preferred from the point of view of accuracy. This conclusion is reinforced when the encodings are compared directly by eye with the matching impressions. Interpreting and encoding an impression viewed in isolation, by whatever method, is a fundamentally different task to the iterative process used by fingerprint experts to determining a match. There is scope for improving the matching accuracy of automatic fingerprint systems. The view is presented that a significant improvement could be achieved if algorithms are developed which are capable of iterative interpretation of the encoded or original data.
78D2A528	Accurate fingerprint orientation is a prerequisite in fingerprint based recognition system. This paper proposes an algorithm for modeling the fingerprint orientation field by using a model based algorithm based on the weighted Legendre basis. Weights required in the modeling are obtained by using symmetric filters, such that: i) high weights should be assigned to the areas near singular points, ii) areas having uniform ridge-valley flow should be given high weights, and iii) areas containing bad quality due to dry/ wet fingerprints, scars, bruises or sensor condition should be given low weights. These conditions ensure accurate reconstruction of fingerprint orientation field for bad quality areas while preserving the true orientation field near singular points. The proposed algorithm has been evaluated on a publicly available database, FVC2004 DB1A. Experimental results reveal that it has better orientation field estimation compared to the various state of the art algorithms.
80AAD859	We study randomized fingerprinting codes that achieve the fundamental capacity limits subject to the so-called Boneh-Shaw marking assumption. Two decoding schemes are studied in particular: the joint decoder is capacity-achieving but computationally intense, while the simple decoder is suboptimal but efficient. We provide tight bounds as well as numerical results for capacities and study the difference between these two schemes. Finally, security strategies for both the fingerprint embedders and the collusive attackers are presented.
80F2D5E3	In this paper, we studied the algorithms of fingerprint image enhancement and put forward a new method to calculate the direction image. Firstly, Radon Transform is used to estimate the direction image, then, the image is filtered by a set directional filters, thus, we get the enhanced image. The experiment results indicate that this method is simple and efficient; which established the firmed foundation to fingerprint image matching.
7A57F0D6	Several years ago the Federal Bureau of Investigation initiated the development of a reader to extract detailed information from single fingerprints. The fine details or minutiae consist of the position and angle of ridge endings and bifurcations, A free standing laboratory reader now exists which accepts an inked fingerprint at its input and outputs this minutia data.
7E9C0F63	In local area network (LAN), the large-scaled peer-to-peer (P2P) application has aroused our attention by its convenience as well as the problems inborn. The network traffic and the resources consumed have severely influenced the usage of other network application, and this makes the study of identification on P2P traffic much more significant. This article advances an identification method based on the host process fingerprint, which can be done on the client computer with P2P application on. Experiments show that method has a high classification capability on the P2P traffic in LAN.
7CC2B662	Fingerprint recognition using a joint transform correlator (JTC) is the most well-known technology among optical fingerprint recognition methods. The JTC method optically compares the reference fingerprint image with the sample fingerprint image then examines match or non-match by acquiring a correlation peak. In contrast to the JTC method, this paper presents a new method to examine fingerprint recognition by producing a computer generated hologram (CGH) of those two fingerprint images and directly comparing them. As a result, we present some parameters to show that fingerprint recognition capability of the CGH direct comparison method is superior to that of the JTC method
7FE858F6	Orientation fields can be used to describe interleaved ridge and valley patterns of fingerprint image, providing features useful for fingerprint recognition. However, for tasks such as fingerprint indexing, additional image alignment is often required to avoid confounding effects caused by pose differences. In this paper, we propose to employ a set of polar complex moments (PCMs) for extraction of rotation invariant fingerprint representation. PCMs are capable of describing fingerprint ridge flow structures, including singular regions, and are tolerant to spurious orientations in noisy fingerprints. From the orientation fields, a set of rotation moment invariants are derived to form a feature vector for comprehensive fingerprint structural description. This feature vector gives a compact and rotation invariant representation that is important for pose-robust fingerprint indexing. A clustering-based fingerprint indexing scheme is employed to facilitate efficient and effective retrieval of the most likely candidates from a fingerprint database. Our experimental results on NIST and FVC fingerprint databases indicate that the proposed invariant representation improves the performance of fingerprint indexing as compared to state-of-the-art methods.
81255706	The uniqueness and randomness of Physical Unclonable Functions (PUFs) allow secret keys to be stored in a tamper-proof package. In this paper, we propose a design for using a 65nm 10T sub-threshold Static Random Access Memory (SRAM) as a PUF. For this PUF, the challenge is the sub-threshold supply voltage and the response is the fingerprint obtained from the initial values of the cells when powered-up. In this design, we achieve significant improvements in power consumption and security over existing designs which makes it promising for low-power security applications.
7E0D9EF7	According to the characteristics of fingerprint images, combined with the human visual properties, this paper presents a de-noising algorithm of fingerprint image processing to decrease the influence of noises. Based on the different characteristics between the noise points and image features, the algorithm decreased the noises of the source images and filtered the corresponding noise coefficients after the wavelet decomposition. As for the coefficient matrices following the wavelet decomposition, after calculating all local gradients of the coefficients, we established the algorithm by using the local gradients of the source images as the judgment basis, and chose the highest Gradient coefficient in the different directions of the source images as the final fusion coefficient. The experimental results indicate that the algorithm can promote the SNR of the source images, protect the details of the images and improve the visual effects.
7812615D	We describe the design and implementation of an online fingerprint verification system which operates in two stages: (i) minutia extraction and (ii) minutia matching. An improved minutia extraction algorithm that is much faster and more accurate than our earlier algorithm has been implemented. For minutia matching, an alignment-based elastic matching algorithm has been developed. This algorithm is capable of finding the correspondences between input minutiae and the stored template without resorting to exhaustive search and has the ability to adaptively compensate for the nonlinear deformations and inexact pose transformations between finger prints. The system has been tested on two sets of finger print images captured with inkless scanners. The verification accuracy is found to be over 99% with a 15% reject rate. Typically, a complete fingerprint verification procedure takes, on an average, about 8 seconds on a SPARC 20 workstation. It meets the response time requirements of on-line verification with high accuracy.
3013FD47	Nowadays, there is a great bunch of applications implementing authentication processes in order to protect user access and users' data. Accessing our mobile device, doing a bank transfer or getting engaged with e-commerce are examples of how the great majority of population is in touch with an authentication process. The most common system to log in web applications is the use of pairs (username, password). It could be said that is rather contended but at the same time could imply problems, normally related to the strength or weakness of the used passwords. On the one hand, if strong passwords are used, users tend to forget them. On the other hand, if weak passwords are used, a non-legitimate user could impersonate a legitimate account attempting against the user's privacy. Bearing this in mind and taking into account that biometrics systems are securer and by far more user-friendly, in this TFG we propose an implementation of a web authentication process based on users' fingerprints. The proposed system is an externalized and standardized solution and, therefore, webApps wishing to use it could be easily integrated. For demonstration purposes, we have implemented three different entities: first, an authentication and authorization server, the authServer; second, a mobile application which stands for the authClient; and finally third, a dummy web application, the webApp. Being the webApp the application that the client will see in its device, the registry and authentication process will be performed by the authClient and authServer in a barely imperceptive way for the user.
7FDE1A1F	Proposes a fingerprint minutia matching technique, which matches the fingerprint minutiae by using both the local and global structures of minutiae. The local structure of a minutia describes a rotation and translation invariant feature of the minutia in its neighborhood. It is used to find the correspondence of two minutiae sets and increase the reliability of the global matching. The global structure of minutiae reliably determines the uniqueness of fingerprint. Therefore, the local and global structures of minutiae together provide a solid basis for reliable and robust minutiae matching. The proposed minutiae matching scheme is suitable for an online processing due to its high processing speed. Experimental results show the performance of the proposed technique.
7576667A	The estimation of fingerprint ridge orientation is an essential step in every automatic fingerprint verification system. The importance of ridge orientation can be deflected from the fact that it is inevitably used for detecting, describing and matching fingerprint features such as minutiae and singular points. In this paper we propose a novel method for fingerprint ridge orientation modelling using Legendre polynomials. One of the main problems it addresses is smoothing orientation data while preserving details in high curvature areas, especially singular points. We show that singular points, which result in a discontinuous orientation field, can be modelled by the zero-poles of Legendre polynomials. The models parameters are obtained in a two staged optimization procedure. Another advantage of the proposed method is a very compact representation of the orientation field, using only 56 coefficients. We have carried out extensive experiments using a state-of-the-art fingerprint matcher and a singular point detector. Moreover, we compared the proposed method with other state-of-the-art fingerprint orientation estimation algorithms. We can report significant improvements in both singular point detection and matching rates.
7213B07D	In this study, two techniques that can improve the authentication process are examined: (i) multiple samples and (ii) multiple biometric sources. We propose the fusion of multiple samples obtained from multiple biometric sources at the score level. By using the average operator, both the theoretical and empirical results show that integrating as many samples and as many biometric sources as possible can improve the overall reliability of the system. This strategy is called the multi-sample multi-source approach. This strategy was tested on a real-life database using neural networks trained in one-versus-all configuration.
812C84C8	Gait is a potential behavioral feature, and many allied studies have demonstrated that it can be served as a useful biometric feature for recognition. This paper described a novel gait recognition technique based on support vector machine fusion of contour projection and skeleton model features. A principal component analysis method was used to lower the dimension of contour projection after segmenting silhouettes from the background in the key frame of gait picture sequence and a skeleton model was built to produce other shape features. The combining features were fused by a support vector machine and tested on the CASIA database at the feature level and decision level based on posterior probability. Experimental results have demonstrated the effectiveness and advantages of the proposed algorithm.
7F20C72C	Modern fingerprint image compression and reconstruction standards used by the US Federal Bureau of Investigation (FBI) are based upon the popular 9/7 discrete wavelet transform. Multiresolution analysis tools have been successfully applied for fingerprint image compression for more than a decade; we propose a novel fingerprint image compression technique based on recently proposed wave atoms decomposition. Wave atoms decomposition has specifically been designed for enhanced representation of oscillatory patterns to convey temporal and spatial information. Our proposed compression scheme is based upon linear vector quantization of decomposed wave atoms representation of fingerprint images. Later quantized information is encoded with arithmetic entropy scheme. The proposed image compression standard outperforms the FBI fingerprint image compression standard, the wavelet scalar quantization (WSQ). Data mining, law enforcement, border security, and forensic applications can potentially benefit from our proposed compression scheme.
7BFF86A8	Noting the advantages of texture-based features over the structural descriptors of vascular trees, we investigated texture-based features from gray level cooccurrence matrix (GLCM) and various wavelet packet energies to classify retinal vasculature for biometric identification. Wavelet packet energy features were generated by Daubechies, Coiflets and Reverse Biorthogonal wavelets. Two different entropy methods, Shannon and logarithm of energy, were used to prune wavelet packet decomposition trees. Next, wrapper methods were used for classification-guided feature selection. Features were ranked based on area under the receiver operating curves, Bhattacharya, and t-test metrics. Using the ranked lists, wrapper methods were used in conjunction with Naïve Bayesian, k-nearest neighbor (k-NN), and Support Vector Machine (SVM) classifiers. Best results were achieved by using features from Reverse Biorthogonal 2.4 wavelet packet decomposition in conjunction with a nearest neighbor classifier, yielding a 3-fold cross validation accuracy of 99.42% with a sensitivity and specificity of 98.33% and 99.47% respectively
76A7D81C	With the development of business activities, the property rights protection for digital content becomes a hot topic. In prior researches, digital fingerprinting techniques are widely used. They find the illegal distributors by traitor tracing techniques. But in the emerging digital wholesale and retail, it is possible that middlemen collude. Then it becomes a development tendency that the fingerprint is added with the group property which improves the group detection performance. Considering the largely existed COX fingerprint which has low computation cost and easy realization, it will save large resources that we classify them to generate the group fingerprint. We utilize four algorithms (k-means, hierarchical clustering, SOM, FCM) to construct the COX fingerprint classifier through which the group fingerprint generating and group traitor tracing algorithms are implemented. The performance of the group fingerprint on the practicability and security are obtained by the colluding attack and multimedia processing experiments. The experiment results show that it is easy to implement the group fingerprinting schemes by the four algorithms. All the classifiers based group fingerprinting schemes withstand the JPEG compressions. The k-means scheme has superior performance than the other three. Even in the averaging attack experiments, in which the other three got the worst performance, k-means obtains acceptable performance.
58596363	A new biometric indicator based on the patterns of conjunctival vasculature is proposed. Conjunctival vessels can be observed on the visible part of the sclera that is exposed to the outside world.These vessels demonstrate rich and specific details invisible light, and can be easily photographed using a regular digital camera. In this paper we discuss methods for conjunctival imaging, preprocessing, and feature extraction in order to derive a suitable conjunctival vascular template for biometric authentication. Commensurate classification methods along  with the observed accuracy are discussed. Experimental results suggest the potential of using conjunctival vasculature as a biometric measure. 
77F03DFE	Fingerprint classification is crucial to reduce the processing time in a large-scale database. In this paper a fingerprint classification based on continuous orientation field and singular points is proposed. The continuous orientation field can not only filter the noises in point directional image,but also represent the basic structural feature of fingerprint more precisely.Singularities are the most important and reliable feature in classification.The reliable and fast classification algorithm is made possible by a simple but effective combination of continuous orientation field and the modified Poincare index in the determination of singular points.The experiment results show the effectiveness of the proposed method in producing good classification result.
7C59D1E3	Biometric systems based on face recognition have been shown unreliable under the presence of face-spoofing images. Hence, automatic solutions for spoofing detection became necessary. In this paper, face-spoofing detection is proposed by searching for Moiré patterns due to the overlap of the digital grids. The conditions under which these patterns arise are first described, and their detection is proposed which is based on peak detection in the frequency domain. Experimental results for the algorithm are presented for an image database of facial shots under several conditions.
7D386E49	In this paper, a fingerprint embedding method better-suited for the AND anti-collusion code (AND-ACC) is proposed. The proposed method embeds both a code and an orthogonal fingerprint using different basis vectors depending on the bit. Although the detection for the embedding method is complex, the performance of the fingerprinting system using proposed embedding method with the AND-ACC against average attack is improved compared with the AND-ACC fingerprinting scheme using code modulation embedding method. The system using the proposed embedding method is robust against the linear combination collusion attack (LCCA) whereas the system using the code modulation is not.
7766147C	Fingerprint recognition has found a reliable application for verification or identification of people in biometrics. Globally, fingerprints can be viewed as valuable traits due to several perceptions observed by the experts; such as the distinctiveness and the permanence on humans and the performance in real applications. Among the main stages of fingerprint recognition, the automated matching phase has received much attention from the early years up to nowadays. This paper is devoted to review and categorize the vast number of fingerprint matching methods proposed in the specialized literature. In particular, we focus on local minutiae-based matching algorithms, which provide good performance with an excellent trade-off between efficacy and efficiency. We identify the main properties and differences of existing methods. Then, we include an experimental evaluation involving the most representative local minutiae-based matching models in both verification and evaluation tasks. The results obtained will be discussed in detail, supporting the description of future directions.
7E3E0BA5	Fingerprint verification is an important biometric technique for personal identification. We describe the design and implementation of a prototype automatic identity-authentication system that uses fingerprints to authenticate the identity of an individual. We have developed an improved minutiae-extraction algorithm that is faster and more accurate than our earlier algorithm (1995). An alignment-based minutiae-matching algorithm has been proposed. This algorithm is capable of finding the correspondences between input minutiae and the stored template without resorting to exhaustive search and has the ability to compensate adaptively for the nonlinear deformations and inexact transformations between an input and a template. To establish an objective assessment of our system, both the Michigan State University and the National Institute of Standards and Technology NIST 9 fingerprint data bases have been used to estimate the performance numbers. The experimental results reveal that our system can achieve a good performance on these data bases. We also have demonstrated that our system satisfies the response-time requirement. A complete authentication procedure, on average, takes about 1.4 seconds on a Sun ULTRA I workstation (it is expected to run as fast or faster on a 200 HMz Pentium).
806C38BC	Most automatic systems for fingerprint comparison are based on minutiae matching. Minutiae are essentially terminations and bifurcations of the ridge lines that constitute a fingerprint pattern. Automatic minutiae detection is an extremely critical process, especially in low-quality fingerprints where noise and contrast deficiency can originate pixel configurations similar to minutiae or hide real minutiae. Several approaches have been proposed in the literature; although rather different from each other, all these methods transform fingerprint images into binary images. In this work we propose an original technique, based on ridge line following, where the minutiae are extracted directly from gray scale images. The results achieved are compared with those obtained through some methods based on image binarization. In spite of a greater conceptual complexity, the method proposed performs better both in terms of efficiency and robustness.
7FB3ABDE	Reliable and accurate fingerprint recognition is a challenging pattern recognition problem, requiring algorithms robust in many contexts. FVC2000 competition attempted to establish the first common benchmark, allowing companies and academic institutions to unambiguously compare performance and track improvements in their fingerprint recognition algorithms. Three databases were created using different state-of-the-art sensors and a fourth database was artificially generated; 11 algorithms were extensively tested on the four data sets. We believe that FVC2000 protocol, databases, and results will be useful to all practitioners in the field not only as a benchmark for improving methods, but also for enabling an unbiased evaluation of algorithms.
7DB6CB3D	Biometrics-based verification, especially fingerprint-based identification, is receiving a lot of attention. There are two major shortcomings of the traditional approaches to fingerprint representation. For a considerable fraction of population, the representations based on explicit detection of complete ridge structures in the fingerprint are difficult to extract automatically. The widely used minutiae-based representation does not utilize a significant component of the rich discriminatory information available in the fingerprints. Local ridge structures cannot be completely characterized by minutiae. Further, minutiae-based matching has difficulty in quickly matching two fingerprint images containing a different number of unregistered minutiae points. The proposed filter-based algorithm uses a bank of Gabor filters to capture both local and global details in a fingerprint as a compact fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the two corresponding FingerCodes and hence is extremely fast. We are able to achieve a verification accuracy which is only marginally inferior to the best results of minutiae-based algorithms published in the open literature. Our system performs better than a state-of-the-art minutiae-based system when the performance requirement of the application system does not demand a very low false acceptance rate. Finally, we show that the matching performance can be improved by combining the decisions of the matchers based on complementary (minutiae-based and filter-based) fingerprint information.
762F072E	A fingerprint classification procedure using a computer is described. It classifies the prints into one of ten defined types. The procedure is implemented using PICAP (picture array processor). The picture processing system includes a TV camera input and a special picture processor. The first part of the procedure is a transformation of the original print to a sampling matrix, where the dominant direction of the ridges for each subpicture is indicated. After smoothing, the lines in this pattern are traced out and converted to strings of symbols. Finally, a syntactic approach is adopted to make the type classification based on this string of symbols.
7B0AA26C	In this paper, we present a device to be a memristor should exhibit three characteristic fingerprints: (a) Pinched hysteresis loop in the voltage v vs. current i plane, namely, the v-i loci always passes through origin for any bipolar periodic input voltage v(t). (b) Beyond a certain critical frequency ω*, the area of the pinched hysteresis lobe decreases monotonically as the frequency of the periodic input signal increases and (c) the shape of the pinched hysteresis loop varies with frequency f and shrinks to a single-valued function through the origin, as the frequency tends to infinity. Examples of memristors exhibiting these three fingerprints are presented.
7AD7D628	A novel method of fault diagnosis is presented based on power grid flow fingerprint identification technology and synchronized phasor measurements. In this paper, the definitions of power grid flow fingerprint and flow fingerprint characteristic point are introduced at first. Then, the proposed method is achieved utilizing the flow fingerprint identification scheme. Meanwhile, two evaluation indexes are developed when the online matching is made between the simulated flow fingerprint and real-time flow fingerprint collected by PMUs. The method also employs improved integer programming technique for optimal PMU placement. The proposed approach is effective to eliminate uncertainty and obtain accurate diagnosis results using less data based on a systematic view. Especially, the application of the proposed method is helpful and beneficial to identify initial fault and accurately capture the nature of cascading failures. Simulation results have proved the validity of this method.
7C22F707	 The joint fingerprinting and decryption (JFD) framework has high efficiency and can provide comprehensive, effective content security protection for remote sensing images. However, the fingerprint is generated by random partial decryption in JFD, which will lead to obvious degradation of image quality after embedding fingerprints and will severely impact the precision and further application of remote sensing images. In order to solve this problem, an improved JFD scheme based on neighborhood similarity is proposed in this paper, where the image is partitioned into different regions, and the regions that affect future application most are excluded from the fingerprint embedding area. Moreover, neighborhood similarity is defined in order to evaluate the change of pixel correlation, and areas with good neighborhood similarity after encryption are chosen to embed fingerprints. The experimental results prove that the fingerprinted image has a good image quality, and it will not affect the following application such as edge extraction and unsupervised classification, etc. Therefore, it is a suitable content security protection method for remote sensing image.
7544EE34	The minimum spanning tree (MST) matching algorithm has been used for searching the partial matching points extracted from fingerprint images. The method, however, has some limitations. To obtain the relationship between the enrolled fingerprints and the input fingerprints the MST is used to generate the tree graph that represents the unique graph for the given minutiae. From the graph, the matching points are estimated. However, the shape of the graph highly depends on the positions of the minutiae. If there are some pseudo minutiae caused by noise, the shape of the graph will be totally different. To overcome the limitations of the MST, we propose the center of rotation method (CRM) that finds true partial matching points. The proposed method is based on the assumption that the input fingerprint minutiae are rigid body. In addition, we employ the polygon wrapping the minutiae for fast matching speed and reliable matching results. In conclusion we have been able to confirm fast performance and high identification ratio by using CRM. (C) 2004 Society of Photo-Optical Instrumentation Engineers.
7AA47C57	Sweat pores and other level 3 features have been proven to provide more discriminatory information about fingerprint characteristics, which is useful for personal identification especially in law enforcement applications. With the advent of high resolution (≥1000 ppi) fingerprint scanning equipment, sweat pores are attracting increasing attention in automatic fingerprint identification system (AFIS), where the extraction of pores is a critical step. This paper presents a scale parameter-estimating method in filtering-based pore extraction procedure. Pores are manually extracted from a 1000 ppi grey-level fingerprint image. The size and orientation of each detected pore are extracted together with local ridge width and orientation. The quantitative relation between the pore parameters (size and orientation) and local image parameters (ridge width and orientation) is statistically obtained. The pores are extracted by filtering fingerprint image with the new pore model, whose parameters are determined by local image parameters and the statistically established relation. Experiments conducted on high resolution fingerprints indicate that the new pore model gives good performance in pore extraction. 
7CA842F6	Biometric based characteristic authentication is an asymmetric authentication technology. This means that the reference biometric data generated during the enrolment process and stored in the biometric database, will never match any freshly offered biometric data exactly (100%). This is commonly accepted due to the nature of the biometric algorithm central to the biometric environment.A password or pin on the other hand, is a symmetric authentication mechanism. This means that an exact match is expected, and if the offered password deviates ever so slightly from the password stored in the password database file, authenticity is rejected.Encryption technologies rely on symmetric authentication to function, as the password or pin is often used as the seed for a random number that will assist in the generation of the cipher. If the password used to encrypt the cipher is not 100% the same as the password supplied to decrypt, the cipher will not unlock.The asymmetric nature of biometrics traditionally renders biometric data unfit to be used as the secret key for an encryption algorithm.This paper introduces a system that allows biometric data to be used as the secret key in an encryption algorithm. This method relies on the BioVault infrastructure. For this reason BioVault will briefly be discussed, followed by a discussion of biometrically based encryption.
7CEAF6EF	In this paper, we have proposed a fingerprint orientation model based on 2D Fourier expansions (FOMFE) in the phase plane. The FOMFE does not require prior knowledge of singular points (SPs). It is able to describe the overall ridge topology seamlessly, including the SP regions, even for noisy fingerprints. Our statistical experiments on a public database show that the proposed FOMFE can significantly improve the accuracy of fingerprint feature extraction and thus that of fingerprint matching. Moreover, the FOMFE has a low-computational cost and can work very efficiently on large fingerprint databases. The FOMFE provides a comprehensive description for orientation features, which has enabled its beneficial use in feature-related applications such as fingerprint indexing. Unlike most indexing schemes using raw orientation data, we exploit FOMFE model coefficients to generate the feature vector. Our indexing experiments show remarkable results using different fingerprint databases
7F0B8455	This paper proposes a new hash-based indexing method to speed up fingerprint identification in large databases. A Locality-Sensitive Hashing (LSH) scheme has been designed relying on Minutiae Cylinder-Code (MCC), which proved to be very effective in mapping a minutiae-based representation (position/angle only) into a set of fixed-length transformation-invariant binary vectors. A novel search algorithm has been designed thanks to the derivation of a numerical approximation for the similarity between MCC vectors. Extensive experimentations have been carried out to compare the proposed approach against 15 existing methods over all the benchmarks typically used for fingerprint indexing. In spite of the smaller set of features used (top performing methods usually combine more features), the new approach outperforms existing ones in almost all of the cases.
7BB6A54C	We have reported the Charge Coupled Device (CCD) fingerprint method for identification of digital still cameras. The CCD fingerprint method utilizes the nonhomogeneous nature of dark currents in CCDs. In this study, we have measured CCD defects patterns of various digital still cameras including professional cameras and cheap ones with various resolution and compression rates. As a result, CCD defect pattern was detected in all cameras except for a low-resolution cheap camera using only one image. Resolution mode change of digital cameras did not affect the position of defect points in general but in some cases, relative pixel intensity varied. Image compression did not affect the pixel position for blank images within normal compression rate, but when there existed light in the background, the pixel position was blurred as the compression rate became high. In conclusion, it is recognized that the CCD fingerprint method can be applied in principle to digital still cameras, that is, individual camera identification can be achieved in principle by using images taken with the camera. 
7A8AB444	Automated fingerprint recognition has received considerable attention over the past decade. Progress has been made on models of the structure of fingerprints, techniques for the acquisition of prints, and the development of commercial automated fingerprint recognition systems. Despite these advances, there remain considerable opportunities for improvement. The speed of retrieval, and the ability to recognize partial or distorted prints are prominent among those areas that require improvement. This study will describe a structural model of fingerprints, based on local structural relations among features, and an associated automated recognition system which addresses the limitations of existing fingerprint models.
7B2093D3	In this paper, the advantages and disadvantages of the gray variance directional image segmentation methods were analyzed. On the base of gray normalization, a new approach to segment the fingerprint images was proposed using the normal distribution model of the fingerprint image gray’s character. This approach makes use of the gray characters of the local and global images. Meanwhile, it has the advantage to choose the threshold automatic without the experience. The experimental results show that the method for the fingerprint image segmentation is effective, adaptive and quick and it can meet the need of fingerprint recognition system.
786AD364	This paper presents a novel method for fingerprint orientation modeling, which executes in two phases. Firstly, the orientation field is reconstructed using a lower order Legendre polynomial to capture the global orientation pattern in the fingerprint structure. Then the preliminary model around the region with presence of fingerprint singularities is dynamically refined using a higher order Legendre polynomial. The singular region is automatically detected through the analysis on the orientation residual field between the original orientation field and the orientation model. The method does not require any prior knowledge on the fingerprint structure. To validate the performance, the method has been applied to fingerprint image enhancement, fingerprint singularity detection and fingerprint recognition using the FVC 2004 data sets. Compared with the recently published Legendre polynomial model, the proposed method attains higher accuracy in fingerprint singularity detection, lower error rates in fingerprint matching.
7BEFDEB7	Fingerprint matching is a key issue in research of an automatic fingerprint identification system. On the basis of triangulation in computational geometry, we develop a kind of method for fingerprint matching based on Delaunay Triangulation net in this paper. Through carrying on Delaunay Triangulation to the topological structure of fingerprint minutiae, minutiae with closer distance link to each other on the space according to the Delaunay criterion and form the Delaunay Triangulation net. Then look for some reference minutiae pairs correctly from the net. According to the reference minutiae pairs, match fingerprint on point pattern. The experimental results on FVC2000 indicate the validity of algorithm.
7DBF4AA0	This correspondence proposes new candidate list reduction criteria for fingerprint indexing approaches. The basic idea is that, given a query fingerprint, the initial set of scores produced by an indexer could contain useful information to reduce the candidate list. Novel reduction criteria have been proposed, and extensive experiments have been carried out over five publicly available benchmarks, using two state-of-the-art fingerprint indexing techniques. Although quite simple, the proposed criteria achieved remarkable results, allowing a substantial reduction of the candidate list: for instance, at 1% error rate, the average penetration rate of a state-of-the-art minutiae-based indexer decreases from 27% to 3.9% on FVC2000 DB2. The new reduction criteria are applicable to any indexing approach, since they only require a list of scores as input.
8055257B	This paper introduces a novel algorithm based on global comprehensive similarity with three steps. To describe the Euclidean space-based relative features among minutiae, we first build a minutia-simplex that contains a pair of minutiae as well as their associated textures, with its transformation-variant and invariant relative features employed for the comprehensive similarity measurement and parameter estimation, respectively. By the second step, we use the ridge-based nearest neighborhood among minutiae to represent the ridge-based relative features among minutiae. With these ridge-based relative features, minutiae are grouped according to their affinity with a ridge. The Euclidean space-based and ridge-based relative features among minutiae reinforce each other in the representation of a fingerprint. Finally, we model the relationship between transformation and the comprehensive similarity between two fingerprints in terms of histogram for initial parameter estimation. Through these steps, our experiment shows that the method mentioned above is both effective and suitable for limited memory AFIS owing to its less than 1k byte template size.
816C3258	Biometric identification is an emerging subject in applications like high-security wireless access and secure transactions across computer networks. Fingerprints are easy to use and provide relatively good performance. Furthermore,fingerprint sensors are cheap and can be integrated easily in wireless hardware.In this paper, methods are presented for the estimation of a high resolution directional field from fingerprints. It is shown how, from the directional field, very accurate detection of the singular points and the orientations of those points can be obtained. These estimates can for instance be used for accurate registration (alignment) of two fingerprints in a fingerprint verification system.
7FCCCBB3	The first subject of the paper is the estimation of a high resolution directional field of fingerprints. Traditional methods are discussed and a method, based on principal component analysis, is proposed. The method not only computes the direction in any pixel location, but its coherence as well. It is proven that this method provides exactly the same results as the "averaged square-gradient method" that is known from literature. Undoubtedly, the existence of a completely different equivalent solution increases the insight into the problem's nature. The second subject of the paper is singular point detection. A very efficient algorithm is proposed that extracts singular points from the high-resolution directional field. The algorithm is based on the Poincare index and provides a consistent binary decision that is not based on postprocessing steps like applying a threshold on a continuous resemblance measure for singular points. Furthermore, a method is presented to estimate the orientation of the extracted singular points. The accuracy of the methods is illustrated by experiments on a live-scanned fingerprint database.
8103D7BE	This paper addresses the design of additive fingerprints that are maximally resilient against Gaussian averaging collusion attacks. The detector performs a binary hypothesis test in order to decide whether a user of interest is among the colluders. The encoder (fingerprint designer) is to imbed additive fingerprints that minimize the probability of error of the test. Both the encoder and the attackers are subject to squared-error distortion constraints. We show that n-simplex fingerprints are optimal in sense of maximizing a geometric figure of merit for the detection test; these fingerprints outperform orthogonal fingerprints. They are also optimal in terms of maximizing the error exponent of the detection test, and maximizing the deflection criteria at the detector when the attacker’s noise is non-Gaussian. Reliable detection is guaranteed provided that the number of colluders K≪N−−√, where N is the length of the host vector.
785052E1	Orientation field represents the topological structure of the interleaved ridge and valley flows in fingerprint images. Although a number of methods have been proposed for orientation estimation, reliable computation of orientation field is still a challenging problem due to the poor quality of some fingerprints. This paper proposes a method to reconstruct fingerprint orientation field by weighted discrete cosine transform (DCT). First, the DCT functions are used to build the basis atoms for linear representation of orientation field. Then, the DCT basis atoms of low and high orders are combined with the weights determined by singularity measurements for orientation reconstruction. The weighted DCT model is further extended for partial fingerprints to gradually and iteratively reconstruct the orientations in noisy or missing parts of fingerprints. The proposed method can perform well in smoothing out the noise while maintaining the orientation details in singular regions. Extensive experiments have been done to compare the proposed method with some existing methods on NIST and FVC fingerprint databases in terms of the reconstruction accuracy of orientation field, fingerprint indexing performance, and fingerprint recognition accuracy. Experimental results illustrate the effectiveness of the proposed method in reconstructing orientation fields, especially for poor quality and partial fingerprints.
80C497D6	In this paper, we introduce the Minutia Cylinder-Code (MCC): a novel representation based on 3D data structures (called cylinders), built from minutiae distances and angles. The cylinders can be created starting from a subset of the mandatory features (minutiae position and direction) defined by standards like ISO/IEC 19794-2 (2005). Thanks to the cylinder invariance, fixed-length, and bit-oriented coding, some simple but very effective metrics can be defined to compute local similarities and to consolidate them into a global score. Extensive experiments over FVC2006 databases prove the superiority of MCC with respect to three well-known techniques and demonstrate the feasibility of obtaining a very effective (and interoperable) fingerprint recognition implementation for light architectures.
27435BF3	Hierarchical clustering is a popular approach in a number of fields with many well known algorithms. However, all existing work to our knowledge implements a greedy heuristic algorithm with no explicit objective function. In this work we formalize hierarchical clustering as an integer linear programming (ILP) problem with a natural objective function and the dendrogram properties enforced as linear constraints. Our experimental work shows that even for small data sets finding the global optimum produces more accurate results. Formalizing hierarchical clustering as an ILP with constraints has several advantages beyond finding the global optima. Relaxing the dendrogram constraints such as transitivity can produce novel problem variations such as finding hierarchies with overlapping clusterings. It is also possible to add constraints to encode guidance such as , , etc. Finally, though exact solvers exist for ILP we show that a simple randomized algorithm and a linear programming (LP) relaxation can be used to provide approximate solutions faster.
805F1CE5	HMM has been largely applied in many fields with great success. To achieve a better performance, an easy way is using more states or more free parameters for a better signal modelling. Thus, state sharing and state clipping methods have been proposed to reduce parameter redundancy and to limit the explosive consummation of system resources. We focus on a simple state sharing method for a hybrid neuro-Markovian on-line handwriting recognition system. At first, a likelihood-based distance is proposed for measuring the similarity between two HMM state models. Afterwards, a minimum quantification error aimed hierarchical clustering algorithm is also proposed to select the most representative models. Here, models are shared to the most under the constraint of the minimum system performance loss. As the result, we maintain about 98% of the system performance while about 60% of the parameters are reduced.
7B2F0A06	Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, clustering algorithms that build meaningful hierarchies out of large document collections are ideal tools for their interactive visualization and exploration as they provide data-views that are consistent, predictable, and at different levels of granularity. This paper focuses on document clustering algorithms that build such hierarchical solutions and (i) presents a comprehensive study of partitional and agglomerative algorithms that use different criterion functions and merging schemes, and (ii) presents a new class of clustering algorithms called constrained agglomerative algorithms, which combine features from both partitional and agglomerative approaches that allows them to reduce the early-stage errors made by agglomerative methods and hence improve the quality of clustering solutions. The experimental evaluation shows that, contrary to the common belief, partitional algorithms always lead to better solutions than agglomerative algorithms; making them ideal for clustering large document collections due to not only their relatively low computational requirements, but also higher clustering quality. Furthermore, the constrained agglomerative methods consistently lead to better solutions than agglomerative methods alone and for many cases they outperform partitional methods, as well.
79B17C36	We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By “categorical data,” we mean tables with fields that cannot be naturally ordered by a metric – e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems.
765DCAF5	A histogram clustering algorithm is suggested, which builds the hierarchy of distributions better in cluster separability. The algorithm optimizes the average cluster separability choosing the system of the data subdomain quantization grid and allows a significant decrease in the number of clusters. Application of the algorithm for uncontrolled Earth’s surface classification by satellite spectral data is shown.
806BFB0C	Mining frequent patterns is a fundamental part of data mining. Most of the previous studies adopt an a priori-like candidate set generation-and-test approach. The a priori is the first algorithm which uses the a priori property to prune the search space. In this paper the AFOPT algorithm is adapted for mining at different levels by using different support. Furthermore, the efficiency of this algorithm is being shown by comparing it to similar algorithms.
791C640A	Self-Organizing Map (SOM) networks have been successfully applied as a clustering method to numeric datasets. However, it is not feasible to directly apply SOM for clustering transactional data. This paper proposes the Transactions Clustering using SOM (TCSOM) algorithm for clustering binary transactional data. In the TCSOM algorithm, a normalized Dot Product norm based dissimilarity measure is utilized for measuring the distance between input vector and output neuron. And a modified weight adaptation function is employed for adjusting weights of the winner and its neighbors. More importantly, TCSOM is a one-pass algorithm, which is extremely suitable for data mining applications. Experimental results on real datasets show that TCSOM algorithm is superior to those state-of-the-art transactional data clustering algorithms with respect to clustering accuracy.
0624B843	Partitioning a large set of objects into homogeneous clusters is a fundamental operation in data mining. The k-means algorithm isbest suited for implementing this operation because of its efficiency in clustering large data sets. However, working only onnumeric values limits its use in data mining because data sets indata mining often contain categorical values. In this paper we present an algorithm, called k-modes, to extend the k-means paradigm to categorical domains. We introduce new dissimilarity measures to deal with categorical objects, replace means of clusters with modes, and use a frequency based method to update modes in the clustering process to minimise the clustering costfunction. Tested with the well known soybean disease data setthe algorithm has demonstrated a very good classification performance. Experiments on a very large health insurance dataset consisting of half a million records and 34 categorical attributes show that the algorithm is scalable in terms of both thenumber of clusters and the number of records.
5CD02299	In this paper, Web mining in a fuzzy environment is devoted. Browsing time staying on a Web page is considered and characterized as a fuzzy variable. Thus, the frequent preferred paths with fuzzy expected values can be gained. With the comparison of the fuzzy expected values, the measure of the interest of people for different Web pages is clear. In order to find more completely frequent fuzzy preferred paths, an efficient algorithm based on the frequent link and access tree(FLAAT) is designed, in which the access tree is traversed using a top-down strategy, and to avoid the loss of useful information, the frequent link is searched to find such nodes that may be neglected, and then the access tree is searched again to find other frequent preferred paths. The gained frequent preferred paths with fuzzy expected values more completely disclose the interest of users. Finally, an example is provided to clearly illustrate the proposed approach. And the results show that our algorithm achieves significant performance improvement over previous work.
7F94DC21	As advances in the technologies of predicting protein interactions, huge data sets portrayed as networks have been available. Identification of functional modules from such networks is crucial for understanding principles of cellular organization and functions. However, protein interaction data produced by high-throughput experiments are generally associated with high false positives, which makes it difficult to identify functional modules accurately. In this paper, we propose a fast hierarchical clustering algorithm HC-PIN based on the local metric of edge clustering value which can be used both in the unweighted network and in the weighted network. The proposed algorithm HC-PIN is applied to the yeast protein interaction network, and the identified modules are validated by all the three types of Gene Ontology (GO) Terms: Biological Process, Molecular Function, and Cellular Component. The experimental results show that HC-PIN is not only robust to false positives, but also can discover the functional modules with low density. The identified modules are statistically significant in terms of three types of GO annotations. Moreover, HC-PIN can uncover the hierarchical organization of functional modules with the variation of its parameter's value, which is approximatively corresponding to the hierarchical structure of GO annotations. Compared to other previous competing algorithms, our algorithm HC-PIN is faster and more accurate.
756A50D3	Johnson has shown that the single linkage and the complete linkage hierarchical clustering algorithms induce a metric on the data known as the ultrametric. Through the use of the Lance and Williams recurrence formula, Johnson's proof is extended to four other common clustering algorithms. It is also noted that two additional methods produce hierarchical structures which can violate the ultrametric inequality.
76B1A066	The location of the author of a social media message is not invariably the same as the location that the author writes about in the message. In applications that mine these messages for information such as tracking news, political events or responding to disasters, it is the geographic content of the message rather than the location of the author that is important. To this end, we present a method to geo-parse the short, informal messages known as microtext. Our preliminary investigation has shown that many microtext messages contain place references that are abbreviated, misspelled, or highly localized. These references are missed by standard geo-parsers. Our geo-parser is built to find such references. It uses Natural Language Processing methods to identify references to streets and addresses, buildings and urban spaces, and toponyms, and place acronyms and abbreviations. It combines heuristics, open-source Named Entity Recognition software, and machine learning techniques. Our primary data consisted of Twitter messages sent immediately following the February 2011 earthquake in Christchurch, New Zealand. The algorithm identified location in the data sample, Twitter messages, giving an F statistic of 0.85 for streets, 0.86 for buildings, 0.96 for toponyms, and 0.88 for place abbreviations, with a combined average F of 0.90 for identifying places. The same data run through a geo-parsing standard, Yahoo! Placemaker, yielded an F statistic of zero for streets and buildings (because Placemaker is designed to find neither streets nor buildings), and an F of 0.67 for toponyms.
7EAD8177	We investigate the effect of weak gravitational lensing in the limit of small angular scales where projected galaxy clustering is strongly nonlinear. This is the regime likely to be probed by future weak lensing surveys. We use well-motivated hierarchical scaling arguments and the plane-parallel approximation to study multi-point statistical properties of the convergence field. These statistics can be used to compute the vertex amplitudes in tree models of hierarchical clustering; these can be compared with similar measurements from galaxy surveys, leading to a powerful probe of galaxy bias. 
7AC42166	A novel algorithm named NB+ which is an extended version of the traditional Naïve Bayesian algorithm has been presented in this paper. An exception occurs when there is an equal probability for the class label value in the Naïve Bayesian algorithm. The approach aims to suggest a solution with the help of a partial matching method. Consequently, the classification accuracy has drastically improved. Experimental evaluation has been done on various databases to show that NB+ algorithm outperforms the traditional Naïve Bayesian algorithm.
82D539EC	Various algorithms have been developed to improve the quantity and quality of information that can be extracted from complex datasets obtained using hyphenated mass spectrometric techniques. While different approaches are possible, the key step often consists in arranging the data into a large series of profiles known as extracted ion profiles. Those profiles, similar to mono-dimensional separation profiles, are then processed to detect potential chromatographic peaks. This allows extracting from the dataset a large number of peaks that are characteristics of the compounds that have been separated. However, with mass spectrometry (MS) detection, the response is usually a complex signal whose pattern depends on the analyte, the MS instrument and the ionization method. When converted to ionic profiles, a single separated analyte will have multiple images at different m/z range. In this manuscript we present a hierarchical agglomerative clustering algorithm to group profiles with very similar feature. Each group aims to contain all profiles that are due to the transport and monitoring of a single analyte. Clustering results are then used to generate a 2 dimensional representation, called clusters plot, which allows an in-depth analysis of the MS dataset including the visualization of poorly separated compounds even when their intensity differs by more than two orders of magnitude. The usefulness of this new approach has been validated with data from capillary electrophoresis time of flight mass spectrometry hyphenated via an electrospray ionization. Using a mixture of 17 low molecular endogenous compounds it was verified that ionic profiles belonging to each compounds were correctly clustered even with very low degree of separation (R below 0.03). The approach was also validated using a urine sample. While with the total ion profile 15 peaks could be distinguished, 70 clusters were obtained allowing a much thorough analysis. In this particular example, the total computing took less than 10min.
7C9D9D1B	Milligan presented the conditions that are required for a hierarchical clustering strategy to be monotonic, based on a formula by Lance and Williams. In the present paper, the statement of the conditions is improved and shown to provide necessary and sufficient conditions.
75EB34A6	Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-ANMI, a new efficient algorithm for clustering categorical data. The k-ANMI algorithm works in a way that is similar to the popular k-means algorithm, and the goodness of clustering in each step is evaluated using a mutual information based criterion (namely, average normalized mutual information – ANMI) borrowed from cluster ensemble. This algorithm is easy to implement, requiring multiple hash tables as the only major data structure. Experimental results on real datasets show that k-ANMI algorithm is competitive with those state-of-the-art categorical data clustering algorithms with respect to clustering accuracy.
7FA715BB	The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints.
7D37CDF7	FP-Growth algorithm recursively generates huge amounts of conditional pattern bases and conditional FP-trees when the dataset is huge. In such a case, both the memory usage and computational cost are expensive, such that, the FP-tree can not meet the memory requirement. In this work, we propose a novel parallel FP-Growth algorithm, which is designed to run on the computer cluster. To avoid memory overflow, this algorithm finds all the conditional pattern bases of frequent items by the projection method without constructing an FP-tree. Hereafter, it splits the mining task into number of independent sub-tasks, executes these sub-tasks in parallel on nodes and then aggregates the results back for the final result. Our algorithm works independently at each node. As a result, it can efficiently reduce the inter-node communication cost. Experiments show that this parallel algorithm not only avoids the memory overflow but accelerate the computational speed. In addition, it achieves much better scalability than that of the FP-Growth algorithm.
7528D3D9	In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on  hierarchical clustering. Our delivery scheme involves cooperation among the participating peers.The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree  structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical  level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.    
674B2B5A	This paper studies an adaptive clustering problem. We focus on re-clustering an object set, previously clustered, when the feature set characterizing the objects increases. We propose an adaptive clustering method based on a hierarchical agglomerative approach, Hierarchical Adaptive Clustering (HAC), that adjusts the partitioning into clusters that was established by applying the hierarchical agglomerative clustering algorithm (HACA) (Han and Kamber, 2001) before the feature set changed. We aim to reach the result more efficiently than running HACA again from scratch on the feature-extended object set. Experiments testing the method's efficiency and a practical distributed systems problem in which the HAC method can be efficiently used (the problem of adaptive horizontal fragmentation in object oriented databases) are also reported.
7592732C	In many applications, data objects are described by both numeric and categorical features. The k-prototype algorithm is one of the most important algorithms for clustering this type of data. However, this method performs hard partition, which may lead to misclassification for the data objects in the boundaries of regions, and the dissimilarity measure only uses the user-given parameter for adjusting the significance of attribute. In this paper, first, we combine mean and fuzzy centroid to represent the prototype of a cluster, and employ a new measure based on co-occurrence of values to evaluate the dissimilarity between data objects and prototypes of clusters. This measure also takes into account the significance of different attributes towards the clustering process. Then we present our algorithm for clustering mixed data. Finally, the performance of the proposed method is demonstrated by a series of experiments on four real world datasets in comparison with that of traditional clustering algorithms.
7E7A888D	This paper presents TreeGNG, a top-down unsupervised learning method that produces hierarchical classification schemes. Tree- GNG extends the Growing Neural Gas algorithm by maintaining a time history of the learned topological mapping. TreeGNG is able to recover from poor decisions made during the construction of the tree, and provides the novel ability to influence the general shape of the hierarchy.
7F0EF70B	Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.
7FA60214	Clustering is a discovery process in data mining. It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. Many advanced algorithms have difficulty dealing with highly variable clusters that do not follow a preconceived model. By basing its selections on both interconnectivity and closeness, the Chameleon algorithm yields accurate results for these highly variable clusters. Existing algorithms use a static model of the clusters and do not use information about the nature of individual clusters as they are merged. Furthermore, one set of schemes (the CURE algorithm and related schemes) ignores the information about the aggregate interconnectivity of items in two clusters. Another set of schemes (the Rock algorithm, group averaging method, and related schemes) ignores information about the closeness of two clusters as defined by the similarity of the closest items across two clusters. By considering either interconnectivity or closeness only, these algorithms can select and merge the wrong pair of clusters. Chameleon's key feature is that it accounts for both interconnectivity and closeness in identifying the most similar pair of clusters. Chameleon finds the clusters in the data set by using a two-phase algorithm. During the first phase, Chameleon uses a graph partitioning algorithm to cluster the data items into several relatively small subclusters. During the second phase, it uses an algorithm to find the genuine clusters by repeatedly combining these subclusters.
801AA81F	The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large data sets, which is critical to data mining applications.
000069BF	A few years ago, Vanecek (1994) suggested to apply a variant of back-face culling to speed-up collision detection between polyhedral objects. However, Vanecek's method is linear in the number of faces in the object, which is unpractical for large models. This paper suggests to add some geometrical information to hierarchies of bounding volumes, typically used in collision detection, and perform conservative back-face culling at the bounding-volume level in constant time. The method described in this paper can be applied to complement any kind of bounding-volumes hierarchy and allows a trade-off between memory and speed. Preliminary experimental results suggest that the method allows a significant speed-up, especially in close proximity situations.
77245C93	Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally “connected,” while the other forms clusters that are optimally “compact.”
802F0036	This paper presents QuoCast, a resource-aware protocol for reliable stream diffusion in unreliable environments, where processes may crash and communication links may lose messages. QuoCast is resource-aware in the sense that it takes into account memory, CPU, and bandwidth constraints. Memory constraints are captured by the limited knowledge each process has of its neighborhood. CPU and bandwidth constraints are captured by a fixed quota on the number of messages that a process can use for streaming. Both incoming and outgoing traffic are accounted for. QuoCast maximizes the probability that each streamed packet reaches all consumers while respecting their incoming and outgoing quotas. The algorithm is based on a tree-construction technique that dynamically distributes the forwarding load among processes and links, based on their reliabilities and on their available quotas. The evaluation results show that the adaptiveness of QuoCast to several contraints provides better reliability when compared to other adaptive approaches.
7D648CAA	This paper first studies the methods of web documents mining and text clustering, and summaries the fuzzy clustering algorithms and similarity measure functions, then proposes a modified similarity function which can solve the problems of feature selection and feature extraction in high-dimensional space. Finally, this paper puts forward to a dynamic fluzzy clustering algorithm(DCFCM) by combining the proposed similarity function with approximated C-mediods. The experiments show that DCFCM can effectively improve he precision of web documents clustering, the method is feasible in web documents mining.
7DAD351F	Parallel algorithms on SIMD (single-instruction stream multiple-data stream) machines for hierarchical clustering and cluster validity computation are proposed. The machine model uses a parallel memory system and an alignment network to facilitate parallel access to both pattern matrix and proximity matrix. For a problem with N patterns, the number of memory accesses is reduced from O(N/sup 3/) on a sequential machine to O(N/sup 2/) on an SIMD machine with N PEs.
7599B1B2	Data mining is a process that analyzes voluminous digital data in order to discover hidden but useful patterns from digital data. However, the discovering of such hidden patterns has statistical meaning and may often disclose some sensitive information. As a result, privacy becomes one of the prime concerns in the data-mining research community. Since distributed association mining discovers association rules by combining local models from various distributed sites, breaching data privacy happens more often than it does in centralized environments. In this work, we present a methodology that generates association rules without revealing confidential inputs such as statistical properties of individual sites, and yet retains a high level of accuracy in the resultant rules. One of the important outcomes of the proposed technique is that it reduces the overall communication costs. Performance evaluation of our proposed method shows that it reduces the communication cost significantly when we compare it with other well-known, distributed association-rule-mining algorithms. Nevertheless, the global rule model generated by the proposed method is based on the exact global support of each item set and hence diminishes inconsistency, which indeed occurs when global models are generated from partial support count of an item set.
8000EE71	Deng et al. [Deng, S., He, Z., Xu, X.: G-ANMI: A mutual information based genetic clustering algorithm for categorical data, Knowledge-Based Systems 23, 144–149(2010)] proposed a mutual information based genetic clustering algorithm named G-ANMI for categorical data. While G-ANMI is superior or comparable to existing algorithms for clustering categorical data in terms of clustering accuracy, it is very time-consuming due to the low efficiency of genetic algorithm (GA). In this paper, we propose a new initialization method for G-ANMI to improve its efficiency. Experimental results show that the new method greatly improves the efficiency of G-ANMI as well as produces higher clustering accuracy.
75AC0D00	Data objects with mixed numeric and categorical attributes are commonly encountered in real world. The k-prototypes algorithm is one of the principal algorithms for clustering this type of data objects. In this paper, we propose an improved k-prototypes algorithm to cluster mixed data. In our method, we first introduce the concept of the distribution centroid for representing the prototype of categorical attributes in a cluster. Then we combine both mean with distribution centroid to represent the prototype of the cluster with mixed attributes, and thus propose a new measure to calculate the dissimilarity between data objects and prototypes of clusters. This measure takes into account the significance of different attributes towards the clustering process. Finally, we present our algorithm for clustering mixed data, and the performance of our method is demonstrated by a series of experiments on four real-world datasets in comparison with that of traditional clustering algorithms.
7BB625D3	Due to enormous growth in both volume and variety of data, clustering a very large database is a time-consuming process. To speed up clustering process, sampling has been recognized as a very utilitarian approach to reduce the dataset size in which a collection of data points are taken as a sample and then a clustering algorithm is applied to partitioning the data points in that sample into clusters. In this approach, the data points, that are not sampled, do not get their cluster labels. The process of allocating unlabeled data points into proper clusters has been well explored purely in numerical or categorical domain only, but not the both. In this paper, we propose a hybrid similarity coefficient to find the resemblance between an unlabeled data point and a cluster, based on the importance of categorical attribute values and the mean values of numerical attributes. Furthermore, we propose a Hybrid Data Labeling Algorithm (HDLA), based on this similarity coefficient to designate an appropriate cluster label to each unlabeled data point. We analyze its time complexity and perform various experiments using synthetic and real world datasets to demonstrate the efficacy of HDLA.
7547B164	A Multilayered Self-Learning Spiking Neural Network and its Learning Algorithm Based on ‘Winner-Takes-More’ Rule in Hierarchical Clustering This paper introduces architecture of multilayered selflearning spiking neural network for hierarchical data clustering. It consists of the layer of population coding and several layers of spiking neurons. Contrary to originally suggested multilayered spiking neural network, the proposed one does not require a separate learning algorithm for lateral connections. Irregular clusters detecting capability is achieved by improving the temporal Hebbian learning algorithm. It is generalized by replacing ‘Winner-Takes-All’ rule with ‘Winner-Takes-More’ one. It is shown that the layer of receptive neurons can be treated as a fuzzification layer where pool of receptive neurons is a linguistic variable, and receptive neuron within a pool is a linguistic term. The network architecture is designed in terms of control systems theory. Using the Laplace transform notion, spiking neuron synapse is presented as a second-order critically damped response unit. Spiking neuron soma is modeled on the basis of bang-bang control systems theory as a threshold detection system. Simulation experiment confirms that the proposed architecture is effective in detecting irregular clusters. A Multilayered Self-Learning Spiking Neural Network and its Learning Algorithm Based on ‘Winner-Takes-More’ Rule in Hierarchical Clustering
7A132388	Recently, there has been enormous growth in the amount of commercial and scientific data, such as protein sequences, retail transactions, and web-logs. Such datasets consist of sequence data that have an inherent sequential nature. However, few existing clustering algorithms consider sequentiality. In this paper, we study how to cluster these sequence datasets. We propose a new similarity measure to compute the similarity between two sequences. In the proposed measure, subsets of a sequence are considered, and the more identical subsets there are, the more similar the two sequences. In addition, we propose a hierarchical clustering algorithm and an efficient method for measuring similarity. Using a splice dataset and synthetic datasets, we show that the quality of clusters generated by our proposed approach is better than that of clusters produced by traditional clustering algorithms.
590717D5	Chapter Eight begins with a review of the clustering task, and the concept of distance. Good clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation. First, hierarchical clustering methods are examined. In hierarchical clustering, a treelike cluster structure (dendrogram) is created through recursive partitioning (divisive methods) or combining (agglomerative) of existing clusters. Single-linkage, complete-linkage, and average-linkage methods are discussed. The single-linkage and complete-linkage clustering algorithms are walked-through, using a small univariate data set. Differences in the resulting dendrogram structure are discussed. The average-linkage algorithm is shown to produce the same dendrogram as the complete-linkage algorithm, for this data set, though not necessarily in general. Next, we turn to the k-means clustering algorithm, beginning with the definition of the steps involved in the algorithm. Cluster centroids are defined. The k-means algorithm is walked-through, using a tiny bivariate data set, showing graphically how the cluster centers are updated. An application of k-means clustering to the large churn data set is undertaken, using SAS Enterprise Miner. The resulting clusters are profiled. Finally, the methodology of using cluster membership for further analysis downstream is illustrated, with the clusters identified by SAS Enterprise Miner helping to predict churn. The exercises include challenges to readers to construct single-linkage, complete-linkage, and k-means clustering solutions for small univariate and bivariate data sets. The hands-on analysis problems include generating k-means clusters using the cereals data set, and applying these clusters to help predict nutrition rating.
7C73C958	This paper evaluates the performance of different criterion functions in the context of partitional clustering algorithms for document datasets. Our study involves a total of seven different criterion functions, three of which are introduced in this paper and four that have been proposed in the past. We present a comprehensive experimental evaluation involving 15 different datasets, as well as an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce. Our experimental results show that there are a set of criterion functions that consistently outperform the rest, and that some of the newly proposed criterion functions lead to the best overall results. Our theoretical analysis shows that the relative performance of the criterion functions depends on (i) the degree to which they can correctly operate when the clusters are of different tightness, and (ii) the degree to which they can lead to reasonably balanced clusters.
79CA8DD3	The original k-means clustering algorithm is designed to work primarily on numeric data sets. This prohibits the algorithm from being directly applied to categorical data clustering in many data mining applications. The k-modes algorithm [Z. Huang, Clustering large data sets with mixed numeric and categorical value, in: Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference. World Scientific, Singapore, 1997, pp. 21–34] extended the k-means paradigm to cluster categorical data by using a frequency-based method to update the cluster modes versus the k-means fashion of minimizing a numerically valued cost. However, as is the case with most data clustering algorithms, the algorithm requires a pre-setting or random selection of initial points (modes) of the clusters. The differences on the initial points often lead to considerable distinct cluster results. In this paper we present an experimental study on applying Bradley and Fayyad's iterative initial-point refinement algorithm to the k-modes clustering to improve the accurate and repetitiveness of the clustering results [cf. P. Bradley, U. Fayyad, Refining initial points for k-mean clustering, in: Proceedings of the 15th International Conference on Machine Learning, Morgan Kaufmann, Los Altos, CA, 1998]. Experiments show that the k-modes clustering algorithm using refined initial points leads to higher precision results much more reliably than the random selection method without refinement, thus making the refinement process applicable to many data mining applications with categorical data.
7E1C5F27	In the above-titled paper (ibid., vol.12, no.11, p.1088-92, Nov. 1990), parallel implementations of hierarchical clustering algorithms that achieve O(n/sup 2/) computational time complexity and thereby improve on the baseline of sequential implementations are described. The latter are stated to be O(n/sup 3/), with the exception of the single-link method. The commenter points out that state-of-the-art hierarchical clustering algorithms have O(n/sup 2/) time complexity and should be referred to in preference to the O(n/sup 3/) algorithms, which were described in many texts in the 1970s. Some further references in the parallelizing of hierarchic clustering algorithms are provided.
76F0F507	This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram, we develop a novel algorithm -- CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms.
7FA96432	It has often been asserted that since hierarchical clustering algorithms require pairwise interobject proximities, the complexity of these clustering procedures is at least O(N2). Recent work has disproved this by incorporating efficient nearest neighbour searching algorithms into the clustering algorithms. A general framework for hierarchical, agglomerative clustering algorithms is discussed here, which opens up the prospect of much improvement on current, widely-used algorithms. This ‘progress report’ details new algorithmic approaches in this area, and reviews recent results
752C5C0B	Ensemble clustering, also known as consensus clustering, aims to generate a stable and robust clustering through the consolidation of multiple base clusterings. In recent years many ensemble clustering methods have been proposed, most of which treat each clustering and each object as equally important. Some approaches make use of weights associated with clusters, or with clusterings, when assembling the different base clusterings. Boosting algorithms developed for classification have also led to the idea of considering weighted objects during the clustering process. However, not much effort has been put towards incorporating weighted objects into the consensus process. To fill this gap, in this paper we propose an approach called Weighted-Object Ensemble Clustering (WOEC). We first estimate how difficult it is to cluster an object by constructing the co-association matrix that summarizes the base clustering results, and we then embed the corresponding information as weights associated to objects. We propose three different consensus techniques to leverage the weighted objects. All three reduce the ensemble clustering problem to a graph partitioning one. We present extensive experimental results which demonstrate that our WOEC approach outperforms state-of-the-art consensus clustering methods and is robust to parameter settings.
7A2AE3B6	Hierarchical clustering is a common method used to determine clusters of similar data points in multidimensional spaces. O(n2) algorithms are known for this problem [3,4,11,19]. This paper reviews important results for sequential algorithms and describes previous work on parallel algorithms for hierarchical clustering. Parallel algorithms to perform hierarchical clustering using several distance metrics are then described. Optimal PRAM algorithms using n/log n processors are given for the average link, complete link, centroid, median, and minimum variance metrics. Optimal butterfly and tree algorithms using n/log n processors are given for the centroid, median, and minimum variance metrics. Optimal asymptotic speedups are achieved for the best practical algorithm to perform clustering using the single link metric on a n/log n processor PRAM, butterfly, or tree.
5DF1BD46	We explore the use of instance and cluster-level constraints with agglomerative hierarchical clustering. Though previous work has illustrated the benefits of using constraints for non-hierarchical clustering, their application to hierarchical clustering is not straight-forward for two primary reasons. First, some constraint combinations make the feasibility problem (Does there exist a single feasible solution?) NP-complete. Second, some constraint combinations when used with traditional agglomerative algorithms can cause the dendrogram to stop prematurely in a dead-end solution even though there exist other feasible solutions with a significantly smaller number of clusters. When constraints lead to efficiently solvable feasibility problems and standard agglomerative algorithms do not give rise to dead-end solutions, we empirically illustrate the benefits of using constraints to improve cluster purity and average distortion. Furthermore, we introduce the new γ constraint and use it in conjunction with the triangle inequality to considerably improve the efficiency of agglomerative clustering.
7CB6B315	A monotone invariant method of hierarchical clustering based on the Mann-Whitney U-statistic is presented. The effectiveness of the complete-link, single-link, and U-statistic methods in recovering tree structures from error perturbed data are evaluated. The U-statistic method is found to be consistently more effective in recovering the original tree structures than either the single-link or complete-link methods.
7CF2C846	Categorical data clustering (CDC) and cluster ensemble (CE) have long been considered as separate research and application areas. The main focus of this paper is to investigate the commonalities between these two problems and the uses of these commonalities for the creation of new clustering algorithms for categorical data based on cross-fertilization between the two disjoint research fields. More precisely, we formally define the CDC problem as an optimization problem from the viewpoint of CE, and apply CE approach for clustering categorical data. Experimental results on real datasets show that CE based clustering method is competitive with existing CDC algorithms with respect to clustering accuracy.
5EC00903	We explore the use of constraints with divisive hierarchical clustering. We mention some considerations on the effects of the inclusion of constraints into the hierarchical clustering process. Furthermore, we introduce an implementation of a semi-supervised divisive hierarchical clustering algorithm and show the influence of including constraints into the divisive hierarchical clustering process. In this task our main interest lies in building stable dendrograms when clustering with different subsets of data.
756A50D3	Johnson has shown that the single linkage and the complete linkage hierarchical clustering algorithms induce a metric on the data known as the ultrametric. Through the use of the Lance and Williams recurrence formula, Johnson's proof is extended to four other common clustering algorithms. It is also noted that two additional methods produce hierarchical structures which can violate the ultrametric inequality.
778095CE	Clustering techniques are usually used in pattern recognition, image segmentation and object detection. Let N be the number of patterns and M be the number of features of each pattern and . In this paper, we first design two O(1) time basic operations for concentrating all nonempty data of size N and computing the proximity matrix using N × N and N × N × M processors, respectively. Then, based on these two operations, a constant time parallel hierarchical clustering algorithm is proposed on a 3-D processor array with reconfigurable bus system using N4 processors. Then, by reducing the number of processors by a factor of N, an O(log2 N) time algorithm for this problem is also derived. Note that no one had ever obtained a constant time algorithm for this problem on the existing parallel computation models
7C9D9D1B	Milligan presented the conditions that are required for a hierarchical clustering strategy to be monotonic, based on a formula by Lance and Williams. In the present paper, the statement of the conditions is improved and shown to provide necessary and sufficient conditions.
76035A9B	Hierarchical agglomerative clustering (HAC) is very useful but due to high CPU time and memory complexity its practical use is limited. Earlier, we proposed an efficient partitioning – partially overlapping partitioning (POP) – based on the fact that in HAC small and closely placed clusters are agglomerated initially, and only towards the end larger and distant clusters are agglomerated. Here, we present the parallel version of POP, pPOP. Theoretical analysis shows that, compared to the existing algorithms, pPOP achieves CPU time speed-up and memory scale-down of O(c) without compromising accuracy where c is the number of cells in the partition. A shared memory implementation shows that pPOP outperforms existing algorithms significantly.
7FA96432	It has often been asserted that since hierarchical clustering algorithms require pairwise interobject proximities, the complexity of these clustering procedures is at least O(N2). Recent work has disproved this by incorporating efficient nearest neighbour searching algorithms into the clustering algorithms. A general framework for hierarchical, agglomerative clustering algorithms is discussed here, which opens up the prospect of much improvement on current, widely-used algorithms. This ‘progress report’ details new algorithmic approaches in this area, and reviews recent results
7F1704E8	Clustering is a basic operation in image processing and computer vision, and it plays an important role in unsupervised pattern recognition and image segmentation. While there are many methods for clustering, the single-link hierarchical clustering is one of the most popular techniques. In this paper, with the advantages of both optical transmission and electronic computation, we design efficient parallel hierarchical clustering algorithms on the arrays with reconfigurable optical buses (AROB). We first design three efficient basic operations which include the matrix multiplication of two N×N matrices, finding the minimum spanning tree of a graph with N vertices, and identifying the connected component containing a specified vertex. Based on these three data operations, an O(log N) time parallel hierarchical clustering algorithm is proposed using N3 processors. Furthermore, if the connectivity of the AROB with four-port connection is allowed, two constant time clustering algorithms can be also derived using N4 and N3 processors, respectively. These results improve on previously known algorithms developed on various parallel computational models.
85C8C653	This paper proposes a hierarchical clustering algorithm based on Saturated Neighbor Graph -- hi-CLUBS and a new concept, natural nearest neighbor, which adopts a parameter-less algorithm of searching the natural neighbors for each point in a dataset. In the work, the Saturated Neighbor Graph is constructed by the natural nearest neighbor firstly. Then modularity is introduced into graph partitioning algorithm, with which the generated graph is partitioned into small sub-clusters without any parameters. Finally, these initial sub-clusters are repeatedly merged with another cluster according to similarity measurement based on connectivity and closeness, until the desired cluster number is reached. The results show that hi-CLUBS produces a set of final clusters achieves better quality than the traditional clustering algorithms.
59658E71	When performing hierarchical clustering, some metric must be used to determine the similarity between pairs of clusters. Traditional similarity metrics either can only deal with simple shapes or are very sensitive to outliers. We propose two potential-based similarity metrics, APES and AMAPES, inspired by the concept of electric potential in physics. The main features of these metrics are: they have strong anti-jamming capability; and they are capable of finding clusters of complex irregular shapes.
7E6AAED5	Among the microarray data analysis clustering methods, K-means and hierarchical clustering are researchers' favorable tools today. However, each of these traditional clustering methods has its limitations. In this study, we introduce a new method, hierarchical K-means regulating divisive or agglomerative approach. The hierarchical K-means firstly employs K-means' algorithm in each cluster to determine K cluster while operating and then employs it on hierarchical clustering technique to shorten merging clusters time while generating a tree-like dendrogram. We apply this method in two original microarray datasets. The result indicates divisive hierarchical K-means is superior to hierarchical clustering on cluster quality and is superior to K-means clustering on computational speed. Our conclusion is that divisive hierarchical K-means establishes a better clustering algorithm satisfying researchers' demand. 	
71CD6C9A	In this paper we introduce the novel class hierarchy construction algorithm (CHCA) in order to create hierarchical clusterings of Web documents. Unlike most clustering methods, CHCA operates on nominal data (the words occurring in each document) and it differs from other hierarchical clustering techniques in that it uses the object-oriented concept of inheritance to create the parent/child relationship between clusters. A prototype system has been developed using CHCA to create cluster hierarchies from web search results returned by conventional search engines. CHCA, without any guidance, creates term-based clusters from the contents of the retrieved pages and assigns each page to a cluster; the clusters correspond to topics and sub-topics in the investigated domain. The performance of our system is compared with a similar web search clustering system (Vivisimo).
76DDB4CD	Non-hierarchical clustering methods are frequently based on the idea of forming groups around 'objects'. The main exponent of this class of methods is the "k"-means method, where these objects are points. However, clusters in a data set may often be due to certain relationships between the measured variables. For instance, we can find linear structures such as straight lines and planes, around which the observations are grouped in a natural way. These structures are not well represented by points. We present a method that searches for linear groups in the presence of outliers. The method is based on the idea of impartial trimming. We search for the 'best' subsample containing a proportion 1 - "&agr;" of the data and the best "k" affine subspaces fitting to those non-discarded observations by measuring discrepancies through orthogonal distances. The population version of the sample problem is also considered. We prove the existence of solutions for the sample and population problems together with their consistency. A feasible algorithm for solving the sample problem is described as well. Finally, some examples showing how the method proposed works in practice are provided. Copyright (c) 2009 Royal Statistical Society. 
06E566AC	this paper provides a good representation of the clustering structure, thus it can be used as a tool to get insight into the distribution of a data set. In addition, the visualization can also reveal hierarchical cluster with different sizes, densities and shapes. 
79249275	Tens, hundreds and even thousands of cores are to be integrated into a single chip. Network on chip appears to offer efficient communication between cores. However, the increased requirement for larger communication bandwidths and lower power consumption challenges the traditional electrical interconnects. Advances in silicon nanophotonics make optical interconnect a promising candidate for network on chip architectures in future. In this paper, we develop a hierarchical cluster-based optical NoC (HCONoC). It is a hybrid electrical/optical on chip network architecture. It connects the lowest level cluster of IP cores with electrical interconnects while optical interconnects are used for inter-cluster communication to improve the efficiency of the network. Three types of routers including electrical and optical router architectures are carefully designed together with an efficient routing algorithm. Packet switching is employed which helps to avoid the overhead of path setup, alleviating the contention. Finally, we simulate the HCONoC for the 64-core architecture and show the network performance including end-to-end delay and network throughput.
7528D3D9	In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on  hierarchical clustering. Our delivery scheme involves cooperation among the participating peers.The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree  structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical  level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.    
7A989C58	In this paper we propose to combine two clustering approaches, namely fuzzy and possibilistic c-means. While fuzzy c-means algorithm finds suitable clusters for groups of data points, obtained memberships of data, however, encounters a major deficiency caused by misinterpretation of membership values of data points. Therefore, membership values cannot correctly interpret compatibility or degree to which data points belong to clusters. As a result, noisy data will be misinterpreted by incorrect memberships assigned, as sum of memberships of each noisy data to all clusters is constrained to be equal to 1. To overcome this, a possibilistic approach has been proposed which removes this constraint. It has, however, caused another shortcoming as cluster centers converge to an identical point. Therefore, possibilities cannot correctly interpret the degrees of compatibilities. To correct this problem, a number of works have been carried out which all try to change possibilistic objective function proposed by Krishnapuram and James M. Keller. In this work, a hierarchical approach has been proposed based on properties of both fuzzy and possibilistic approaches to overcome this deficiency. Sensitivities of both methods have been studied together with analyzing results obtained by both methods. Superiority of the proposed method as opposed to conventional possibilistic c-means is shown to be conspicuous. 
04208BBF	 Clustering analysis is an important technique in many applications, such as in biology, medicine, psychology, pattern recognition, image processing, marketing, and data engineering. The large number of existing clustering algorithms can be broadly classified into two types: (1) hierarchical and (2) partitional. Depending on the algorithmic approach taken, a hierarchical structure begins with N clusters, one per pattern, and grows a sequence of clusterings until all N patterns are in a single cluster (the agglomerative approach), or begins with one cluster containing all N patterns and successively divides clusters until N clusters are achieved (the divisive approach). Hierarchical clustering is a sequence of nested partitions in the form of a tree diagram or a dendrogram, whereas a partitional clustering is a single partition. Some fuzzy partitional clustering algorithms and their convergence properties have been given in the literature, but no fuzzy hierarchical clustering algorithm has yet been presented. In this study, fuzzy hierarchical clustering algorithms for the agglomerative approach and the divisive approach, respectively, are proposed. A performance comparison among the two proposed algorithms with different parameter values is included. The two proposed algorithms are compared with two existing algorithms. Additionally, to teduce computational time, the corresponding parallel versions of the two proposed algorithms are developed. Some experimental results show the feasibility of the proposed approaches. 
7731DE76	The processing power of parallel coprocessors like the Graphics Processing Unit (GPU) is dramatically increasing. However, until now only a few approaches have been presented to utilize this kind of hardware for mesh clustering purposes. In this paper, we introduce a Multilevel clustering technique designed as a parallel algorithm and solely implemented on the GPU. Our formulation uses the spatial coherence present in the cluster optimization and hierarchical cluster merging to significantly reduce the number of comparisons in both parts. Our approach provides a fast, high-quality, and complete clustering analysis. Furthermore, based on the original concept, we present a generalization of the method to data clustering. All advantages of the mesh-based techniques smoothly carry over to the generalized clustering approach. Additionally, this approach solves the problem of the missing topological information inherent to general data clustering and leads to a Local Neighbors k-means algorithm. We evaluate both techniques by applying them to Centroidal Voronoi Diagram (CVD)-based clustering. Compared to classical approaches, our techniques generate results with at least the same clustering quality. Our technique proves to scale very well, currently being limited only by the available amount of graphics memory.
77560214	Attribute discretization is one of the basis pre-treatment methods for data stream mining. For the reason that the attribute discretization algorithm cannot be applied with the highspeed data stream directly, we firstly build a synopsis structure for data stream, then proposed an attribute discretization algorithm based on a synopsis structure, finally, the simulation experiment results show that the method has achieved the problem of data stream attribute discretization.
7DA4FF3A	This paper focuses on document clustering algorithms that build hierarchical solutions. In this paper is evaluate the performance of different criterion functions for the problem of clustering documents.
798FC174	Hierarchical clustering (HC) is a widely used approach both in pattern recognition and data mining and has rich solutions in the literature. But all these existing solutions have some restrictions when the clustered dataset has complex structure. Spectral clustering is a graph-based, simple and outperforming method with the ability to find complex structure in dataset using spectral properties of the dataset-associated affinity matrix. In this paper, we propose a novel effective HC algorithm called SHC base on the techniques of spectral method. The experiment results both on artificial and real data sets show that our algorithm can hierarchically cluster complex data effectively and naturally.
7D4C763F	The ordering of nodes with respect to destinations of interest by means of spatial information (e.g., distances, path constituency, complete or partial topology) has been a fundamental aspect of all routing protocols in wireless networks. This spatial ordering has also included the use of geographical or virtual coordinates denoting the location of nodes. We propose the use of ordering of nodes based on time rather than space, and without the need to establish any clock synchronization among nodes. We demonstrate for the first time that using the relative times when each node receives and transmits packets is sufficient to establish multiple loop-free paths to destinations, and that such time-based ordering renders more efficient loop-free routing than the spatial ordering of nodes. With the use of self-adjusted delays, nodes can manipulate their ordering so that the resulting routing choices are more robust to failures than routing choices based solely on times driven by the physical topology. Furthermore, we show that the problem of resetting sequence numbers, which is a network-wide operation with traditional spatial ordering, is trivial with temporal ordering. We introduce the Time Ordered Routing Protocol (TORP) and compare it against routing protocols based on spatial ordering to demonstrate that temporal ordering can lead to superior performance in multi-hop wireless networks.
76DB1BAE	Pairwise Broadcast is a novel low-power wireless sensor network clock synchronization method. A subset of sensor nodes are synchronized by overhearing the timing message exchanges of a pair of sensor nodes. Therefore, a group of sensor nodes can be synchronized without sending any extra messages. This paper mainly account for the problem that the original Pairwise Broadcast Synchronization in absence of the clock skew between the pairwise broadcast node, proposed that use MLE(Maximum Likelihood Estimation) for accurate estimation of the clock offset and clock skew of nodes under Gaussian delay model. The simulation results indicate this method to ensure high accuracy simultaneously effectively reduce energy consumption.
5B0E344F	Time Synchronization in wireless networks is extremely important for basic communication, but it also provides the ability to detect movement, location, and proximity. The synchronization problem consists of four parts: send time, access time, propagation time, and receive time. Three current synchronization protocol Reference Broadcast Synchronization, Timing-sync Protocol for Sensor Networks, and Flooding Time Synchronization Protocol are presented and how they attempt solve the synchronization problem is also discussed. Security concerns as well as an industry case are also presented. 
77104C17	Wireless sensor networks are a nascent type of ad-hoc networks that had drawn the interest of the research community in last few years. They require working together for distributed tasks. Accurate and reliable time is one of the needs for some of its applications. The use of traditional time synchronization protocols for wired network is restricted by severe energy constraint in sensor network. The realization of time synchronized network poses many challenges, which are the subject of active research in the fileld. In this paper we argue that clock synchronization provides a lot of overhead, and can be eliminated in some of the cases. We advocate that sensor should be allowed to run unsynchronized. The time when any event of interest occurs, the sensor node records that event in its memory with timestamp from its cluster head's clock, instead of its local clock.
7C1B594D	An improved approach, called receiver only synchronization (ROS), relative to timing synch protocol for sensor networks (TPSN) was devised based on a two-way timing message exchange mechanism between two nodes. In ROS, all the nodes in their transmission range overhear the messages and adjust their clock parameters without having to communicate by themselves, hence saving a lot of energy. In this paper, we are proving that in ROS, one-way timing message transmission from the root node to the neighboring nodes located within its transmission range is superior to the two-way timing message exchange. Also, we are showing that choosing the more accurate model instead of the linear regression model yields further performance gains for the clock parameters estimation.
5A68FCE2	The   possibility   of   establishing   the chronology   of   events   in   a   widely   distributed network,      or      even      stronger,      the timesynchronization  of  all  nodes  in  the  network  is often  needed  for  applications  of  wireless  sensor networks. In  this  paper  we describethe Flooding Time    Synchronization    Protocol    (FTSP) that providestime   synchronization   service   in   such networks. The  protocol was  designed  to utilize low   communication   bandwidth,   scale   well   for medium  ized  multi-hop  networks,  and  be  robust against  topology  changes  and  node  failures.  The FTSP achieves ts robustness by utilizing periodic radio broadcast of synchronization messages and implicit   dynamic   opology   update.   MAC-layer time-stamping,   comprehensive compensation   oferrors  and linear  regression  re  used  to  achieve high accuracy. The resulting time synchronization rror of the FTSP is significantly ower than that  the  existing  RBS  and  TPSN  algorithms.The data  from a  comprehensive  multi-hop  experiment shows the  average  network-wide  synchronization error to   be in   the   microsecond   range.   The protocol  was  further  validated  as  part  of  our countersniper system that was field tested in a US military facility.
7F95D7EE	Time synchronization may play a key role in wireless sensor networks to meet real-time and energy-saving requirements and improve data-fusion and multiplexing efficiency. In this paper, we introduce a clock synchronization algorithm between the base station and sensor node used in wireless check meter system. The algorithm can not only been extended more conveniently, but also less affected by environment. The algorithm is implemented in the platform of s3c2410 solidified OS of Windows CE.Net. So the most important interrupt delay contributor, ISR and IST, can not be ignored, A result for the maximum clock skew of the hardware-based synchronization is presented, which can fulfill the need of wireless sensor networks built for wireless check meter system.
77B2E72B	Time synchronization is extremely important for wireless sensor networks. Since the sensors have very limited energy resources and it is very difficult to change batteries for the sensor nodes, energy efficiency is one of primary importance for a sensor network. To prolong the lifetime of a sensor network, we proposed a cluster based on demand time synchronization in wireless sensor networks(COD). In this protocol many nodes are set to “sleep” modes in most cases, for the formation of clustering and time synchronization procedure is initiated only when the event is detected. Compared to other clock synchronizat- ion protocols, the COD simulation results show that it is more energy efficient than RBS and FTSP.
061A259D	Wireless sensor networks (WSNs) assume a collection of tiny sensing devices connected wirelessly and which are used to observe and monitor a variety of phenomena in the real physical world. Time synchronization is an important issue in wireless sensor networks.  Many applications based on these WSNs assume local clocks need to be synchronized to a common view of clock at each sensor node. Some essential limitations of sensor networks such as limited energy resources, storage, computation, and bandwidth, combined with potentially high density of nodes make traditional synchronization methods incompatible for these networks. Hence, an increasing research focus on designing synchronization schemes is required. This paper reviews existing time synchronization protocols and the need for synchronization in sensor networks and then presents the proposed algorithm to construct adhoc tree structure of sensor network along with the process of clock synchronization. 
79D02E4A	Having access to an accurate time is a vital building block in all networks; in wireless sensor networks even more so, because wireless media access or data fusion may depend on it. Starting out with a novel analysis, we show that orthodox clock synchronization algorithms make fundamental mistakes. The state-of-the-art clock synchronization algorithm FTSP exhibits an error that grows exponentially with the size of the network, for instance. Since the involved parameters are small, the error only becomes visible in midsize networks of about 10--20 nodes. In contrast, we present PulseSync, a new clock synchronization algorithm that is asymptotically optimal. We evaluate PulseSync on a Mica2 testbed, and by simulation on larger networks. On a 20 node network, the prototype implementation of PulseSync outperforms FTSP by a factor of 5. Theory and simulation show that for larger networks, PulseSync offers an accuracy which is several orders of magnitude better than FTSP. To round off the presentation, we investigate several optimization issues, e.g. media access and local skew.
815E7D30	Time synchronization is an important research issue in wireless sensor networks (WSNs). Many applications based on these WSNs assume local clocks at each sensor node that need to be synchronized to a common notion of time. Some inherent properties of sensor networks such as limited energy, storage, computation, and bandwidth resources, combined with potentially high density of nodes make conventional synchronization methods unsuitable for these networks. Therefore, an increasing research focus on designing synchronization algorithms is required. In this paper we explore various time synchronization protocols and present theoretical analysis of protocols based on quantitative and qualitative criteria with the proposed Tree Structured Time Synchronization Protocol. The comparative study shows that it is an excellent conciliation among synchronization accuracy, computational complexity, and convergence time.
7913EB96	Many applications of Wireless Sensor Networks (WSNs) require accurate time synchronization for data consistency and coordination. While the existing protocols for time synchronization provide sufficient accuracy, they consume high energy and poorly synchronize the distant nodes. We propose a Recursive Time Synchronization Protocol (RTSP) which provides global clock synchronization in an accurate and energy-efficient way. It achieves better performance by using a novel blend of techniques such as the MAC-layer time-stamping based on Start of Frame Delimiter (SFD) byte, fewer broadcasts by the reference node, compensation of the propagation delay and adjustment of the timestamps at each hop, estimation of the relative skew and offset using least square linear regression on two data points (2LR), adaptive re-synchronization interval, aggregation of the synchronization requests and energy-awareness. We also analyze the sources of errors and efficiency. Simulation results show an average accuracy of 0.3μs in a large multi-hop network while using only 1/5th of the energy consumed by FTSP in the long run, i.e., the RTSP outperforms all other protocols of its class including RBS, TPSN and FTSP.
76684966	The International Atomic Time TAI is a physically realized time scale which is ultimately used for comparisons between observations and dynamical theories. Its definition should tell unambiguously what an ideal TAI should be. For terrestrial applications, TAI has been defined as a geocentric coordinate time. In Solar System Dynamics, a barycentric coordinate time is needed. In general, it is not possible to convert a coordinate time into another coordinate time. But a specific clock synchronized on TAI in the terrestrial system can be considered as reading a ‘modified’, proper time [TAI]i, which can be converted into a barycentric coordinate time. In this conversion appears a small location dependent term. By this process all the clocks of the TAI system give an unique barycentric time with the same metrological properties as TAI.
7F980C5B	A blind method for jointly estimating and correcting time, carrier frequency and sampling clock frequency synchronization in OFDM based systems is proposed. The algorithm is particularlly suitable for low-cost terminal implementations, since it provides a great efficiency in terms of hardware complexity. Its performance has been tested on a real system, taking 3GPP-LTE as target technology and FPGAs as hardware platform. Results show its good performance while keeping computational complexity unincreased.
7AC21292	This paper addresses the implementation of global time in transputer systems by software synchronization of local processor timers. After an introduction on the synchronization problem, SYNC_WAVE, a synchronization algorithm for multicomputer architectures, is briefly described. The rest of the paper gives details of an existing implementation of SYNC_WAVE for transputer-based systems. Most of the devised code is presented and commented on, showing the methods used to improve synchronization accuracy and to read the global clock from within a user process. The performance results obtained are shown, and the usefulness and portability of the proposed code are discussed.
7D58E42C	A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock synchronization algorithms. Its use is illustrated by presenting a time service which maintains externally (and hence, internally) synchronized clocks in the presence of process, communication and clock failures.
816D7F63	Accurately synchronized clocks are crucial for many applications in sensor networks. Existing time synchronization algorithms provide on average good synchronization between arbitrary nodes, however, as we show in this paper, close-by nodes in a network may be synchronized poorly. We propose the Gradient Time Synchronization Protocol (GTSP) which is designed to provide accurately synchronized clocks between neighbors. GTSP works in a completely decentralized fashion: Every node periodically broadcasts its time information. Synchronization messages received from direct neighbors are used to calibrate the logical clock. The algorithm requires neither a tree topology nor a reference node, which makes it robust against link and node failures. The protocol is implemented on the Mica2 platform using TinyOS. We present an evaluation of GTSP on a 20-node testbed setup and simulations on larger network topologies.
7F1F639A	We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node, such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.Gradient clock synchronization has been widely studied in the static setting. We show that the bounds for the static case also apply to our highly dynamic setting: if two nodes remain at distance d from each other for sufficiently long, it is possible to synchronize their clocks to within O(d log(D/d)), where D is the diameter of the network. This is known to be optimal for static networks, and since a static network is a special case of a dynamic network, it is optimal for dynamic networks as well. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length d appears between two nodes, the time required until the skew between the two nodes is reduced to O(d log(D/d)) is O(D), which we prove is optimal.
76E0C500	We describe a new fault-tolerant algorithm for solving a variant of Lamport's clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtaines its local time. The algorithm solves the problem of maintaining closely synchronized local times, assuming that processes' local times are closely synchronized initially. The algorithm is able to tolerate the failure of just under one-third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time and its uncertainty, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the basic alborithm. A similar style algorithm can also be used to achieve synchronization initially.
79826E19	We present a novel clock synchronization algorithm and prove tight upper and lower bounds on the worst-case clock skew that may occur between any two participants in any given distributed system. More importantly, the worst-case clock skew between neighboring nodes is (asymptotically) at most a factor of two larger than the best possible bound. While previous results solely focused on the dependency of the skew bounds on the network diameter, we prove that our techniques are optimal also with respect to the maximum clock drift, the uncertainty in message delays, and the imposed bounds on the clock rates. The presented results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds. Furthermore, our algorithm exhibits a number of other highly desirable properties. First, the algorithm ensures that the clock values remain in an affine linear envelope of real time. A better worst-case bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. Second, the algorithm minimizes the number and size of messages that need to be exchanged in a given time period. Moreover, only a small number of bits must be stored locally for each neighbor. Finally, our algorithm can easily be adapted for a variety of other prominent synchronization models. 
80081FB9	Large propagation delay and node movement are considered to be two significant attributes that differentiate an underwater wireless sensor network (UWSN) from a ground wireless sensor network (WSN). Considering the effects of both propagation delay and movement, we propose a time synchronization algorithm suitable for a UWSN. An underwater node can move out of and into another node's range frequently. With the proposed algorithm, no time synchronization is necessary if the time stamps of the received data packets are within the tolerance. In this fashion, the network underwater does not need to perform global time synchronizations periodically, which reduces the time used to synchronize clocks among sensor nodes. The simulation results show the time cost for synchronization is linear to the data packets exchanged.
5DDE6ADB	Clock synchronization is a crucial service in many distributed systems, including wireless ad-hoc networks. This paper studies external clock synchronization, in which nodes should bring their clocks close to the value of some external reference time, which is provided in the system by one or more source clocks.Reference broadcast synchronization (RBS) is a known approach that exploits the broadcast nature of wireless networks for a single hop. However, when networks are large in physical extent, additional mechanisms must be employed.Using multi-hop algorithms that re-broadcast time information to short distances reduces the energy consumed for clock synchronization. The reason is that energy costs grow more than linearly with the broadcast distance. On the other hand, the quality of the clock synchronization, as measured in the closeness of the clocks, deteriorates as the number of hops increases.This paper shows how to balance these two contradictory goals, achieving optimal clock synchronization while adhering to an energy budget at each node. In particular, a distributed algorithm is presented that uses multi-hop broadcasting over a shallow infrastructure to synchronize the clocks. The closeness of clock synchronization achieved by the algorithm is proved to be optimal for the given energy constraints.
76367B21	A number of time synchronization protocols for wireless sensor networks (WSNs) have been recently proposed aiming at maximizing the accuracy and minimizing the power efficiency. This paper proposes novel clock skew estimators for the protocols based on two-way timing message exchanges to achieve long term reliability of synchronization. The proposed clock synchronization mechanism is far more power efficient than the conventional ones by significantly increasing the re-synchronization period. Moreover, it can be applied to the conventional protocols without any additional overhead. In fact, the proposed estimators assume simple steps and low complexity, a feature which is strongly demanding for WSNs consisting of cheap and small nodes
7534C71C	We describe hardware and software schemes for achieving precise clock synchronization on SP2 parallel system nodes. The SP2 multistage interconnection network has an unusual hardware feature, a set of distributed counters that the processor nodes may utilize for synchronizing their time–of–day clocks. We describe an algorithm for synchronizing the counters to within less than 200 nanoseconds of each other in a network of up to 512 processor nodes. This is 4–5 orders of magnitude better than what can be achieved by existing software schemes. We also describe experimental system software, called, for synchronizing the node clocks to the Internet time of day, utilizing the synchronous counters in the SP2 network.synchronizes the node clocks typically within 5 μs of each other, which is up to 2–3 orders of magnitude better than could be achieved by previous methods on the SP2 system. Synchronized clocks are useful in parallel and distributed environments, for example for performance measurement, tuning, tracing, debugging, gang scheduling of parallel processes, and timestamping of transactions. We also measure the performance of a widely used time synchronization utility, the Network Time Protocol, using the synchronous counters of the SP2 interconnection network.
7DD1F597	A general protocol for atomic broadcast in networks is presented. The protocol tolerates loss, duplication, reordering, delay of messages, and network partitioning in an arbitrary network of fail-stop sites (i.e. no Byzantine site behavior is tolerated). The protocol is based on majority-concensus decisions to commit on unique ordering of received broadcast messages. Under normal operating conditions, the protocol requires three phases to complete and approximately 4N/V messages where N is the number of sites. This overhead is distributed among the messages of which the delivery decision is made and the heavier the broadcast message traffic, the lower the overhead per broadcast message becomes. Under abnormal operating conditions, a decentralized termination protocol (also presented) is invoked. A performance analysis of this protocol is presented, showing that this protocol commits with high probability under realistic operating conditions without invoking termination protocol if N is sufficiently large. The protocol retains its efficiency in wide-area networks where broadcast communication media are unavailable.
80C888C7	Time-based localization approaches attract a lot of interest due to their high accuracy and potentially low cost for wireless sensor networks (WSNs). However, time-based localization is tightly coupled with clock synchronization. Thus, the reliability of timestamps in time-based localization becomes an important yet challenging task to deal with. In this paper, we propose robust time-based localization strategies to locate a target node with the help of anchors (nodes with known positions) in asynchronous networks. Two kinds of asynchronous networks are considered: one only with clock offsets, labeled quasi-synchronous networks, whereas the other with not only clock offsets but also clock skews, labeled fully asynchronous networks. A novel ranging protocol is developed for both networks, namely asymmetric trip ranging (ATR), to reduce the communication load and explore the broadcast property of WSNs. Regardless of the reliability of the timestamp report from the target node, closed-form least-squares (LS) estimators are derived to accurately estimate the target node position. As a result, we counter the uncertainties caused by the target node by ignoring the timestamps from this node. Furthermore, in order to simplify the estimator in fully asynchronous networks, localization and synchronization are decoupled. A simple yet efficient method is proposed to first Calibrate the Clock Skews of the anchors, and then Estimate the Node Position (CCS-ENP). Finally, Cramér-Rao bounds (CRBs) and simulation results corroborate the efficiency of our localization schemes.
7CF95A36	This letter proposes an energy-efficient clock synchronization scheme for Wireless Sensor Networks (WSNs) based on a novel time synchronization approach. Within the proposed synchronization approach, a subset of sensor nodes are synchronized by overhearing the timing message exchanges of a pair of sensor nodes. Therefore, a group of sensor nodes can be synchronized without sending any extra messages. This paper brings two main contributions: 1. Development of a novel synchronization approach which can be partially or fully applied for implementation of new synchronization protocols and for improving the performance of existing time synchronization protocols. 2. Design of a time synchronization scheme which significantly reduces the overall network-wide energy consumption without incurring any loss of synchronization accuracy compared to other well-known schemes.
7FD9D0A2	The cost of synchronizing a multicomputer increases with system size. For large multicomputers, the time and resources spent to enable each node to estimate the clock value of every other node in the system can be prohibitive. We show how to reduce the cost of synchronization by assigning each node to one or more groups, then having each node estimate the clock values of only those nodes with which it shares a group. Since each node estimates the clock value of only a subset of the nodes, the cost of synchronization can be significantly reduced. We also provide a method for computing the maximum skew between any two nodes in the multicomputer, and a method for computing the maximum time between synchronizations. We also show how the fault tolerance of the synchronization algorithm may be determined.
7849ABA5	Recent advances in micro-electromechanical (MEMS) technology have led to the development of small, low-cost, and low-power sensors. Wireless sensor networks (WSNs) are large-scale networks of such sensors, dedicated to observing and monitoring various aspects of the physical world. In such networks, data from each sensor is agglomerated using data fusion to form a single meaningful result, which makes time synchronization between sensors highly desirable. This paper surveys and evaluates existing clock synchronization protocols based on a palette of factors like precision, accuracy, cost, and complexity. The design considerations presented here can help developers either in choosing an existing synchronization protocol or in defining a new protocol that is best suited to the specific needs of a sensor-network application. Finally, the survey provides a valuable framework by which designers can compare new and existing synchronization protocols.
7D466127	Recently, a few efficient timing synchronization protocols for wireless sensor networks (WSNs) have been proposed with the goal of maximizing the accuracy and minimizing the power utilization. This paper proposes novel clock skew estimators assuming different delay environments to achieve energy-efficient network-wide synchronization for WSNs. The proposed clock skew correction mechanism significantly increases the re-synchronization period, which is a critical factor in reducing the overall power consumption. The proposed synchronization scheme can be applied to the conventional protocols without additional overheads. Moreover, this paper derives the Cramer-Rao lower bounds and the maximum likelihood estimators under different delay models and assumptions. These analytical metrics serves as good benchmarks for the thus far reported experimental results
7D833076	Since wireless ad-hoc networks use shared communication medium, accesses to the medium must be coordinated to avoid packet collisions. Transmission scheduling algorithms allocate time slots to the nodes of a network such that if the nodes transmit only during the allocated time slots, no collision occurs. For real-time applications, by ensuring deterministic channel access, transmission scheduling algorithms have the added significance of making guarantees on transmission latency possible. In this paper we present a distributed transmission scheduling algorithm for hexagonal wireless ad-hoc networks with a particular focus on Wireless Sensor Networks. Afforded by the techniques of ad-hoc networks topology control, hexagonal meshes enable trivial addressing and routing protocols. Our transmission scheduling algorithm constructs network-wide conflict-free packet transmission schedule for hexagonal networks, where the overhead of schedule construction in terms of message exchanges is zero above and beyond that for topology control and other network control related functions. Furthermore, the schedule is optimal in the sense that the bottleneck node does not idle. We also present an implicit clock synchronization algorithm to facilitate scheduling. We derive the real time capacity of our scheduling algorithm. We present evaluations of our scheduling algorithm in the presence of topological irregularities using simulation.
80C59B4C	Over the last years, large-scale decentralized computer networks such as peer-to-peer and mobile ad hoc networks have become increasingly prevalent. The topologies of many of these networks are often highly dynamic. This is especially true for ad hoc networks formed by mobile wireless devices.In this paper, we study the fundamental problem of clock synchronization in dynamic networks. We show that there is an inherent trade-off between the skew S guaranteed along sufficiently old links and the time needed to guarantee a small skew along new links: for any sufficiently large initial skew on a new link, there are executions in which the time required to reduce the skew on the link to O(S) is at least Ω(n/S).We show that this bound is tight for moderately small values of S. Assuming a fixed set of n nodes, an arbitrary pattern of edge insertions and removals, and a weak dynamic connectivity requirement, we present an algorithm that always maintains a skew of O(n) between any two nodes in the network. For a parameter S=Ω(ρn−−√), where ρ is the maximum hardware clock drift, it is further guaranteed that if a communication link between two nodes u,v persists in the network for Θ(n/S) time, the clock skew between u and v is reduced to no more than O(S).
7FC275BB	We introduce the distributed gradientclock synchronization problem. As in traditional distributed clock synchronization, we consider a network of nodes equipped with hardware clocks with bounded drift. Nodes compute logical clock values based on their hardware clocks and message exchanges, and the goal is to synchronize the nodes' logical clocks as closely as possible, while satisfying certain validity conditions. The new feature of gradient clock synchronization GCS for short) is to require that the skew between any two nodesy' logical clocks be bounded by a nondecreasing function of the uncertainty in message delay (call this the distance) between the two nodes, and other network parameters. That is, we require nearby nodes to be closely synchronized, and allow faraway nodes to be more loosely synchronized. We contrast GCS with traditional clock synchronization, and discuss several practical motivations for GCS, mostly arising in sensor and ad-hoc networks. Our main result is that the worst case clock skew between two nodes at distance d or less from each other is Ω(d + logDloglogD), where D is the diameter of the network. This means that clock synchronization is not a localproperty, in the sense that the clock skew between two nodes depends not only on the distance between the nodes, but also on the size of the network. Our lower bound implies, for example, that the TDMA protocol with a fixed slot granularity will fail as the network grows, even if the maximum degree of each node stays constant.
8022410E	This paper presents a synchronization protocol for real-time multimedia application in wireless networks. A multimedia stream is multicast from one to some mobile terminals (MTs). Source and receivers can both be mobile. The protocol allows MTs to receive a multicast stream without breaks in playback as MTs move from cell to cell. The scheme is general because we make no assumption about clocks synchronization and messages are not time-stamped. However, it is used in a network having bounded delays. Verifications show that synchronization properties are respected. Moreover, simulations show that the strategy of known locations reduces the handoff duration compared to the one of unknown locations and then preserves the synchronization.
5F021DDB	Optical networks are widely regarded as the ultimate solution to the bandwidth needs of future communication systems. With fiber-optic links deployed between nodes, the electronic devices at the switch nodes, rather than the transmission medium, limit the bandwidth of a network. In an optically-switched network, a photonic switch can control connection paths without optical-to-electrical (O-E) conversion of the signal, thereby avoiding speed and data-format restrictions im- posed by such conversions. Despite the recognized potential of optically-switched multiwavelength networks, however, their overall effectiveness remains hampered by problems related to signal impairment such as noise, crosstalk and dispersion ac- cumulating over a transparent path, and a finite number of wavelengths available limiting the network size. Moreover, the absence of O-E conversions prevents their application to certain areas, most notably, packet switches.
7DFBE219	Using active Techniques to measure networks, that is by injecting probe packets, has proved to be quite challenging for properties beyond simple end-to-end delay and loss. Some of the greatest difficulties have resulted from our inability to design techniques robust to multi-hop queueing effects. This difficulty is only compounded by the need to keep measurements non-intrusive, that is to minimally affect ongoing data flows. In this paper, we show that novel network primitives based on hop-dependent priority queueing are very effective in addressing these challenges. By enabling these primitives, network operators can perform a variety of active measurements accurately. Such measurement-friendliness results from many factors including ease of applying fundamentally single-hop methods, better measurement capabilities, and easier clock synchronization. Other advantages of our architecture include ease of deployment, simplicity, low overhead and generality, i.e., no constraints on scheduling policies for data packets. We also discuss the challenges faced, for example, in coping with small but unavoidable inaccuracies and with exposing the primitives to end-users.
7A92DF05	Synchronization and localization are critical challenges for the coherent functioning of a wireless network, which are conventionally solved independently. Recently, various estimators have been proposed for pairwise synchronization between immobile nodes, based on time stamp exchanges via two-way communication. In this paper, we consider a network of mobile nodes for which a novel joint time-range model is presented, treating both unsynchronized clocks and the pairwise distances as a polynomial functions of true time. For a pair of nodes, a least squares solution is proposed for estimating the pairwise range parameters between the nodes, in addition to estimating the clock offsets and clock skews. Extending these pairwise solutions to network-wide ranging and clock synchronization, we present a central data fusion based global least squares algorithm. A unique solution is nonexistent without a constraint on the cost function e.g., a clock reference node. Ergo, a constrained framework is proposed and a new Constrained Cramér-Rao Bound (CCRB) is derived for the joint time-range model. In addition, to alleviate the need for a single clock reference, various clock constraints are presented and their benefits are investigated using the proposed solutions. Simulations are conducted and the algorithms are shown to approach the theoretical limits.
7BDEF745	We describe a new fault-tolerant algorithm for solving a variant of Lamport's clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtaines its local time. The algorithm solves the problem of maintaining closely synchronized local times, assuming that processes' local times are closely synchronized initially. The algorithm is able to tolerate the failure of just under one-third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time and its uncertainty, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the basic alborithm. A similar style algorithm can also be used to achieve synchronization initially.
178CABEE	In this paper, a novel clock synch algorithm for WSN based on Pairwise Broadcast (PB) is investigated. Within the proposed approach, a cluster based time synch topology is constructed by broadcast and two-way message exchange mechanism. In each cluster, several assistant Cluster Heads (CHs) are voted upon the neighborhood of member nodes, resulting in fine-gradient cluster segmentation. On the basis of the topology, an improved clock estimator with unknown fixed delay is addressed. Herein, we induce clock parameters by General Linear Model as well as the fixed delay by Maximum Likelihood Estimation. Moreover, the performance analysis of the estimator is carried out. The simulation results reveal that the proposed algorithm is prominent on synch overhead and that it achieves better performance in terms of synch accuracy.
78F80572	Intermittent connection of wireless links, caused by low duty-cycle radio operation, harsh working environment, movement of sensor nodes, etc., makes clock synchronization a challenging task. Prior synchronization approaches in wireless sensor networks (WSNs) typically require that nodes exchange time messages frequently with the reference clock, which is difficult in networks with low or intermittent connectivity. This poster presents RobSync, a robust design for clock synchronization in intermittent-connected wireless networks. Having recognized that clock skew is highly correlated to the voltage supply, we use the local voltage information as a reference for clock self-calibration, which helps reduce the frequency of time-stamp exchanges. To prevent a misuse of the voltage information, leading to error accumulation, a re-synchronization interval adjustment design is developed to make a trade-off between accuracy and energy consumption. We present the theory behind RobSync, and provide preliminary results by experiments to compare our approach and the recent approach.
77881AE4	We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node, such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.Gradient clock synchronization has been widely studied in the static setting. We show that the bounds for the static case also apply to our highly dynamic setting: if two nodes remain at distance d from each other for sufficiently long, it is possible to synchronize their clocks to within O(d log(D/d)), where D is the diameter of the network. This is known to be optimal for static networks, and since a static network is a special case of a dynamic network, it is optimal for dynamic networks as well. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length d appears between two nodes, the time required until the skew between the two nodes is reduced to O(d log(D/d)) is O(D), which we prove is optimal.
7F0F7624	A clock synchronization scheme that strikes a balance between hardware and software solutions is proposed. The proposed scheme is a software algorithm that uses minimal additional hardware to achieve reasonably tight synchronization. Unlike other software solutions, the guaranteed worst-cast skews can be made insensitive to the maximum variation of message transit delay in the system. The scheme is particularly suitable for large partially connected distributed systems with topologies that support simple point-to-point broadcast algorithms. Examples of such topologies include the hypercube and the mesh interconnection structures.
79291B39	Circuit emulation service (CES) allows time-division multiplexing (TDM) services (T1/E1 and T3/E3 circuits) to be transparently extended across a packet network. With circuit emulation over IP, for instance, TDM data received from an external device at the edge of an IP network is converted to IP packets, sent through the IP network, passed out of the IP network to its destination, and reassembled into TDM bit stream. Clock synchronization is very important for CES. This paper presents a clock synchronization scheme based on a double exponential filtering technique and a linear process model. The linear process model is used to describe the behaviour of clock synchronization errors between a transmitter and a receiver. In the clock synchronization scheme, the transmitter periodically sends explicit time indications or timestamps to a receiver to enable the receiver to synchronize its local clock to the transmitter's clock. A phase-locked loop (PLL) at the receiver processes the transmitted timestamps to generate timing signal for the receiver. The PLL has a simple implementation and provides both fast responsiveness (i.e. fast acquisition of transmitter frequency at a receiver) and significant jitter reduction in the locked state. Copyright © 2006 John Wiley & Sons, Ltd.
7941F5FC	In large scale of WSNs (wireless sensor networks), a centralized algorithm is not suitable for WSNs. Distributed algorithm has the obvious advantage over traditional time synchronization algorithm. The paper proposes a distributed time synchronization algorithm for WSNs, called SNOWBALL effect time synchronization. In the algorithm, a node needs only to communicate with its neighbor node, synchronizing itself clock based on the neighbor nodes information. The algorithm is a continuous synchronization process. The network will reach balance and clock consistency after repeated algorithm iterations. In the algorithm, we achieve network energy optimization by reducing unnecessary message exchange.
7CE46771	We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the first known solution that achieves optimal accuracy—the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.
7FD2FE79	We show that CSMA is able to spontaneously synchronize transmissions in a wireless network with constant-size packets, and that this property can be used to devise efficient synchronized CSMA scheduling mechanisms without message passing. Using tools from queuing theory, we prove that for any connected wireless networks with arbitrary interference constraints, it is possible to implement self-synchronizing TDMA schedules without any explicit message passing or clock synchronization besides transmitting the original data packets, and the interaction can be fully local in that each node decides when to transmit next only by overhearing its neighbors' transmissions. We also provide a necessary and sufficient condition on the emergence of self-synchronization for a given TDMA schedule, and prove that such conditions for self-synchronization can be checked in a finite number of steps for a finite network topology.
7F7D7BBD	The development of tiny, low-cost, low-power and multifunctional sensor nodes equipped with sensing, data processing, and communicating components, have been made possible by the recent advances in micro-electro-mechanical systems (MEMS) technology. Wireless sensor networks (WSNs) assume a collection of such tiny sensing devices connected wirelessly and which are used to observe and monitor a variety of phenomena in the real physical world. Many applications based on these WSNs assume local clocks at each sensor node that need to be synchronized to a common notion of time. This paper reviews the existing clock synchronization protocols for WSNs and the methods of estimating clock offset and clock skew in the most representative clock synchronization protocols for WSNs.
756A3BB8	The problem of achieving optimal clock synchronization in a communication network with arbitrary topology and perfect clocks (that do not drift) is studied. Clock synchronization algorithms are presented for a large family of delay assumptions. Our algorithms are modular and consist of three major components. The first component holds for any type of delay assumptions; the second component holds for a large, natural family of local delay assumptions; the third component must be tailored for each specific delay assumption.Optimal clock synchronization algorithms are derived for several types of delay assumptions by appropriately tuning the third component. The delay assumptions include lower and upper delay bounds, no bounds at all, and bounds on the difference of the delay in opposite directions. In addition, our model handles systems where some processors are connected by broadcast networks in which every message arrives at all the processors at approximately the same time. A composition theorem allows combinations of different assumptions for different links or even for the same link; such mixtures are common in practice.Our results achieve the best possible precision in each execution. This notion of optimality is stronger than the more common notion of worst-case optimality. The new notion of optimality applies to systems where the worst-case behavior of any clock synchronization algorithm is inherently unbounded. 
7868EEE7	This paper describes an experiment designed to evaluate the accuracy of one-way clock synchronization using geostationary satellites with the propagation delays calculated from the satellite's orbital elements. Propagation delays from a ground transmitter via satellite to each of five locations in the North and South American continents were measured and compared with the calculated values. Three months of data are presented along with descriptions of the equipment, timing signal format, and methods for delay calculation and time recovery. The results show that within two weeks of epoch for the orbital elements, clocks can be synchronized to 150 ??s using the Tactical Communications Satellite (TACSAT). If one of the observers of the timing signals was already synchronized to the master clock, his delay measurement could improve the results for TACSAT to 75 ??s. By the same method and within 12 hours of epoch, the results for the Lincoln Experimental Satellite-6 (LES-6) indicated that synchronization to 25 ??s was possible.
76E618BA	We present a novel clock synchronization algorithm and prove tight upper and lower bounds on the worst-case clock skew that may occur between any two participants in any given distributed system. More importantly, the worst-case clock skew between neighboring nodes is (asymptotically) at most a factor of two larger than the best possible bound. While previous results solely focused on the dependency of the skew bounds on the network diameter, we prove that our techniques are optimal also with respect to the maximum clock drift, the uncertainty in message delays, and the imposed bounds on the clock rates. The presented results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds.Furthermore, our algorithm exhibits a number of other highly desirable properties. First, the algorithm ensures that the clock values remain in an affine linear envelope of real time. A better worst-case bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. Second, the algorithm minimizes the number and size of messages that need to be exchanged in a given time period. Moreover, only a small number of bits must be stored locally for each neighbor. Finally, our algorithm can easily be adapted for a variety of other prominent synchronization models.
63A0B5A5	We consider the problem of clock synchronization in a wireless setting where processors must minimize the number of times their radios are used to save energy. Energy efficiency is a central goal in wireless networks, especially if energy resources are severely limited, as occurs in sensor and ad hoc networks, and in many other settings. The problem of clock synchronization is fundamental and intensively studied in the field of distributed algorithms. In the current setting, the problem is to synchronize clocks of m processors that wake up in arbitrary time points, such that the maximum difference between wake-up times is bounded by a positive integer n. (Time intervals are appropriately discretized to allow communication of all processors that are awake in the same discrete time unit.) Currently, the best-known results for synchronization for single-hop networks of m processors is a randomized algorithm due to Bradonjic et al. [2009] of O(√n/m ⋅ poly-log(n)) radio use times per processor, and a lower bound of Ω (√n/m). The main open question left in their work is to close the poly-log gap between the upper and the lower bound, and to derandomize their probabilistic construction and eliminate error probability. This is exactly what we do in this article. That is, we show a deterministic algorithm with radio use of Θ (√n/m), which exactly matches the lower bound proven in Bradonjic et al. [2009] to a small multiplicative constant. Therefore, our algorithm is optimal in terms of energy efficiency and completely resolves a long sequence of works in this area [Bradonjic et al. 2009; Moscribroda et al. 2006; McGlynn and Borbash 2001; Polastre et al. 2004]. Moreover, our algorithm is optimal in terms of running time as well. To achieve these results, we devise a novel adaptive technique that determines the times when devices power their radios on and off. This technique may be of independent interest.In addition, we prove several lower bounds on the energy efficiency of algorithms for multihop networks. Specifically, we show that any algorithm for multihop networks must have radio use of Ω (√n) per processor. Our lower bounds hold even for specific kinds of networks, such as networks modeled by unit disk graphs and highly connected graphs. Our results imply that the simple deterministic algorithm devised for two-processor networks in Bradonjic et al. [2009] with efficiency O(√n) can be used in multihop networks, and it is the most efficient solution in terms of energy use.
78DA4353	It is known that clock synchronization can be achieved in the presence of faulty processors as long as the nonfaulty processors are connected, provided that some authentication technique is used. Without authentication the number of faults that can be tolerated has been an open question. Here we show that if we restrict logical clocks to running within some linear functions of real time, then clock synchronization is impossible without authentication when one-third or more of the processors are faulty. We also provide a lower bound on the closeness to which simultaneity can be achieved in the network as a function of the transmission and processing delay properties of the network.
7641A8AA	We consider the problem of synchronizing the clocks of processors in a failure-free distributed system when the hardware clocks do not drift but there is uncertainty in the message delays. Our goal is to develop closed form expressions for how closely the clocks can be synchronized. For an arbitrary undirected topology with arbitrary (symmetric) uncertainties, we prove a lower bound of diam/2 on the closeness of synchronization achievable, where diam is the diameter of the graph when the edges are weighted with the uncertainties. We then consider the class of topologies described as k-ary m-cubes, both with and without wrap-around. We assume that every edge has the same uncertainty, u. For the k-ary m-cube without wrap-around, we show that our lower bound diam/2 = um(k-1)/2 is tight, by analyzing the synchronization achieved by a simple algorithm. Since chains, meshes, and hypercubes are special cases of this topology, we have tight bounds for them. For the k-ary m-cube with wrap-around, we show using the same algorithm that our lower bound diam/2 = u*m*floor(k/2)/2 is tight when k is even and is almost tight when k is odd. This result implies tight or almost tight bounds for rings and tori. 
5C270BFF	CPM waveforms, which are documented in MIL-STD-188-181B, are typically demodulated in superheterodyne receivers. This paper provides an outline for an alternate method of demodulation. The pulse count demodulation scheme is typically used as a method for demodulation of FM voice signals. The pulse count demodulator does not require fast A/D converters or automatic gain control anywhere in the receive path of the radio. Thus, the receive radio architecture can be designed without these components, decreasing the power requirements and size of the RF portion of the receiver. This paper investigates the performance characteristics of the CPM signal as demodulated through the pulse count demodulator. Some analysis is performed on the extent of the limitations in amplitude and phase accuracy and resolution. The design of the PCD is discussed and this paper provides the simulated results that represent the ultimate performance of the CPM waveform as demodulated through the pulse count demodulator.
77B78231	We present an improved and optimized formulation for the estimation of the multiscale amplitude-modulation frequency-modulation (AM-FM) estimates when (i) non-separable filters are used and (ii) the variable spacing, local linear phase method is used. Also, we introduce the use of multiscale directional filterbanks for the feature extraction of images. Recently, AM-FM methods have shown promising results in a variety of medical image analysis applications. The 2D optimized AM-FM demodulation described here presents advantages for feature extraction at different frequency scales and orientations that can be used to detect different patterns, directions, or structures in an image. We test the new formulation using a Gaussian amplitude-modulated Quadratic frequency-modulated synthetic image and natural images. The results show that the optimized estimation produces better results, up to 4.9 times for the IF estimation and in 3 orders of magnitude for the IA estimation, for noise-free signals compared to the state-of-the-art methods.
7EDAABCF	The problem of optimum demodulation of PM and FM signals when the phase modulator employed in the transmitter of the communication system exhibits saturation effects is studied. Quasi-optimum receivers with zero and finite lag are designed by making use of the state variable techniques. Performance results are presented graphically for a class of message signals, and it turns out that the effect of saturation in the phase modulator on the average performance of the system is essentially to reduce the modulation index.
7BB04FC7	In this paper, a measurement method for DTMB modulator which performs the estimation of timing and frequency offsets, channel impulse response (CIR), as well as modulation error ratio (MER) by using only two consecutive frames in an iterative manner is proposed to precisely measure the modulator performance in short time. Both simulation and lab testing results show that the measurement method could reflect the performance of DTMB modulator with high accuracy.
7E3A5596	We develop a mathematical framework for quantifying and understanding multidimensional frequency modulations in digital images. We begin with the widely accepted definition of the instantaneous frequency vector (IF) as the gradient of the phase and define the instantaneous frequency gradient tensor (IFGT) as the tensor of component derivatives of the IF vector. Frequency modulation bounds are derived and interpreted in terms of the eigendecomposition of the IFGT. Using the IFGT, we derive the ordinary differential equations (ODEs) that describe image flowlines. We study the diagonalization of the ODEs of multidimensional frequency modulation on the IFGT eigenvector coordinate system and suggest that separable transforms can be computed along these coordinates. We illustrate these new methods of image pattern analysis on textured and fingerprint images. We envision that this work will find value in applications involving the analysis of image textures that are nonstationary yet exhibit local regularity. Examples of such textures abound in nature
7FB42927	Error diffusion halftoning is a popular method of producing frequency modulated (FM) halftones for printing and display. FM halftoning fixes the dot size (e.g., to one pixel in conventional error diffusion) and varies the dot frequency according to the intensity of the original grayscale image. We generalize error diffusion to produce FM halftones with user-controlled dot size and shape by using block quantization and block filtering. As a key application, we show how block-error diffusion may be applied to embed information in hardcopy using dot shape modulation. We enable the encoding and subsequent decoding of information embedded in the hardcopy version of continuous-tone base images. The encoding-decoding process is modeled by robust data transmission through a noisy print-scan channel that is explicitly modeled. We refer to the encoded printed version as an image barcode due to its high information capacity that differentiates it from common hardcopy watermarks. The encoding/halftoning strategy is based on a modified version of block-error diffusion. Encoder stability, image quality versus information capacity tradeoffs, and decoding issues with and without explicit knowledge of the base image are discussed
752793FB	Phase Generated Carrier (PGC) with directly Frequency Modulation (FM) is one of the most important demodulation methods for optical fiber interferometric sensor system. Previous research has confirmed that system performance using Orthogonal Demodulation Type PGC (ODT-PGC) method is determined by many parameters, such as signal phase delay, FM depth, laser intensity accompanying modulation. This article proposes a new PGC demodulation method based on Fixed Phase Delay (FPD-PGC) by 3x2 directional coupler, using second-harmonic components of two interferometric signals to demodulate. The demodulation principle of the new method is described in detail and its performances have been studied. Theoretical analysis and experimental results show that the new method combines main advantages of directional coupler method and ODT-PGC method, and eliminates, to a great extent, the impacts of FM depth, signal phase delay, intensity modulation. Signal-to-total-Harmonic Ratio (SHR) of new method increases more than 30dB compare with ODT-PGC method under the condition of intensity modulation coefficient is 0.4. Besides that, Signal to Noise Ratio (SNR) also improves significantly.
8019E2A9	We propose a digital receiver for FM subcarrier systems including the radio broadcast data systems (RBDS) and the radio data systems (RDS). The RBDS receiver is demodulating the 57 kHz modulated digital signal to the baseband RBDS digital signal. It contains carrier Costas loop synchronization techniques, a symbol timing recovery and decoding techniques, and an RBDS decoder. The RBDS decoder is used to recover the digital signal to the information data and sent to the output of the RBDS system. The main functions in the RBDS decoder are block error detection/correction, block synchronization, and block type detection.
811A640A	In this paper, a measurement method for DTMB modulator which performs the estimation of timing and frequency offsets, channel impulse response (CIR), as well as modulation error ratio (MER) by using only two consecutive frames in an iterative manner is proposed to precisely measure the modulator performance in short time. Both simulation and lab testing results show that the measurement method could reflect the performance of DTMB modulator with high accuracy.
7F884997	Discrete time state-space models for multiple source frequency modulated communications environments are developed and coupled digital phase-locked loop (CDPLL) estimator structures are derived based on the extended Kalman filter (EKF). Unlike related efforts found in the literature, the receiver presented includes both passband and baseband coupling between the DPLLs. Separability of two sources is investigated by examination of state observability and by simulations of the derived estimators. The relationship between the EKF and the CDPLL is presented and their linear and nonlinear behavior discussed. As the CDPLLs exhibit chaotic behavior during acquisition and cannot be linearized during tracking, acquisition and tracking characteristics are extracted from simulations. One simulation result is the effect of state observability on the coupling between the DPLLs; when the states are “strongly” observable, the states are not as tightly coupled. It is also shown that the loop bandwidths decrease during periods of “weak” observability.
80F9FEF7	Many motion-compensated image reconstruction (MCIR) methods have been proposed to correct for subject motion in medical imaging. MCIR methods incorporate motion models to improve image quality by reducing motion artifacts and noise. This paper analyzes the spatial resolution properties of MCIR methods and shows that nonrigid local motion can lead to nonuniform and anisotropic spatial resolution for conventional quadratic regularizers. This undesirable property is akin to the known effects of interactions between heteroscedastic log-likelihoods (e.g., Poisson likelihood) and quadratic regularizers. This effect may lead to quantification errors in small or narrow structures (such as small lesions or rings) of reconstructed images. This paper proposes novel spatial regularization design methods for three different MCIR methods that account for known nonrigid motion. We develop MCIR regularization designs that provide approximately uniform and isotropic spatial resolution and that match a user-specified target spatial resolution. Two-dimensional PET simulations demonstrate the performance and benefits of the proposed spatial regularization design methods.
7DAE9282	An algorithm for the separation and energy-based demodulation of two-component mixtures of AM-FM signals is presented. The proposed algorithm is based on the generating differential or difference equation of the mixture signal and nonlinear differential energy operators.
7F374D53	We propose the use of Amplitude-Modulation Frequency-Modulation (AM-FM) features for representing and retrieving X-Ray images with pneumoconiosis. The AM-FM features are estimated using multiscale filterbanks with wave-lengths related with the standard sizes for grading the level of opacities in X-Rays. The extracted AM-FM features represent opacity profusion in terms of instantaneous frequency (IF) and instantaneous amplitude (IA) features. Here, IF estimates in the medium and high scale frequencies can be used to capture early disease symptoms. AM-FM features from the low and medium scale frequencies are associated with advanced disease stages. We demonstrate the performance of the system in X-ray image retrieval and classification applications.
809138D8	It is shown that the nonlinear energy-tracking signal operator Psi (x)=(dx/dt)/sup 2/-xd/sup 2/x/dt/sup 2/ and its discrete-time counterpart can estimate the AM and FM modulating signals. Specifically, Psi can approximately estimate the amplitude envelope of AM signals and the instantaneous frequency of FM signals. Bounds are derived for the approximation errors, which are negligible under general realistic conditions. These results, coupled with the simplicity of Psi , establish the usefulness of the energy operator for AM and FM signal demodulation. These ideas are then extended to a more general class of signals that are sine waves with a time-varying amplitude and frequency and thus contain both an AM and an FM component; for such signals it is shown that Psi can approximately track the product of their amplitude envelope and their instantaneous frequency. The theoretical analysis is done for both continuous- and discrete-time signals.
7EFC98DF	Multidimensional amplitude-modulation frequency-modulation (AM-FM) models allow us to describe continuous-scale modulations in digital images. AM-FM models have led to a wide range of applications ranging from image and video compression, video image segmentation, to image retrieval in digital libraries. We present new, two-dimensional algorithms that provide significant improvements in both accuracy and speed over previously reported non-parametric approaches. Results are shown for both real and synthetic images.
786B632E	A two-dimensional (2-D) inverse synthetic aperture radar (ISAR) return signal model that employs stepped frequency (SF) modulation is developed. The geometry of the examined ISAR scenario is described by analytical geometrical equations. The target to be imaged is represented by a rectangular grid of point scatterers, moving along a rectilinear trajectory at constant speed, without any rotational motion. Thus, the inverse synthetic aperture results from the translational motion of the target for a short period of time. The process of ISAR signal modelling through coherent summation of the SF-modulated signals reflected from different point scatterers of the target is thoroughly described. Moreover, an efficient ISAR image reconstruction approach, including cross-correlation-based range compression and fast-Fourier-transform-based azimuth compression, is presented through analytical mathematical expressions. Numerical simulations are carried out for various SF ISAR scenarios and high-resolution ISAR images are obtained by applying the proposed ISAR image reconstruction approach. Simulation results (ISAR images and corresponding entropy values) indicate the validity of the proposed 2-D SF ISAR return signal model and the efficiency of the proposed imaging algorithms. Finally, a numerical simulation result is illustrated, which shows the comparison of the performance of the proposed ISAR image reconstruction algorithms based on SF and linear frequency modulation waveforms. It is shown that the two waveforms attain almost the same ISAR image resolution.
7870F50A	A new fault diagnosis method based on empirical mode decomposition (EMD) and homomorphic filtering demodulation is proposed for rolling bearing. The vibration signal of fault rolling bearing is decomposed into a series of intrinsic mode functions (IMFs) by EMD, then extract the envelopes from the outstanding IMFs with various fault characteristic information by homomorphic filtering demodulation and Hilbert envelope demodulation, and do the comparison analysis. The research results show that homomorphic filtering demodulation is superior to Hilbert envelope demodulation, and the combination of EMD and homomorphic filtering demodulation is an effective approach for rolling bearing fault diagnosis.
79EBCADB	In this paper, a new spectral precoding scheme is developed for constant-envelope orthogonal frequency-division multiplexing (CE-OFDM) signals to provide very small power spectral sidelobes. It is shown that the proposed spectrally precoded CE-OFDM scheme can provide higher spectral compactness than previously designed spectrally precoded CE-OFDM block scheme and continuous-phase CE-OFDM scheme.
7DD853D5	Position-sensitive detectors (PSDs), or lateral-effect photodiodes, are commonly used for high-speed, high-resolution optical position measurement. This paper describes the instrument design for multidimensional position and orientation measurement based on the simultaneous position measurement of multiple modulated sources using frequency-domain-multiplexed (FDM) PSDs. The important advantages of this optical configuration in comparison with laser/mirror combinations are that it has a large angular measurement range and allows the use of a probe that is small in comparison with the measurement volume. We review PSD characteristics and quantitative resolution limits, consider the lock-in amplifier measurement system as a communication link, discuss the application of FDM to PSDs, and make comparisons with time-domain techniques. We consider the phase-sensitive detector as a multirate DSP problem, explore parallels with Fourier spectral estimation and filter banks, discuss how to choose the modulation frequencies and sample rates that maximize channel isolation under design constraints, and describe efficient digital implementation. We also discuss hardware design considerations, sensor calibration, probe construction and calibration, and 3-D measurement by triangulation using two sensors. As an example, we characterize the resolution, speed, and accuracy of an instrument that measures the position and orientation of a 10 mm times 5 mm probe in 5 degrees of freedom (DOF) over a 30-mm cube with 4-mum peak-to-peak resolution at 1-kHz sampling.
75EB8088	To have higher resolution of distance in the laser scanner using the phase demodulation method, to demodulate using an intermediate frequency is advantageous. This method is called the multiple phase demodulation method. In the multiple phase demodulation method several frequencies are used for signal processing. These signals are made from the oscillator and the programmable clock source. Even though the clock source is well synchronized with the oscillator, the initial phase error problem can be occurred since the operating points of both signal sources are not exactly simultaneous. In this work, the experimental results are presented to show how the initial phase error problem is solved.
7F6A882D	In wireless sensor networks (WSN), location information acquisition is critical to guarantee their performance. This paper presents a new positioning method in WSN with high precision at reasonable implementation cost for 3D case. Reference nodes with known locations transmit linear frequency modulation continuous wave (FMCW), while other sensor nodes estimate the range difference to them based on the received signals' frequency difference, called time frequency difference arrival (TFDA). The location information can be obtained by solving a set of hyperbolic equations. Two different positioning methods: Taylor iterative method and Chan's method are inspected and compared in terms of accuracy, constraints and computational complexity. This proposed technique is cost-effective, scalable and easy to implement. The simulation results show that the new method enjoys high precision.
7E99AABF	The Teager-Kaiser energy operator and the related energy separation algorithm (ESA) find numerous applications in various problems related to monocomponent AM-FM demodulation. This energy-operator-based approach, however, applies only to signals with narrowband frequency content, i.e., the information bandwidth and frequency deviation about the carrier are small relative to the carrier frequency. For signals with large frequency deviation, modulation indices, this approximation fails, and the ESA incurs large frequency/amplitude demodulation errors. In this letter, we develop a generalized energy-operator-based approach that uses frequency transformations derived from multirate operations such as decimation/interpolation and heterodyning. This generalized approach is shown to produce significant reduction of the demodulation error over the conventional ESA, particularly where the modulation index or frequency deviation is large.
7FFE99D0	This report discusses heterodyne holographic interferometry and time-average holography with a frequency shifted reference beam. Both methods will be used for the measurement and visualization of internal transonic flows, where the target facility is a flutter cascade. The background and experimental requirements for both methods are reviewed. Measurements using heterodyne holographic interferometry are presented. The performance of the laser required for time-average holography of time-varying transonic flows is discussed. 
7EE520BE	In this paper, we investigate the problem of choosing the best domain for embedding watermarks in digital still images. During this paper, twelve watermarking techniques were implemented and evaluated. Four techniques were selected to represent different approaches of embedding data in spatial domain, three approaches using discrete cosine transform (DCT) domain, two approaches using discrete wavelet transform domain (DWT) domain, and two combined DWT-DCT techniques. The algorithms were chosen to represent a range of computational complexities and implementation structures. The performance of the selected algorithms was evaluated with respect to many perceptibility parameters.
7E84C3E6	Two methods of vestigial sideband (VSB) realization, the staggered modulation (SM) and cosine modulation (CM), are analyzed. It is shown that if the SM method is used to eliminate the frequency spectral redundancy, the modulation error rate (MER) performance is lower than CM's. However, the CM scheme is considered to be higher computational complexity. To resolve the existing drawbacks, the improved staggering modulation (ISM) method based on staggered quadrature amplitude modulated (SQAM) is proposed. Utilizing the interpolation filter instead of the time shifting filter, ISM approach simplifies the complexity of modulation without loss of the MER performance.
78962906	An improved demodulation method by solving a quadratic equation was proposed for the phase generated carrier (PGC) scheme with frequency modulation to suppress the impact of laser intensity modulation (LIM). The influence of LIM on total harmonic distortion (THD) was analyzed for PGC algorithm considering laser intensity modulation coefficient (LIMC), signal amplitude, initial phase and modulation depth. According to our analysis, the maximum THD of PGC algorithm is more than -24dB when LIMC is 0.1. While the maximum THD is less than -65dB by using the improved method when LIMC is less than 0.5, which is limited by the low-pass filter. For an experiment system with 0.285 LIMC, the THD was -9dB for PGC algorithm. By using the improved method, the THD approached -50dB
8000711F	This paper presents a computer-aided diagnostic (CAD) system for analyzing chest radiographs based on the International Labor Organization (ILO) standards. We introduce an amplitude-modulation frequency-modulation (AM-FM) based methodology by which a computer-based system will extract AM-FM features and detect those with suspected interstitial lung diseases. For classification, we use Partial Least Squares (PLS) using a low number of extracted factors (making the system robust). We consider several different AM-FM classifiers based on extracting features from individual scales as well as a final classifier that combines results from the individuals scales. We validate our methodology on 11 standard images graded according to the ILO standard. For several scales, as well as for the combined classifier that uses information from all scales, we get excellent classification results (area under the receiver operator characteristics curve equal to 1.0) using a limited number of latent PLS factors.
7ABD9C76	We propose a modular architecture for long-haul optical networks supporting flexible-bandwidth superchannels. Colorless transceivers can be designed to modulate/detect up to M = 4 subcarriers, each at a symbol rate of 12.5 Gbaud, achieving a maximum bit rate of 200 Gbit/s, assuming polarization-multiplexed quadrature phase-shift keying (PM-QPSK). A set of N synchronized transceivers can cooperate to modulate/detect a superchannel comprising N·M subcarriers using no-guard-interval orthogonal frequency-division multiplexing, enabling transmission at bit rates beyond 1 Tbit/s. We analyze and simulate the performance of the proposed architecture in the presence of linear fiber impairments and synchronization errors and establish design requirements for practical deployment of the architecture. Simulation results are shown for transmission of a superchannel comprising 24 subcarriers, which conveys approximately 1.1 Tbit/s with a spectral efficiency of 3.5 bits/s/Hz using PM-QPSK.
7D0C33EE	In this paper, a novel adaptive digital image watermarking model based on modified Fuzzy C-means clustering is proposed. For watermark embedding process, we used Discrete Wavelet Transform (DWT). A segmentation technique XieBeni integrated Fuzzy C-means clustering (XFCM) is used to identify the segments of original image to expose suitable locations for embedding watermark. We also pre-processed the host image using Particle Swarm Optimization (PSO) to lend a hand to the clustering process. The goal is to focus on proper segmentation of the image so that the embedded watermark can withstand common image processing attacks and provide security to digital images. Several attacks were performed on the watermarked images and original watermark was extracted. Performance measures like PSNR, MSE, CC were computed to test the extracted watermarks with and without attacks. Experimental results show that the proposed scheme has performed well in terms of imperceptibility and robustness when compared to other watermarking models.
7D712204	We develop new amplitude-modulated frequency-modulated (AM-FM) based methods to address some issues associated with the semantic gap between visual and mathematical features presented by retinal diseases such as age-related macular degeneration (AMD). Through the processing of simulated and real, clinical retinal images we gain an understanding of the effects of basic morphological characteristics of lesions associated with AMD. Through synthetic simulations, we discuss how histograms of the instantaneous amplitude and the instantaneous frequency magnitude, extracted from different scales, can be used to differentiate between images of different sizes and edge sharpness, while maintaining invariance with respect to rotations. We show that AM-FM features extracted from low and very-low frequency scales can clearly differentiate between retinal images containing Temporal Concentrated Drusen (TCD) and Geographic Atrophy (GA). Shape, size, distribution and edge sharpness are visual features used by ophthalmologists in identifying lesions such as drusen. We propose the use of new AM-FM derived features to quantitatively define these visual descriptions.
7914A0A6	Previously investigated multicomponent AM-FM demodulation techniques either assume that the individual component signals are spectrally isolated from each other or that the components can be isolated by linear time-invariant filtering techniques and, consequently, break down in the case where the components overlap spectrally or when one of the components is stronger than the other. In this paper, we present a nonlinear algorithm for the separation and demodulation of discrete-time multicomponent AM-FM signals. Our approach divides the demodulation problem into two independent tasks: algebraic separation of the components based on periodicity assumptions and then monocomponent demodulation of each component by instantaneously tracking and separating its source energy into its amplitude and frequency parts. The proposed new algorithm avoids the shortcomings of previous approaches and works well for extremely small spectral separations of the components and for a wide range of relative amplitude/power ratios. We present its theoretical analysis and experimental results and outline its application to demodulation of cochannel FM voice signals.
7D7D5BA3	Video communication is often afflicted by various forms of losses, such as packet loss over the Internet. This paper examines the question of whether the packet loss pattern, and in particular, the burst length, is important for accurately estimating the expected mean-squared error distortion resulting from packet loss of compressed video. We focus on the challenging case of low-bit-rate video where each P-frame typically fits within a single packet. Specifically, we: 1) verify that the loss pattern does have a significant effect on the resulting distortion; 2) explain why a loss pattern, for example a burst loss, generally produces a larger distortion than an equal number of isolated losses; and 3) propose a model that accurately estimates the expected distortion by explicitly accounting for the loss pattern, inter-frame error propagation, and the correlation between error frames. The accuracy of the proposed model is validated with H.264/AVC coded video and previous frame concealment, where for most sequences the total distortion is predicted to within plusmn0.3 dB for burst loss of length two packets, as compared to prior models which underestimate the distortion by about 1.5 dB. Furthermore, as the burst length increases, our prediction is within plusmn0.7 dB, while prior models degrade and underestimate the distortion by over 3 dB. The proposed model works well for video-telephony-type of sequences with low to medium motion. We also present a simple illustrative example, of how knowledge of the effect of burst loss can be used to adapt the schedule of video streaming to provide improved performance for a burst loss channel, without requiring an increase in bit rate.
7FA170FD	We consider the problem of error control for receiver-driven layered multicast of audio and video over the Internet. The sender injects into the network multiple source layers and multiple channel coding (parity) layers, some of which are delayed relative to the source, Each receiver subscribes to the number of source layers and the number of parity layers that optimizes the receiver's quality for its available bandwidth and packet loss probability. We augment this layered FEC system with layered pseudo-ARQ. Although feedback is normally problematic in broadcast situations, ARQ can be simulated by having the receivers subscribe and unsubscribe to the delayed parity layers to receive missing information. This pseudo-ARQ scheme avoids an implosion of repeat requests at the sender and is scalable to an unlimited number of receivers, We show gains of 4-18 dB on channels with 20% loss over systems without error control and additional gains of 1-13 dB when FEC is augmented by pseudo-ARQ in a hybrid system, Optimal error control in the hybrid system is achieved by an optimal policy for a Markov decision process.
7FE46870	The quality of service limitation of today's best-effort networks poses major challenge for low-latency video communication. To combat network losses for real-time and on-demand video communication, which exhibits stronger dependency across packets, a network-adaptive coding scheme is employed to dynamically manage the packet dependency using optimal reference picture selection. The selection of the reference is achieved within a rate-distortion optimization framework and is adapted to the varying network conditions. For network-adaptive streaming of prestored video, based on an accurate loss-distortion model, a prescient scheme that optimizes the dependency of a group of packets is proposed to achieve global optimality as well as improved rate-distortion performance. With the improved trade-off between compression efficiency and error resilience, the proposed system does not require retransmission of lost packets, which makes less than one-second low-latency communication possible.
773F1213	We consider an unconventional procedure for communicating to the server the receipt of media packets for Internet video streaming. Instead of separately acknowledging each media packet as it arrives, we periodically send to the server a single acknowledgment packet, denoted rich acknowledgment, that contains information about all media packets that have arrived at the client by the time the rich acknowledgment is sent. We investigate rate-distortion optimized sender-driven streaming that employs rich acknowledgments. Performance gains of up to 1.3 dB for streaming packetized video content are observed over rate-distortion optimized senderdriven systems that employ conventional acknowledgments. 
8123EE44	This paper addresses the problem of streaming packetized media over a lossy packet network in a rate-distortion optimized way. We show that although the data units in a media presentation generally depend on each other according to a directed acyclic graph, the problem of rate-distortion optimized streaming of an entire presentation can be reduced to the problem of error-cost optimized transmission of an isolated data unit. We show how to solve the latter problem in a variety of scenarios, including the important common scenario of sender-driven streaming with feedback over a best-effort network, which we couch in the framework of Markov decision processes. We derive a fast practical algorithm for nearly optimal streaming in this scenario, and we derive a general purpose iterative descent algorithm for locally optimal streaming in arbitrary scenarios. Experimental results show that systems based on our algorithms have steady-state gains of 2-6 dB or more over systems that are not rate-distortion optimized. Furthermore, our systems essentially achieve the best possible performance: the operational distortion-rate function of the source at the capacity of the packet erasure channel.
764E3831	The directional intra prediction (IP) in H.264/AVC and HEVC tends to cause the residue to be anisotropic. To transform the IP residue, Mode Dependent Directional Transform (MDDT) based on Karhunen Loève transform (KLT) can achieve better energy compaction than DCT, with one transform assigned to each prediction mode. However, due to the data variation, different residue blocks with the same IP mode may not have the same statistical properties. Instead of constraining one transform for each IP mode, in this paper, we propose a novel rate-distortion optimized transform (RDOT) scheme which allows a set of specially trained transforms to be available to all modes, and each block can choose its preferred transform to minimize the rate-distortion (RD) cost. We define a cost function which is an estimate of the true RD cost and use the Lloyd-type algorithm (a sequence of transform optimization and data reclassification alternately) to find the optimal set of transforms. The proposed RDOT scheme is implemented in HM9.0 software of HEVC. Experimental results suggest that RDOT effectively achieves 1.6% BD-Rate reduction under the Intra Main condition and 1.6% BD-Rate reduction under the Intra High Efficiency (HE) 10bit condition.
78CFB32A	We consider rate-distortion optimized strategies for dropping frames from multiple conversational and streaming videos sharing limited network node resources. The dropping strategies are based on side information that is extracted during encoding and is sent along the regular bitstream. The additional transmission overhead and the computational complexity of the proposed frame dropping schemes are analyzed. Our experimental results show that a significant improvement in end-to-end performance is achieved compared to priority-based random early dropping.
0319D9B8	The general problem of optimized video encoding has received a great deal of attention in recent years. This paper focuses on the optimization of video coding with frameskip. We propose models that estimate the distortion for coded frames as well as non-coded frames. Using these models in conjunction with well-know models that estimate the rate allows us to formulate a rate control problem that trades-off spatial and temporal quality. Simulation results indicate moderate improvements for low motion test sequences.
7EA6573E	Motion information scalability is an important requirement for a fully scalable video codec, especially in low bit rate or small resolution decoding scenarios, for which the fully scalable motion model (SMM) has been proposed. SMM can collaborate flawlessly with other scalabilities, such as spatial, temporal and quality, in a scalable video codec. It performs better than the nonscalable motion model. To further improve the SMM, this correspondence extends the algorithm to support the hierarchical B frame structure and bidirectional or multidirectional motion estimation. Furthermore, the corresponding rate distortion optimized estimation for improved efficiency in several scenarios is discussed. Several simulation results based upon the updated framework are presented to verify the advantage of this extension.
591D1690	In this paper, we discuss how to apply the rate distortion technique to select the optimal mode in the scalable coding. Firstly, we analyze this problem from a general scalable model and point out that the complicated dependencies among the different macroblocks and layers make the original independency assumption no longer a right approximation. Secondly, we propose an EOD function to estimate this dependency and derive a simple formula of this function. We apply the proposed algorithm to the H.26L PFGS, and the experimental results show that the algorithm significantly improves the coding efficiency of the H.26L PFGS. Further studies on how to design the EOD function more accurately is quite interesting and significant.
808A4297	We describe an effective method for increasing error resilience of video transmission over bit error prone networks. Rate-distortion optimized mode selection and synchronization marker insertion algorithms are introduced. The resulting video communication system takes into account the channel condition and the error concealment method used by the decoder, to optimize video coding mode selection and placement of synchronization markers in the compressed bit stream. The effects of mismatch between the parameters used by the encoder and the parameters associated with the actual channel condition and the decoder error concealment method are evaluated. Results for the binary symmetric channel and wideband code division multiple access mobile network models are presented in order to illustrate the advantages of the proposed method.
80EF36FE	Video distortion metrics based on models of the human visual system have traditionally used comparisons between the distorted signal and a reference signal to calculate distortions objectively. In video coding applications, this is not prohibitive. In quality monitoring applications, however, access to the reference signal is often limited. This paper presents a computationally efficient video distortion metric that can operate in full- or reduced-reference mode as required. The metric is based on a model of the human visual system implemented using the wavelet transform and separable filters. The visual model is parameterized using a set of video frames and the associated quality scores. The visual model's hierarchical structure, as well as the limited impact of fine scale distortions on quality judgments of severely impaired video, are exploited to build a framework for scaling the bitrate required to represent the reference signal. Two applications of the metric are also presented. In the first, the metric is used as the distortion measure in a rate-distortion optimized rate control algorithm for MPEG-2 video compression. The resulting compressed video sequences demonstrate significant improvements in visual quality over compressed sequences with allocations determined by the TM5 rate control algorithm operating with MPEG-2 at the same rate. In the second, the metric is used to estimate time series of objective quality scores for distorted video sequences using reference bitrates as low as 10 kb/s. The resulting quality scores more accurately model subjective quality recordings than do those estimated using the mean squared error as a distortion metric, while requiring a fraction of the bitrate used to represent the reference signal. The reduced-reference metric's performance is comparable to that of the full-reference metrics tested in the first Video Quality Experts Group evaluation.
8048EDA2	Error resilient, low latency video coding for interactive video applications requires progressive intra coding of macroblocks (MBs) to contain the error propagation. Both refresh MB selection (RMS) and refresh rate selection (RRS) impact the subjective video quality in the presence of packet losses. Joint source-channel rate distortion optimization methods attempt to find the best trade-off between compression efficiency and end-to-end distortion at an MB-level and are typically computationally expensive in addition to not being optimal at a picture level. While probabilistic error propagation tracking is used for refresh MB selection in previous work, these picture-level optimal RRS methods model source-channel distortion by mimicking the effect of periodic intra frame coding which does not match well with content adaptive refresh MB selection. In this paper, we propose a frame-level approach to RRS that aligns the joint source-channel rate-distortion trade-off modeling with an enhanced RMS process to achieve an optimal end-to-end distortion that is content, bit-rate and channel adaptive. For typical videoconferencing content, the proposed approach is quite low in complexity, works on par with off-line multi-pass identification of the optimal fixed refresh rate and is quite competitive when compared to the H.264 Joint Model’s lossy rate-distortion optimization technique.
7FB93FC8	This paper addresses the problem of streaming packetized media over a lossy packet network through an intermediate proxy server to a client, in a rate-distortion optimized way. The proxy, located at the junction of the backbone network and the last hop to the client, coordinates the communication between the media server and the client using hybrid receiver/sender-driven streaming in a rate-distortion optimization framework. The framework enables the proxy to determine at every instant which packets, if any, it should either request from the media server or retransmit directly to the client, in order to meet a constraint on the average transmission rate while minimizing the average end-to-end distortion. Performance gains of up to 1.5 dB and up to 4 dB are observed over rate-distortion optimized sender-driven systems for the case when the last hop is wireline and wireless, respectively.
7FEDFE11	New standardization activities have been recently launched by the JCT-VC experts group in order to challenge the current video compression standard H.264/AVC. Several improvements of this standard, previously integrated in the JM key technical area software, are already known and gathered in the high efficiency video coding test model. In particular, competition-based motion vector prediction has proved its efficiency. However, the targeted 50% bitrate saving for equivalent quality is not yet achieved. In this context, this paper proposes to reduce the signaling information resulting from this motion vector competition, by using data hiding techniques. As data hiding and video compression traditionally have contradictory goals, an advanced study of data hiding schemes is first performed. Then, an original way of using data hiding for video compression is proposed. The main idea of this paper is to hide the competition index into appropriately selected chroma and luma transform coefficients. To minimize the prediction errors, the transform coefficients modification is performed via a rate-distortion optimization. The proposed scheme is evaluated on several low and high resolution sequences. Objective improvements (up to 2.40% bitrate saving) and subjective assessment of the chroma loss are reported.
779C3AFF	In this paper, a novel adaptive Lagrange multiplier selection model in rate-distortion optimization (RDO) is proposed to determine the motion estimation mode during motion compensated temporal filter (MCTF) decomposition in the context of 3D wavelet-based scalable video codec (SVC). First, the motion activity of temporal subbands is investigated. Then, the model parameters for different MCTF levels are estimated. Finally, an optimization model for each temporal subband is obtained from the adaptive Lagrange multiplier selection. We demonstrate the accuracy and performance of our proposed model through extensive numerical simulations. Experimental results illustrate that the proposed model is adaptive with the characteristics of the temporal subbands, suggesting that our model can effectively improve the video quality in terms of both the PSNR and the mean structural similarity index (mean SSIM).
7E0721E4	Block-based discrete cosine transform (DCT) has been successfully adopted into several international image/video coding standards, e.g., MPEG-2, H.264/AVC, as it can achieve a good tradeoff between performance and complexity. Although DCT theoretically approximates the optimum Karhunen-Loève transform under first-order Markov conditions, one fixed set of transform basis functions (TBF) cannot handle all the cases efficiently due to the non-stationary nature of video contents. To further improve the performance of block-based transform coding, in this paper, we present the design of rate-distortion optimized transform (RDOT) which contributes to both intraframe and interframe coding. The most important property which makes a difference between RDOT and the conventional DCT is that, in the proposed method, transform is implemented with multiple TBF candidates which are obtained from off-line training. With this feature, for coding each residual block, the encoder is capable to select the optimal set of TBF in terms of rate-distortion performance, and better energy compaction is achieved in the transform domain. To obtain an optimum group of candidate TBF, we have developed a two-step iterative optimization technique for the off-line training, with which the TBF candidates are refined at each iteration until the training process becomes converged. Moreover, analysis on the optimal group of candidate TBF is also presented in this paper, with a detailed description of a practical implementation for the proposed algorithm on the latest VCEG key technical area software platform. Extensive experimental results show that, compared with the conventional DCT-based transform scheme adopted into the state-of-the-art H.264/AVC video coding standard, significant improvement of coding performance has been achieved for both intraframe and interframe coding with our proposed method.
7870E33D	Recent advances in video capturing and display technologies, along with the exponentially increasing demand of video services, challenge the video coding research community to design new algorithms able to significantly improve the compression performance of the current H.264/AVC standard. This target is currently gaining evidence with the standardization activities in the High Efficiency Video Coding (HEVC) project. The distortion models used in HEVC are mean squared error (MSE) and sum of absolute difference (SAD). However, they are widely criticized for not correlating well with perceptual image quality. The structural similarity (SSIM) index has been found to be a good indicator of perceived image quality. Meanwhile, it is computationally simple compared with other state-of-the-art perceptual quality measures and has a number of desirable mathematical properties for optimization tasks. We propose a perceptual video coding method to improve upon the current HEVC based on an SSIM-inspired divisive normalization scheme as an attempt to transform the DCT domain frame prediction residuals to a perceptually uniform space before encoding. Based on the residual divisive normalization process, we define a distortion model for mode selection and show that such a divisive normalization strategy largely simplifies the subsequent perceptual rate-distortion optimization procedure. We further adjust the divisive normalization factors based on local content of the video frame. Experiments show that the proposed scheme can achieve significant gain in terms of rate-SSIM performance when compared with HEVC.
656ECA98	Raster document coders are typically based on the use of a binary mask layer that efficiently encodes the text and graphic content. While these methods can yield much higher compression ratios than natural image compression methods, the binary representation tends to distort fine document details, such as thin lines, and text edges. In this paper, we describe a method for encoding and decoding the binary mask layer that substantially improves the decoded document quality at a fixed bit rate. This method, which we call resolution enhanced rendering (RER), works by adaptively dithering the encoded binary mask, and then applying a nonlinear predictor to decode a gray level mask at the same or higher resolution. We present experimental results illustrating that the RER method can substantially improve document quality at high compression ratios.
80A44F9B	We present an effective framework for increasing the error-resilience of low bit-rate video communications over an error-prone packet-switched network. Our framework is based on the principle of layered coding with transport prioritization. We introduce a rate-distortion optimized mode-selection algorithm for our prioritized layered framework. This algorithm is based on a joint source/channel-coding approach and trades off source coding efficiency for increased bitstream error-resilience to optimize the video coding mode selection within and across layers. The algorithm considers the channel conditions, as well as the error recovery and concealment capabilities, of the channel codec and source decoder, respectively. Important framework parameters including the packetization scheme, decoder error concealment method, and channel codec error-protection strength are considered. The effects of mismatch between the parameters employed by the encoder and the actual channel conditions are considered. Results are presented for a wide range of packet loss rates in order to illustrate the benefits of the proposed framework.
8015072B	The curved wavelet transform (CWT) was developed to enhance compactness of the wavelet transform (WT) representation. Curve determination is critical for the CWT because a well-defined curve set can increase the performance gain in terms of the rate-distortion (R-D). Conventionally, the image to be encoded is divided into blocks and the curve orientation in each block is independently determined through the minimization of its high-pass CWT energy. In this paper, we propose an R-D optimization algorithm for the curve determination, in which variable block size and the impact of neighboring blocks are taken into account. To reduce the computational cost, an alternative sampling strategy is exploited. Experiment results with natural images show that the proposed algorithm can provide better image quality, measured objectively or subjectively, compared to the conventional CWT coding algorithm. Importantly, the proposed approach overcomes the hurdles of computational cost and optimization at the global level opens the door for further performance enhancements of applications with the CWT.
7EC2A91B	This paper addresses the problem of streaming packetized media over a lossy packet network to a wireless client, in a rate-distortion optimized way. We introduce an incremental redundancy error-correction scheme that combats the effects of both packet loss and bit errors in an end-to-end fashion, without support from the underlying network or from an intermediate base station. The scheme is employed within an optimization framework that enables the sender to compute which packets it should send, out of all the packets it could send at a given transmission opportunity, in order to meet an average transmission-rate constraint while minimizing the average end-to-end distortion. Experimental results show that our system is robust and maintains quality of service over a wide range of channel conditions. Up to 8 dB performance gains are registered over systems that are not rate-distortion optimized, at bit-error rates as large as 10/sup -2/.
7DEE704C	In this paper, we introduce a new strategy for processing real-time video packets in programmable network nodes or active routers. We first discuss challenges in transmission of video streams over bandwidth-limited networks, followed by the active approach as an advance for streaming real-time video. In our model, each programmable node (or router) makes admission decision for video frames based on evaluating their potential value. Frames "bid" their expected distortion price and the node chooses the best ones first until resource is fully utilised. We also analyse complexity and overhead to show its clear benefit against other known strategies. Simulation experiments demonstrate consistent outperformance of the framework in comparison to Lagrangian-based rate-distortion optimized schemes.
7D8084CF	This paper addresses the problem of encoder optimization in a macroblock-based multimode video compression system. An efficient solution is proposed in which, for a given image region, the optimum combination of macroblock modes and the associated mode parameters are jointly selected so as to minimize the overall distortion for a given bit-rate budget. Conditions for optimizing the encoder operation are derived within a rate-constrained product code framework using a Lagrangian formulation. The instantaneous rate of the encoder is controlled by a single Lagrange multiplier that makes the method amenable to mobile wireless networks with time-varying capacity. When rate and distortion dependencies are introduced between adjacent blocks (as is the case when the motion vectors are differentially encoded and/or overlapped block motion compensation is employed), the ensuing encoder complexity is surmounted using dynamic programming. Due to the generic nature of the algorithm, it can be successfully applied to the problem of encoder control in numerous video coding standards, including H.261, MPEG-1, and MPEG-2. Moreover, the strategy is especially relevant for very low bit rate coding over wireless communication channels where the low dimensionality of the images associated with these bit rates makes real-time implementation very feasible. Accordingly, in this paper, the method is successfully applied to the emerging H.263 video coding standard with excellent results at rates as low as 8.0 Kb per second. Direct comparisons with the H.263 test model, TMN5, demonstrate that gains in peak signal-to-noise ratios (PSNR) are achievable over a wide range of rates.
7A7264D5	We propose new models and methods for rate-distortion (RD) optimal video delivery over IP, when packets with bit errors are also delivered. In particular, we propose RD optimal methods for slicing and unequal error protection (UEP) of packets over IP allowing transmission of packets with bit errors. The proposed framework can be employed in a classical independent-layer transport model for optimal slicing, as well as in a cross-layer transport model for optimal slicing and UEP, where the forward error correction (FEC) coding is performed at the link layer, but the application controls the FEC code rate with the constraint that a given IP packet is subject to constant channel protection. The proposed method uses a novel dynamic programming approach to determine the optimal slicing and UEP configuration for each video frame in a practical manner, that is compliant with the AVC/H.264 standard. We also propose new rate and distortion estimation techniques at the encoder side in order to efficiently evaluate the objective function for a slice configuration. The cross-layer formulation option effectively determines which regions of a frame should be protected better; hence, it can be considered as a spatial UEP scheme. We successfully demonstrate, by means of experimental results, that each component of the proposed system provides significant gains, up to 2.0 dB, compared to competitive methods
7DE54C8A	This paper presents a rate-distortion optimization approach to hybrid sound coding. The advantages of sinusoidal and transform coding are combined by a rate-distortion optimization mechanism, using a perceptually relevant distortion measure based on spectral auditory masking. As a result, the coder can adapt to the input signal and to constraints such as bit rate. Listening test results show improved performance of the hybrid coder compared to the individual coding paradigms. There is a good correlation between the improved performance as reported by the listeners and the differences in distortion resulting from the perceptually relevant distortion measure. This confirms that the distortion measure used in the optimization is useful; moreover, it shows the feasibility of the rate-distortion optimization approach for hybrid sound coding.
7FC42BEF	This paper presents an algorithm to optimize the tradeoff between rate and expected end-to-end distortion of a video sequence transmitted over a packet network. The approach optimizes the source coding parameters, slicing, network QoS class selection and/or error control coding parameters, and accounts for the effects of compression, packetization, error propagation, and concealment at the decoder. It builds on, and substantially extends the applicability of, the recursive optimal per-pixel estimate (ROPE) technique for end-to-end distortion estimation. A trellis-based algorithm is introduced in order to overcome macroblock interdependencies in the estimation procedure, and allow adaptive slicing. Moreover, we propose a complementary packetization scheme to efficiently arrange the slices into packets for FEC protection while minimizing rate loss due to padding. Simulations demonstrate consistent gains over currently used techniques.
06F0B7DE	Rate-distortion optimization (RDO) plays a significant role in video coding. However, in most RDO methods, the distortion measurement metrics consider only the spatial distortion of statistical pixel errors. People have concerns about not only the information of independent pixels, but also the spatial and temporal correlations between them. In order to make the distortion assessment more consistent with human perception, temporal information of the successive images and the characteristics of human visual perception should be considered as well. In this paper, we propose a rate-distortion model based on spatio-temporal video structural similarity (stVSSIM) index, which takes both spatial and temporal visual quality into account. Meanwhile, to obtain a reasonable trade-off between bit-rate and visual quality dynamically, a perceptual adaptive Lagrange multiplier selection method is presented. Simulation results show that the proposed method averagely reduces 20% bit-rate under the equal visual quality and the adaptive Lagrange multiplier can further improve the results.
7624C006	In this paper we present a state of the art, practical, realtime, region of interest (ROI) video encoder implemented on the Texas Instruments TMS320DM3x SOC. The proposed algorithm is a novel rate distortion optimized ROI coding algorithm with low complexity making it ideal for implementing on embedded video SOCs with low computational and memory resources while achieving excellent perceptual quality. The proposed solution is a complete solution incorporating ROI processing in the entire video chain from front-end face detection to back-end video compression. It is probably one of the first video capture and compression system implemented on an embedded SOC which relies on specialized rate distortion method for ROI coding using object detection methods from the front end or user inputs. Extensive subjective evaluation has been performed on the proposed algorithm for various resolutions ranging from CIF to 1080p video resolutions at different bitrates for over 300 test cases. Significant subjective quality enhancements have been observed for video sequences over all the different video resolutions at various different bitrates. With the proposed algorithm competitive subjective quality is achieved for video conferencing sequences at 300 kbps for 720p and at 96 kbps for CIF when compared to the case where no ROI based rate distortion methods for coding are used. On the Texas Instruments TMS320DM3x SOC the ROI videoencoder achieved realtime performance for 1080p video resolution at 30 fps.
7DDF5EAE	H.264/AVC encoder employs a complex mode-decision technique based on rate-distortion optimization. It calculates rate-distortion cost (RDcost) for all possible modes to choose the best one having the minimum RDcost. This paper presents a frame-layer rate control for H.264/AVC that computes the Lagrange multiplier (/spl lambda//sub MODE/) for mode decision by using a quantization parameter (QP) which may be different from that used for encoding. At the same time, we also compare actual bits produced by previous macroblocks (MBs) with the total bits allocated to these MBs to further modify /spl lambda//sub MODE/. The objective of these measures aims to produce bits as close to the frame target bits for rate control as possible. This is very important in the case of low-bit-rate tight buffer applications. In order to obtain an accurate QP for a frame, we employ a complexity-based bit-allocation scheme and a QP adjustment method. Simulation results comparing with the H.264 Joint Video Team (JVT) rate control method show that the H.264 encoder, using the proposed algorithm, achieves a visual quality improvement of about 0.56 dB, performs better for buffer overflow and underflow, and achieves a smaller PSNR deviation.
03510265	In this paper, we improve the performance of the embedded coder by reorganising its output bitstream in the rate-distortion (R-D)sense. In the proposed rate-distortion optimized embedding (RDE), the coding bit is first allocated to the coefficient with the steepest R-D slope, i.e. the biggest distortion decrease per coding bit. To avoid transmission of the position of the coded coefficient, RDE uses the expectation R-D slope that can be calculated by the coded bits that have already been transmitted to the decoder. RDE also takes advantage of the probability estimation table of the QM-coder so that the calculation of the R-D slope is just a lookup table operation. Extensive experimental results show that RDE significantly improves the coding efficiency.
7F721715	In this paper, a rate estimation technique using the transform coefficient variance is proposed in the rate-distortion cost function for intra mode decision. The experiment shows that the rate estimation using the absolute transform coefficient variance can predict rates more accurate than the one using ρ-domain model. The new cost function brings out an average speed-up factor of 13.8 when compared with the RDO mode decision, but with considerable degradation in the rate-distortion performance. To improve the coding performance, we propose a SATD-based mode decision algorithm, which incorporates both SATD and the variance information into the RDO mode decision method. The results show that the SATD-based mode decision scheme can achieve a significant improvement in computation with negligible PSNR loss.
78DC4B55	For rate-distortion optimized rate allocation in JVT Scalable Video Coding (SVC), the distortion impact of every FGS NAL unit on the global reconstruction quality is calculated by repeatedly bitstream decoding, which leads to high complexity. In this paper, a fast rate allocation algorithm by modeling distortion estimation is proposed. Based on the hypothesis that DCT residual coefficients follow Laplacian distribution, we establish the distortion estimation model by calculating quantization error of each FGS NAL unit and analyzing the prediction in hierarchical B coding structure. Besides, the parameter in the model is updated according to the distribution of residual coefficients decoded at the base layer within every frame. Experimental results show that compared to the existing method of R-D optimized rate allocation in SVC, the proposed method results in a reduction in decoding time of nearly 50%, and save the runtime of rate allocation by 45.3%, while the PSNR loss of decoded sequence is only 0.04 dB on average.
80407F44	Traditional video compression methods remove spatial and temporal redundancy based on the signal statistical correlation. However, to reach higher compression ratios without perceptually degrading the reconstructed signal, the properties of the human visual system (HVS) need to be better exploited. Research effort has been dedicated to modeling the spatial and temporal just-noticeable-distortion (JND) based on the sensitivity of the HVS to luminance contrast, and accounting for spatial and temporal masking effects. This paper describes a foveation model as well as a foveated JND (FJND) model in which the spatial and temporal JND models are enhanced to account for the relationship between visibility and eccentricity. Since the visual acuity decreases when the distance from the fovea increases, the visibility threshold increases with increased eccentricity. The proposed FJND model is then used for macroblock (MB) quantization adjustment in H.264/advanced video coding (AVC). For each MB, the quantization parameter is optimized based on its FJND information. The Lagrange multiplier in the rate-distortion optimization is adapted so that the MB noticeable distortion is minimized. The performance of the FJND model has been assessed with various comparisons and subjective visual tests. It has been shown that the proposed FJND model can increase the visual quality versus rate performance of the H.264/AVC video coding scheme.
7D687567	We propose a rate-distortion optimization (RDO) scheme based on the structural similarity (SSIM) index, which was found to be a better indicator of perceived image quality than mean-squared error, but has not been fully exploited in the context of image and video coding. At the frame level, an adaptive Lagrange multiplier selection method is proposed based on a novel reduced-reference statistical SSIM estimation algorithm and a rate model that combines the side information with the entropy of the transformed residuals. At the macroblock level, the Lagrange multiplier is further adjusted based on an information theoretical approach that takes into account both the motion information content and perceptual uncertainty of visual speed perception. Finally, the mode for H.264/AVC coding is selected by the SSIM index and the adjusted Lagrange multiplier. Extensive experiments show that the proposed scheme can achieve significantly better rate-SSIM performance and provide better visual quality than conventional RDO coding schemes.
80DEF79C	In today's hybrid video coding, Rate-Distortion Optimization (RDO) plays a critical role. It aims at minimizing the distortion under a constraint on the rate. Currently, the most popular RDO algorithm for one-pass coding is the one recommended in the H.264/AVC reference software. It, or HR-lambda for convenience, is actually a kind of universal method which performs the optimization only according to the quantization process while ignoring the properties of input sequences. Intuitively, it is not efficient all the time and an adaptive scheme should be better. Therefore, a new algorithm Lap- lambda is presented in this paper. Based on the Laplace distribution of transformed residuals, the proposed Lap-lambda is able to adaptively optimize the input sequences so that the overall coding efficiency is improved. Cases which cannot be well captured by the proposed models are considered via escape methods. Comprehensive simulations verify that compared with HR-lambda , Lap-lambda shows a much better or similar performance in all scenarios. Particularly, significant gains of 1.79 dB and 1.60 dB in PSNR are obtained for slow sequences and B-frames, respectively.
7FDADB2D	An important motivation for the development of the emerging H.263+ and MPEG-4 coding standards is to enhance the quality of highly compressed video for two-way, real-time communications. In these applications, the delay produced by bits accumulated in the encoder buffer must be very small, typically below 100 ms, and the rate control strategy is responsible for encoding the video with high quality and maintaining a low buffer delay. In this work, we present a simple rate control technique that achieves these two objectives by smartly selecting the values of the quantization parameters in typical discrete cosine transform video coders. To do this, we derive models for bit rate and distortion in this type of coders, in terms of the quantization parameters. Using Lagrange optimization, we minimize distortion subject to the target bit constraint, and obtain formulas that indicate how to choose the quantization parameters. We implement our technique in H.263 and MPEG-4 coders, and compare its performance to TMN7 and VM7 rate control when the encoder buffer is small, for a variety of video sequences and bit rates. This new method has been adopted as a rate control tool in the test model TMN8 of H.263+ and (with some modifications) in the verification model VM8 of MPEG-4.
75114DD2	The SSIM-based rate-distortion optimization (RDO) has been verified to be an effective tool for H.264/AVC to promote the perceptual video coding performance. However, the current SSIM-based RDO is not efficient for improving the perceptual quality of the video streaming application over the error-prone network, because it does not consider the transmission induced distortion in the encoding process. In this paper, a SSIM-based error-resilient RDO scheme for H.264/AVC is proposed to improve the wireless video streaming performance. Firstly, with the help of the SSE-based RDO, we present a low-complexity Lagrange multiplier decision method for the SSIM-based RDO video coding in the error-free environment. Then, the SSIM-based decoding distortion of the user end is estimated at the encoder and is correspondingly introduced into the RDO to involve the transmission induced distortion into the encoding process. Further, the Lagrange multiplier is theoretically derived to optimize the encoding mode selection in the error-resilient RDO process. Experimental results show that the proposed SSIM-based error-resilient RDO can obtain superior perceptual video quality (more structural information) to the traditional SSE-based error-resilient RDO for wireless video streaming at the same bit rate condition.
7D5A3CAD	We propose a new real-time packet scheduling algorithm for streaming scalable H.264. Our algorithm makes use of a packet importance measure, which we define, that takes into consideration transmission history, channel conditions, and the unique decoding dependencies due to the temporal wavelet encoding. Our algorithm utilizes this importance measure to minimize the expected reconstruction distortion at the decoder under a certain rate constraint. In our experimental results we show gains of more than 3 dB in decoded video quality when transmissions are controlled with our algorithm as compared to existing schedulers
7DB51E7D	We introduce two models for predicting the rate and distortion of the matching-pursuit video codec. The first model is based on a pre-coding analysis pass using the full matching-pursuit dictionary. The second model is based on a reduced-complexity analysis pass. We evaluate these models for use within existing rate-distortion optimization techniques. Our prediction results suggest that the models have sufficient accuracy to be useful in this context, and that significant complexity reductions could be achieved compared to exact rate-distortion computation.
7975585E	Reliable transmission of compressed video in a packet lossy environment cannot be achieved without error recovery mechanisms. We describe an effective method for increasing error resilience of video transmission over packet lossy networks such as the Internet. Intra coding (without reference to a previous picture) is a well-known technique for eliminating temporal error propagation in a predictive video coding system. Randomly intra coding of blocks increases error resilience to packet loss. However, when the error concealment used by the decoder is known, intra encoding following a method that optimizes the tradeoffs between compression efficiency and error resilience is a better alternative. In this paper, we present a rate-distortion optimized mode selection method for packet lossy environments that takes into account the network conditions and the error concealment method used at the decoder. We present results for different packet loss rates and typical packet sizes of the Internet, that illustrate the advantages of the proposed method.
81538A7C	This paper presents a scalable rate control (SRC) scheme based on a more accurate second-order rate-distortion model. A sliding-window method for data selection is used to mitigate the impact of a scene change. The data points for updating a model are adaptively selected such that the statistical behavior is improved. For video object (VO) shape coding, we use an adaptive threshold method to remove shape-coding artifacts for MPEG-4 applications. A dynamic bit allocation among VOs is implemented according to the coding complexities for each VO. SRC achieves more accurate bit allocation with low latency and limited buffer size. In a single framework, SRC offers multiple layers of controls for objects, frames, and macroblocks (MBs). At MB level, SRC provides finer bit rate and buffer control. At multiple VO level, SRC offers superior VO presentation for multimedia applications. The proposed SRC scheme has been adopted as part of the International Standard of the emerging ISO MPEG-4 standard.
597EB3E2	In this paper, we introduce a novel architecture for programmable network nodes that work with a large number of real-time video streams. We first discuss challenges in transmission of video streams over bandwidth-limited networks, followed by the active approach as an advance for streaming real-time video. In our model, each programmable node makes admission decision for video frames based on evaluating their potential value. Frames “bid” their expected distortion price and the node chooses the best ones first until resource is fully utilized. Analysis of complexity and overhead shows clear benefit of our framework. Simulation experiments demonstrate its consistent outperformance in comparison to lagrangian-based Rate-Distortion Optimized schemes.
7681C6D9	Most conventional distortion metrics regard a video frame as a static image, and seldom exploit using the motion information of video frames in succession. Moreover, these methods usually calculate the visual distortion based on the independent spatial pixels. Recently, many researches show that the way people perceive the video signals is similar to the way filters process signals in the frequency domain. Therefore, in order to achieve better visual quality, we introduce a novel distortion measurement into the video coding system, which is consistent with human visual perception, and establish a perception-based rate-distortion optimization model. In this paper, we adopt Gabor filter family to decompose the video signals into frequency domain, and combine the video motion information to measure the perceptual distortion. We call it Motion tuned Distortion metric For Video coding (MDFV). After that we set up an MDFV based rate-distortion optimization model to select the best encoding mode. The experimental results show that the proposed approach is effective.
814AC5B0	The rate-distortion efficiency of video compression schemes is based on a sophisticated interaction between various motion representation possibilities, waveform coding of differences, and waveform coding of various refreshed regions. Hence, a key problem in high-compression video coding is the operational control of the encoder. This problem is compounded by the widely varying content and motion found in typical video sequences, necessitating the selection between different representation possibilities with varying rate-distortion efficiency. This article addresses the problem of video encoder optimization and discusses its consequences on the compression architecture of the overall coding system. Based on the well-known hybrid video coding structure, Lagrangian optimization techniques are presented that try to answer the question: what part of the video signal should be coded using what method and parameter settings?.
7F8DB94E	We propose in this paper a novel error resilient transcoding scheme that can be placed at the boundary between wired and wireless networks via heterogeneous network links. This error resilient transcoder shall seamlessly complement the standard Scalable Video Coding (SVC) bitstream to offer additional error resilient adaptation capability for receiving devices. The novel error resilient transcoding scheme consists of three different modules; each is designed to meet various levels of complexity need. The three modules are all based on the Loss-Aware Rate-Distortion Optimization (LA-RDO) mode decision algorithm we have previously developed for SVC. However, each individual module can be tailored to different complexity requirements depending on whether and how the LA-RDO mode decision is implemented. Another innovation of this approach is the design of a fast rate control algorithm in order to maintain consistent bitrates between input and output of the transcoder. This rate control algorithm only needs picture-level bit information for training target quantization parameters. Simulation results demonstrate that, comparing with standard SVC, the proposed approach is able to achieve up to 4 dB gain for the enhancement layer video and up to 1 dB gain for the base layer video.
80D8FBD0	We present a rate-distortion optimal joint source/channel coding framework for efficient and robust low bit rate video communications in packet networks. The proposed algorithm combines layered coding with transport prioritization in a novel operational rate-distortion optimization framework that trades off source coding performance for channel coding protection. The system is demonstrated to achieve significant improvement in reconstructed video quality for a range of packet loss rates.
7CC011D6	This paper proposes an efficient wavelet-based coding scheme with fine grain scalability, where the base layer is encoded with a novel designed of wavelet-based coder, and the enhancement layer is encoded with progressive fine granularity scalable (PFGS) coding. This algorithm involves multi-frame motion compensation, rate-distortion optimizing strategy with Lagrangian cost function and context-based adaptive arithmetic coding. In order to improve efficiency ofthe enhancement layer coding, an improved motion estimation scheme that uses both information from the base layer and the enhancement layer is also proposed in this paper. The novel designed of wavelet-based coder significantly improves the coding efficiency ofthe base layer compared with MPEG-4 ASP (Advanced Simple Profile) and H.26L TML9. The PFGS coding is a significant improvement over MPEG-4 FGS at the enhancement layer. Experiments show that single layer coding efficiency gain of the proposed scheme is about 2.0-3.0dB and 0.3-1.0dB compared with MPEG-4 ASP and H.26L TML9, respectively. The overall coding efficiency gain of the proposed scheme is about 4.0-5.0dB compared with MPEG-4 FGS.
7EF88D59	This work addresses the transmission of pre-encoded JPEG2000 video within a video-on-demand scenario. The primary requirement for the rate allocation algorithm deployed in the server is to match the real-time processing demands of the application. Scalability in terms of complexity must be provided to supply a valid solution by a given instant of time. The FAst rate allocation through STeepest descent (FAST) method introduced in this work selects an initial (and possibly poor) solution, and iteratively improves it until time is exhausted or the algorithm finishes execution. Experimental results suggest that FAST commonly achieves solutions close to the global optimum while employing very few computational resources.
7687FE2D	A high definition video coding technique using super-macroblocks is investigated in this work. Our research is motivated by the observation that the macroblock-based partition in H.264/AVC may not be efficient for high definition video since the maximum macroblock size of 16 × 16 is relatively small against the whole image size. In the proposed super-macboblock based video coding scheme, the original block size M × N in H.264 is scaled to 2M × 2N. Along with the super-macroblock prediction framework, a low-complexity 16 × 16 discrete cosine transform (DCT) is proposed. As compared with the 1D 8 × 8 DCT, only 16 additions are added for a 1D 16 points 16 × 16 DCT. Furthermore, an adaptive scheme is proposed for the selection the best coding mode and best transform size. It is shown by experimental results that the super-macroblock coding scheme can achieve a higher coding gain.
811D8DB5	This paper proposes a scheme for low-delay robust transmission of video signals over packet erasure channels. In applications such as video conferencing, the permissible delay between encoding and playback may be too low to allow retransmission or channel coding approaches which require buffering several video packets. For such a scenario, we present a scheme that provides error robustness using redundant video descriptions applied to pertinent portions of the video signal. In the H.264/AVC specification, this can be efficiently implemented using redundant slices and flexible macroblock ordering (FMO). We describe a model that determines the bit rate of the redundant descriptions such that the expected distortion at the decoder is minimized. Across all the video test sequences used, the average video quality delivered by the proposed scheme is 3.7 dB higher than decoder-based error concealment, and 1.2 dB higher than encoder-based loss-aware rate-distortion optimization.
7A29564A	Motion information scalability is an important requirement for a fully scalable video codec, especially in low bit rate or small resolution decoding scenarios, for which the fully scalable motion model (SMM) has been proposed. SMM can collaborate flawlessly with other scalabilities, such as spatial, temporal and quality, in a scalable video codec. It performs better than the nonscalable motion model. To further improve the SMM, this correspondence extends the algorithm to support the hierarchical B frame structure and bidirectional or multidirectional motion estimation. Furthermore, the corresponding rate distortion optimized estimation for improved efficiency in several scenarios is discussed. Several simulation results based upon the updated framework are presented to verify the advantage of this extension.
757BA39E	The real-time speech hiding is to hide the secret speech into a cover speech in real-time communication systems. By hiding one secret speech into the cover speech, we can get a stego speech, which sounds meaningful and indistinguishable from the original cover speech. Therefore, even if the attackers catch the audio packets on Internet, they would not notice that there is another speech hidden inside it. In this paper, we propose a scheme for speech hiding in a real-time communication system such as voice over Internet Protocol (VoIP). We propose a novel design of real-time speech hiding for G.711 codec, which is widely supported by almost every VoIP device. Experimental results show that the processing time for the proposed algorithm takes only 0.257 ms, which is suitable for real-time VoIP applications.
7619646B	Multimedia applications have been the key driving force in converging fixed, mobile and IP networks. A major hurdle in the realisation of this convergence is obtaining Quality of Service from a heterogeneous, best-effort service network. Interactive voice requires strict bounds on delay, jitter and packet losses, for Different Network Traffic Intensity, whereas video adds significant bandwidth requirements to the network, while Internet only makes its best effort to deliver a packet. Hence, the end-to-end QoS management of heterogeneous networks supporting multimedia services is of paramount importance. We present an empirical performance study of multimedia applications over 802.11 networks within metropolitan area networking environments. Specifically, we study the QoS performance of Voice over IP (VoIP) applications over 802.11-based networks, while sharing the network resources with HTTP and video applications. Using the OPNET simulator, we simulate several realistic application traffic scenarios, and we investigate the performance of VoIP applications by analyzing QoS parameters, such as delay, jitter, MOS, and packet loss ratio. Subsequently, the performance characteristics data of the network, which we obtain through simulations, are used to build a Markov model of the network performance to extend our analysis and gain further insight into the network performance dynamics.
5A8F9D97	We propose a novel online monitoring approach to distinguish between attacks and normal activity in SIP-based Voice over IP environments. We demonstrate the efficiency of the approach even when only limited data sets are used in learning phase. The solution builds on the monitoring of a set of 38 features in VoIP flows and uses Support Vector Machines for classification. We validate our proposal through large offline experiments performed over a mix of real world traces from a large VoIP provider and attacks locally generated on our own testbed. Results show high accuracy of detecting SPIT and flooding attacks and promising performance for an online deployment are measured.
7D39AC23	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
8133D5AC	Real-time services such as VoIP are becoming popular and are major revenue earners for network service providers. These services are no longer confined to the wired domain and are being extended over wireless networks. Although some of the existing wireless technologies can support some low-bandwidth applications, the bandwidth demands of many multimedia applications exceed the capacity of these technologies. The IEEE 802.16-based WiMax promises to be one of the wireless access technologies capable of supporting very high bandwidth applications. In this paper, we exploit the rich set of flexible features offered at the medium access control (MAC) layer of WiMax for the construction and transmission of MAC protocol data units (MPDUs) for supporting multiple VoIP streams. We study the quality of VoIP calls, usually given by R-score, with respect to the delay and loss of packets. We observe that loss is more sensitive than delay; hence, we compromise the delay performance within acceptable limits in order to achieve a lower packet loss rate. Through a combination of techniques like forward error correction, automatic repeat request, MPDU aggregation, and minislot allocation, we strike a balance between the desired delay and loss. Simulation experiments are conducted to test the performance of the proposed mechanisms. We assume a three-state Markovian channel model and study the performance with and without retransmissions. We show that the feedback-based technique coupled with retransmissions, aggregation, and variable length MPDUs are effective and increase the R-score and mean opinion score by about 40 percent.
812A382D	The paper investigates the effects of packet loss and delay jitter on speech quality in voice over Internet protocol (VoIP) scenarios. A new formula is proposed to quantify these effects and incorporated into ITU-T G.107, the E-model. In the simulation, codecs ITU-T G.723.1 and G.729 are used; random packet loss and Pareto distributed network delay are introduced. The prediction errors range between -0.20 and +0.12 MOS (mean opinion score). The formula extends the coverage of the current E-model, and is very useful in MOS prediction as well as network planning.
6E870075	Perceived conversational speech quality is a key quality of service (QoS) metric for voice over IP (VoIP) applications. Speech quality is mainly affected by network impairments, such as delay, jitter and packet loss. Playout buffer algorithms are used to compensate for jitter based on a tradeoff between delay and loss, but can have a significant effect on perceived quality. The main aim in this paper is to assess how buffer algorithms affect perceived speech quality and how to choose the best algorithm and its parameters to obtain optimum perceived speech quality (in terms of an objective mean opinion score). The contributions of the paper are three-fold. First, we introduce a new methodology for predicting conversational speech quality (conversational mean opinion score or MOSc) which combines the latest ITU-T speech quality measurement algorithm (PESQ) and the concepts of the E-model. Second, we assess different playout buffer algorithms using the new MOSc metric on Internet trace data. Our findings indicate that, in general, end-to-end delay has a major effect on the selection of a buffer algorithm and its parameters. For small end-to-end delays, an algorithm that seeks to minimise loss is preferred, whereas for large end-to-end delays, an algorithm that aims at a minimum buffer delay is best. Third, we propose a modified buffer algorithm together with an adaptive parameter adjustment scheme. Preliminary results show that this can achieve an "optimum" perceived speech quality for all the traces considered. The results are based on Internet trace data measurements between UK and USA, UK and China, UK and Germany.
80F515A4	This paper presents an adaptive steganography scheme for Voice over IP (VoIP). Differing from existing steganography techniques for VoIP, this scheme enhances the embedding transparency by taking into account the similarity between Least Significant Bits (LSBs) and embedded messages. Moreover, we introduce the notion of Partial Similarity Value (PSV). By properly setting the threshold PSV, we can adaptively balance the embedding transparency and capacity. We evaluate the effectiveness of this scheme with G.729a as the codec of the cover speech in StegTalk, a covert communication system based on VoIP. The experimental results demonstrate that our technique provides better performance than the traditional method.
7DE3AE79	Transcoding Steganography (TranSteg) is a fairly new IP telephony steganographic method that is characterized by a high steganographic bandwidth, low introduced distortions, and high undetectability. TranSteg utilizes compression of the overt data to free space for the secret data bits. In this paper, we focus on evaluating different possibilities for TranSteg detection. Building on the previous works, we perform a wide analysis of different steganalysis methods to assess the possibility of TranSteg detection and identify the most ‘undetectable’ pairs of voice codecs. 
5F8051C3	According to former results from (Dittmann et al., 2005) in this paper we summarize the design principles from the general approach and introduce extended experimental test results of a voice-over-IP (VoIP) framework including a steganographic channel based on (Dittmann et al., 2005), (Dittmann and Hesse, 2004), (Kraetzer et al., 2006) and (Vogel et al., 2006). We show that using this framework it is largely secure to transmit hidden messages during a VoIP session and demonstrate results with respect to perceptibility for music and speech data
79250463	With the spread of new and innovative Internet services such as SIP-based communications, the challenge of protecting and defending these critical applications has been raised. In particular, SIP firewalls attempt to filter the signaling un- wanted activities and attacks based on the knowledge of the SIP protocol. Optimizing the SIP firewall configuration at real-time by selecting the best filtering rules is problematic because it depends on both natures of the legal traffic and the unwanted activities. More precisely, we do not know exactly how the unwanted activities are reflected in the SIP messages and in what they differ from the legal ones. In this paper, we address the case of Spam over Internet Telephony (SPIT) mitigation. We propose an adaptive solution based on extracting signatures from learnt decision trees. Our sim- ulations show that quickly learning the optimal configura- tion for a SIP firewall leads to reduce at lowest the unso- licited calls as reported by the users under protection. Our results promote the application of machine learning algo- rithms for supporting network and service resilience against such new challenges. 
77FE1304	The design of a VoIP media stream encryption device based on ARM9 CPU is introduced. The device can be deployed between the Soft Switch or IP-PBX and the VoIP terminal, dedicatedly used for the encryption/de-encryption of the VoIP signal and the RTP voice packet. As an illustration, the encryption flow of the packet is described when the VoIP protocol is SIP and the encryption algorithm is RC4. Then a test is implemented to compare the packet before the encryption and after that, the effectiveness of the design is proved.
5D0F37C5	Voice over Internet Protocol (VoIP) refers to the technology used to transport voice in a digitised form over an IP-based network. As a network service, VoIP inherits all the vulnerabilities a best-effort service network suffers since voice frames can often travel over hostile environments such as the Internet. In this paper we perform an experimental analysis on the effects of encryption under burst VoIP traffic conditions in terms of call volumes under the CS-CELP G.729 coder. The effects of encryption have been illustrated by using the NS-2 simulation tool and a fully automated Python script.
7D5CAE14	The IEEE 802.16 system called WiMAX (Worldwide Interoperability for Microwave Access) provides quality of service (QoS) of several types for different service. The WiMAX is expected to support QoS for real time application, such as Voice over IP (VoIP). In this paper, when network congestion occurs, the VoIP bit-rate needs to be adapted to achieve the best speech quality. We propose a new scheme called Adaptive VoIP Level Coding (AVLC). VoIP is sensitive to delay and loss. According to different network conditions such as varied modulation, packet delay, packet loss, and residual time slot, we use G.722.2 codec to adapt each connection’s data rate. Simulation experiments are conducted to test the performance (network delay, packet loss, and R-score) of the proposed mechanisms. In result, we increase the R-score about 40% to 50%.
81670C60	Voice over Internet Protocol (VoIP) is becoming popular to end-users and sparking great interests in broadband wireless networks. As a mobile version of Worldwide Interoperability for Microwave Access (WiMAX), IEEE 802.16e plays an important role in the evolution towards 4G. In this paper, we have investigated the actual quality of VoIP in the 4G field trial, which aims to better understand the mobile WiMAX performance for delay-sensitive services. Distinct from the past research works, we set up two different evaluation systems, professional and user-friendly. The performance measurements utilize Perceptual Evaluation of Speech Quality (PESQ) to evaluate the voice quality, the packet loss, jitter and delay for network tests. Our test results show that the mobile WiMAX network is able to support well the delay-sensitive VoIP service. We find that the VoIP quality on the downlink is perfect. On the uplink, the quality is degraded but still adequate at both cases of the cell edge and handover. Our work makes a contribution in better understanding the mobile WiMAX performance for both industry and academia.
7E951C8D	The quality of service limitation of today's Internet is a major challenge for real-time voice communications. Excessive delay, packet loss, and high delay jitter all impair the communication quality. A new receiver-based playout scheduling scheme is proposed to improve the tradeoff between buffering delay and late loss for real-time voice communication over IP networks. In this scheme the network delay is estimated from past statistics and the playout time of the voice packets is adaptively adjusted. In contrast to previous work, the adjustment is not only performed between talkspurts, but also within talkspurts in a highly dynamic way. Proper reconstruction of continuous playout speech is achieved by scaling individual voice packets using a time-scale modification technique based on the Waveform Similarity Overlap-Add (WSOLA) algorithm. Results of subjective listening tests show that this operation does not impair audio quality, since the adaptation process requires infrequent scaling of the voice packets and low playout jitter is perceptually tolerable. The same time-scale modification technique is also used to conceal packet loss at very low delay, i.e., one packet time. Simulation results based on Internet measurements show that the tradeoff between buffering delay and late loss can be improved significantly. The overall audio quality is investigated based on subjective listening tests, showing typical gains of 1 on a 5-point scale of the Mean Opinion Score.
8046B9CD	This paper examines the capacity and performance characteristics of the digital video broadcasting (DVB)/Digital Audio-Visual Council (DAVIC) cable television protocol for the delivery of low rate isochronous streams for a cable population of up to 700 nodes. Streams (ranging from 8 to 128 kbps) suitable for timing critical services such as compressed/uncompressed voice (e.g., VoIP: G711 and G.7231), audio and low quality video, were considered in order to study the effects on channel capacity when using reservation and fixed access for the delivery of timing critical services. The analysis focuses on the,performance of the upstream channel, which is the limiting factor of community antenna television (CATV) networks and is critical in the delivery of services to individual subscribers on demand. Simulation results indicated that such streams, within the given protocol limitations, can be supported for a particular system population with trade-offs in terms of system throughput and channel utilization. Network capacity, in terms of the number of simultaneous streams supported and link utilization, is significantly affected by packet size. Analysis of the results indicated that for different streams, packet sizes and combined with header suppression, the benefits from the use of fixed access is essential for the support of timing critical services.
7EB78B3B	Traffic engineering and admission control schemes for QoS support over the Internet share the hypothesis that offered traffic must be policed. Recently, measurement-based techniques have been proposed as effective QoS-guarantee mechanisms. For them, a key element is a thorough understanding of the characteristics of regulated traffic. In this paper, we present an analysis of the traffic output process for a leaky-bucket-regulated voice source with silence suppression, which would be a prime candidate for VoIP service. Specifically, we present a procedure for calculating the mean and the variance of the amount of regulated traffic as a function of the measurement period. We evaluate the quality of our results via simulation.
7F900EF9	In this paper, we exploit the flexible features in the medium access control (MAC) layer of WiMax for construction and transmission of MAC protocol data units (MPDU) for supporting multiple VoIP streams over a WiMax link. Quality of VoIP calls, usually given by R-score, is studied with respect to delay and loss of packets. We observe that loss is more sensitive than delay, hence we trade delay for loss. We propose a combination of techniques that exploit the flexibility of the WiMax MAC layer to strike a balance between loss and delay. These techniques are forward error correction, automatic repeat request, MPDU aggregation, and minislot allocation. Simulation experiments are conducted to test the performance of the proposed mechanisms. We assume a three-state Markovian channel model and study the performance with and without retransmissions. We show that the feedback-based technique coupled with retransmissions, aggregation, and variable length MPDUs are effective and increases the R-score by about 40%.
7E86AB80	Voice over IP (VoIP), also known as Internet telephony, is gaining market share rapidly and now competes favorably as one of the visible applications of the Internet. Nevertheless, being an application running over the TCP/IP suite, it is susceptible to flooding attacks. If flooded, as a time-sensitive service, VoIP may show noticeable service degradation and even encounter sudden service disruptions. Because multiple protocols are involved in a VoIP service and most of them are susceptible to flooding, an effective solution must be able to detect and overcome hybrid floods. As a solution, we offer the VoIP flooding detection system (vFDS)-an online statistical anomaly detection framework that generates alerts based on abnormal variations in a selected hybrid collection of traffic flows. It does so by viewing collections of related packet streams as evolving probability distributions and measuring abnormal variations in their relationships based on the Hellinger distance-a measure of variability between two probability distributions. Experimental results show that vFDS is fast and accurate in detecting flooding attacks, without noticeably increasing call setup times or introducing jitter into the voice streams.
80DBB73F	When voice traffic is transported over a packet-based network, a number of conditions different from the ones in the traditional circuit-switched network will have an influence on the quality of the speech signal as perceived by the users. In particular, distortion of the voice signal due to delay jitter and partial loss of signal caused by packet loss will have an important impact. Therefore, in order to determine the influence of delay jitter and packet loss on the perceived quality, an objective speech quality assessment system, called DSLA, is used to predict the mean opinion score for voice connections, which are set up on a test-bed, subject to artificially introduced network conditions. A decomposition of the voice degradation is made into different parts, due to sampling, digitization, encoding/decoding, packet loss and delay jitter, respectively. It is found that delay jitter has a devastating influence on the perceived quality, when no dejittering buffer is used. However, when the received signal is dejittered, the degradation due to jitter is similar to the one caused by packet loss.
7F88127A	This paper presents a VoIP teleconferencing service that does not required expensive equipment and that can be administrated on a user level. The system is designed and developed so as to be independent of the user environment and to offer high QoS by reducing processing delay. This is achieved through the introduction of a high-speed mixing method to synchronize and mix RTP voice streams. The performance of a trial implementation of a teleconferencing server using this service is evaluated, and its effectiveness is confirmed.
6FF3092D	Voice signal processing algorithms such as P.861 PSQM (perceptual speech quality measure) and P.862 PESQ (perceptual evaluation of speech quality) have been recommended by the ITU-T for the objective measurement of subjective speech quality. The algorithms have been implemented in commercially available products to measure the end-to-end speech quality of voice-over-IP (VoIP) calls. Such measurements are carried out by transmitting a packetized reference speech signal and then applying PSQM or PESQ to the received signal and the original reference to obtain an objective quality score. The measurements are intrusive since test calls are injected into the network. We develop a novel passive method for measuring and monitoring speech quality in live (i.e. in progress) VoIP calls. In contrast to the intrusive methods, the passive method does not require the transmission of any reference speech signal. Instead, the method operates directly on VoIP packet streams copied from the network and applies the ITU-T objective speech quality processing algorithms to the copied voice signals and specially constructed 'pseudo-packet' stream signals. Thus there is no need for any sending devices. The proposed passive method inherits the accuracy that is already provided by the ITU-T objective methods.
7C2CDBE3	Extensible Messaging and Presence Protocol (XMPP) is an open, XML-based protocol aimed at near-real-time, extensible instant messaging (IM) and presence information. It has been expanded into the broader realm of message-oriented middleware. Built to be extensible, the protocol has been extended with features such as Voice over IP and file transfer signaling. XMPP protocol has been used by many social networking platforms including gtalk, and facebook; collaborative services like google wave, and gradient; geo-presence systems like Nokia Ovi Contacts; multiplayer games like chesspark, and by many online live customer support and technical support services. In this manuscript, I will introduce XMPP and its extensions. In the tutorials I will demonstrate how you can leverage some of the available open source software and libraries for rapid development of XMPP enabled services and communication platforms. The tutorial slides and supplementary materials will be available on my website: http://drozturk.com/talks.
7A87B931	The success of Skype has inspired a generation of peer-to-peer-based solutions for real-time multimedia services over the Internet. However, there lacks still a robust metric quantifying the perceptual quality of a Skype call. The widely-used PESQ (Perceptual Evaluation of Speech Quality) falls short of modeling super-wideband calls, which are characteristics of SILK Skype's codec made public in 2011. Towards a robust QoE (Quality of Experience) metric for VoIP call analysis, we propose a model, referred to as WF-Regression model, to capture the call rate and perceptual quality relationship. The model is shown through a user study that it is robust, R-square = 0.9990 and outperform PESQ modeling the quality of Skype calls, error ratio = 3.68% vs. 14.59%.
7DF83938	In the network devices for real time streaming, e.g. VoIP phone, video phone and online game, the playout algorithm controls the playout delay to eliminate the jitter and to minimize the overflow packet loss. Conventional algorithms did this based on network delay only; they did not consider the user perceived quality, and aware the codec and communication duplex mode. Therefore, we present two novel approaches: codec aware adaptive playout (CAAP) and duplex aware adaptive playout (DAAP), they intend to optimize the user perceived quality based on codec and communication duplex mode respectively. Because of their different characteristics, CAAP and DAAP can work either alone or together; and their improvements are accumulative. The out-performance of CAAP and DAAP are superior the prior algorithm in our substantial but conservative evaluation. Since no objective mechanisms for measuring the speech quality of two-way communication exist; a new LMOS-DMOS measurement mechanism is also proposed. Although VoIP is an exemplary embodiment in this paper, we formally model our algorithms into a generalized user perceived QoS control as well.
80AA5E5F	Receiver playout buffers are required to smooth network delay variations for multimedia streams. Playout buffer algorithms such as those commonly used in the Internet, autoregressively measure the network delay and variation and adjust the buffer delay accordingly, to avoid packets arriving too late. In this work, we attempt to adjust the buffer delay based on a prediction of the network delay and a similar measure of variation. The philosophy here is that the use of an accurate prediction will adjust the buffer delay more effectively by tracking rapid fluctuations more accurately. Proper buffer delay can lead to either (or both) a lower total end-to-end delay for a fixed packet lateness percentage or fewer late packets for a fixed total end-to-end delay which are both important metrics for applications such as IP telephony. We present a playout algorithm based on a simple normalized least-mean-square (NLMS) adaptive predictor and demonstrate using Internet packet traces that it can yield reductions in average total end-to-end delays.
7DDF15E2	In voice over IP (VoIP) networks, multiple voice frames can be sent within one packet to increase the network efficiency. When packet loss happens, the voice decoder often tries to conceal the erased frames from received parameters. This paper discuss the quantitative dependency of concealment quality on the packetization rate in terms of LPC distortion. The performance of an erasure-robust. concealment method is analyzed and simulated in comparison with the standard G.729 concealment method. Results show that around 1.2 dB improvement on spectral distortion can be obtained with erasure robust. concealment compared with G.729 under 1-6 frames/packet, packetization. A method to determine the maximum packet size given the network load and expected concealment quality is introduced. The proposed method serves as the packetization guidance during the call setup procedure in VoIP sessions.
7DCBCB60	Steganography, as one of alternative techniques for secure communications, has drawn more and more attentions. This paper presents a covert communication model based on least significant bits (LSB) steganography in Voice over IP (VoIP). The model aims at providing nice security of secret messages and real-time performance that is vital for VoIP. Therefore, we employ a simple encryption of secret messages before embedding them. This encryption strikes a good balance between adequate short-term protection for secret messages and low latency for VoIP. Furthermore, we design a structure of embedded messages. It can provide flexible length and avoid effectually both extraction attack and deceptive attack. We evaluate the model with ITU-T G.729a as the codec of the cover speech in StegTalk, our platform for study on covert communications theory in VoIP. In this case, the proposed model can provide two optional covert transmission speeds, i.e. 0.8kb/s and 2.6 kb/s, where the maximum payload ratio is 99.98%. The experimental results show that our method has negligible effects on speech quality and well meets the real-time requirement of VoIP.
01014831	In VoIP applications, packet loss can have a major impact on perceived speech quality. The impact is affected by factors such as packet loss size, loss pattern and loss locations. In this paper, we report an investigation into the impact of loss location on perceived speech quality and the relationships between convergence time and loss location for three different codecs (G.729, G.723.1 and AMR) using perceptual-based objective measurement methods (PSQM+, MNB and EMBSD). Our results show that loss location has a severe effect on perceived speech quality. The loss at unvoiced speech segments has little impact on perceived speech quality for all codecs. However, the loss at the beginning of voiced segments has the most severe impact on perceived speech quality. The convergence time depends on the speech content (voiced/unvoiced). For unvoiced segments, the convergence time is stable whereas for voiced segments it varies but has an upper bound at the end of the segment. Our method allows a more accurate measurement of the exact effect of packet loss on perceived speech quality. This could help in the development of a perceptually relevant packet loss metric, which could be valuable in non-intrusive VoIP measurements.
7E1E9CC9	We measured the capacity for VoIP traffic in an IEEE 802.11b wireless testbed and compared it with the theoretical capacity and our simulation results. We identified factors that have been commonly overlooked in past studies but affect experiments and simulations. We found that in many papers, the capacity for VoIP traffic has been measured via simulations or experiments without considering these factors, showing different capacity in each paper. After these corrections, simulations and experiments yielded a capacity estimate of 15 calls for 64 kb/s CBR VoIP traffic with 20 ms packetization interval and 38 calls for VBR VoIP traffic with a 0.39 activity ratio. Furthermore, we measured the capacity for VoIP traffic using each access category introduced in the 802.11e standard and the effect of the TCP traffic on VoIP traffic. We found that while the 802.11e standard can protect the QoS of VoIP against TCP traffic, it does not improve the capacity due to the significant retransmissions during TXOP.
7E923534	We propose a streaming authentication scheme for the use over the IP telephony. To clear the strict real-time transmission requirements of the IP telephony, the latency and interval between signatures are adjusted dynamically according to the transmission quality of the network. This provides efficient signing and continuous authentication during real-time streaming. We show our scheme's advantages over previously proposed schemes by comparing delay on the sender and receiver side, and tolerance to packet loss.
7F80D600	We propose a new stream cipher suitable for high-speed network applications, called HSSC (high-speed stream cipher). HSSC uses simple structure to allow detailed analysis and high software performance. The software implementation using C is about 2.07 cycles per byte on a Pentium II 450MHz platform. HSSC uses twelve 32-bit variables as internal states, four predefined constants, four key-derived constants and a set of rules to combine the internal states into 128-bit output per round.
7E03F6EF	Telephony over IP is exposed to multiple security threats. Conventional protection mechanisms do not fit into the highly dynamic, open and large-scale settings of VoIP infrastructures, and may significantly impact on the performance of such a critical service. We propose in this paper a runtime risk management strategy based on anomaly detection techniques for continuously adapting the VoIP service exposure. This solution relies on support vector machines (SVM) and exploits dynamic security safeguards to reduce risks in a progressive manner. We describe how SVM parameters can be integrated into a runtime risk model, and show how this framework can be deployed into an Asterisk VoIP server. We evaluate the benefits and limits of our solution through a prototype and an extensive set of experimental results.
652368DD	We present a new and powerful approach for transmitting video over the Internet via dial-up modem links. Currently, the source and channel coding are both done in the user PC. In fact, joint source channel coding has become quite popular. In strong contrast to this, we propose to spatially separate the source and channel coding so that the source coding is performed in the user PC and the channel coding is performed in the remote access concentrator. This results in significantly improved video quality since the modem connection has a very small packet loss rate compared to the Internet. In addition, the bit rate of the modem connection is fixed and much lower than the potentially available bit rate of the Internet. Clearly, using a channel coding scheme powerful enough to protect against the high packet loss rate of the Internet is not necessary for the modem connection, and in fact, it is a waste of the already very constrained modem connection bandwidth. By moving the channel coding from the user PC to the remote access concentrator, we free up bits on the highly reliable modem link for the source coding, which in turn results in a compressed video of higher quality. In the remote access concentrator we then employ a channel coding scheme which increases the overall bit rate but also makes the real time stream robust to the highly likely packet drops in the Internet.
7CEC8964	This paper presents a formal framework for identifying and filtering SPIT calls (SPam in Internet Telephony) in an outbound scenario with provable optimal performance. In so doing, our work is largely different from related previous work: our goal is to rigorously formalize the problem in terms of mathematical decision theory, find the optimal solution to the problem, and derive concrete bounds for its expected loss (number of mistakes the SPIT filter will make in the worst case).This goal is achieved by considering an abstracted scenario amenable to theoretical analysis, namely SPIT detection in an outbound scenario with pure sources. Our methodology is to first define the cost of making an error (false positive and false negative), apply Wald's sequential probability ratio test to the individual sources, and then determine analytically error probabilities such that the resulting expected loss is minimized.The benefits of our approach are: (1) the method is optimal (in a sense defined in the paper); (2) the method does not rely on manual tuning and tweaking of parameters but is completely self-contained and mathematically justified; (3) the method is computationally simple and scalable. These are desirable features that would make our method a component of choice in larger, autonomic frameworks. 
7D662126	In this paper we experimentally study the relationship between resource utilization in the wireless LAN and the quality of VoIP calls transmitted over the wireless medium. Specifically we evaluate how its overall capacity is shared between three basic MAC bandwidth components (load, access, and free) as the number of VoIP calls increases and how it influences transmission impairments (delay, loss, and jitter) and thus call quality. Resource utilization (under the MAC bandwidth components framework) is calculated by a WLAN probe application that passively "sniffs" packets at the L2/MAC layer of the wireless medium and analyses their headers and temporal characteristics. The quality of VoIP calls is predicted using an extended version of the ITU-T E-model, which estimates user satisfaction from time varying transmission impairments. Through experimentation with various codecs and packetization schemes we found that as the load (number of calls) reaches the available capacity level, packet delays and jitter increase dramatically resulting in the call quality becoming degraded. We show how these MAC bandwidth components maybe used to assess the VoIP call quality on 802.11 WLANs
6D785A11	The paper presents a new hidden data insertion procedure based on estimated probability of the remaining time of the call for steganographic method called LACK (Lost Audio PaCKets steganography). LACK provides hidden communication for real-time services like Voice over IP. The analytical results presented in this paper concern the influence of LACK's hidden data insertion procedures on the method's impact on quality of voice transmission and its resistance to steganalysis. The proposed hidden data insertion procedure is also compared to previous steganogram insertion approach based on estimated remaining average call duration. 
58CF9412	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
764AF17F	Cloud Consulting combines open source grid computing for distributed cloud computing and Enterprise Resource Modeling (ERP) to provide Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) through a simple, unified API. Cloud communications refers to using internet-based or cloud-based voice and data communications services, where telecommunications applications, switching, and storage are managed generally by third parties. These services can include capabilities that range from Voice over IP (VoIP) communications to hosted PBX and unified communications delivering voice, fax, video, and data requirements. Provisioning for these services is known as Communication as a Service (CaaS).
8054009E	Steganography is an ancient art that encompasses various techniques of information hiding, the aim of which is to embed secret information into a carrier message. Steganographic methods are usually aimed at hiding the very existence of the communication. Due to the rise in popularity of IP telephony, together with the large volume of data and variety of protocols involved, it is currently attracting the attention of the research community as a perfect carrier for steganographic purposes. This article is a first survey of the existing Voice over IP (VoIP) steganography methods and their countermeasures.
75EAC64A	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
7EFA283A	The adaptive codebook used in CELP (code excited linear prediction) codecs to model the pitch excitation allows the attainment of a high quality of synthesized speech but introduces a strong inter-frame dependency and consequently causes error propagation in case of frame erasure. In a previous work we showed that the error propagation can be greatly reduced by constraining, at the encoder side, the innovative codebook to partially model the pitch excitation. In this paper we extend this work by exploiting, at the decoder side, the pitch-related information present in the innovative excitation to speed up the recovery of the decoder. The method consists in adequately shifting the last pitch pulse present within the corrupted adaptive codebook memory so that it is resynchronized with the excitation parameters of the frame that follows the erased one
7C8D0145	VoIP applications are becoming popular these days. A lot of Internet traffic are being generated by them. Detection of VoIP traffic is becoming important because of QoS issues and security concerns. A VoIP client typically opens a number of network connection between VoIP client and VoIP client, VoIP client and VoIP server. In the case of peer to peer VoIP applications like Skype network, connections may be between client to client, client to Super Node, client to login server, Super Node to Super Node. Typically, VoIP media traffic are carried by UDP unless firewalls blocks UDP, in which case media and signalling traffic are carried by TCP. Many VoIP applications uses RTP to carry media traffic. Notable examples includes GTalk, Google+ Hangouts, Asterisk based VoIP and Apple's FaceTime. On the other hand, Skype uses a proprietary protocol based on P2P architecture. It uses encryption for end to end communications and adopts obfuscation and anti reverse engineering techniques to prevent reverse engineering of the Skype protocol. This makes the detection of Skype flows a challenging task. Although Skype encrypts all communications, still a portion of Skype payload header known as Start of Message (SoM) is left unecrypted. In this paper, we develop a method for detection of VoIP flows in UDP media streams. Our detection method relies on signalling traffic generated by VoIP applications and heuristics based on the information contained in Skype SoM and RTP/RTCP headers.
5C691BA5	The paper presents a new hidden data insertion procedure based on estimated probability of the remaining time of the call for steganographic method called LACK (Lost Audio PaCKets steganography). LACK provides hidden communication for real-time services like Voice over IP. The analytical results presented in this paper concern the influence of LACK's hidden data insertion procedures on the method's impact on quality of voice transmission and its resistance to steganalysis. The proposed hidden data insertion procedure is also compared to previous steganogram insertion approach based on estimated remaining average call duration. 
7F6DAD9F	This paper presents validation and evaluation of mobile VoIP support over WiMAX networks using the FMIPv6-based cross layer handover scheme. A software module has been implemented for the FMIPv6-based handoff scheme. The handoff delay components are formulated. To evaluate its support of mobile VoIP, we carefully assess the handoff delay, the total delay, and the R factor which is a representation of voice user satisfactory degree. Simulation results show that the cross-layering handoff scheme, as compared with the non-cross-layer scheme, successfully decreases layer-3 handoff delay by almost 50%, and is therefore thriving to support mobile VoIP services. We believe this is the first performance evaluation work for the FMIPv6-based cross-layer scheme, and hence an important work for the WiMAX research community.
58C68905	We propose an original method to geoposition an audio/video stream with multiple emitters that are at the same time receivers of the mixed signal. The achieved method is suitable for those comes where a list of positions within a designated area is encoded with a degree of precision adjusted to the visualization capabilities; and is also easily extensible to support new requirements. This method extends a previously proposed protocol, without incurring in any performance penalty.
7FCD235E	Voice over internet protocol (VoIP) provided voice service using Internet. It receives footlights when it escapes an initial curiosity. Interest to VoIP increased, because it can transfer existing phone service and deliver voice data through Internet technology. Communication service providers introduce Next Generation Network and Broadband convergence Network with VoIP as an added service to the consumer. In addition, session initiation protocol (SIP) service can be applied even outside the Internet phone service. However, it is needed to ensure secrecy of VoIP call in a special situation. It is relatively difficult to eavesdrop in the commonly used Public Switched Telephone Network (PSTN) as it is connected with 1:1 circuit. In this paper, we propose a new model of Internet telephone for eavesdrop prevention enabling VoIP (using SIP protocol) to use the virtual private network (VPN) protocol and establish the probability of practical use comparing it with Internet telephone. Copyright © 2010 John Wiley & Sons, Ltd.
78F4BCB6	Existing solutions for securing multimedia streams such as RTP and Ogg, particularly those based on encrypting large portions of the data stream, are highly unscalable, and infringe on the Quality of Service (QoS) requirements as well. In this paper, we propose an efficient selective encryption technique for securing Ogg formatted VoIP/video streams. The proposed solution encrypts only 1.5% of the stream while guaranteeing security/privacy of the entire bit stream. Experimental results based on example hardware platforms while simulating different digital attacks are presented to verify the robustness of the proposed method.
80843370	Recently voice over IP (VoIP) is experiencing a phenomenal growth. Being a real-time service, VoIP is more susceptible to denial-of-service (DoS) attacks than regular Internet services. Moreover, VoIP uses multiple protocols for call control and data delivery, making it vulnerable to various DoS attacks at different protocol layers. An attacker can easily disrupt VoIP services by flooding TCP SYN packets, UDP-based RTP packets, or SIP-based INVITE messages, which pose a critical threat to IP telephony. In this paper, we present an online statistical detection mechanism, called vFDS, to detect DoS attacks in the context of VoIP. The core of vFDS is based on Hellinger distance method, which computes the variability between two probability measures. Using Hellinger distance, we characterize normal protocol behaviors and then detect the traffic anomalies caused by flooding attacks. Our experimental results show that vFDS achieves fast and accurate detection of DoS attacks
80D69E67	With constantly increasing costs of energy, we ask ourselves what we can say about the energy efficiency of existing VoIP systems. To answer that question, we gather information about the existing client-server and peer-to-peer VoIP systems, build energy models for these systems, and evaluate their power consumption and relative energy efficiency through analysis and a series of experiments.Contrary to the recent work on energy efficiency of peer-to-peer systems, we find that even with efficient peers a peer-to-peer architecture can be less energy efficient than a client-server architecture. We also find that the presence of NATs in the network is a major obstacle in building energy efficient VoIP systems. We then provide a number of recommendations for making VoIP systems more energy efficient.
80508BA5	TCP and UDP are the dominant transport protocols today, with TCP being preferred because of the lack of fairness mechanisms in UDP. Some time-dependent applications with small bandwidth requirements, however, occasionally suffer from unnecessarily high latency due to TCP retransmission mechanisms that are optimized for high-throughput streams. Examples of such thin-stream applications are Internet telephony and multiplayer games. For such interactive applications, the high delays can be devastating to the experience of the service. To address the latency issues, we explored application-transparent, sender-side modifications. We investigated whether it is possible to bundle unacknowledged data to preempt the experience of packet loss and improve the perceived latency in time-dependent systems. We implemented and tested this idea in Linux. Our results show that we can reduce the application latency by trading it against bandwidth.
7D6DEB86	Multi media stream service provided by broadband wireless networks has emerged as a important technology and has attracted much attention today. Due to its large coverage area, low cost of deployment and high speed data rates, WiMAX is a promising technology for providing wireless last-mile connectivity. Physical and MAC layer of this technology refer to the IEEE 802.16e standard, which defines 5 different data delivery service classes that can be used in order to satisfy Quality of Service (QoS) requirements of different applications, such as VoIP, videoconference, FTP, Web, etc. In this paper we examine a case of QoS deployment over a cellular WiMAX network and in particular the performance obtained using two different QoS configurations namely ertPS and UGS has been compared. Results indicate that for delay-sensitive traffic ertPS has an edge over UGS and rtPS.
7550F29B	Implement VoIP Based IP Telephony with Open Source Asterisk ArchitectureAsterisk is a leading open source telephony software/system, easily implemented over intranet and internet. Asterisk empowers developers and integrators to create advanced communication solutions. An Asterisk system is a low cost type of a traditional PBX system. Any phone controlled by an Asterisk system can call a VoIP or analog phone controlled or managed by a traditional telephone system or by Asterisk telephone system. In this paper, the authors focus on the deployment and testing of various Open Source Asterisk Services in an enterprise level communication system. Selected services are listed in this paper that can be used to implement a telephone system with good Quality of Services QoS and good Quality of Experience QoE from the personal user to enterprise level users.
036916A5	This paper presents the design and analysis of a multilayer protection scheme against denial-of-service (DoS) attacks in IP telephony enabled enterprise networks. While there are many types of DoS attacks, we focus on flood-based attacks using application layer and transport layer signaling messages in IP telephony. We design sensors to detect and control these types attacks and consider different location of these sensors in the enterprise network. The algorithm for detecting these attacks is based on the well established non-parametric cumulative sum method. The response to the attack uses standard protocol features of IP telephony to control the number of incoming application and transport layer setup requests. We consider different recovery algorithms and compare their performance using our emulation toolkit. Our results show that the detection algorithm can quickly detect both transport and application layer attacks and is robust against various types of attacks. We also show that with proper choice of sensor parameters, the detection algorithm is effective over a wide range of call volumes.
7839BC52	The IP Multimedia Subsystem (IMS), standardized by the 3GPP, is the most promising contender for replacing legacy, voice-dedicated mobile networks with an All-IP technology. As 3GPP IMS standards are in their embryonic state it becomes very crucial to test IMS clients during their development on a platform that simulates the IMS network. IMS client testbed also allows the developers to experiment all error circumstances which are not possible on a live network. Furthermore, it is also very costly to use an operator's live network for testing purpose. This paper highlights challenges involved and our experience of building an IMS client test bed using open source tools like OpenIMScore.
79208D58	The name MPEG-4 high-efficiency AAC (HE-AAC) refers to a family of recent audio coders that was developed by the International Organization for Standardization/International Electrotechnical Commission (ISO/IEC) Moving Picture Experts Group (MPEG) by subsequent extension of the established Advanced Audio Coding (AAC) architecture. These algorithmic extensions facilitate a significant increase in coding efficiency relative to previous standards and other known systems. Thus, they provide a representation for generic audio/music signals that offers high audio quality also to applications limited in transmission bandwidth or storage capacity, such as digital audio broadcasting and wireless music access for cellular phones. This article presents a compact overview of the evolution, technology, and performance of the MPEG-4 HE-AAC coding family.
7B1DD2CD	Based on the knowledge and experiences from existing image steganalysis techniques, the overall objective of the paper is to evaluate existing audio steganography with a special focus on attacks in ad-hoc end-to-end media communications on the example of Voice over IP (VoIP) scenarios. One aspect is to understand operational requirements of recent steganographic techniques for VoIP applications. The other aspect is to elaborate possible steganalysis approaches applied to speech data. In particular we have examined existing VoIP applications with respect to their extensibility to steganographic algorithms. We have also paid attention to the part of steganalysis in PCM audio data which allows us to detect hidden communication while a running VoIP communication with the usage of the PCM codec. In our impelementation we use Jori's Voice over IP library by Jori Liesenborgs (JVOIPLIB) that provides primitives for a voice over IP communication. Finally we show first results of our prototypic implementation which extents the common VoIP scenario by the new feature of steganography. We also show the results for our PCM steganalyzer framework that is able to detect this kind of hidden communication by using a set of 13 first and second order statistics.
7D6F6428	Differing from applying steganography on storage cover media, steganography on voice over IP (VoIP) must often delicately balance between providing adequate security and maintaining low latency for real-time services. This paper presents a novel real-time steganography model for VoIP that aims at providing good security for secret messages without sacrificing real-time performance. We achieve this goal by employing the well-known least-significant-bits (LSB) substitution approach to provide a reasonable tradeoff between the adequate information hiding requirement (good security and sufficient capacity) and the low latency requirement for VoIP. Further, we incorporate the M-sequence technique to eliminate the correlation among secret messages to resist the statistical detection based on the fact that the distribution of the LSBs in the stego-speech is not uniform and to provide a short-term security protection of secret messages. To accurately recover secret messages at the receiver side, we design a synchronization mechanism based on the RSA key agreement and the synchronized sequence transmission using techniques of the protocol steganography, which can effectively enhance the flexibility of the covert communication system and be extended to other steganography schemes based on real-time systems. We evaluate the effectiveness of our model with ITU-T G.729a as the codec of the cover speech in StegTalk, a covert communication system based on VoIP. The experimental results demonstrate that our technique provides good security and transparency for transmitting secret messages while adequately meeting the real-time requirement of VoIP.
7D35A58A	For the limitation of the network resources, to support real-time service in mobile ad hoc networks is challenging, especially for the delay sensitive voice sessions. Firstly, the delay spike phenomenon is described and analyzed in detail in this paper. Then, to alleviate its impairment on the voice quality, based on the prediction of delay spikes, a new source rate adjusting mechanism is proposed, which changes the number of voice frames in the RTP packet according to the network conditions adaptively. The results have shown that our scheme can improve the voice quality obviously; moreover, network congestion can be relieved or even avoided effectively.
75D2EB80	The paper presents a new steganographic method for IP telephony called TranSteg (Transcoding Steganography). Typically, in steganographic communication it is advised for covert data to be compressed in order to limit its size. In TranSteg it is the overt data that is compressed to make space for the steganogram. The main innovation of TranSteg is to, for a chosen voice stream, find a codec that will result in a similar voice quality but smaller voice payload size than the originally selected. Then, the voice stream is transcoded. At this step the original voice payload size is intentionally unaltered and the change of the codec is not indicated. Instead, after placing the transcoded voice payload, the remaining free space is filled with hidden data. TranSteg proof of concept implementation was designed and developed. The obtained experimental results are enclosed in this paper. They prove that the proposed method is feasible and offers a high steganographic bandwidth while introducing small voice degradation. Moreover, TranSteg detection is difficult to perform when compared with existing VoIP steganography methods.
58FF219F	Skype, one of the popular VoIP applications, has its own redundancy mechanism to mitigate the impact of packet loss at the expense of additional bandwidth usage. However, the benefit of voice quality improvement through redundancy is reduced greatly in the case of consecutive loss of Skype packets. When Skype is running over wireless networks, the high likelihood of having consecutive packet loss in a noisy wireless environment could lead to an interesting scenario: using more bandwidth for a lower voice quality.In this thesis, we study the impact of the HARQ retransmission mechanism of WiMAX to the voice quality of Skype. Two key parameters for the HARQ mechanism of WiMAX (i.e. Maximum numbers of retransmission and the delay of sending ACK) and their interplay with voice quality are investigated in this research. While HARQ retransmission may reduce the redundancy usage in Skype, it introduces additional bandwidth usage during the retransmission. The overhead of WiMAX retransmission with different HARQ parameters and overall bandwidth usage of Skype are also discussed in this thesis. Numerical analysis and simulation of voice quality and bandwidth usage of Skype over WIMAX are presented. Results show that HARQ retransmission can reduce the impact of bursty noise and improve the voice quality of skype up to 22%. The overall bandwidth consumption can be reduced by 76% when HARQ is engaged. Combining our studies on voice quality and bandwidth usage, we attempt to obtain the optimal HARQ parameters that can lead to the highest voice quality over bandwidth usage ratio for Skype over WiMAX.
0F885325	Divide-and-rule strategy is popularly adopted in steganography based on Voice-over-IP (VoIP), which often divides a cover into many small parts and performs embedding operation on each one to maintain the real-time requirement of VoIP services. No existing study, however, notices that the cover length of the parts may seriously affect the embedding performance. The goal of this paper is to fill in the gap and present a novel steganographic scheme to achieve the optimal embedding. To attain the goal, we first present an adjustable matrix encoding (AME) approach, which can adaptively generate a guide matrix to accommodate to the given cover length and provide the best performance while guaranteeing the desired embedding capacity. Further, the optimal embedding is modeled as a typical combinatorial optimization problem, of which the optimal solution is an AME or a combination of multiple AMEs. We evaluate the proposed scheme with ITU-T G. 711 (A-law) as the codec of the cover speech and compare it with previous methods. The experimental results demonstrate that the proposed scheme is really feasible in both theory and practice, and can provide consistently optimal embedding performance in any case. We present an optimal embedding scheme for Voice over IP.A novel adjustable matrix encoding (AME) is proposed to suit the limited cover.The optimal embedding using AME is modeled as a combinatorial optimization problem.An algorithm to search the optimal solution is provided.Our scheme can provide consistently optimal embedding performances in any case.
7D3C3318	Steganography is a data hiding process in which information is secured, while transferring data from sender to receiver. In audio steganography, encoding process is carried nut through inactive frames of low bit rate audio streams by performing iLBC (internet Low Bit rate Codec). This methodology can be used in applications such as VoIP (Voice Over Internet Protocol), streaming audio, archival and messaging. Traditionally, data embedding is carried out in the inactive frames rather than the active frame of streams; that is inactive frame has large embedding capacity. In addition VAD (Voice Activity Detection) algorithm is used for detecting inactive frames. The concealment of analysis is encoded by iLBC, that supports two basic frame lengths, giving a bit-rate of 13.3 kbps with an encoding frame length of 30 ms and 15.2 kbps with an encoding frame length of 20 nis . To enhance security in steganography PCC (Parabolic Curve Cryptography) algorithm is implemented.
84F40BB8	Governments and their agencies are often challenged by high cost and flexible telephonic and data services. Emerging technologies, such as those of Voice over Internet Protocol (VoIP) that allow convergent systems where voice and data networks can utilise the same network to provide both services, can be used to improve such services. However, these convergent networks are based on the classical (best-effort) characteristics that come with some weaknesses in respect of quality of service and fair access to network resources. This is true for multimedia applications that need bounds on delay and minimum bandwidth. VoIP is an implementation of these convergence networks that are capable of transporting voice over Internet Protocol (IP) based networks. In order to deploy a VoIP network capable of providing the traditional Public Switched Telephone Network–Private Branch Exchange (PSTN-PBX) scale solution, a number of issues such as services to be offered, end-user terminal, quality of service, security, bandwidth, signalling, protocol and operating legislations must be addressed. The implementation also requires an application software, which can be an open source or a proprietary software. This study examined how Asterisk, an open source VoIP software can be deployed to serve the needs of an educational institution. The educational institution in this case is the University of Namibia which is currently using a conventional PSTN system for voice and fax communication services, as well as the local area network connected to Internet for data services. Like any other open source software, Asterisk comes free of any proprietary costs. The study investigated how this software could be deployed for a longer period at the University of Namibia. Asterisk was deployed on a pilot basis to provide for a larger scale model to cater for the entire university. It was found out that the University of Namibia has a potential to implement the project although implementation can be scaled down so as to support sustainability. Since the software recommended for installation is open source, the project could be used as a source of information by students who specialize in real-time multi-media systems.
2EC0FE68	A fundamental issue in real-time interactive voice transmissions over unreliable IF networks is the loss or late arrival of packets for playback. This problem is especially serious when transmitting low bit rate-coded speech with pervasive dependencies introduced. In this case, the loss or late arrival of a single packet will lead to the loss of subsequent dependent frames. We study end-to-end loss-concealment schemes for ensuring high quality in playback. We propose a novel multiple description-coding method for concealing packet losses in transmitting low bit rate-coded speech. Based on high correlations observed in linear predictor parameters-in the form of Line Spectral Pairs (LSPs)-of adjacent frames, we generate multiple descriptions in senders by interleaving LSPs, and reconstruct lost LSPs in receivers by linear interpolations. As excitation codewords have low correlations, we further enlarge the segment size for excitation generation and replicate excitation codewords in all the descriptions in order to maintain the same transmission bandwidth. Our proposed scheme can be extended easily to more than two descriptions and can adapt its number of descriptions dynamically to network-loss conditions. Experimental results on FS-1016 CELP, ITU G.723.1, and FS MELP coders show good performance of our scheme.
7EEEAF03	Voice over IP (VoIP) applications can choose a plethora of different speech codecs, which differ in bandwidth, listening speech quality, and resilience to quality degradation under packet loss. However, VoIP Codecs also exhibit differences in facets such as computational complexity or traffic generated that impact on the energy consumption of smartphones due to the use of processor.In this work deals with the study of energy consumption differences among VoIP codecs. We compare the execution time required to encode/decode reference conversations. Our results show that computational complexity has a significant impact on battery consumption (a factor of up to 10 was found between different codecs). Based on our results, we provide a ranking of energy efficiency. We also propose a simple algorithm for codec dynamic selection considering the dimensions of quality, energy and bandwidth. Our algorithm reacts to network conditions choosing the codec that provides less battery consumption constrained to user-defined targets for minimum quality and maximum codec bitrate.
800A2FE1	The paper presents a method to improve the recovery of a speech decoder after the reception of one or several late frames. Rather than considering a late frame as "lost", we propose to use it in order to update the internal state of the decoder. This limits, and, in some cases, stops, the error propagation caused by the concealment. Evaluation results show that there is much to be gained in a voice over IP environment, where late frames can be used to improve the robustness against jitter without increasing the overall end-to-end delay.
025B47E4	This paper presents the first formal framework for identifying and filtering SPIT calls (SPam in Internet Telephony) in an outbound scenario with provable optimal performance. In so doing, our work deviates from related earlier work where this problem is only addressed by ad-hoc solutions. Our goal is to rigorously formalize the problem in terms of mathematical decision theory, find the optimal solution to the problem, and derive concrete bounds for its expected loss (number of mistakes the SPIT filter will make in the worst case). This goal is achieved by considering a scenario amenable to theoretical analysis, namely SPIT detection in an outbound scenario with pure sources. Our methodology is to first define the cost of making an error, apply Wald’s sequential probability ratio test, and then determine analytically error probabilities such that the resulting expected loss is minimized. The benefits of our approach are: (1) the method is optimal (in a sense defined in the paper); (2) the method does not rely on manual tuning and tweaking of parameters but is completely self-contained and mathematically justified; (3) the method is computationally simple and scalable. These are desirable features that would make our method a component of choice in larger, autonomic frameworks.
7A5925AC	This paper presents the design and the implementation of Elliptic Curve Cryptography in an Asterisk VoIP server which serves as an exchange for placing voice calls over the internet. Voice over internet protocol refers to the transmission of speech encoded into data packets transmitted across networks. VoIP networks are prone to confidentiality threats due to the weak keys used by the AES algorithm for encryption of the VoIP packets. So, in order to strengthen the key for encryption/decryption, Elliptic Curve Diffie-Hellman (ECDH) Algorithm key agreement scheme is employed with smaller key sizes resulting in faster computations. The elliptic curve used in this paper is a modified NIST P-256 curve and key generation algorithm using split exponents for fast exponentiation has been implemented to speed up and increase the randomness of key generation. The implementation of split exponents also help in increasing the security of the keys generated. The key generated by ECDH is highly secure because the discrete logarithmic problem is very difficult in this scheme. This Method is successfully carrying out voice calls on VoIP clients connected to the internet. This ECDH key exchanging mechanism for voice calls in real time is implemented on an Asterisk PBX (Private Branch eXchange), using AGI(Asterisk Gateway Interface) server.
685518E3	In a mobile ad hoc network, all the nodes will act as the client/server and it has the capability to forward the packets. Security is a major concern in a wireless network due to decentralised topology. Intruders may attack the transmission path between the source and destination. Existing technique used an ad hoc on demand vector AODV routing protocol to detect the attackers by an on demand basis. But it is not sufficient to detect all the denial of a service attack. Therefore, the proposed new protocol as an on-demand attack detection ODAD protocol is designed to detect various attacks in different ways. It contains three modes to detect the various attacks. They are: 1 finding drop node FDN; 2 acknowledgement ACK; 3 self-node correction SNC. These modes are used to detect the attackers inside the mobile ad hoc network and then inform about the intruders to the remaining node in the network. Experimental results analyse the performance of throughput, packet delivery rate, drop rate and delay. This technique decreases the delay and drop rate, and increases the delivery rate and throughput compared with the existing technique.
01DB3F19	A Distributed Denial of Service (DDoCS) attack consumes the resources of a remote host or network by sending a massive amount ofIP packets from many distributed hosts. It is a pressing problem on the Internet as demonstrated by recent attacks on major e-commerce servers andISPs. Since the attack is distributed and the attack tools evolve at a rapid and alarming rate, an effective solution must be formulated using a distributed and adaptive approach. In this paper, we propose a countermeasure againstDDoCS attacks using a method we call Active Shaping. Our method employs the Active Networks technologies, which incorporates programmability into network nodes. The Active Networks technology enables us to deter congestion and bandwidth consumption of the backbone network caused byDDoCS attacks, and to prevent our system from dropping packets of legitimate users mistakenly. This paper introduces the concept of our method, system design and evaluates the effectiveness of our method using a prototype.
80FFEEEE	This paper presents a simple and robust mechanism, called change-point monitoring (CPM), to detect denial of service (DoS) attacks. The core of CPM is based on the inherent network protocol behavior and is an instance of the sequential change point detection. To make the detection mechanism insensitive to sites and traffic patterns, a nonparametric cumulative sum (CUSUM) method is applied, thus making the detection mechanism robust, more generally applicable, and its deployment much easier. CPM does not require per-flow state information and only introduces a few variables to record the protocol behaviors. The statelessness and low computation overhead of CPM make itself immune to any flooding attacks. As a case study, the efficacy of CPM is evaluated by detecting a SYN flooding attack - the most common DoS attack. The evaluation results show that CPM has short detection latency and high detection accuracy
80EC4C12	Distributed Denial of Service (DDoS) flooding attacks are one of the biggest concerns for security professionals. DDoS flooding attacks are typically explicit attempts to disrupt legitimate users' access to services. Attackers usually gain access to a large number of computers by exploiting their vulnerabilities to set up attack armies (i.e., Botnets). Once an attack army has been set up, an attacker can invoke a coordinated, large-scale attack against one or more targets. Developing a comprehensive defense mechanism against identified and anticipated DDoS flooding attacks is a desired goal of the intrusion detection and prevention research community. However, the development of such a mechanism requires a comprehensive understanding of the problem and the techniques that have been used thus far in preventing, detecting, and responding to various DDoS flooding attacks. In this paper, we explore the scope of the DDoS flooding attack problem and attempts to combat it. We categorize the DDoS flooding attacks and classify existing countermeasures based on where and when they prevent, detect, and respond to the DDoS flooding attacks. Moreover, we highlight the need for a comprehensive distributed and collaborative defense approach. Our primary intention for this work is to stimulate the research community into developing creative, effective, efficient, and comprehensive prevention, detection, and response mechanisms that address the DDoS flooding problem before, during and after an actual attack.
8139352D	Bluetooth, a protocol designed to replace peripheral cables, has grown steadily over the last five years and includes a variety of applications. The Bluetooth protocol operates on a wide variety of mobile and wireless devices and is nearly ubiquitous. Several attacks exist that successfully target and exploit Bluetooth enabled devices. This paper describes the implementation of a network intrusion detection system for discovering malicious Bluetooth traffic. The work improves upon existing techniques, which only detect a limited set of attacks (based on measuring anomalies in the power levels of the Bluetooth device). The new method identifies reconnaissance, denial of service, and information theft attacks on Bluetooth enabled devices, using signatures of the attacks. Furthermore, this system includes an intrusion response component to detect attacks in progress, based on the attack classification. This paper presents the implementation of the Bluetooth intrusion detection system and demonstrates its detection, analysis, and response capabilities. The tool includes a visualization interface to facilitate the understanding of Bluetooth enabled attacks. The experimental results show that the system can significantly improve the overall security of an organization by identifying and responding to threats posed to the Bluetooth protocol.
80802D68	Denial of Service (DoS) attacks constitute one of the major threats and among the hardest security problems in today’s Internet. Of particular concern are Distributed Denial of Service (DDoS) attacks, whose impact can be proportionally severe. With little or no advance warning, a DDoS attack can easily exhaust the computing and communication resources of its victim within a short period of time. Because of the seriousness of the problem many defense mechanisms have been proposed to combat these attacks. This paper presents a structural approach to the DDoS problem by developing a classification of DDoS attacks and DDoS defense mechanisms. Furthermore, important features of each attack and defense system category are described and advantages and disadvantages of each proposed scheme are outlined. The goal of the paper is to place some order into the existing attack and defense mechanisms, so that a better understanding of DDoS attacks can be achieved and subsequently more efficient and effective algorithms, techniques and procedures to combat these attacks may be developed.
754763FE	Recently many prominent web sites face so called Distributed Denial of Service Attacks (DDoS). While former security threats could be faced by a tight security policy and active measures like using rewalls, vendor patches etc. these DDoS are new in suchway that there is no completely satisfying protection yet. In this paper we categorize different forms of attacks and give an overview over the most common DDoS tools. Furthermore we present a solution based on Class Based Routing mechanisms in the Linux kernel that will prevent the most severe impacts of DDoS on clusters of web servers with a prepended load balancing server. The goal is to keep the web servers under attack responding to the normal client requests. Some performance tests and a comparison to other approaches conclude our paper.
7D1512FE	Nowadays, web servers are suffering from application layer distributed denial of service (DDoS) attacks, to which network layer solutions is not applicable as attackers are indistinguishable based on packets or protocols. In this study, the authors propose trust management helmet (TMH) as a partial solution to this problem, which is a lightweight mitigation mechanism that uses trust to differentiate legitimate users from attackers. Its key insight is that a server should give priority to protecting the connectivity of good users during application layer DDoS attacks, instead of identifying all the attack requests. The trust to clients is evaluated based on their visiting history and used to schedule the service to their requests. The authors introduce license, for user identification (even beyond NATs) and storing the trust information at clients. The license is cryptographically secured against forgery or replay attacks. The authors realise this mitigation mechanism and implement it as a Java package and use it for evaluation. The simulation results show that TMH is effective in mitigating session flooding attack: even with 20 times number of attackers, more than 99% of the sessions from legitimate users are accepted with TMH; whereas less than 18% are accepted without it. Moreover, we found that the additional computation cost on the deployed server is neglectable and the bandwidth overhead is acceptable.
753525DC	Self-organized maps (SOM) use an unsupervised learning technique to independently organize a set of input patterns into various classes. In this paper, we use an ensemble of SOMs to identify computer attacks and characterize them appropriately using the major classes of computer attacks (denial of service, probe, user-to-root and remote-to-local). The procedure produces a set of confidence levels for each connection as a way to describe the connection's behavior.
8163EF49	This paper presents a new distributed approach to detecting DDoS (distributed denial of services) flooding attacks at the traffic-flow level The new defense system is suitable for efficient implementation over the core networks operated by Internet service providers (ISPs). At the early stage of a DDoS attack, some traffic fluctuations are detectable at Internet routers or at the gateways of edge networks. We develop a distributed change-point detection (DCD) architecture using change aggregation trees (CAT). The idea is to detect abrupt traffic changes across multiple network domains at the earliest time. Early detection of DDoS attacks minimizes the floe cling damages to the victim systems serviced by the provider. The system is built over attack-transit routers, which work together cooperatively. Each ISP domain has a CAT server to aggregate the flooding alerts reported by the routers. CAT domain servers collaborate among themselves to make the final decision. To resolve policy conflicts at different ISP domains, a new secure infrastructure protocol (SIP) is developed to establish mutual trust or consensus. We simulated the DCD system up to 16 network domains on the Cyber Defense Technology Experimental Research (DETER) testbed, a 220-node PC cluster for Internet emulation experiments at the University of Southern California (USC) Information Science Institute. Experimental results show that four network domains are sufficient to yield a 98 percent detection accuracy with only 1 percent false-positive alarms. Based on a 2006 Internet report on autonomous system (AS) domain distribution, we prove that this DDoS defense system can scale well to cover 84 AS domains. This security coverage is wide enough to safeguard most ISP core networks from real-life DDoS flooding attacks.
7E50D7D3	A low-rate distributed denial of service (DDoS) attack has significant ability of concealing its traffic because it is very much like normal traffic. It has the capacity to elude the current anomaly-based detection schemes. An information metric can quantify the differences of network traffic with various probability distributions. In this paper, we innovatively propose using two new information metrics such as the generalized entropy metric and the information distance metric to detect low-rate DDoS attacks by measuring the difference between legitimate traffic and attack traffic. The proposed generalized entropy metric can detect attacks several hops earlier (three hops earlier while the order α = 10 ) than the traditional Shannon metric. The proposed information distance metric outperforms (six hops earlier while the order α = 10) the popular Kullback-Leibler divergence approach as it can clearly enlarge the adjudication distance and then obtain the optimal detection sensitivity. The experimental results show that the proposed information metrics can effectively detect low-rate DDoS attacks and clearly reduce the false positive rate. Furthermore, the proposed IP traceback algorithm can find all attacks as well as attackers from their own local area networks (LANs) and discard attack traffic.
252C6489	Distributed Denial-of-Service attack (DDoS) is one of the most outstanding menaces on the Internet. A DDoS attack generally attempts to overwhelm the victim in order to deny their services to legitimate users. A number of approaches have been proposed for defending against DDoS attacks accurately in real time. However, existing schemes have limits in terms of detection accuracy an d delay if the IDRS (Intrusion Detection and Response System) deployed only at a specific location detects and responds against attacks. As in this case, it is not able to catch the characteristic of the attack which is distributed in large-scale. Moreover, the existing detection schemes have vulnerabilities to intellectual DDoS attacks which are able to avoid its detection threshold or delay its detection time. This paper suggests the effective DDoS defense system which uses the collaborative scheme among distributed IDRSs located in the vicinity of the attack source or victim network. In proposed scheme, both victim and source-end IDRS work synergistically to identify the attack and avoid false alarm rate up to great extent. Additionally, we propose the duplicate detection window scheme to detect various attacks dynamics which increase the detection threshold gradually in early stage. The proposed scheme can effectively detect and respond against these diverse DDoS attack dynamics.
5C00C81D	Traditional networking is being progressively replaced by Software Defined Networking (SDN). It is a new promising approach to designing, building and managing networks. In comparison with traditional routed networks, SDN enables programmable and dynamic networks. Although it promises more flexible network management, one should be aware of current and upcoming security threats accompanied with its deployment. Our goal is to analyze SDN accompanied with OpenFlow protocol from the perspective of Distributed Denial of Service attacks (DDoS). In this paper, we outline our research questions related to an analysis of current and new possibilities of realization, detection and mitigation of DDoS attacks in this environment.
8066C0BF	The interaction between TCP and various active queue management (AQM) algorithms has been extensively analyzed for the last few years. However, the analysis usually assumed that routers and TCP flows are not under any network attacks. In this paper, we investigate how the performance of TCP flows is affected by denial-of-service (DoS) attacks under the drop tail and various AQM schemes. In particular, we consider two types of DoS attacks-the traditional flooding-based DoS (FDDoS) attacks and the recently proposed pulsing DoS (PDoS) attacks. Both analytical and simulation results support that the PDoS attacks are more effective than the FDDoS attacks under the same average attack rate. Moreover, the drop tail surprisingly outperforms the RED-like AQMs when the router is under a PDoS attack, whereas the RED-like AQMs perform better under a severe FDDoS attack. On the other hand, the Adaptive Virtual Queue algorithm can retain a higher TCP throughput during PDoS attacks as compared with the RED-like AQMs.
5D2253F3	The Border Gateway Protocol (BGP) is a fundamental component of the current Internet infrastructure. Due to the inherent trust relationship between peers, control of a BGP router could enable an attacker to redirect traffic allowing man-in-the-middle attacks or to launch a large-scale denial of service. It is known that BGP has weaknesses that are fundamental to the protocol design. Many solutions to these weaknesses have been proposed, but most require resource intensive cryptographic operations and modifications to the existing protocol and router software. For this reason, none of them have been widely adopted. However, the threat necessitates an effective, immediate solution.We propose a system that is capable of detecting malicious inter-domain routing update messages through passive monitoring of BGP traffic. This approach requires no protocol modifications and utilizes existing monitoring infrastructure. The technique relies on a model of the autonomous system connectivity to verify that route advertisements are consistent with the network topology. By identifying anomalous update messages, we prevent routers from accepting invalid routes. Utilizing data provided by the Route Views project, we demonstrate the ability of our system to distinguish between legitimate and potentially malicious traffic.
7B55777A	Distributed Denial of Service (DDoS) attacks have caused continuous critical threats to the Internet services. DDoS attacks are generally conducted at the network layer. Many DDoS attack detection methods are focused on the IP and TCP layers. However, they are not suitable for detecting the application layer DDoS attacks. In this paper, we propose a scheme based on web user browsing behaviors to detect the application layer DDoS attacks (app-DDoS). A clustering method is applied to extract the access features of the web objects. Based on the access features, an extended hidden semi-Markov model is proposed to describe the browsing behaviors of web user. The deviation from the entropy of the training data set fitting to the hidden semi-Markov model can be considered as the abnormality of the observed data set. Finally experiments are conducted to demonstrate the effectiveness of our model and algorithm. 
7ED0AEF8	In this poster, based on our previous work in building a lightweight DDoS (Distributed Denial-of-Services) attacks detection mechanism for web server using TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) and genetic algorithm based instance selection methods, we further propose a more efficient and effective instance selection method, named E-FCM (Extend Fuzzy C-Means). By using this method, we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance. Therefore, the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment.
8010F0E4	In a Delay-Tolerant Network (DTN), data originating from a source node may be delivered to the destination node, despite the non-existence of end-to-end connectivity between them at all times. In an adversarial environment such as a battlefield, DTN nodes could be compromised to launch Denial-of-Service (DoS) attacks by generating excess data, to cause an overflow of the limited resources of the legitimate nodes, hence decreasing the network throughput. A node may also display selfish behavior by generating more data than allowed, to increase its throughput and to decrease the latency of its data packets. In this paper, we term such a DoS attack and selfish data generation behavior, a resource-misuse attack. We study two types of resource-misuse attacks, breadth attacks and depth attacks. Accordingly, we propose different schemes to detect these attacks. Trace-driven simulations using both a synthetic and a real-world trace show that our detection schemes have low average detection latency and additionally, probabilistic detection of the depth attack has low false positive and false negative rates.
7E768027	Mobile Ad Hoc Networks are vulnerable to various types of Denial of Service (DoS) attacks for the absence of fixed network infrastructure. The Gray Hole attack is a type of DoS attacks. In this attack, an adversary silently drops some or all of the data packets sent to it for further forwarding even when no congestion occurs. Firstly, DSR protocol, aggregate signature algorithm and network model were introduced. Secondly, we proposed to use aggregate signature algorithm to trace packet dropping nodes. The proposal was consisted of three related algorithms: the creating proof algorithm, the checkup algorithm and the diagnosis algorithm. The first was for creating proof, and the second was for checking up source route nodes, and the last was for locating the malicious nodes. Finally, the efficiency of the proposal was analyzed. The simulation results using ns-2 show that in a moderately changing network, most of the malicious nodes could be detected, the routing packet overhead was low, and the packet delivery rate has been improved.
80B22E01	Launching a denial of service (DoS) attack is trivial, but detection and response is a painfully slow and often a manual process. Automatic classification of attacks as single- or multi-source can help focus a response, but current packet-header-based approaches are susceptible to spoofing. This paper introduces a framework for classifying DoS attacks based on header content, and novel techniques such as transient ramp-up behavior and spectral analysis. Although headers are easily forged, we show that characteristics of attack ramp-up and attack spectrum are more difficult to spoof. To evaluate our framework we monitored access links of a regional ISP detecting 80 live attacks. Header analysis identified the number of attackers in 67 attacks, while the remaining 13 attacks were classified based on ramp-up and spectral analysis. We validate our results through monitoring at a second site, controlled experiments, and simulation. We use experiments and simulation to understand the underlying reasons for the characteristics observed. In addition to helping understand attack dynamics, classification mechanisms such as ours are important for the development of realistic models of DoS traffic, can be packaged as an automated tool to aid in rapid response to attacks, and can also be used to estimate the level of DoS activity on the Internet.
81EDE1E2	In recent years, DoS (Denial of Service) attack and more powerful DDoS (Distributed DoS) attack pose security problems on the Internet. As the measure to these attacks, it is important to trace attackers and stop the attacks. However, since information of the attacker is ``spoofed'', it is difficult to trace. Therefore, the method of specifying attackers is required. Savage et al. proposed a method to trace flooding attacks by ``marked'' packets. This method, however, has some problems gathering the attack packets through a lot of hops. In this paper, we propose a method to solve this problem by observing the feature of attack traffic and change the ``marking probability'' of the routers. We implement algorithms both of our proposed method and extending marking method to estimate the efficiency of them. From the results of some experiments, we will conclude the effectiveness of our proposed scheme 
80B88823	In this paper we consider how to optimize a new generation of pulsing denial-of-service (PDoS) attacks from the attackers' points of views. The PDoS attacks are 'smarter' than the traditional attacks in several aspects. The most obvious one is that they require fewer attack packets to cause a similar damage. Another is that the PDoS attacks can be tuned to achieve different effects. This paper concentrates on the attack tuning part. In particular, we consider two conflicting goals involved in launching a PDoS attack: (1) maximizing the throughput degradation and (2) minimizing the risk of being detected. To address this problem, we first analyze the TCP throughput and quasi-global synchronization phenomenon caused by the PDoS attack. We then propose a family of objective functions to incorporate the two conflicting goals, and obtain the optimal attack settings. To validate the analytical results, we have carried out extensive experiments using both ns-2 simulation and a test-bed. The overall experimental results match well with the analytical results.
79E6CA97	We describe a two-dimensional architecture for defending against denial of service attacks. In one dimension,the architecture accounts for all resources consumed by each I/O path in the system; this accounting mechanism is implemented as an extension to the path object in the Scout operating system. In the second dimension, the various modules that define each path can be configured in separate protection domains; we implement hardware enforced protection domains, although other implementations are possible. The resulting system—which we call Escort—is the first example of a system that simultaneously does end-to-end resource accounting (thereby protecting against resource based denial of service attacks where principals can be identified) and supports multiple protection domains (thereby allowing untrusted modules to be isolated from each other). The paper describes the Escort architecture and its implementation in Scout, and reports a collection of experiments that measure the costs and benefits of using Escort to protect a web server from denial of service attacks.
7A652F84	Security experts generally acknowledge that the long-term solution to distributed denial of service attacks is to increase the security level of Internet computers. Attackers would then be unable to find zombie computers to control. Internet users would also have to set up globally coordinated filters to stop attacks early. However, the critical challenge in these solutions lies in identifying the incentives for the Internet's tens of millions of independent companies and individuals to cooperate on security and traffic control issues that do not appear to directly affect them. We give a brief introduction to: network weaknesses that DDoS attacks exploit; the technological futility of addressing the problem solely at the local level; potential global solutions; and why global solutions require an economic incentive framework.
7FDCED3F	The paper analyzes a network based denial of service attack for IP (Internet Protocol) based networks. It is popularly called SYN flooding. It works by an attacker sending many TCP (Transmission Control Protocol) connection requests with spoofed source addresses to a victim's machine. Each request causes the targeted host to instantiate data structures out of a limited pool of resources. Once the target host's resources are exhausted, no more incoming TCP connections can be established, thus denying further legitimate access. The paper contributes a detailed analysis of the SYN flooding attack and a discussion of existing and proposed countermeasures. Furthermore, we introduce a new solution approach, explain its design, and evaluate its performance. Our approach offers protection against SYN flooding for all hosts connected to the same local area network, independent of their operating system or networking stack implementation. It is highly portable, configurable, extensible, and requires neither special hardware, nor modifications in routers or protected end systems.
7E0481D7	DDoS attack flows distributed in many links exhibit directional nature, they are usually generated by certain tools and originate from different nodes, but have inherent dependencies in spatial when transit in network. This will cause correlation between the traffic where they traverse deviate from norm. By taking advantage of this feature, we propose a spatial correlation detection method deploying in backbone network to combat DDoS attack. In doing so, we first approximately estimate abnormality of every origin destination (OD) flow through comparing observations with predictions, then for OD flows with same destination, extracting spatial correlation between their abnormality estimations by principle component analysis(PCA). Abrupt change of spatial correlation indicates DDoS attack occurs. We evaluate the detection performance of our method in detecting synthetic DDoS attack that injected on real backbone traffic through receiver operating characteristic (ROC) curve. The contribution of this paper is utilizing spatial correlation between attack flows, rather than the volume of attack purely, facilitates us to detect relatively small attack being masked in tremendous traffic of backbone network. Moreover, contrary to the centralized computation of previous network-wide anomaly detection method, our method can be deployed separately in each node, in such a way that our method can adapt to different size of network, and thus scalable.
7DC336D3	Different signature or misuse based intrusion detection techniques; anomaly detection is accomplished of detecting novel attacks. However, the use of anomaly detection in practice is vulnerable by a high rate of false alarms. Pattern based techniques have been shown to make a low rate of false alarms, but are not as efficient as anomaly detection in detecting novel attacks,particularly when it comes to network probing and Denial-Of-Service (DOS) attacks. In this paper we find a new approach that merge pattern-based and anomaly-based intrusion detection, mitigating the weak point of the two approaches while increasing their strengths. Our approach begins with network protocols, and expands these state machines with information about statistics that need to be maintained to detect anomalies. 
80DF889E	Denial of service (DoS) attack on the Internet has become a pressing problem. In this paper, we describe and evaluate route-based distributed packet filtering (DPF), a novel approach to distributed DoS (DDoS) attack prevention. We show that DPF achieves proactiveness and scalability, and we show that there is an intimate relationship between the effectiveness of DPF at mitigating DDoS attack and power-law network topology.The salient features of this work are two-fold. First, we show that DPF is able to proactively filter out a significant fraction of spoofed packet flows and prevent attack packets from reaching their targets in the first place. The IP flows that cannot be proactively curtailed are extremely sparse so that their origin can be localized---i.e., IP traceback---to within a small, constant number of candidate sites. We show that the two proactive and reactive performance effects can be achieved by implementing route-based filtering on less than 20% of Internet autonomous system (AS) sites. Second, we show that the two complementary performance measures are dependent on the properties of the underlying AS graph. In particular, we show that the power-law structure of Internet AS topology leads to connectivity properties which are crucial in facilitating the observed performance effects.
80B9C13E	While there are many ongoing research efforts for Denial-of-Service (DoS) attacks in the general Internet environment, there is insufficient research on voice networks. In this paper, we present the design and evaluation of a SIP-TRW algorithm for detection of DDoS attack traffic in VoIP networks. We analyzed existing TRW algorithms for detection of DDoS attacks in the Internet. In order to apply existing algorithms to voice networks, we designed connection establishment and release processes, and defined the probability function.In order to verify the proposed algorithm, we determined the threshold value and defined the variables for the virtual traffic and the environment. Numerical results from the equation showed that there is 70% probability that the connection will break. It also showed that attacks will be detected within 1.2 seconds when the rate of attack is 10 packets per second. The detection time is within 0.5 seconds when the rate is 20 packets per second. We used NS-2 simulators to measure detection ratio by attack traffic type, and the detection time by attack speed. The results showed that detection took 4.3 seconds when one INVITE packet was sent every 0.1 seconds. The proposed algorithm detected 13280 out of 15000 different attack types, resulting in an 89% detection ratio.
5A8DE225	Distributed Denial-of-Service (DDoS) attacks are a major problem in the Internet today. In one form of a DDoS attack, a large number of compromised hosts send unwanted traffic to the victim, thus exhausting the resources of the victim and preventing it from serving its legitimate clients. One of the main mechanisms that have been proposed to deal with DDoS is filtering, which allows routers to selectively block unwanted traffic. Given the magnitude of DDoS attacks and the high cost of filters in the routers today, the successful mitigation of a DDoS attack using filtering crucially depends on the efficient allocation of filtering resources. In this paper, we consider a single router, typically the gateway of the victim, with a limited number of available filters. We study how to optimally allocate filters to attack sources, or entire domains of attack sources, so as to maximize the amount of good traffic preserved, under a constraint on the number of filters. We formulate the problem as an optimization problem and solve it optimally using dynamic programming, study the properties of the optimal allocation, experiment with a simple heuristic and evaluate our solutions for a range of realistic attack-scenarios. First, we look at a single-tier where the collateral damage is high due to the filtering at the granularity of domains. Second, we look at the two-tier problem where we have an additional constraint on the number of filters and the filtering is performed on the granularity of attackers and domains. 
778A3BAF	The basis of denial of service (DoS)/distributed DoS (DDoS) attacks lies in overwhelming a victim's computer resources by flooding them with enormous traffic. This is done by compromising multiple systems that send a high volume of traffic. The traffic is often formulated in such a way that it consumes finite resources at abnormal rates either at victim or network level. In addition, spoofing of source addresses makes it difficult to combat such attacks. This paper adopts a twofold collaborative mechanism, wherein the intermediate routers are engaged in markings and the victim uses these markings for detecting and filtering the flooding attacks. The markings are used to distinguish the legitimate network traffic from the attack so as to enable the routers near the victim to filter the attack packets. The marked packets are also helpful to backtrack the true origin of the spoofed traffic, thus dropping them at the source rather than allowing them to traverse the network. To further aid in the detection of spoofed traffic, Time to Live (TTL) in the IP header is used. The mappings between the IP addresses and the markings along with the TTLs are used to find the spurious traffic. We provide numerical and simulated experimental results to show the effectiveness of the proposed system in distinguishing the legitimate traffic from the spoofed. We also give a statistical report showing the performance of our system.
800818D7	This paper presents a new spectral template-matching approach to countering shrew distributed denial-of-service (DDoS) attacks. These attacks are stealthy, periodic, pulsing, and low-rate in attack volume, very different from the flooding type of attacks. They are launched with high narrow spikes in very low frequency, periodically. Thus, shrew attacks may endanger the victim systems for a long time without being detected. In other words, such attacks may reduce the quality of services unnoticeably. Our defense method calls for collaborative detection and filtering (CDF) of shrew DDoS attacks. We detect shrew attack flows hidden in legitimate TCP/UDP streams by spectral analysis against pre-stored template of average attack spectral characteristics. This novel scheme is suitable for either software or hardware implementation.The CDF scheme is implemented with the NS-2 network simulator using real-life Internet background traffic mixed with attack datasets used by established research groups. Our simulated results show high detection accuracy by merging alerts from cooperative routers. Both theoretical modeling and simulation experimental results are reported here. The experiments achieved up to 95% successful detection of network anomalies along with a low 10% false positive alarms. The scheme cuts off malicious flows containing shrew attacks using a newly developed packet-filtering scheme. Our filtering scheme retained 99% of legitimate TCP flows, compared with only 20% TCP flows retained by using the Drop Tail algorithm. The paper also considers DSP, FPGA, and network processor implementation issues and discusses limitations and further research challenges.
795F12BD	The most popular method of authenticating users is through passwords. Though passwords are the most convenient means of authentication, they bring along themselves the threat of dictionary attacks. While offline dictionary attacks are possible only if the adversary is able to collect data for a successful protocol execution by eavesdropping on the communication channel and can be successfully countered by using public key cryptography, online dictionary attacks can be performed by anyone and there is no satisfactory solution to counter them. In this paper, we propose an authentication protocol which is easy to implement without any infrastructural changes and yet prevents online dictionary attacks. Our protocol uses only one way hash functions and eliminates online dictionary attacks by implementing a challenge-response system. This challenge-response system is designed in a fashion that it hardly poses any difficulty to a genuine user but is extremely burdensome, time consuming and computationally intensive for an adversary trying to launch as many as hundreds of thousands of authentication requests as in case of an online dictionary attack. The protocol is perfectly stateless and thus less vulnerable to denial of service (DoS) attacks.
7B0DB9B9	In order to detect the DoS attack (Denial-of-Service attack) when wireless mesh networks adopt AODV routing protocol of Ad Hoc networks. Such technologies as an end-to-end authentication, utilization rate of cache memory, two pre-assumed threshold value and distributed voting are used in this paper to detect DoS attacker, which is on the basic of hierarchical topology structure in wireless mesh networks. Through performance analysis in theory and simulations experiment, the scheme would improve the flexibility and accuracy of DoS attack detection, and would obviously improve its security in wireless mesh networks.
76C0F2C7	This paper proposes a novel feedback-based control technique that tackles distributed denial of service (DDoS) attacks in four consecutive phases. While protection routers close to the server control inbound traffc rate andkeeps the server alive (phase 1), the server negotiate with upstream routers close to traffc sources to install leaky-buckets for its IP address. The negotiation continues until a defense router on each traffc link accepts the request (phase 2). Next, the server through a feedback-control process adjusts size of leaky-buckets until inbound traffc locates in a desired range (phase 3). Then through a ﬁngerprint test, the server detects which port interfaces of defense routers purely carry good traffc and subsequently asks corresponding defense routers to remove the leaky-bucket limitations for those port interfaces. Additionally, the server amends size of leaky-buckets for the defense routers proportional to amount of good traffc that each one carries (phase 4). Simulation-based results shows that our technique effectively, defenses a victim server against various DDoS attacks such that in most cases more than 90% of good inbound traffc reaches the server while the DDoS attack has been controlled as well.
7FA83BCC	A solo attack may cause a big loss in computer and network systems, its prevention is, therefore, very inevitable. Precise detection is very important to prevent such losses. Such detection is a pivotal part of any security tools like intrusion detection system, intrusion prevention system, and firewalls etc. Therefore, an approach is provided in this paper to analyze denial of service attack by using a supervised neural network. The methodology used sampled data from Kddcup99 dataset, an attack database that is a standard for judgment of attack detection tools. The system uses multiple layered perceptron architecture and resilient backpropagation for its training and testing. The developed system is then applied to denial of service attacks. Moreover, its performance is also compared to other neural network approaches which results more accuracy and precision in detection rate.
7AC99BB6	Denial of Service (DoS) attacks are currently one of the biggest risks any organization connected to the Internet can face. Hence, the congestion handling techniques at the edge router(s), such as Active Queue Management (AQM) schemes must take into account such attacks. Ideally, an AQM scheme should (a) ensure that each network flow gets its fair share of bandwidth, and (b) identify attack flows so that corrective actions (e.g. drop flooding traffic) can be explicitly taken against them to further mitigate the DoS attacks. This paper presents a proof-of-concept work on devising such an AQM scheme, which we name Deterministic Fair Sharing (DFS). Most of the existing AQM schemes do not achieve the above goals or have significant room for improvement. DFS uses the concept of weighted fair share (wfs) that allows it to dynamically self-adjust the router buffer usage based on the current level of congestion, while aiding in identifying malicious flows. By using multiple data structures (a comprehensive repository and a cache) for keeping state of legitimate and malicious flows, DFS is able to optimize its runtime performance (e.g. higher bandwidth flows being handled by the cache). We demonstrate the performance advantage of DFS via extensive simulation while comparing against other existing AQM techniques.
7A6EFF3B	In recent years, DoS (Denial of Service) attacks and more powerful DDoS (Distributed DoS) attacks have posed security problems on the Internet. For countermeasure to these attacks, it is important to trace attackers and stop the attacks. However, since information about the attackers is spoofed, such attacks are difficult to trace. Therefore, a method of identifying attackers is required. Savage and colleagues proposed a method of tracing flooding attacks by using marked packets. This method, however, involves problems in gathering the attack packets through numerous hops. In this paper, we propose a method of resolving this problem by observing the features of the attack traffic and changing the marking probability in the routers. We implement algorithms of both our proposed method and the extended marking method to estimate their efficiency. The experimental results confirm the effectiveness of the proposed method
12A33609	Malicious bot programs, the source of distributed denial of service attack, are widespread and the number of PCs which were infected by malicious bot program are increasing geometrically thesedays. The continuous distributed denial of service attacks are happened constantly through these bot PCs and some financial incident cases have found lately. Therefore researches to response distributed denial of service attack are necessary so we propose an effective feature generation method for distributed denial of service attack detection using entropy. In this paper, we apply our method to both the DARPA 2000 datasets and also the distributed denial of service attack datasets that we composed and generated ourself in general university. And then we evaluate how the proposed method is useful through classification using bayesian network classifier.
7953D3EE	The recently proposed TCP-targeted Low-rate Distributed Denial-of-Service (LDDoS) attacks send fewer packets to attack legitimate flows by exploiting the vulnerability in TCP’s congestion control mechanism. They are difficult to detect while causing severe damage to TCP-based applications. Existing approaches can only detect the presence of an LDDoS attack, but fail to identify LDDoS flows. In this paper, we propose a novel metric – Congestion Participation Rate (CPR) – and a CPR-based approach to detect and filter LDDoS attacks by their intention to congest the network. The major innovation of the CPR-base approach is its ability to identify LDDoS flows. A flow with a CPR higher than a predefined threshold is classified as an LDDoS flow, and consequently all of its packets will be dropped. We analyze the effectiveness of CPR theoretically by quantifying the average CPR difference between normal TCP flows and LDDoS flows and showing that CPR can differentiate them. We conduct ns-2 simulations, test-bed experiments, and Internet traffic trace analysis to validate our analytical results and evaluate the performance of the proposed approach. Experimental results demonstrate that the proposed CPR-based approach is substantially more effective compared to an existing Discrete Fourier Transform (DFT)-based approach – one of the most efficient approaches in detecting LDDoS attacks. We also provide experimental guidance to choose the CPR threshold in practice.
797F0A61	Modern organizations rely on passwords for preventing illicit access to valuable data and resources. A well designed password policy helps users create and manage more effective passwords. This paper offers a novel model and tool for understanding, creating, and testing password policies. We present a password policy simulation model which incorporates such factors as simulated users, accounts, and services. This model and its implementation enable administrators responsible for creating and managing password policies to test them before giving them to actual users. It also allows researchers to test how different password policy factors impact security, without the time and expense of actual human studies. We begin by presenting our password policy simulation model. We next discuss prior work and validate the model by showing how it is consistent with previous research conducted on human users. We then present and discuss experimental results derived using the model
7B9E6441	Password systems are a first line of defense that can prevent, deter, and detect abusive acts. They are one of the most cost effective computer resource control mechanisms presently available. This piece explores some of the more salient aspects of password system design, including objectives of password controls, design philosophies, man-machine interface design, system administration, and technical system implementation.
78185880	A very severe issue in today's computing world is the ‘ACCESS’ to an authenticated user. The most common and popularauthentication mechanism is to use alphanumerical usernames and passwords including special characters and digits. But, textual passwords are most of the time prone to dictionary attacks, shoulder surfing, etc. Hence, graphical passwords have been introducedin this paper as an alternative to authentication schemes. Though the graphical password schemes help in generating more userfriendly passwords, they are still vulnerable to shoulder surfing. To address this issue, text along with images can be combined togenerate more secured passwords. Session passwords, as the name suggests can only be entered once and then, a new password isgenerated for the next session; thereby making the authentication process much stronger.
7DC02752	Password restriction policies and advice on creating secure passwords have limited effects on password strength. Influencing users to create more secure passwords remains an open problem. We have developed Persuasive Text Passwords (PTP), a text password creation system which leverages Persuasive Technology principles to influence users in creating more secure passwords without sacrificing usability. After users choose a password during creation, PTP improves its security by placing randomly-chosen characters at random positions into the password. Users may shuffle to be presented with randomly-chosen and positioned characters until they find a combination they feel is memorable. In this paper, we present an 83-participant user study testing four PTP variations. Our results show that the PTP variations significantly improved the security of users' passwords. We also found that those participants who had a high number of random characters placed into their passwords would deliberately choose weaker pre-improvement passwords to compensate for the memory load. As a consequence of this compensatory behaviour, there was a limit to the gain in password security achieved by PTP.
8025530F	This paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. The present design was the result of countering observed attempts to penetrate the system. The result is a compromise between extreme security and ease of use.
7ABA0670	In this paper, two password authentication schemes, based on triangle and straight line encoding, respectively, are proposed. In our schemes, a one-way function and a strong cryptographic operation such as DES (data encryption standard) are adopted. Besides, in each scheme, the system only needs to store a secret key pair, and each user can select his own password freely. Instead of storing a password verification table inside the computer system, both methods proposed only have to store a corresponding table of identities, which is used by the computer system for validating the submitted passwords. Owing to these two schemes quickly and efficiently responding to any log-in attempt, they are suitable for real-time applications. Furthermore, in each scheme, the system does not need to reconstruct any term of the existing key table when a new user is inserted into the system. Thus, these two schemes are suitable for practical implementation.
5E313D6B	 Untraceable dynamic identity-based remote user authentication scheme with verifiable password update.proposed a dynamic identity-based remote user authentication scheme with verifiable password update. They also proved that their scheme could withstand various attacks. Unfortunately, by proposing concrete attacks, we show that their scheme is vulnerable to three kinds of attacks. We also point out that their scheme cannot provide untraceability. The analysis shows that the scheme of Chang et al. is not suitable for practical applications.
7D376B3C	Users rarely choose passwords that are both hard to guess and easy to remember. To determine how to help users choose good passwords, the authors performed a controlled trial of the effects of giving users different kinds of advice. Some of their results challenge the established wisdom.
813378A5	People enjoy the convenience of on-line services, Automated Teller Machines (ATMs), and pervasive computing, but online environments, ATMs, and pervasive computing may bring many risks. In this paper, we discuss how to prevent users’ passwords from being stolen by adversaries. We propose a virtual password concept involving a small amount of human computing to secure users’ passwords in on-line environments, ATMs, and pervasive computing. We adopt user-determined randomized linear generation functions to secure users’ passwords based on the fact that a server has more information than any adversary does. We analyze how the proposed schemes defend against phishing, key logger, and shoulder-surfing attacks. To the best of our knowledge, our virtual password mechanism is the first one which is able to defend against all three attacks together.
7979C29A	Many security primitives are based on hard mathematical problems. Using hard AI problems for security is emerging as an exciting new paradigm, but has been under-explored. In this paper, we present a new security primitive based on hard AI problems, namely, a novel family of graphical password systems built on top of Captcha technology, which we call Captcha as graphical passwords (CaRP). CaRP is both a Captcha and a graphical password scheme. CaRP addresses a number of security problems altogether, such as online guessing attacks, relay attacks, and, if combined with dual-view technologies, shoulder-surfing attacks. Notably, a CaRP password can be found only probabilistically by automatic online guessing attacks even if the password is in the search set. CaRP also offers a novel approach to address the well-known image hotspot problem in popular graphical password systems, such as PassPoints, that often leads to weak password choices. CaRP is not a panacea, but it offers reasonable security and usability and appears to fit well with some practical applications for improving online security.
7A771AF1	Text-based passwords remain the dominant authentication method in computer systems, despite significant advancement in attackers' capabilities to perform password cracking. In response to this threat, password composition policies have grown increasingly complex. However, there is insufficient research defining metrics to characterize password strength and using them to evaluate password-composition policies. In this paper, we analyze 12,000 passwords collected under seven composition policies via an online study. We develop an efficient distributed method for calculating how effectively several heuristic password-guessing algorithms guess passwords. Leveraging this method, we investigate (a) the resistance of passwords created under different conditions to guessing, (b) the performance of guessing algorithms under different training sets, (c) the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements, and (d) the relationship between guess ability, as measured with password-cracking algorithms, and entropy estimates. Our findings advance understanding of both password-composition policies and metrics for quantifying password security.
5F5D0465	There have been many protocols proposed over the years for password authenticated key exchange in the three-party scenario, in which two clients attempt to establish a secret key interacting with one same authentication server. However, little has been done for password authenticated key exchange in the more general and realistic four-party setting, where two clients trying to establish a secret key are registered with different authentication servers. In fact, the recent protocol by Yeh and Sun seems to be the only password authenticated key exchange protocol in the four-party setting. But, the Yeh-Sun protocol adopts the so called “hybrid model”, in which each client needs not only to remember a password shared with the server but also to store and manage the server’s public key. In some sense, this hybrid approach obviates the reason for considering password authenticated protocols in the first place; it is difficult for humans to securely manage long cryptographic keys. In this paper, we propose a new protocol designed carefully for four-party password authenticated key exchange that requires each client only to remember a password shared with its authentication server. To the best of our knowledge, our new protocol is the first password-only authenticated key exchange protocol in the four-party setting.
7DF5E1C8	Text-based passwords are the most common mechanism for authenticating humans to computer systems. To prevent users from picking passwords that are too easy for an adversary to guess, system administrators adopt password-composition policies (e.g., requiring passwords to contain symbols and numbers). Unfortunately, little is known about the relationship between password-composition policies and the strength of the resulting passwords, or about the behavior of users (e.g., writing down passwords) in response to different policies. We present a large-scale study that investigates password strength, user behavior, and user sentiment across four password-composition policies. We characterize the predictability of passwords by calculating their entropy, and find that a number of commonly held beliefs about password composition and strength are inaccurate. We correlate our results with user behavior and sentiment to produce several recommendations for password-composition policies that result in strong passwords without unduly burdening users.
7A407108	 User authentication and key management are two important security issues in WSNs (Wireless Sensor Networks). In WSNs, for some applications, the user needs to obtain real-time data directly from sensors and several user authentication schemes have been recently proposed for this case. We found that a two-factor mutual authentication scheme with key agreement in WSNs is vulnerable to gateway node bypassing attacks and user impersonation attacks using secret data stored in sensor nodes or an attacker's own smart card. In this paper, we propose an improved scheme to overcome these security weaknesses by storing secret data in unique ciphertext form in each node. In addition, our proposed scheme should provide not only security, but also efficiency since sensors in a WSN operate with resource constraints such as limited power, computation, and storage space. Therefore, we also analyze the performance of the proposed scheme by comparing its computation and communication costs with those of other schemes. 
789936C7	System-generated random passwords have maximum password security and are highly resistant to guessing attacks. However, few systems use such passwords because they are difficult to remember. In this paper, we propose a system-initiated password scheme called "Surpass" that lets users replace few characters in a random password to make it more memorable. We conducted a large-scale online study to evaluate the usability and security of four Surpass policies, varying the number of character replacements allowed from 1 to 4 in randomly-generated 8-character passwords. The study results suggest that some Surpass policies (with 3 and 4 character replacements) outperform by 11% to 13% the original randomly-generated password policy in memorability, while showing a small increase in the percentage of cracked passwords. When compared to a user-generated password complexity policy (that mandates the use of numbers, symbols, and uppercase letters) the Surpass policy with 4-character replacements did not show statistically significant inferiority in memorability. Our qualitative lab study showed similar trends. This Surpass policy demonstrated significant superiority in security though, with 21% fewer cracked passwords than the user-generated password policy.
77C25DD1	Passwords are an ubiquitous and critical component of many security systems. As the information and access guarded by passwords become more necessary, we become ever more dependent upon the security passwords provide. The creation and management of passwords is crucial, and for this we must develop and deploy password policies. This paper focuses on defining and modeling password policies for the entire password policy lifecycle. The paper first discusses a language for specifying password policies. Then, a simulation model is presented with a comprehensive set of variables and the algorithm for simulating a password policy and its impact. Finally, the paper presents several simulation results using the password policy simulation tool.
75B0F4D4	Securing computer systems has traditionally been the domain of system administrators using technology to protect computer systems from attack. However, in many systems, human users are critical to the security process. There's a growing realization that technology alone can't protect security if users don't properly deploy and utilize it. Ultimately, it's users who create passwords and choose whether to adhere to security procedures.
7BE61015	While there are many highly secure authentication systems, the user ID and password credential pair remains the most commonly used means of authentication even though it is one of the easiest to break. It is vulnerable to technical and social attacks but proper application of people, process and technical controls in the selection, use and monitoring of passwords can decrease the likelihood of compromise. This paper investigates the technical and social uses, weaknesses, vulnerabilities, attacks and defenses of the user ID and password combination. This reviews existing technology, use and attacks of user IDs and passwords; it will consider what can be done as well as suggest what should be done. Academic research and industry practice are addressed.
7A9CEECB	The aim of the research is to realize a better form of personal identification number (PIN) authentication for a mobile phone without lowering usability and acceptability. Design/methodology/approach – The authors’ approach is to extend the input operation of PIN authentication by allowing more than one number at a time using a multi-touch-enabled screen. The authors also introduced substitution keys to be able to type any combination of a PIN value and an input pattern by multi-touch typing. Findings – The authors conducted a user evaluation study using a Web-based prototype system. The results of the study indicate that PIN input time, input errors and secret memorability of the proposed scheme were no worse than those of conventional PIN authentication. The theoretical security level of the proposed scheme is almost three and a half times than that of the conventional scheme. Originality/value – The paper introduced a multi-touch-allowed secret input operation into a PIN authentication. Though the introduction affected not only an input operation but also a PIN input interface and secret information, it makes possible to realize a better security level without a drastic change of a user interface and taking a longer input time. 
7A28050B	The movement of koi, a highly colored variant of common carp Cyprinus carpio, was monitored for 18 months in the Waikato River, New Zealand, with both radiotelemetry and acoustic telemetry. Koi had large total linear ranges (mean, 39 km), and most fish (74%) monitored for more than 250 d traveled between the Waikato River and lateral habitat. Differences in total linear range and mean daily movement between years suggest a behavioral response associated with reduced habitat availability during periods of low flow. The movements of koi were highly variable and infrequent, suggesting that important long-range movements are missed in short-term studies (<250 d), resulting in underestimations of total linear range. Our data suggest that large numbers of koi could have been intercepted traveling between lateral habitat and the Waikato River during 1 of 2 years, indicating that point source removal would be highly effective.
7A5EC172	As the Internet has grown, its user community has changed from a small tight-knit group of researchers to a loose gathering of people on a global network. The amazing and constantly growing numbers of machines and users ensures that untrustworthy individuals have full access to that network. High speed inter-machine communication and even higher speed computational processors have made the threats of system “crackers”, data theft, and data corruption very real. This paper outlines some of the problems of current password security by demonstrating the ease by which individual at counts may be broken. Various techniques used by crackers are outlined, and finally one solution to this point of system vulnerability, a proactive password checker, is documented.
7C5F4DDD	The authors have developed a software methodology that improves security by using typing biometrics to reinforce password authentication mechanisms. Typing biometrics is the analysis of a user's keystroke patterns. Each user has a unique way of using the keyboard to enter a password; for example, each user types the characters that constitute the password at different speeds. The methodology employs fuzzy logic to measure the user's typing biometrics. This reinforcement is transparent-indiscernible to the users while they are entering the normal authentication.
7CAED90C	This paper presents the first large-scale study of the success of password expiration in meeting its intended purpose, namely revoking access to an account by an attacker who has captured the account's password. Using a dataset of over 7700 accounts, we assess the extent to which passwords that users choose to replace expired ones pose an obstacle to the attacker's continued access. We develop a framework by which an attacker can search for a user's new password from an old one, and design an efficient algorithm to build an approximately optimal search strategy. We then use this strategy to measure the difficulty of breaking newly chosen passwords from old ones. We believe our study calls into question the merit of continuing the practice of password expiration.
7DD95D8C	Despite considerable research on passwords, empirical studies of password strength have been limited by lack of access to plaintext passwords, small data sets, and password sets specifically collected for a research study or from low-value accounts. Properties of passwords used for high-value accounts thus remain poorly understood.We fill this gap by studying the single-sign-on passwords used by over 25,000 faculty, staff, and students at a research university with a complex password policy. Key aspects of our contributions rest on our (indirect) access to plaintext passwords. We describe our data collection methodology, particularly the many precautions we took to minimize risks to users. We then analyze how guessable the collected passwords would be during an offline attack by subjecting them to a state-of-the-art password cracking algorithm. We discover significant correlations between a number of demographic and behavioral factors and password strength. For example, we find that users associated with the computer science school make passwords more than 1.5 times as strong as those of users associated with the business school. while users associated with computer science make strong ones. In addition, we find that stronger passwords are correlated with a higher rate of errors entering them.We also compare the guessability and other characteristics of the passwords we analyzed to sets previously collected in controlled experiments or leaked from low-value accounts. We find more consistent similarities between the university passwords and passwords collected for research studies under similar composition policies than we do between the university passwords and subsets of passwords leaked from low-value accounts that happen to comply with the same policies.
7B04A913	In 2001, Lin et al. proposed an optimal strong-password authentication protocol called the OSAP protocol. However, Chen and Ku pointed out that it is vulnerable to the stolen-verifier attack. In this paper, we shall propose an improved version of the OSAP protocol to enhance the security.
7DCE7511	TMN is applicable to management of a diversity of equipment, networks and services. This diversity and in particular the huge differences in complexity and cost of network elements (e.g. digital switches compared with regenerators) complicates the specification and standardization of security in TMN. However, equipment cost is not itself very useful and appropriate security measures should be based on other factors such as potential loss of revenue. The need for security of TMN is evident even from a generic evaluation of different information assets together with the specification of top-level security objectives. Existing TMN recommendation only allow for a modest level of security. In a number of known management systems and network elements only seldom changed passwords are used. Such password are often transmitted in the clear thus providing only a very limited degree of security. Additional security measures requires that existing TMN protocols are modified to allow for exchange of security information and communication of protected transfer syntax. In addition, management of security in TMN is essential. GULS (generic upper layer security) is a promising candidate facilitating such security exchanges and security transformations. However, the details of which security techniques (e.g. type of authentication) is not specified by GULS and must be provided by additional standards. Even so, GULS (or similar mechanisms) allow for the introduction of a security infrastructure within the TMN which enables different security techniques to be applied when standardised or as needed.
7BFE2F5C	Passwords are the only ubiquitous form of authentication currently available on the web. Unfortunately, passwords are insecure. In this paper we therefore propose the use of strong cryptography, using the fact that users increasingly own a smartphone that can perform the required cryptographic operations on their behalf. This is not as trivial as it sounds. Services will not migrate to new forms of authentication if few users have the means to use it. Similarly, users will not acquire the means if there are few services that accept them. Moreover, enabling one's smartphone to seamlessly sign in at a website when browsing on an arbitrary PC is non-trivial.We propose a system, based on a smartphone app, that can be used to sign in with username and password to arbitrary websites using an arbitrary PC or laptop. We describe the protocol and implementation to achieve this without the need for typing usernames and passwords. Furthermore, we propose an authentication protocol based on public key cryptography, integrated in the same smartphone app. This allows websites to seamlessly migrate towards a much more secure authentication method on the web, independently of each other. A prototype of our system has been developed.
79D8254D	Many portable devices need a simple authentication system to protect them from being used by an unauthenticated person such as a thief. The security of traditional methods such as pin codes or passwords is limited by shoulder surfing where a casual or intentional observer observes an authentication session and derives all information necessary for authentication. Graphical authentication systems have been developed to forestall this attack. We present here an especially simple variant of a graphical authentication system based on the capacity of humans to recognize faces well. In our challenge-response scheme, a user is presented with a row of typically three faces and needs to decide whether the number of “friends” is even or odd. We present here an analysis of security and usability of this scheme.
76771A1D	Text-based passwords are still the most commonly used authentication mechanism in information systems. We took advantage of a unique opportunity presented by a significant change in the Carnegie Mellon University (CMU) computing services password policy that required users to change their passwords. Through our survey of 470 CMU computer users, we collected data about behaviors and practices related to the use and creation of passwords. We also captured users' opinions about the new, stronger policy requirements. Our analysis shows that, although most of the users were annoyed by the need to create a complex password, they believe that they are now more secure. Furthermore, we perform an entropy analysis and discuss how our findings relate to NIST recommendations for creating a password policy. We also examine how users answer specific questions related to their passwords. Our results can be helpful in designing better password policies that consider not only technical aspects of specific policy rules, but also users' behavior in response to those rules.
76239220	We propose a four-party password authenticated inter-domain key exchange protocol which makes use of properties of identity-based cryptography and secret public keys. Being password-based and certificate-free, our protocol is lightweight and is suited to lightweight computing environments, such as pervasive computing. Apart from resistance against offline and active attacks, our protocol additionally provides perfect forward secrecy. We provide heuristic analysis of various security properties. Performance comparisons against other related protocols show that our protocol is efficient.
7FED7E74	To encourage strong passwords, system administrators employ password-composition policies, such as a traditional policy requiring that passwords have at least 8 characters from 4 character classes and pass a dictionary check. Recent research has suggested, however, that policies requiring longer passwords with fewer additional requirements can be more usable and in some cases more secure than this traditional policy. To explore long passwords in more detail, we conducted an online experiment with 8,143 participants. Using a cracking algorithm modified for longer passwords, we evaluate eight policies across a variety of metrics for strength and usability. Among the longer policies, we discover new evidence for a security/usability tradeoff, with none being strictly better than another on both dimensions. However, several policies are both more usable and more secure that the traditional policy we tested. Our analyses additionally reveal common patterns and strings found in cracked passwords. We discuss how system administrators can use these results to improve password-composition policies.
7D2DA1ED	Personal information and organizational information need to be protected, which requires that only authorized users gain access to the information. The most commonly used method for authenticating users who attempt to access such information is through the use of username–password combinations. However, this is a weak method of authentication because users tend to generate passwords that are easy to remember but also easy to crack. Proactive password checking, for which passwords must satisfy certain criteria, is one method for improving the security of user-generated passwords. The present study evaluated the time and number of attempts needed to generate unique passwords satisfying different restrictions for multiple accounts, as well as the login time and accuracy for recalling those passwords. Imposing password restrictions alone did not necessarily lead to more secure passwords. However, the use of a technique for which the first letter of each word of a sentence was used coupled with a requirement to insert a special character and digit yielded more secure passwords that were more memorable.
77905A47	We describe a sequence of five experiments on network security that cast students successively in the roles of computer user, programmer, and system administrator. Unlike experiments described in several previous papers, these experiments avoid placing students in the role of attacker. Each experiment starts with an in-class demonstration of an attack by the instructor. Students then learn how to use open-source defense tools appropriate for the role they are playing and the attack at hand. Threats covered include eavesdropping, dictionary, man-in-the-middle, port scanning, and fingerprinting attacks. Defense skills gained by students include how to forward ports with OpenSSH, how to prevent weak passwords with CrackLib, how to salt passwords, how to set up a simple certifying authority, issue and verify certificates, and guarantee communication confidentiality and integrity using OpenSSL, and how to set up firewalls and IPsec-based virtual private networks. At two separate offerings, tests taken before and after each experiment showed that each has a statistically significant and large effect on students' learning. Moreover, surveys show that students finish the sequence of experiments with high interest in further studies and work in the area of security. These results suggest that the experiments are well-suited for introductory security or networking courses.
7C325CE0	Watermarking of multimedia content is important to authenticate, copy-control and ownership detection. In this paper, multimedia authentication and tamper detection scheme is proposed with the security of AES(Advanced Encryption Standard) ciphered watermarking and hash function. The algorithm embeds two watermarks in the host image for authentication and tamper detection. We first used Unique Identification Code (UIC) as first robust watermark which is then embedded using the 2-level discrete wavelet transform. Hash code of host image is calculated and used as secondary watermark for tamper detection. This method is blind in nature. The PSNR, SSIM values are used as a metric to test the efficiency of the enhanced watermarking technique. The results show that the imperceptibility of method is high as compared to the existing.
5BC1AE79	We describe a digital watermarking method for use in audio, image, video and multimedia data. We argue that a watermark must be placed in perceptually significant components of a signal if it is to be robust to common signal distortions and malicious attack. However, it is well known that modification of these components can lead to perceptual degradation of the signal. To avoid this, we propose to insert a watermark into the spectral components of the data using techniques analogous to spread sprectrum communications, hiding a narrow band signal in a wideband channel that is the data. The watermark is difficult for an attacker to remove, even when several individuals conspire together with independently watermarked copies of the data. It is also robust to common signal and geometric distortions such as digital-to-analog and analog-to-digital conversion, resampling, and requantization, including dithering and recompression and rotation, translation, cropping and scaling. The same digital watermarking algorithm can be applied to all three media under consideration with only minor modifications, making it especially appropriate for multimedia products. Retrieval of the watermark unambiguously identifies the owner, and the watermark can be constructed to make counterfeiting almost impossible. Experimental results are presented to support these claims.
79DCAAEC	A novel wavelet domain based semi-fragile watermarking scheme is presented for securing digital content and to concisely determine the regions where the integrity fails. In addition, another watermark is embedded to perform self-recovery in case of malicious attack. The security weaknesses of the traditional block-based approaches are circumvented by correlating the watermark bits with wavelet coefficients of the approximation subband of the host image. Semi-fragility exhibits robustness to JPEG compression, while recovery attribute makes the scheme suitable for video surveillance and remote sensing applications. Experimental investigations are performed to evaluate the performance of the proposed multiple semi-fragile watermarks and shows the suitability of the proposed approach for accurate authentication and recovery based applications.
7562FC29	Aiming to balance the robustness and imperceptibility of database watermark, propose a wavelet transform (DWT) based blind watermarking algorithm. The algorithm screens candidate attributes that can be embedded watermark and conducts subset segmentation and rearrangement, and then performs DWT transformation to the data subsets and the scrambled watermark image respectively. Embed the compressed low-frequency part of the watermark into the High-frequency part of the data set to achieve data fusion. Theoretical analysis and experiments show that the algorithm enjoys strong robustness and good invisibility.
809D24E9	Reversible semi-fragile authentication watermark (RSAW) is required in an integrated and powerful authentication system. An effective RSAW scheme should have the desirable features: tamper detection and localization, good perceptual invisibility, detection without requiring explicit knowledge of the original image, robustness against lossy compression, noise attack and low-pass filtering to some extent, reversibility on condition that marked image has not been disturbed, and high security against forge attack. To our best knowledge, RSAW schemes that are presented in the literature are not effective enough. This paper proposes a new RSAW scheme which is effective and has additional features as tamper discerning, computational efficiency and multiple encryption keys supporting. Experimental results demonstrate the validity of the proposed RSAW scheme
77E2777E	Digital watermarking is the process of introducing small modifications into a copy of a digital document that can be detected later. The embedded information can be used to determine the document's owner or simply to distinguish several copies. However, coincidental or malicious “attacks” can degrade the robustness of watermark detection. Here, uniform scalar quantization of watermarked documents is investigated theoretically, extending results from theory of dithered quantization, and experimentally. The watermark is embedded by an independent additive pseudo-noise sequence. The statistical distribution of the quantization errors depending on the statistics of the host signal and the watermark is used to determine the robustness of watermark detection via correlation. Experiments with JPEG compression of an image with a DCT-domain additive watermark demonstrate the usefulness of the presented theory.
7578EEF3	In this letter, we present a new audio watermarking technique using -box transformation. Furthermore, we analyze the strength of the proposed watermarking technique with MSE analysis, PSNR analysis, SSIM analysis, robustness analysis, compression attack analysis, noise attack analysis, cropping analysis and capacity analysis and conclude that the proposed method of watermarking is better than many existing techniques.
7D3743F2	This paper presents a new semi-fragile watermarking algorithm for audio authentication, which takes full advantage of scrambing algorithm. Watermark scrambling algorithm can dispel the pixel space relationship of the binary watermark image. After scrambing, the white pixels and black ones of original watermark image are comparatively orderliness, which is robustness against signal processing and fragility to malicious attack. Therefore the proposed scheme can tolerate general signal processing, e.g. MPEG Audio Layer 3 (MP3) compression, and can detect any spiteful tamper on watermarked audio; locate spatial regions from the ruleless of pixels without the help from the original watermark, and even can evaluate the temper degree. Experimental results show that the algorithm can identify intentional content modification and incidental tampering, and also indicate the location where a modification takes place.
79BF98B1	H.264/AVC is a widespread standard for high definition video (HD) for example DVD and HD videos on the internet. To prevent unauthorized modifications, video authentication can be used. In this paper, we present a cryptanalysis of a H.264/AVC video authentication scheme proposed by Saadi et al. [1] at EUSIPCO 2009. Our result will prevent situations where newer schemes are developed from the scheme thus amplifying the flaw. The designers claimed that the scheme can detect modifications on watermarked video. However, we show that an attacker can modify the watermarked video and compute a valid watermark such that the recipient will retrieve a watermark from the modified watermarked video that will match what the recipient computes during video authentication check. Thus, the recipient will think the tampered video is authentic. The first main problem of the scheme is its use of hash functions for watermark generation. Since hash functions are public functions not depending on any secret, the attacker can modify the watermarked video and feed this through the hash function to compute a new watermark. The second problem is that it is possible for the attacker to perform watermark embedding thus producing a modified watermarked video. On receiving the modified video, the recipient recomputes the watermark and compares this with the watermark extracted from the video. They will match because the embedded watermark and recomputed watermark use the same hash function based watermark generation and the same input i.e. the modified video. Our cryptanalysis strategy applies to any watermarking based video authentication scheme where the watermark and embedding are not functions of secrets. As countermeasure, the functions should be designed so that only legitimate parties can perform them. We present two improved schemes that solve this problem based on private key signing functions and message authentication functions respectively. 
7F411E8F	This paper addresses issues that arise in copyright protection systems of digital images, which employ blind watermark verification structures in the discrete cosine transform (DCT) domain. First, we observe that statistical distributions with heavy algebraic tails, such as the alpha-stable family, are in many cases more accurate modeling tools for the DCT coefficients of JPEG-analyzed images than families with exponential tails such as the generalized Gaussian. Motivated by our modeling results, we then design a new processor for blind watermark detection using the Cauchy member of the alpha-stable family. The Cauchy distribution is chosen because it is the only non-Gaussian symmetric alpha-stable distribution that exists in closed form and also because it leads to the design of a nearly optimum detector with robust detection performance. We analyze the performance of the new detector in terms of the associated probabilities of detection and false alarm and we compare it to the performance of the generalized Gaussian detector by performing experiments with various test images.
80B4B08A	One of the threats to watermarking security is unauthorized removal. In this paper, we present a framework based on extended TPM to solve this problem. As to the spread spectrum (SS) watermarking schemes we believe that it has the capability of read- but-not-remove if the platform software environment on which watermark detection process run is trusted. We think using Trusted Computing Platform could enhance the security of watermarking detection. Firstly, to support our framework, we point out that standard TCG must be extended. Secondly, we discuss the security and feasibility about our method. Finally, we use a SS watermarking scheme as an example and come to a conclusion that a traditional watermarking scheme could get the capability of secure detection after being transformed based on our framework.
7811A20D	A ROI (region of interest) of a medical image is an area including important information and must be stored without any distortion. In order to achieve optimal compression as well as satisfactory visualization of medical images, we compress the ROI by lossless compression, and the rest by lossy compression. Furthermore, security is an important issue in web-based medical information system. Watermarking skill is often used for protecting medical images. In this paper, we present a robust technique embedding the watermark of signature information or textual data around the ROI of a medical image based on genetic algorithms. A fragile watermark is adopted to detect any unauthorized modification. The embedding of watermark in the frequency domain is more difficult to be pirated than in spatial domain.
777FAEBB	The paper is focused on the evolution of the watermarking schemes from pixel-based approaches to content based methods. Firstly, the field of watermarking is presented, its implication in the real word is outlined. The second section gives a brief overview of different approaches in the spatial and the frequential domain. We then present different content-based approaches used to improve the invisibility or the synchronisation of the mark. The last section describes our content-based scheme; the synchronisation of the mark after image processing lies in the use of feature point detectors. The image is decomposed into a set of triangles and each triangle is individually marked.
128FCFCC	In-network processing presents a critical challenge for data authentication in wireless sensor networks (WSNs). Current schemes relying on Message Authentication Code (MAC) cannot provide natural support for this operation since even a slight modification to the data invalidates the MAC. Although some recent works propose using privacy homomorphism to support in-network processing, they can only work for some specific query-based aggregation functions, e.g. SUM, average, etc. In this paper, based on digital watermarking, we propose an end-to-end, statistical approach for data authentication that provides inherent support for in-network processing. In this scheme, authentication information is modulated as watermark and superposed on the sensory data at the sensor nodes. The watermarked data can be aggregated by the intermediate nodes without incurring any en route checking. Upon reception of the sensory data, the data sink is able to authenticate the data by validating the watermark, thereby detecting whether the data has been illegitimately altered. In this way, the aggregation–survivable authentication information is only added at the sources and checked by the data sink, without any involvement of intermediate nodes. Furthermore, the simple operation of watermark embedding and complex operation of watermark detection provide a natural solution of function partitioning between the resource limited sensor nodes and the resource abundant data sink. In addition, the watermark can be embedded in both spatial and temporal domains to provide the flexibility between the detection time and detection granularity. The simulation results show that the proposed scheme can successfully authenticate the sensory data with high confidence.
753DC3A7	A  new watermarking scheme having the ability of sharing secret with multi-users is proposed. It splits the original watermark into two shares and embeds one share into the cover image to increase the security. A polarization procedure is performed to establish a polarity stream from the cover image. The second share and the polarity stream are used to generate a master key and several normal keys. In this system, only the super-user can reveal the genuine watermark directly. Other users possess the normal keys can obtain shadow watermarks merely. By combining the shadow watermarks together, the real watermark can be recovered.
802F0C1D	A method is presented for marking high-quality digital images with a robust and invisible watermark. A broad definition of robustness, stated as fundamental, is used. It requires the invisible mark to survive and remain detectable through all image manipulations that in themselves does not damage the image beyond useability. These manipulations include JPEG "lossy" compression and, in the extreme, the printing and rescanning of the image. The watermark is imparted onto an image as a random, bur reproducible, small modulation of its pixel brightnesses, and becomes a permanent part of the marked image. Detecting the imparted watermark, especially after image manipulation, is a daunting task. It is one of detecting the presence of a known small modulation of a random carrier where the carrier is composed of the pixel brightness values of the unmarked image. The method presented exploits the not well understood but superb ability of the human visual system to recognize a correlated pattern in a scatter diagram called a "visualizer-coincidence image." Results of application of the method are presented.
7B8AF693	This paper presents a watermarking scheme, which is robust and can sustain almost all attack on audio file. In the proposed watermarking scheme use a robust watermark which can resist all attacks. The feature of the proposed scheme is to resist minimum mean square error (MMSE) estimation attack. In audio samples an energy efficient watermark is embedded which satisfies the power spectrum condition (PSC). In PSC compliant technique watermark's power spectrum is directly proportional to that of the original signal. The watermark satisfying PSC are proven to be most robust. Energy efficient watermarking scheme resist MMSE as much as possible. Experiments justify that the proposed scheme is inaudible and robust against various attack such as noise adding, resampling, requantization, random cropping and MP3 compression.
76441826	In this paper, a DC-based approach to robust watermarking is proposed. Classical watermarking schemes widely for frequency domain hide information in various frequency components. Due to codec operations, hidden information usually is corrupted and damaged. The lack of robustness against possible codec attack is an important issue of advanced watermarking technique. In this study, we find the polarity of DC components in each image block is robust to the DCT/IDCT operations. Thus, a new watermarking scheme based DC-component is developed. In this proposed scheme, information bits are encoded and hidden in the DC-components of each image block. The experimental results indicate that (1) hidden information can be recovered from compression/decompression attacks and (2) the information-hiding process is low computation- complexity. Besides, the relationship between block size and the amount of hidden information is also investigated to illustrate the deployment strategy in real applications.
7FAA1196	The growth of new imaging technologies has created a need for techniques that can be used for copyright protection of digital images and video. One approach for copyright protection is to introduce an invisible signal, known as a digital watermark, into an image or video sequence. In this paper, we describe digital watermarking techniques, known as perceptually based watermarks, that are designed to exploit aspects of the the human visual system in order to provide a transparent (invisible), yet robust watermark. In the most general sense, any watermarking technique that attempts to incorporate an invisible mark into an image is perceptually based. However, in order to provide transparency and robustness to attack, two conflicting requirements from a signal processing perspective, more sophisticated use of perceptual information in the watermarking process is required. We describe watermarking techniques ranging from simple schemes which incorporate common-sense rules in using perceptual information in the watermarking process, to more elaborate schemes which adapt to local image characteristics based on more formal perceptual models. This review is not meant to be exhaustive; its aim is to provide the reader with an understanding of how the techniques have been evolving as the requirements and applications become better defined.
808BBE9F	3D face recognition is one of the most popular researches at present. However, 3D face model stored in databases can be tampered or overwritten with attacker's own, which results in that legitimate user failed in authentication or lost their identity. In this paper, we provide a security enhancement scheme for biometric system and implement it in 3D face recognition. In our scheme, a digital watermark algorithm for 3D face model authentication is proposed to guarantee that the 3D face model in databases can not be modified. The difference of recognition performance between method with authentication and method without authentication is measured to evaluate our security enhancement scheme. Experiment results show the scheme and 3D face model authentication algorithm we proposed is viable and valid.
770D38E1	In this paper a fragile watermarking technique based on Karhunen-Loève transform (KLT) and genetic algorithms (GA) is proposed. To achieve high sensibility to content manipulations, the proposed algorithm slightly modifies the middle-frequency KLT coefficients while maintaining the visual imperceptibility of the watermark (i.e., the average peak signal-to-noise ratio reported was 52.49 dB). It uses a GA to compensate the errors in the extraction of the authentication information, which is a pseudo-random generated watermark sequence. The experimental tests carried out show that the scheme is able to detect pixel-level alterations localizing the block in which the pixel(s) was modified.
80A861F9	The paper discusses the feasibility of coding an "undetectable" digital water mark on a standard 512/spl times/512 intensity image with an 8 bit gray scale. The watermark is capable of carrying such information as authentication or authorisation codes, or a legend essential for image interpretation. This capability is envisaged to find application in image tagging, copyright enforcement, counterfeit protection, and controlled access. Two methods of implementation are discussed. The first is based on bit plane manipulation of the LSB, which offers easy and rapid decoding. The second method utilises linear addition of the water mark to the image data, and is more difficult to decode, offering inherent security. This linearity property also allows some image processing, such as averaging, to take place on the image, without corrupting the water mark beyond recovery. Either method is potentially compatible with JPEG and MPEG processing.
7AAAD725	In this paper, we present two watermarking approaches that are robust to geometric distortions. The first approach is based on image normalization, in which both watermark embedding and extraction are carried out with respect to an image normalized to meet a set of predefined moment criteria. We propose a new normalization procedure, which is invariant to affine transform attacks. The resulting watermarking scheme is suitable for public watermarking applications, where the original image is not available for watermark extraction. The second approach is based on a watermark resynchronization scheme aimed to alleviate the effects of random bending attacks. In this scheme, a deformable mesh is used to correct the distortion caused by the attack. The watermark is then extracted from the corrected image. In contrast to the first scheme, the latter is suitable for private watermarking applications, where the original image is necessary for watermark detection. In both schemes, we employ a direct-sequence code division multiple access approach to embed a multibit watermark in the discrete cosine transform domain of the image. Numerical experiments demonstrate that the proposed watermarking schemes are robust to a wide range of geometric attacks.
7EF782AE	The issue of copyright protection of digital multimedia data has attracted a lot of attention during the last decade. An efficient copyright protection method that has been gaining popularity is watermarking, i.e., the embedding of a signature in a digital document that can be detected only by its rightful owner. Watermarks are usually blindly detected using correlating structures, which would be optimal in the case of Gaussian data. However, in the case of DCT-domain image watermarking, the data is more heavy-tailed and the correlator is clearly suboptimal. Nonlinear receivers have been shown to be particularly well suited for the detection of weak signals in heavy-tailed noise, as they are locally optimal. This motivates the use of the Gaussian-tailed zero-memory nonlinearity, as well as the locally optimal Cauchy nonlinearity for the detection of watermarks in DCT transformed images. We analyze the performance of these schemes theoretically and compare it to that of the traditionally used Gaussian correlator, but also to the recently proposed generalized Gaussian detector, which outperforms the correlator. The theoretical analysis and the actual performance of these systems is assessed through experiments, which verify the theoretical analysis and also justify the use of nonlinear structures for watermark detection. The performance of the correlator and the nonlinear detectors in the presence of quantization is also analyzed, using results from dither theory, and also verified experimentally.
78FFE60B	Recently, several semi-fragile watermarking approaches with the additional capability of image recovery have been proposed. However, the security, robustness, and image recovery aspect of these approaches have certain shortcomings. In this paper, a novel semi-fragile watermarking framework using integer transform based information embedding and extraction is proposed, which allows accurate authentication and recovery of the image. It is based on integer wavelet transform with improved security against collage attack, enhanced robustness, and capability of producing better quality recovered image. Security is enhanced by correlating the to-be-embedded watermark with the approximation subband of wavelet transform. Similarly, no unprotected area is left for attacks on the image, either in spatial or transform domain. Robustness is enhanced by using the idea of embedding in largest coefficient inside a group and correlating it with the quantized version of the mean of the group. In particular, the recovery approach is improved by introducing lossless compression and BCH coding of the integer DCT based low-pass version of the cover image itself. Alteration sensitivity is improved compared to traditional block-based approaches and thus the proposed approach can concisely determine the regions where the integrity verification fails. Experimental comparisons with existing approaches validate the usefulness of the proposed multiple semi-fragile watermarking approach.
7DAFE534	With rapidly growing interest in ways to hide information, a large number of schemes have been proposed for watermarks and other information in digital pictures, video, audio and other multimedia objects. In an attempt to overcome this problem, watermarking has been suggested in the literature as the most effective means for copyright protection and authentication. The main focus of this thesis is the problem of joint watermarking and compression of images due to bandwidth or storage constraints.
56521F97	The paper proposes the use of digital watermark based authentication for intrusion detection in IEC 61850-automated substations. The watermark can be embedded into the Least Significant Bits of the measurements without visible deterioration in precision. When Intelligent Electronics Devices gets measurements, the watermark in the measurement can be retrieved to determine whether it has been attacked and detect malicious intrusion. The proposed approach is appropriate for the time critical and resource constrained applications in substation automation system for its simplicity. Numerical simulation shows that the process latency and error incurred by watermarking is acceptable and will not impact performance of protective function in IEC 61850 automated substations.
75C44EE3	In-network processing presents a critical challenge for data authentication in wireless sensor networks (WSNs). Current schemes relying on message authentication code (MAC) cannot provide natural support for this operation since even a slight modification to the data invalidates the MAC. In this paper, based on digital watermarking, we propose an end-to-end approach for data authentication in WSNs that provides inherent support for in-network processing. In this scheme, authentication information is modulated as watermark and superposed to the sensory data at the sensor nodes. The watermarked data can be aggregated by the intermediate nodes without incurring any en-route checking. Upon reception of the sensory data, possibly distorted by the operations along the route, the data sink is able to authenticate the data by validating the watermark, detecting whether the data has been altered and where it has occurred. In this way, the aggregation-survivable authentication information is only added at the sources and checked by the data sink, without any involvement of intermediate nodes. Furthermore, the simple operation of watermark embedding and complex operation of watermark detection provide a natural solution of function partitioning between the resource limited sensor nodes and resource abundant data sink. The simulation results show that the proposed scheme can successfully authenticate the sensory data with high confidence.
7C2F314E	In this paper two watermarking algorithms for image content authentication with localization and recovery capability of the tampered regions are proposed. In both algorithms, a halftone version of the original gray-scale image is used as an approximated version of the host image (image digest) which is then embedded as a watermark sequence into given transform domains of the host image. In the first algorithm, the Integer Wavelet Transform (IWT) is used for watermark embedding which is denominated WIA-IWT (Watermarking-based Image Authentication using IWT), while in the second one, the Discrete Cosine Transform (DCT) domain is used for this purpose, we call this algorithm WIA-DCT (Watermarking-based Image Authentication using DCT). In the authentication stage the tampered regions are detected using the Structural Similarity index (SSIM) criterion, which are then recovered using the extracted halftone image. In the recovery stage, a Multilayer Perceptron (MLP) neural network is used to carry out an inverse halftoning process to improve the recovered image quality. The experimental results demonstrate the robustness of both algorithms against content preserved modifications, such as JPEG compression, as well as an effective authentication and recovery capability. Also the proposed algorithms are compared with some previously proposed content authentication algorithms with recovery capability to show the better performance of the proposed algorithms.
76FB3E9A	We present an innovative scheme of blindly extracting message bits when a watermarked image is distorted. In this scheme, we have exploited the capabilities of machine learning (ML) approaches for nonlinearly classifying the embedded bits. The proposed technique adaptively modifies the decoding strategy in view of the anticipated attack. The extraction of bits is considered as a binary classification problem. Conventionally, a hard decoder is used with the assumption that the underlying distribution of the discrete cosine transform coefficients do not change appreciably. However, in case of attacks related to real world applications of watermarking, such as JPEG compression in case of shared medical image warehouses, these coefficients are heavily altered. The sufficient statistics corresponding to the maximum likelihood based decoding process, which are considered as features in the proposed scheme, overlap at the receiving end, and a simple hard decoder fails to classify them properly. In contrast, our proposed ML decoding model has attained highest accuracy on the test data. Experimental results show that through its training phase, our proposed decoding scheme is able to cope with the alterations in features introduced by a new attack. Consequently, it achieves promising improvement in terms of bit correct ratio in comparison to the existing decoding scheme.
7883FD41	Compared with Generalized Gaussian distribution (GGD), Cauchy distribution is superior to describe the statistical distribution of the Intra-coded DCT coefficients in H.264/AVC For the bipolar additive watermark in H.264/AVC video stream, a Cauchy distribution based detection algorithm is proposed by ternary hypothesis testing. Experimental results show that the proposed approach can achieve more than 80% on average for the accuracy of watermark detection.
7CAC2376	Watermarking techniques are applied to digital media to protect their integrity and copyright. The embedding of a watermark, however, often distorts the quality of the protected image. This may be intolerable since the protected media is for preserving artistic and valuable images. Hence, engineers have proposed removable solutions permitting authorized users to restore watermarked images to unmarked images with satisfactory quality. Unfortunately, these mechanisms cannot resist signal processing attacks to protect the ownership. In this article, we propose a novel watermarking mechanism by utilizing pair-difference correlations upon subsampling and the technique of JND. This new approach can guarantee the robust essentials of watermarking schemes. Experimental results reveal that the new method outperforms others in terms of restored image quality. More specifically, this novel approach can resist various attacks to which related works are vulnerable.
82802DDF	A content authentication and tamper recovery scheme for digital speech signal is proposed. In this paper, a new compression method for speech signal based on discrete cosine transform is discussed, and the compressed signals obtained are used to tamper recovery. One block-based large capacity embedding method is explored, which is used for embedding the compressed signals. For the scheme proposed, watermark is generated by frame number and compressed signal. If watermarked speech is attacked, the attacked frames can be located by frame number, and reconstructed by using the compressed signal. Theoretical analysis and experimental results demonstrate that the scheme not only improves the security of watermark system, but also can locate the attacked frames precisely and reconstruct the attacked frames.
7D49B3E7	The widespread use of digital multimedia data has increased the need for effective means of copyright protection. Watermarking has attracted much attention, as it allows the embedding of a signature in a digital document in an imperceptible manner. In practice, watermarking is subject to various attacks, intentional or unintentional, which degrade the embedded information, rendering it more difficult to detect. A very common, but not malicious, attack is quantization, which is unavoidable for the compression and transmission of digital data. The effect of quantization attacks on the nearly optimal Cauchy watermark detector is examined in this paper. The quantization effects on this detection scheme are examined theoretically, by treating the watermark as a dither signal. The theory of dithered quantizers is used in order to correctly analyze the effect of quantization attacks on the Cauchy watermark detector. The theoretical results are verified by experiments that demonstrate the quantization effects on the detection and error probabilities of the Cauchy detection scheme.
6C8550AF	For verification and authentication of the video material and recovery of the original video several security mechanisms are required. The security techniques to realize this solution are introduced in: The verification of the integrity is verified by hash functions.Authenticity is verified by digital signatures using asymmetric cryptography and hash functions. The introduced scheme from [1] uses RSA signatures. The private key of the digital signature mechanism is used to sign the data and the corresponding public key is used for verification of the encrypted data. If the data can be verified the corresponding private key was used for the digital signature generation and the data seems to be authentic as well integer.Furthermore the original content can be reproduced by inverting of the watermark with the well know techniques of Fridrich et al. [2]. Additional secret key cryptography (symmetric crypt function) protects the reproduction. The invertibility is necessary, because we use a digital watermark to embed the authentication message and signature into the media itself. Digital watermark, in this application fragile watermark, changes the data and the original data cannot be reconstructing. To invert the data, the watermark must be removed and the original data reconstructed. The watermark embeds the information into a non visual or acoustical channel of the data after the original data of the channel were compressed and encrypted. The compression realizes the new space for the watermark consisting of the encrypted selected data and security information.
7E430A10	We have entered an era where inexpensive and readily-available equipment can produce perfect copies of digital multimedia materials, such as CD-quality audio, publication-quality images, or digital video. In this environment, it has become easier for malicious parties to make salable copies of copyrighted content without compensation to the content owner. Many media content owners are concerned about the potential loss of revenue from multimedia piracy, especially when the content will be exposed to the Internet. Digital watermarking is seen by many as a potential solution to this problem. Many different watermarking schemes have been proposed. Often, however there is little discussion of how effective a proposed watermarking technique may be at solving a particular problem. We describe a number of proposed image-watermarking application scenarios and form a small number of watermark-application categories. Then, with these applications in mind, we discuss the desired technical properties of watermarks for each category. Finally we discuss some watermarking techniques developed by the authors, in light of the desired properties.
7DB1BDB2	Watermarks allow embedded signals to be extracted from audio and video content for a variety of purposes. One application is for copyright control, where it is envisaged that digital video recorders will not permit the recording of content that is watermarked as "never copy". In such a scenario, it is important that the watermark survive both normal signal transformations and attempts to remove the watermark so that an illegal copy can be made. We discuss to what extent a watermark can be resistant to tampering and describe a variety of possible attacks.
7D3761ED	The watermarking of digital images, audio, video, and multimedia products in general has been proposed for resolving copyright ownership and verifying originality of content. This paper studies the contribution of watermarking for developing protection schemes. A general watermarking framework (GWF) is studied and the fundamental demands are listed. The watermarking algorithms, namely watermark generation, embedding, and detection, are analyzed and necessary conditions for a reliable and efficient protection are stated. Although the GWF satisfies the majority of requirements for copyright protection and content verification, there are unsolved problems inside a pure watermarking framework. Particular solutions, based on product registration and related network services, are suggested to overcome such problems.
7FA69773	This paper proposes a novel watermarking scheme with flexible self-recovery quality. The embedded watermark data for content recovery are calculated from the original discrete cosine transform (DCT) coefficients of host image and do not contain any additional redundancy. When a part of a watermarked image is tampered, the watermark data in the area without any modification still can be extracted. If the amount of extracted data is large, we can reconstruct the original coefficients in the tampered area according to the constraints given by the extracted data. Otherwise, we may employ a compressive sensing technique to retrieve the coefficients by exploiting the sparseness in the DCT domain. This way, all the extracted watermark data contribute to the content recovery. The smaller the tampered area, the more available watermark data will result in a better quality of recovered content. It is also shown that the proposed scheme outperforms previous techniques in general.
7A5CF78E	Many digital watermarking algorithms are proposed in the literature. Broadly these watermarking algorithms can be classified into two main categories. The first category of algorithms uses a pseudo random Gaussian sequence (PRGS) watermark whereas the second category of algorithms uses a binary logo as a watermark. The main advantage of PRGS based watermarking scheme is its ability to detect the presence of watermark without manual intervention. However the main drawback is calculating reliable threshold value. In the similar manner the main advantage of binary logo watermark is that there is no need to calculate threshold value but requires manual intervention to detect the presence of watermark. The advantage and disadvantage of either approach is quite clear hence it would be a good idea to design a watermarking scheme which inherits the advantages from both these approaches. In this paper we present one such approach which is termed as bar-code watermarking. The proposed scheme offers objective as well as subjective detection. A PRGS sequence watermark is represented as a bar-code on a binary logo and embedded in the host image. Watermark detection can be either done subjectively or objectively
77B4C1C7	The work presented here deals with watermarking algorithms. The goal is to show how the Human Visual System (H.V.S) properties can be taken into account in the conception of such algorithms. The construction of the watermarking algorithm presented in this paper needs three steps. In the first one the selection of auspicious sites for the watermark embedding is described. The selection exploits a multi-channel model of the Human Visual System which decomposes the visual input into seventeen perceptual components. Medium and high frequencies are then selected to generate a sites map. This latter is improved by considering some high level uniform areas. The second step deals with the choice of the strength to apply to the selected sites. The strength is determined by considering the H.V.S. sensitivity to the local band limited contrast. In the third step, examples of spatial watermarking embedding and extraction are given. The same perceptual mask has been successfully used in other studies. The watermark results from a binary pseudo-random sequence, of length 64, which is circularly shifted so as to occupy all the sites mentioned above. The watermark extraction exploits the detection theory and requires both the perceptual mask and the original watermark. The extracted watermark is then compared to the original and a normalized correlation coefficient is computed. This coefficient value allows the detection of the copyright.
7FB6F52A	Robustness is one of the crucial issues in digital watermarking. Especially the robustness against geometric distortion and JPEG compression at the same time remains challenging. In this paper, a locally linear embedding (LLE) based watermarking algorithm that is robust against affine transformation is proposed. This algorithm improves the robustness via the intrinsic robustness of the LLE. A random generated watermark is embedded in the coefficients of reconstruction weights of the locally linear embedding. In watermark extraction, the watermark can be extracted almost the same process as the watermark embedding. Experimental results have demonstrated that the proposed watermarking scheme is more robust than other watermarking algorithms reported in the literature. Specifically, it is robust against almost all affine transform related testing functions in StirMark 3.1. While the approach is presented for gray-level images, it can be applied to color images and video sequences.
7DAEB614	Traditional intrusion detection systems (IDS) detect attacks by comparing current behavior to signatures of known attacks. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. Behavior that deviates from the learned normal model signals possible novel attacks. Our IDS is unique in two respects. First, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. This prevents alarm floods. Second, the IDS learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. On the 1999 DARPA IDS evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (IP addresses and ports, as modeled by most other systems) and protocol anomalies. Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants, which implies that they could be combined to increase coverage.
7566A357	As the information technology grows interests in the intrusion detection system (IDS), which detects unauthorized usage, misuse by a local user and modification of important data, has been raised. In the field of anomaly-based IDS several data mining techniques such as hidden Markov model (HMM), artificial neural network, statistical techniques and expert systems are used to model network packets, system call audit data, etc. However, there are undetectable intrusion types for each measure and modeling method because each intrusion type makes anomalies at individual measure. To overcome this drawback of single-measure anomaly detector, this paper proposes a multiple-measure intrusion detection method. We measure normal behavior by systems calls, resource usage and file access events and build up profiles for normal behavior with hidden Markov model, statistical method and rule-base method, which are integrated with a rule-based approach. Experimental results with real data clearly demonstrate the effectiveness of the proposed method that has significantly low false-positive error rate against various types of intrusion.
7FACBDE4	The network intrusion detection (NIDS) is faced with the question to detect many kinds of intrusion. In order to detect the complex attack, Network Intrusion detection system need to analysis massive data captured form different network safety equipments. So a new multi relational mining algorithm MRA2 is proposed. MRA2 depend on the association rules mining technology and the probability function dependency method which is proposed through extending the theory of function dependency. MRA2 is able to synthesize the various datalog resources to detect intrusion effectively and reappear to the complex network attack scenario.
7E7969F7	A new approach to representing computer penetrations is introduced called penetration state transition analysis. This approach models penetrations as a series of state transitions described in terms of signature actions and state descriptions. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule-based expert system for detecting penetrations, referred to as STAT.
7DCBAD4B	The paper focuses on intrusion detection and countermeasures with respect to widely-used operating systems and networks. The design and architecture of an intrusion detection system built from distributed agents is proposed to implement an intelligent system on which data mining can be performed to provide global, temporal views of an entire networked system. A starting point for agent intelligence in the system is the research into the use of machine learning over system call traces from the privileged sendmail program on UNIX. The authors use a rule learning algorithm to classify the system call traces for intrusion detection purposes and show the results.
5C2490E0	A modified RBF (radial basis function)-based neural network is proposed for network anomaly detection. Special attention is given to the determination of the parameters of the hidden layer. We propose a novel grid-based approach to compress and cluster the training data. The number, center and radii of the RBFs are determined according to the clustering result. At the detecting stage, we expand each input node with a sigmoid function to meet the type of input data. Experimental result on KDD 99 intrusion detection datasets shows that our RBF based IDS has high detection rate while maintaining a low false positive rate. It also shows the remarkable ability of our IDS to detect new type of attacks.
7E0DD16C	Classification of intrusion attacks and normal network traffic is a challenging and critical problem in pattern recognition and network security. In this paper, we present a novel intrusion detection approach to extract both accurate and interpretable fuzzy IF–THEN rules from network traffic data for classification. The proposed fuzzy rule-based system is evolved from an agent-based evolutionary framework and multi-objective optimization. In addition, the proposed system can also act as a genetic feature selection wrapper to search for an optimal feature subset for dimensionality reduction. To evaluate the classification and feature selection performance of our approach, it is compared with some well-known classifiers as well as feature selection filters and wrappers. The extensive experimental results on the KDD-Cup99 intrusion detection benchmark data set demonstrate that the proposed approach produces interpretable fuzzy systems, and outperforms other classifiers and wrappers by providing the highest detection accuracy for intrusion attacks and low false alarm rate for normal network traffic with minimized number of features.
75F2DC32	This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers’ (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks.
7D83A406	Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection.
7B75FDC9	Traffic anomalies and attacks are commonplace in today's networks, and identifying them rapidly and accurately is critical for large network operators. Intrusion detection systems are an important component of defensive measures protecting computer systems and networks from abuse.For an intrusion detection system, it is important to detect previously known attacks with high accuracy. However, detecting previously unseen attacks is equally important in order to minimize the losses as a result of a successful intrusion. It is also equally important to detect attacks at an early stage in order to minimize their impact. To address these challenges, this paper proposes to improve the efficiency of the network intrusion detection process by including an Event Calculus based specification to detect the registered and expected behaviour of the whole network.
7ED7AE7B	As complete prevention of computer attacks is not possible, intrusion detection systems (IDSs) play a very important role in minimizing the damage caused by different computer attacks. There are two intrusion detection methods: namely misuse- and anomaly-based. A collaborative, intelligent intrusion detection system (CIIDS) is proposed to include both methods, since it is concluded from recent research that the performance of an individual detection engine is rarely satisfactory. In particular, two main challenges in current collaborative intrusion detection systems (CIDSs) research are highlighted and reviewed: CIDSs system architectures and alert correlation algorithms. Different CIDSs system, architectures are explained and compared. The use of CIDSs together with other multiple security systems raise certain issues and challenges in, alert correlation. Several different techniques for alert correlation are discussed. The focus will be on correlation of CIIDS alerts. Computational, Intelligence approaches, together with their applications on IDSs, are reviewed. Methods in soft computing collectively provide understandable, and autonomous solutions to IDS problems. At the end of the review, the paper suggests fuzzy logic, soft computing and other AI techniques, to be exploited to reduce the rate of false alarms while keeping the detection rate high. In conclusion, the paper highlights opportunities for an integrated solution to large-scale CIIDS.
7BAE61AD	There are some issues for the shuffle network intrusion detection, such as high loss detection rates and time-consuming procedures. This paper proposes a shuffle network intrusion detection method fusing the misuse behavior analysis and analyzes the network misuse behavior procedures. According to the damaged data flow balance features by network misuse behavior, the paper applies the hypothesis test in probability theory to evaluate whether the confidence interval excesses 0. If the confidence interval does not contain zero, it indicates the presence of feed-forward network intrusion; otherwise, there is no feed-forward network intrusion. The experimental results show that this method can effectively solve the multi-packet collaborative intrusion problems. Compared to traditional methods, the test speed and accuracy of the method is significantly improved. 
788D804C	Security has become an important issue for networks. Intrusion detection technology is an effective approach in dealing with the problems of network security. In this paper, we present an intrusion detection model based on hybrid fuzzy logic and neural network. The key idea is to take advantage of different classification abilities of fuzzy clustering and neural network for intrusion detection system. The new model has ability to recognize an attack, to differentiate one attack from another (i.e. classifying attacks), and the most important, to detect new attacks with high detection rate and low false negative. Training and testing data were obtained from the Defense Advanced Research Projects Agency intrusion detection evaluation data set. 
75F80375	In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal compression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average. 
772CD65D	 Eight sites participated in the second DARPA off-line intrusion de-tection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False alarm rates were low (less than 10 per day). Best de-tection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. Best overall performance would have been provided by a com-bined system that used both host-based and network-based intrusion detection. De-tection accuracy was poor for previously unseen new, stealthy, and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because protocols and TCP services were not analyzed at all or to the depth required, because signatures for old attacks did not gen-eralize to new attacks, and because auditing was not available on all hosts. Promising capabilities were demonstrated by host-based systems, by anomaly detection systems, and by a system that performs forensic analysis on file system data.
7E496032	Presented in this paper is the way of getting large signal parameters of MESFET model by the use of small signal S-parameters. Small-signal S-parameters allow expressing to nonlinearity of model, which are used in account of large-signal MESFET model.
5BCE3A64	As network attacks have increased in number and severity over the past few years, intrusion detection system (IDS) is increasingly becoming a critical component to secure the network. Due to large volumes of security audit data as well as complex and dynamic properties of intrusion behaviors, optimizing performance of IDS becomes an important open problem that is receiving more and more attention from the research community. The uncertainty to explore if certain algorithms perform better for certain attack classes constitutes the motivation for the reported herein. In this paper, we evaluate performance of a comprehensive set of classifier algorithms using KDD99 dataset. Based on evaluation results, best algorithms for each attack category is chosen and two classifier algorithm selection models are proposed. The simulation result comparison indicates that noticeable performance improvement and real-time intrusion detection can be achieved as we apply the proposed models to detect different kinds of network attacks.
7F88C3B5	Intrusion detection based upon computational intelligence is currently attracting considerable interest from the research community. Characteristics of computational intelligence (CI) systems, such as adaptation, fault tolerance, high computational speed and error resilience in the face of noisy information, fit the requirements of building a good intrusion detection model. Here we want to provide an overview of the research progress in applying CI methods to the problem of intrusion detection. The scope of this review will encompass core methods of CI, including artificial neural networks, fuzzy systems, evolutionary computation, artificial immune systems, swarm intelligence, and soft computing. The research contributions in each field are systematically summarized and compared, allowing us to clearly define existing research challenges, and to highlight promising new research directions. The findings of this review should provide useful insights into the current IDS literature and be a good source for anyone who is interested in the application of CI approaches to IDSs or related fields.
7EF318D2	Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. We study one such observable-sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences; comparison of relative frequencies of different sequences; a rule induction technique; and hidden Markov models (HMMs). We discuss the factors affecting the performance of each method and conclude that for this particular problem, weaker methods than HMMs are likely sufficient.
77CCBE06	In this paper, a four-angle-star based visualized feature generation approach, FASVFG, is proposed to evaluate the distance between samples in a 5-class classification problem. Based on the four angle star image, numerical features are generated for network visit data from KDDcup99, and an efficient intrusion detection system with less features is proposed. The FASVFG-based classifier achieves a high generalization accuracy of 94.3555% in validation experiment, and the average Mathews correlation coefficient reaches 0.8858.
7D9C83BB	The paper presents a new approach to representing and detecting computer penetrations in real time. The approach, called state transition analysis, models penetrations as a series of state changes that lead from an initial secure state to a target compromised state. State transition diagrams, the graphical representation of penetrations, identify precisely the requirements for and the compromise of a penetration and present only the critical events that must occur for the successful completion of the penetration. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule based expert system for detecting penetrations, called the state transition analysis tool (STAT). The design and implementation of a Unix specific prototype of this expert system, called USTAT, is also presented. This prototype provides a further illustration of the overall design and functionality of this intrusion detection approach. Lastly, STAT is compared to the functionality of comparable intrusion detection tools.
7FFD6B99	Intrusion detection systems monitor system activities to identify unauthorized use, misuse, or abuse. IDSs offer a defense when your system's vulnerabilities are exploited and do so without requiring you to replace expensive equipment. The steady growth in research on intrusion detection systems has created a demand for tools and methods to test their effectiveness. The authors have developed a software platform that both simulates intrusions and supports their systematic methodology for IDS testing.
7A1333F9	Wireless sensor networks (WSNs) are vulnerable to a variety of malicious attacks, especially the packet dropping attack, making security an important research field. Since prevention based techniques are less helpful for guarding against inside attacks, intrusion detection (ID) techniques are indispensable to provide advanced protection. This paper proposes an innovative cellular ID framework for packet dropping attack, which takes the deployment of passive listening nodes into consideration. Performance evaluation made in VisualSense demonstrates its high detection accuracy.
5B7700D1	Wireless ad hoc network is becoming a new research frontier, in which security is an important issue. Usually some nodes act maliciously and they are able to do different kinds of Denial of Service (Dos). Because of the limited resource, intrusion detection system (IDS) runs all the time to detect intrusion of the attacker which is a costly overhead. We use game theory to model the interactions between the intrusion detection system and the attacker, and a realistic model is given by using Bayesian game. We solve the game by finding the Bayesian Nash equilibrium. The results of our analysis show that the IDS could work intermittently without compromising its effectiveness. At the end of this paper, we provide an experiment to verify the rationality and effectiveness of the proposed model.
79406BD7	Intrusion Detection Systems (IDS) have nowadays become a necessary component of almost every security infrastructure. So far, many different approaches have been followed in order to increase the efficiency of IDS. Swarm Intelligence (SI), a relatively new bio-inspired family of methods, seeks inspiration in the behavior of swarms of insects or other animals. After applied in other fields with success SI started to gather the interest of researchers working in the field of intrusion detection. In this paper we explore the reasons that led to the application of SI in intrusion detection, and present SI methods that have been used for constructing IDS. A major contribution of this work is also a detailed comparison of several SI-based IDS in terms of efficiency. This gives a clear idea of which solution is more appropriate for each particular case.
78FC71D4	With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.
7054D090	There is often the need to update an installed intrusion detection system (IDS) due to new attack methods or upgraded computing environments. Since many current IDSs are constructed by manual encoding of expert knowledge, changes to IDSs are expensive and slow. We describe a data mining framework for adaptively building Intrusion Detection (ID) models. The central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session, and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. These rules can then be used for misuse detection and anomaly detection. New detection models are incorporated into an existing IDS through a meta-learning (or co-operative learning) process, which produces a meta detection model that combines evidence from multiple models. We discuss the strengths of our data mining programs, namely, classification, meta-learning, association rules, and frequent episodes. We report on the results of applying these programs to the extensively gathered network audit data for the 1998 DARPA Intrusion Detection Evaluation Program.
7A1919E6	Many different demands can be made of intrusion detection systems. An important requirement is that it be effective i.e. that it should detect a substantial percentage of intrusions into the supervised system, while still keeping the false alarm rate at an acceptable level.This paper aims to demonstrate that, for a reasonable set of assumptions, the false alarm rate is the limiting factor for the performance of an intrusion detection system. This is due to the base-rate fallacy phenomenon, that in order to achieve substantial values of the Bayesian detection rate, P(Intrusion|Alarm), we have to achieve—a perhaps unattainably low—false alarm rate.A selection of reports of intrusion detection performance are reviewed, and the conclusion is reached that there are indications that at least some types of intrusion detection have far to go before they can attain such low false alarm rates.
028BE359	Intrusion detection corresponds to a suite of techniques that are used to identify attacks against computers and network infrastructures. Anomaly detection is a key element of intrusion detection in which perturbations of normal behavior suggest the presence of intentionally or unintentionally induced attacks, faults, defects, etc. This paper focuses on a detailed comparative study of several anomaly detection schemes for identifying different network intrusions. Several existing supervised and unsupervised anomaly detection schemes and their variations are evaluated on the DARPA 1998 data set of network connections [9] as well as on real network data using existing standard evaluation techniques as well as using several specific metrics that are appropriate when detecting attacks that involve a large number of connections. Our experimental results indicate that some anomaly detection schemes appear very promising when detecting novel intrusions in both DARPA’98 data and real network data.
61E164F2	Misuse detection is the process of attempting to identify instances of network attacks by comparing current activity against the expected actions of an intruder. Most current approaches to misuse detection involve the use of rule-based expert systems to identify indications of known attacks. However, these techniques are less successful in identifying attacks which vary from expected patterns. Artificial neural networks provide the potential to identify and classify network activity based on limited, incomplete, and nonlinear data sources. We present an approach to the process of misuse detection that utilizes the analytical strengths of neural networks, and we provide the results from our preliminary analysis of this approach.
78EEC707	A simple CAD model is proposed for the short-channel enhancement-mode MOSFET. The conventional use of drain bias modulation of channel length to describe saturation characteristics has been discarded and replaced by drain bias enhancement of channel velocity. The model possesses continuity of current, transconductance and output conductance throughout the triode, and saturation ranges of operation. It has been tested against experimental transistors and against two-dimensional numerically simulated transistors, and has given satisfactory results in all cases. The model is based on good physics, is easy to understand, is straightforward to use, and is computationally efficient. 
7518A3AF	Today's computer systems are vulnerable both to abuse by insiders and to penetration by outsiders, as evidenced by the growing number of incidents reported in the press. To close all security loopholes from today's systems is infeasible, and no combination of technologies can prevent legitimate users from abusing their authority in a system; thus auditing is viewed as the last line of defense.Over the past several years, the computer security community has been developing automated tools to analyze computer system audit data for suspicious user behavior. This paper describes the use of such tools for detecting computer system intrusion and describes further technologies that may be of use for intrusion detection in the future.
8042453A	Intrusion detection systems (IDSs) attempt to identify unauthorized use, misuse, and abuse of computer systems. In response to the growth in the use and development of IDSs, the authors have developed a methodology for testing IDSs. The methodology consists of techniques from the field of software testing which they have adapted for the specific purpose of testing IDSs. They identify a set of general IDS performance objectives which is the basis for the methodology. They present the details of the methodology, including strategies for test-case selection and specific testing procedures. They include quantitative results from testing experiments on the Network Security Monitor (NSM), an IDS developed at UC Davis. They present an overview of the software platform that has been used to create user-simulation scripts for testing experiments. The platform consists of the UNIX tool expect and enhancements that they have developed, including mechanisms for concurrent scripts and a record-and-replay feature. They also provide background information on intrusions and IDSs to motivate their work.
7C8F7EE3	In order to solve the problems of high false alarm rate and fail rate in intrusion detection system of Computer Integrated Process System (CIPS) network, this paper takes advantage that Genetic Algorithm (GA) possesses overall optimization seeking ability and neural network has formidable approaching ability to the non-linear mapping to propose an intrusion detection model based on Genetic Algorithm Neural Network (GANN) with self-learning and adaptive capacity, which includes data collection module, data preprocessing module, neural network analysis module and intrusion alarm module. To overcome the shortcomings that GA is easy to fall into the extreme value and searches slowly, it improves the adjusting method of GANN fitness value and optimizes the parameter settings of GA. The improved GA is used to optimize BP neural network. Simulation results show that the model makes the detection rate of the system enhance to 97.11%.
811F9DC4	 The author presents the design and implementation of a real-time intrusion detection tool, called USTAT, a state transition analysis tool for UNIX. This is a UNIX-specific implementation of a generic design developed by A. Porras and R.A. Kemmerer (1992) as STAT, a state transition analysis tool. State transition analysis is a new approach to representing computer penetrations. In STAT, a penetration is identified as a sequence of state changes that take the computer system from some initial state to a target compromised state. The development of the first USTAT prototype, which is for SunOS 4.1.1, is discussed. USTAT makes use of the audit trails that are collected by the C2 basic security module of SunOS, and it keeps track of only those critical actions that must occur for the successful completion of the penetration. This approach differs from other rule-based penetration identification tools that pattern match sequences of audit records.
03F42860	In the Clinical Decision Support System (CDSS), over-fitting phenomenon may appear when decision tree algorithm was used. For this problem, this paper will make use of the Rough Set theory to the training set for attribute reduction, the decision tree built by using the decision tree algorithm was used to predict the test data. In this paper, 46 copies of coronary heart disease clinical data were used to test the improved algorithm. Comparing the accuracy of the algorithm and the improved algorithm, we can know that, the improved algorithm has a better recognition rate for the diagnosis of coronary heart disease, and effectively solves the over-fitting phenomenon in the Decision Tree Algorithm.
77531D91	Identify individual and environmental variables associated with caregiver stability and instability for children in diverse permanent placement types (i.e., reunification, adoption, and long-term foster care/guardianship with relatives or non-relatives), following 5 or more months in out-of-home care prior to age 4 due to substantiated maltreatment.Participants were 285 children from the Southwestern site of Longitudinal Studies of Child Abuse and Neglect (LONGSCAN). Caregiver instability was defined as a change in primary caregiver between ages 6 and 8 years. Classification and regression tree (CART) analysis was used to identify the strongest predictors of instability from multiple variables assessed at age 6 with caregiver and child reports within the domains of neighborhood/community characteristics, caregiving environment, caregiver characteristics, and child characteristics.One out of 7, or 14% of the 285 children experienced caregiver instability in their permanent placement between ages 6 and 8. The strongest predictor of stability was whether the child had been placed in adoptive care. However, for children who were not adopted, a number of contextual factors (e.g., father involvement, expressiveness within the family) and child characteristics (e.g., intellectual functioning, externalizing problem behaviors) predicted stability and instability of permanent placements.Current findings suggest that a number of factors should be considered, in addition to placement type, if we are to understand what predicts caregiver stability and find stable permanent placements for children who have entered foster care. These factors include involvement of a father figure, family functioning, and child functioning.Adoption was supported as a desired permanent placement in terms of stability, but results suggest that other placement types can also lead to stability.In fact, with attention to providing biological parents, relative, and non-relative caregivers with support and resources the likelihood that a child will have a stable caregiver may be increased.
80DFDDF3	A common approach to split selection in classification trees is to search through all possible splits generated by predictor variables. A splitting criterion is then used to evaluate those splits and the one with the largest criterion value is usually chosen to actually channel samples into corresponding subnodes. However, this greedy method is biased in variable selection when the numbers of the available split points for each variable are different. Such result may thus hamper the intuitively appealing nature of classification trees. The problem of the split selection bias for two-class tasks with numerical predictors is examined. The statistical explanation of its existence is given and a solution based on the P-values is provided, when the Pearson chi-square statistic is used as the splitting criterion.
7E8F41FE	A new model for supervised classification based on probabilistic decision graphs is introduced. A probabilistic decision graph (PDG) is a graphical model that efficiently captures certain context specific independencies that are not easily represented by other graphical models traditionally used for classification, such as the Naïve Bayes (NB) or Classification Trees (CT). This means that the PDG model can capture some distributions using fewer parameters than classical models. Two approaches for constructing a PDG for classification are proposed. The first is to directly construct the model from a dataset of labelled data, while the second is to transform a previously obtained Bayesian classifier into a PDG model that can then be refined. These two approaches are compared with a wide range of classical approaches to the supervised classification problem on a number of both real world databases and artificially generated data.
7DD171A0	Classification trees are a popular tool in applied statistics because their heuristic search approach based on impurity reduction is easy to understand and the interpretation of the output is straightforward. However, all standard algorithms suffer from a major problem: variable selection based on standard impurity measures as the Gini Index is biased. The bias is such that, e.g., splitting variables with a high amount of missing values—even if missing completely at random (MCAR)—are artificially preferred. A new split selection criterion that avoids variable selection bias is introduced. The exact distribution of the maximally selected Gini gain is derived by means of a combinatorial approach and the resulting -value is suggested as an unbiased split selection criterion in recursive partitioning algorithms. The efficiency of the method is demonstrated in simulation studies and a real data study from veterinary gynecology in the context of binary classification and continuous predictor variables with different numbers of missing values. The proposed method is extendible to categorical and ordinal predictor variables and to other split selection criteria such as the cross-entropy.
79406A62	In many medical applications, longitudinal data sets are available. Longitudinal data, as well as observations from paired organs, show a dependency structure which should be respected in the evaluation. Adler et al. (Comput Stat Data Anal 53(3):718–729, 2009) proposed various bootstrapping strategies for ensemble methods based on classification trees for two measurements of paired organs. These strategies have shown to improve the classification performance compared to the traditional approach, where only one observation per subject is used. We extend the methodology to the situation, where an arbitrary number of observations per individual are available and investigate the performance of the proposed methods with bagged classification trees (bagging) and random forests in the situation of longitudinal data. Moreover, we adapt the estimation of classification performance criteria to repeated measurements data. The clinical data set consists of morphological examinations of both eyes of glaucoma patients and healthy controls over a time period of up to 7 years. The performance of our modified classifiers is evaluated by a subject-based leave-one-out bootstrap ROC analysis. Simulation results and results for the glaucoma data set demonstrate that our proposal is an improvement of adhoc strategies and of the use all measurements of each subject or block strategy.
589C9711	This paper describes the evaluation of a WSD method withinSENSEVAL. This method is based on Semantic Classification Trees (SCTs)and short context dependencies between nouns and verbs. The trainingprocedure creates a binary tree for each word to be disambiguated. SCTsare easy to implement and yield some promising results. The integrationof linguistic knowledge could lead to substantial improvement.
6A1B7A68	Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.
7AF08408	In recent years, classification learning for data streams has become an important and active research topic. A major challenge posed by data streams is that their underlying concepts can change over time, which requires current classifiers to be revised accordingly and timely. To detect concept change, a common methodology is to observe the online classification accuracy. If accuracy drops below some threshold value, a concept change is deemed to have taken place. An implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as a symptom of concept change. Unfortunately however, this assumption is often violated in the real world where data streams carry noise that can also introduce a significant reduction in classification accuracy. To compound this problem, traditional noise cleansing methods are incompetent for data streams. Those methods normally need to scan data multiple times whereas learning for data streams can only afford one-pass scan because of data’s high speed and huge volume. Another open problem in data stream classification is how to deal with missing values. When new instances containing missing values arrive, how a learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored. To solve these problems, this paper proposes a novel classification algorithm, flexible decision tree (FlexDT), which extends fuzzy logic to data stream classification. The advantages are three-fold. First, FlexDT offers a flexible structure to effectively and efficiently handle concept change. Second, FlexDT is robust to noise. Hence it can prevent noise from interfering with classification accuracy, and accuracy drop can be safely attributed to concept change. Third, it deals with missing values in an elegant way. Extensive evaluations are conducted to compare FlexDT with representative existing data stream classification algorithms using a large suite of data streams and various statistical tests. Experimental results suggest that FlexDT offers a significant benefit to data stream classification in real-world scenarios where concept change, noise and missing values coexist.
560B2421	Knowledge of the composition and areal distribution of aquatic vegetation types, as well as their seasonal and interannual variations, is crucial for managing and maintaining the balance of lake ecosystems. In this study, a series of remotely sensed images with a resolution of 30 m (HJ-CCD and Landsat TM) were collected and used to map the distribution of aquatic vegetation types in Taihu Lake, China. Seasonal and interannual dynamics of aquatic vegetation types were explored and analyzed. The distribution areas of Type I (emergent, floating-leaved and floating vegetation) and Type II (submerged vegetation) were used to model their growing season phenology by double logistic functions. The resulting double logistic models showed, the area of Type I reached its peak in mid-August, and the maximum area for Type II occurred in mid-September. From 1984 to 2013, Type I area increased continuously from 59.75 km2 to 148.00 km2 (R2 = 0.84), whereas the area covered by Type II first increased and then decreased, with a trend conforming to a significant quadratic curve (R2 = 0.83). The eutrophication and stable state of Taihu Lake was assessed using a simple indicator which was expressed as a ratio of Type II area to Type I area. The results showed that the eutrophication in the lake might have been increasing in the area studied since 2000. Additionally, the results showed that air temperature had likely a direct effect on the growth of Type I (R2 = 0.66) and a significant, but delayed, effect on the growth of Type II.
76CDF761	Economic evaluation of a new oil well is important for decision-making in the petroleum industry, and this evaluation is based on a good prediction on a well's production. However, it is difficult to accurately predict a well's production due to the complex subsurface conditions of reservoirs. The industrial standard approach is to use either curve-fitting methods or complex and time-consuming reservoir simulations. In this paper, an enhanced decision tree learning approach called neural-based decision tree (NDT) model is applied in an attempt to investigate its performance in predicting petroleum production. The primary strength of this model is that it can capture dependencies among attributes, and therefore, it is likely to provide an improved or more accurate prediction (Lee and Yen, 2002).This paper presents an application of the NDT model for petroleum prediction. Our models were developed based on the five most significant parameters that affect oil production: permeability, porosity, first shut-in pressure, residual oil and saturation of water. The five parameters were used as input variables, and oil production is the output variable for modeling. Four different models were generated in the modeling process, and each involves a different combination of parameters. First, an overall oil production model is developed using the three geoscience parameters of permeability, porosity and first shut-in pressure. Secondly, two different models, with different input parameters, were developed to predict production in the post-water flooding stage only. The results of the above models indicate that data-driven models may not be effective for classifying the data set. Hence, a trend model was developed in an attempt to improve the effectiveness and accuracy of the predictive model. The result shows that the trend model can provide an improved performance, and its performance is comparable to that of the artificial neural network.
74267BE0	This paper presents a novel decision-tree induction for a multi-objective data set, i.e. a data set with a multi-dimensional class. Inductive decision-tree learning is one of the frequently-used methods for a single-objective data set, i.e. a data set with a single-dimensional class. However, in a real data analysis, we usually have multiple objectives, and a classifier which explains them simultaneously would be useful and would exhibit higher readability. A conventional decision-tree inducer requires transformation of a multi-dimensional class into a single-dimensional class, but such a transformation can considerably worsen both accuracy and readability. In order to circumvent this problem we propose a bloomy decision tree which deals with a multi-dimensional class without such transformations. A bloomy decision tree has a set of split nodes each of which splits examples according to their attribute values, and a set of flower nodes each of which predicts a class dimension of examples. A flower node appears not only at the fringe of a tree but also inside a tree. Our pruning is executed during tree construction, and evaluates each class dimension based on Cramér’s V. The proposed method has been implemented as D3-B (Decision tree in Bloom), and tested with eleven data sets. The experiments showed that D3-B has higher accuracies in nine data sets than C4.5 and tied with it in the other two data sets. In terms of readability, D3-B has a smaller number of split nodes in all data sets, and thus outperforms C4.5.
58772DB1	Numerical data poses a problem to symbolic learning methods, since numerical value ranges inherently need to be partitioned into intervals for representation and handling. An evaluation function is used to approximate the goodness of different partition candidates. Most existing methods for multisplitting on numerical attributes are based on heuristics, because of the apparent efficiency advantages. We characterize a class of well-behaved cumulative evaluation functions for which efficient discovery of the optimal multisplit is possible by dynamic programming. A single pass through the data suffices to evaluate multisplits of all arities. This class contains many important attribute evaluation functions familiar from symbolic machine learning research. Our empirical experiments convey that there is no significant differences in efficiency between the method that produces optimal partitions and those that are based on heuristics. Moreover, we demonstrate that optimal multisplitting can be beneficial in decision tree learning in contrast to using the much applied binarization of numerical attributes or heuristical multisplitting.
7DA028E8	The available e-data throughout the Web are growing at such a high rate that data mining on the web is considered the biggest challenge of information technology. As a result it is crucial to find new and innovative ways for classifying and mining those huge amounts of data. In this paper we present an implementation of a state-of-the-art data mining algorithm on a modern FPGA. This is one of the first approaches utilizing the resources of an FPGA to accelerate certain very CPU intensive data-mining/data classification schemes and our real-world results from actual runs on hardware demonstrate that it is a highly promising one. In particular, our FPGA-based system achieves, depending on the data classified, a speedup from 4x and up to 50x (on average 25x) when compared with a state-of-the art multi-core CPU, including I/O overhead.
757153C9	The basic concepts of a multi-stage classification strategy, the decision tree classifier, are presented. The two main methods to design decision trees are presented and discussed along with some experimental results. An attempt is made to describe an Applicable Logic for the design of decision trees. Advantages and disadvantages of the both design approaches are discussed.
75881784	Given learning samples from a raster data set, spatial decision tree learning aims to find a decision tree classifier that minimizes classification errors as well as salt-and-pepper noise. The problem has important societal applications such as land cover classification for natural resource management. However, the problem is challenging due to the fact that learning samples show spatial autocorrelation in class labels, instead of being independently identically distributed. Related work relies on local tests (i.e., testing feature information of a location) and cannot adequately model the spatial autocorrelation effect, resulting in salt-and-pepper noise. In contrast, we recently proposed a focal-test-based spatial decision tree (FTSDT), in which the tree traversal direction of a sample is based on both local and focal (neighborhood) information. Preliminary results showed that FTSDT reduces classification errors and salt-and-pepper noise. This paper extends our recent work by introducing a new focal test approach with adaptive neighborhoods that avoids over-smoothing in wedge-shaped areas. We also conduct computational refinement on the FTSDT training algorithm by reusing focal values across candidate thresholds. Theoretical analysis shows that the refined training algorithm is correct and more scalable. Experiment results on real world data sets show that new FTSDT with adaptive neighborhoods improves classification accuracy, and that our computational refinement significantly reduces training time.
7E7B8D13	Support vector machines (SVMs) have shown strong generalization ability in a number of application areas, including protein structure prediction. However, the poor comprehensibility hinders the success of the SVM for protein structure prediction. The explanation of how a decision made is important for accepting the machine learning technology, especially for applications such as bioinformatics. The reasonable interpretation is not only useful to guide the "wet experiments," but also the extracted rules are helpful to integrate computational intelligence with symbolic AI systems for advanced deduction. On the other hand, a decision tree has good comprehensibility. In this paper, a novel approach to rule generation for protein secondary structure prediction by integrating merits of both the SVM and decision tree is presented. This approach combines the SVM with decision tree into a new algorithm called SVM_DT, which proceeds in three steps. This algorithm first trains an SVM. Then, a new training set is generated through careful selection from the output of the SVM. Finally, the obtained training set is used to train a decision tree learning system and to extract the corresponding rule sets. The results of the experiments of protein secondary structure prediction on RS126 data set show that the comprehensibility of SVM_DT is much better than that of the SVM. Moreover, the generalization ability of SVM_DT is better than that of C4.5 decision trees and is similar to that of the SVM. Hence, SVM_DT can be used not only for prediction, but also for guiding biological experiments
7FB04BF9	The ideal use of small multilayer nets at the decision nodes of a binary classification tree to extract nonlinear features is proposed. The nets are trained and the tree is grown using a gradient-type learning algorithm in the multiclass case. The method improves on standard classification tree design methods in that it generally produces trees with lower error rates and fewer nodes. It also reduces the problems associated with training large unstructured nets and transfers the problem of selecting the size of the net to the simpler problem of finding a tree of the right size. An efficient tree pruning algorithm is proposed for this purpose. Trees constructed with the method and the CART method are compared on a waveform recognition problem and a handwritten character recognition problem. The approach demonstrates significant decrease in error rate and tree size. It also yields comparable error rates and shorter training times than a large multilayer net trained with backpropagation on the same problems.
7887D583	The classification of large dimensional data sets arising from the merging of remote sensing data with more traditional forms of ancillary data causes a significant computational problem. Decision tree classification is a popular approach to the problem. This type of classifier is characterized by the property that samples are subjected to a sequence of decision rules before they are assigned to a unique class. If a decision tree classifier is well designed, the result in many cases is a classification scheme which is accurate, flexible, and computationally efficient. This correspondence provides an automated technique for effective decision tree design which relies only on a priori statistics. This procedure utilizes canonical transforms and Bayes table look-up decision rules. An optimal design at each node is derived based on the associated decision table. A procedure for computing the global probability of correct classification is also provided. An example is given in which class statistics obtained from an actual Landsat scene are used as input to the program. The resulting decision tree design has an associated probability of correct classification of 0.75 compared to the theoretically optimum 0.79 probability of correct classification associated with a full dimensional Bayes classifier. Recommendations for future research are included.
7C66892E	A critical issue in classification tree design-obtaining right-sized trees, i.e. trees which neither underfit nor overfit the data-is addressed. Instead of stopping rules to halt partitioning, the approach of growing a large tree with pure terminal nodes and selectively pruning it back is used. A new efficient iterative method is proposed to grow and prune classification trees. This method divides the data sample into two subsets and iteratively grows a tree with one subset and prunes it with the other subset, successively interchanging the roles of the two subsets. The convergence and other properties of the algorithm are established. Theoretical and practical considerations suggest that the iterative free growing and pruning algorithm should perform better and require less computation than other widely used tree growing and pruning algorithms. Numerical results on a waveform recognition problem are presented to support this view.
7EAD4110	A survey is presented of current methods for decision tree classifier (DTC) designs and the various existing issues. After considering potential advantages of DTCs over single-state classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. The relation between decision trees and neutral networks (NN) is also discussed.
5FE9C8DA	The design of algorithms that explore multiple representation languages and explore different search spaces has an intuitive appeal. In the context of classification problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based on a combination of attributes. The same applies to model-tree algorithms in regression domains, but using linear models at leaf nodes. In this paper, we study where to use combinations of attributes in decision tree learning. We present an algorithm for multivariate tree learning that combines a univariate decision tree with a discriminant function by means of constructive induction. This algorithm is able to use decision nodes with multivariate tests, and leaf nodes that predict a class using a discriminant function. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree. Functional trees can be seen as a generalization of multivariate trees. Our algorithm was compared against to its components and two simplified versions using 30 benchmark data sets. The experimental evaluation shows that our algorithm has clear advantages with respect to the generalization ability and model sizes at statistically significant confidence levels.
7E4C6FAC	Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.
79BCE1C3	Supervised classification is one of the important tasks in remote sensing image interpretation, in which the image pixels are classified to various predefined land use/land cover classes based on the spectral reflectance values in different bands. In reality some classes may have very close spectral reflectance values that overlap in feature space. This produces spectral confusion among the classes and results in inaccurate classified images. To remove such spectral confusion one requires extra spectral and spatial knowledge. This report presents a decision tree classifier approach to extract knowledge from spatial data in form of classification rules using Gini Index and Shannon Entropy (Shannon and Weaver, 1949) to evaluate splits. This report also features calculation of optimal dataset size required for rule generation, in order to avoid redundant Input/output and processing.
8020CD31	This paper describes research to obtain continental and global scale maps of urban land cover from remotely sensed imagery, specifically utilizing newly available one kilometer data from the MODIS sensor. Defining the extent of urban land is crucial, since knowledge of the size and spatial distribution of cities is important on a number of fronts, from resource management to economic development planning to regional and global climate modeling. The algorithm used for this work is a supervised decision tree classifier, and the technique of boosting is exploited to improve classification accuracy and to provide a means to correct major sources of error using available prior information. First results for North America indicate that the incorporation of. ancillary information successfully improves urban classification results, resolving confusion between the urban and barren classes that normally occurs when only MODIS data is used.
80B879CA	This paper introduces a hybrid learning methodology that integrates genetic algorithms (GAs) and decision tree learning (ID3) in order to evolve optimal subsets of discriminatory features for robust pattern classification. A GA is used to search the space of all possible subsets of a large set of candidate discrimination features. For a given feature subset, ID3 is invoked to produce a decision tree. The classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set, which, in turn, is used by the GA to evolve better feature sets. This GA-ID3 process iterates until a feature subset is found with satisfactory classification performance. Experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data. The results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection.
79D84E5F	An algorithm to map burnt areas has been developed for SPOT VEGETATION (VGT) data in Australian woodland savannas. A time series of daily VGT images (15 May to 15 July 1999) was composited into 10-day periods by applying a minimum value criterion to the near-infrared band (0.78–0.89 mm). The algorithm was developed using a classification tree methodology that was confirmed as a powerful means of image classification. This methodology allowed the identification of three classes of burnt surfaces that appear to be differentiated by the proportion of the pixel that is burnt, the intensity of the fire and the density of the tree layer. The performance of the algorithm was assessed by classification of one VGT composite image (31 May–9 June) using, as representative of the ground truth, burnt areas extracted from two Landsat TM scenes (9 June). We randomly extracted 30 windows (each of ~14 km by 14 km) for which we compared the percentage of area burnt as derived from TM and VGT. The estimated mean absolute deviation in the percentage of the area burnt in each window is ±6.3%. In the area common to the two datasets a total amount of 6473 km2 was estimated to be burnt in the VGT classification against 7536 km2 that was burnt according to TM images. The accuracy of the classification was found to vary with the vegetation type being the most accurate estimate in low woodland with an underestimation error of 8.6%. These results show that VGT could be a very useful sensor for burnt area mapping over large woodland areas, although the low spatial resolution and the lack of a thermal band can be a limitation in certain conditions (e.g. understorey burns). The same methodology will be applied to map burnt areas for the entire Australian continent.
7ABDCCD6	The purposes of this study are to identify the strongest clinical parameters in relation to in-hospital mortality, which are available in the earliest phase of the hospitalization of patients, and to create an easy tool for the early identification of patients at risk.The classification and regression tree analysis was applied to data from the Acute Heart Failure Database-Main registry comprising patients admitted to specialized cardiology centers with all syndromes of acute heart failure. The classification model was built on derivation cohort (n = 2543) and evaluated on validation cohort (n = 1387).The classification tree stratifies patients according to the presence of cardiogenic shock (CS), the level of creatinine, and the systolic blood pressure (SBP) at admission into the 5 risk groups with in-hospital mortality ranging from 2.8% to 66.2%. Patients without CS and creatinine level of 155 μmol/L or less were classified into very-low-risk group; patients without CS, creatinine level greater than 155 μmol/L, and SBP greater than 103 mm Hg, into low-risk group, whereas patients without CS, creatinine level greater than 155 μmol/L, and SBP of 103 mm Hg or lower, into intermediate-risk group. The high-risk group patients had CS and creatinine of 140 μmol/L or less; patients with CS and creatinine level greater than 140 μmol/L belong to very-high-risk group. The area under receiver operating characteristic curve was 0.823 and 0.832, and the value of Brier's score was estimated on level 0.091 and 0.084, for the derivation and the validation cohort, respectively.The presented classification model effectively stratified patients with all syndromes of acute heart failure into in-hospital mortality risk groups and might be of advantage for clinical practice.
79931D06	Classification trees (CT) have been used successfully in the past to classify aquatic vegetation from spectral indices (SI) obtained from remotely-sensed images. However, applying CT models developed for certain image dates to other time periods within the same year or among different years can reduce the classification accuracy. In this study, we developed CT models with modified thresholds using extreme SI values (CTm) to improve the stability of the models when applying them to different time periods. A total of 903 ground-truth samples were obtained in September of 2009 and 2010 and classified as emergent, floating-leaf, or submerged vegetation or other cover types. Classification trees were developed for 2009 (Model-09) and 2010 (Model-10) using field samples and a combination of two images from winter and summer. Overall accuracies of these models were 92.8% and 94.9%, respectively, which confirmed the ability of CT analysis to map aquatic vegetation in Taihu Lake. However, Model-10 had only 58.9–71.6% classification accuracy and 31.1–58.3% agreement (i.e., pixels classified the same in the two maps) for aquatic vegetation when it was applied to image pairs from both a different time period in 2010 and a similar time period in 2009. We developed a method to estimate the effects of extrinsic (EF) and intrinsic (IF) factors on model uncertainty using Modis images. Results indicated that 71.1% of the instability in classification between time periods was due to EF, which might include changes in atmospheric conditions, sun-view angle and water quality. The remainder was due to IF, such as phenological and growth status differences between time periods. The modified version of Model-10 (i.e. CTm) performed better than traditional CT with different image dates. When applied to 2009 images, the CTm version of Model-10 had very similar thresholds and performance as Model-09, with overall accuracies of 92.8% and 90.5% for Model-09 and the CTm version of Model-10, respectively. CTm decreased the variability related to EF and IF and thereby improved the applicability of the models to different time periods. In both practice and theory, our results suggested that CTm was more stable than traditional CT models and could be used to map aquatic vegetation in time periods other than the one for which the model was developed.
75542B44	Using 1998 and 1999 singleton birth data of the State of Florida, we study the stability of classification trees. Tree stability depends on both the learning algorithm and the specific data set. In this study, test samples are used in statistical learning to evaluate both stability and predictive performance. We also use the resampling technique bootstrap, which can be regarded as data self-perturbation, to evaluate the sensitivity of the modeling algorithm with respect to the specific data set. We demonstrate that the selection of the cost function plays an important role in stability. In particular, classifiers with equal misclassification costs and equal priors are less stable compared to those with unequal misclassification costs and equal priors.
7767905F	The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.
7598242E	When managers and researchers encounter a data set, they typically ask two key questions: (1) Which model (from a candidate set) should I use? And (2) if I use a particular model, when is it going to likely work well for my business goal? This research addresses those two questions and provides a rule, i.e., a decision tree, for data analysts to portend the “winning model” before having to fit any of them for longitudinal incidence data. We characterize data sets based on managerially relevant (and easy-to-compute) summary statistics, and we use classification techniques from machine learning to provide a decision tree that recommends when to use which model. By doing the “legwork” of obtaining this decision tree for model selection, we provide a time-saving tool to analysts. We illustrate this method for a common marketing problem (i.e., forecasting repeat purchasing incidence for a cohort of new customers) and demonstrate the method's ability to discriminate among an integrated family of a hidden Markov model (HMM) and its constrained variants. We observe a strong ability for data set characteristics to guide the choice of the most appropriate model, and we observe that some model features (e.g., the “back-and-forth” migration between latent states) are more important to accommodate than are others (e.g., the inclusion of an “off” state with no activity). We also demonstrate the method's broad potential by providing a general “recipe” for researchers to replicate this kind of model classification task in other managerial contexts (outside of repeat purchasing incidence data and the HMM framework).
5D25C398	Decision tree learning has become a popular and practical method in data mining because of its high predictive accuracy and ease of use. However, a set of if-then rules generated from large trees may be preferred in many cases because of at least three reasons: (i) large decision trees are difficult to understand as we may not see their hierarchical structure or get lost in navigating them, (ii) the tree structure may cause individual subconcepts to be fragmented (this is sometimes known as the “replicated subtree” problem), (iii) it is easier to combine new discovered rules with existing knowledge in a given domain. To fulfill that need, the popular decision tree learning system C4.5 applies a rule post-pruning algorithm to transform a decision tree into a rule set. However, by using a global optimization strategy, C4.5rules functions extremely slow on large datasets. On the other hand, rule post-pruning algorithms that learn a set of rules by the separate-and-conquer strategy such as CN2, IREP, or RIPPER can be scalable to large datasets, but they suffer from the crucial problem of overpruning, and do not often achieve a high accuracy as C4.5. This paper proposes a scalable algorithm for rule post-pruning of large decision trees that employs incremental pruning with improvements in order to overcome the overpruning problem. Experiments show that the new algorithm can produce rule sets that are as accurate as those generated by C4.5 and is scalable for large datasets.
78504EB4	We present a framework for flexible nonlinear contextual image classification. The framework integrates classical and recent models for image classification, ranging from a multivariate Gaussian classifier, to MLP neural nets, classification trees and recent regression models based on general additive models, and combines them with a Markov random field for spatial context. The effect of using the different nonlinear discriminant functions is compared with the effect of using an MRF model for spatial context. In general, the use of an MRF model results in larger improvements in classification accuracy than using different nonlinear discriminant functions, but the combination of them can give large improvements.
7E53AC45	Decision tree classifiers have received much recent attention, particularly with regards to land cover classifications at continental to global scales. Despite their many benefits and general flexibility, the use of decision trees with high spatial resolution data has not yet been fully explored. In support of the National Park Service (NPS) Vegetation Mapping Program (VMP), we have examined the feasibility of using a commercially available decision tree classifier with multitemporal satellite data from the Enhanced Thematic Mapper-Plus (ETM+) instrument to map 11 land cover types at the Delaware Water Gap National Recreation Area near Milford, PA. Ensemble techniques such as boosting and consensus filtering of the training data were used to improve both the quality of the input training data as well as the final products.Using land cover classes as specified by the National Vegetation Classification Standard at the Formation level, the final land cover map has an overall accuracy of 82% (κ=0.80) when tested against a validation data set acquired on the ground (n=195). This same accuracy is 99.5% when considering only forest vs. nonforest classes. Usage of ETM+ scenes acquired at multiple dates improves the accuracy over the use of a single date, particularly for the different forest types. These results demonstrate the potential applicability and usability of such an approach to the entire National Park system, and to high spatial resolution land cover and forest mapping applications in general.
7683DF85	Multiseason reflectance data from radiometrically and geometrically corrected multispectral SPOT-5 images of 10-m resolution were combined with thorough field campaigns and land cover digitizing using a binary classification tree algorithm to estimate the area of marshes covered with common reeds (Phragmites australis) and submerged macrophytes (Potamogeton pectinatus, P. pusillus, Myriophyllum spicatum, Ruppia maritima, Chara sp.) over an area of 145,000 ha. Accuracy of these models was estimated by cross-validation and by calculating the percentage of correctly classified pixels on the resulting maps. Robustness of this approach was assessed by applying these models to an independent set of images using independent field data for validation. Biophysical parameters of both habitat types were used to interpret the misclassifications. The resulting trees provided a cross-validation accuracy of 98.7% for common reed and 97.4% for submerged macrophytes. Variables discriminating reed marshes from other land covers were the difference in the near-infrared band between March and June, the Optimized Soil Adjusted Vegetation Index of December, and the Normalized Difference Water Index (NDWI) of September. Submerged macrophyte beds were discriminated with the shortwave-infrared band of December, the NDWI of September, the red band of September and the Simple Ratio index of March. Mapping validations provided accuracies of 98.6% (2005) and 98.1% (2006) for common reed, and 86.7% (2005) and 85.9% (2006) for submerged macrophytes. The combination of multispectral and multiseasonal satellite data thus discriminated these wetland vegetation types efficiently. Misclassifications were partly explained by digitizing inaccuracies, and were not related to biophysical parameters for reedbeds. The classification accuracy of submerged macrophytes was influenced by the proportion of plants showing on the water surface, percent cover of submerged species, water turbidity, and salinity. Classification trees applied to time series of SPOT-5 images appear as a powerful and reliable tool for monitoring wetland vegetation experiencing different hydrological regimes even with a small training sample (N = 25) when initially combined with thorough field measurements.
7C8CCCB4	Aquatic vegetation plays an important role in maintaining the balance of lake ecosystems. Thus, classifying and mapping aquatic vegetation is a priority for lake management. Classification tree (CT) approaches have been used successfully to map aquatic vegetation from spectral indices obtained from remotely sensed images. However, due to the effects of extrinsic and intrinsic factors, applying a CT model developed for imagery from one date to imagery from another date or a different dataset likely would reduce the classification accuracy. In this study, three spectral features (SFs) were selected to develop a CT model for identifying aquatic vegetation in Taihu Lake. Three traditional CT models with three SFs were developed using CT analysis based on satellite images acquired on 11 July, 16 August and 26 September 2013, and corresponding ground-truth samples, from the Huangjing-1A/B Charge-Coupled Device (HJ-CCD) images, environment and disaster reduction small satellites that were launched by China Center for Resources Satellite Data and Application (CRESDA). The overall accuracies of traditional CT models were 82%, 80% and 84%. We then tested two methods to modify CT model thresholds to adjust the traditional CT models based on image date to determine if the results would enable us to map and classify aquatic vegetation for periods when no ground-based data were available. We assessed the results with ground-truth samples and area agreement with traditional CT models. Results showed that CT models modified from a linear adjustment based on the relationship between ranked values of SFs between two image dates produced map accuracies comparable with those obtained from the traditional CT models and suggest that the method we proposed is feasible for mapping aquatic vegetation types in lakes when ground data are not available.
76EB6675	Decision tree algorithm is a kind of data mining model to make induction learning algorithm based on examples. It is easy to extract display rule, has smaller computation amount, and could display important decision property and own higher classification precision. For the study of data mining algorithm based on decision tree, this article put forward specific solution for the problems of property value vacancy, multiple-valued property selection, property selection criteria, propose to introduce weighted and simplified entropy into decision tree algorithm so as to achieve the improvement of ID3 algorithm. The experimental results show that the improved algorithm is better than widely used ID3 algorithm at present on overall performance.
75087187	In previous attempts to identify aquatic vegetation from remotely-sensed images using classification trees (CT), the images used to apply CT models to different times or locations necessarily originated from the same satellite sensor as that from which the original images used in model development came, greatly limiting the application of CT. We have developed an effective normalization method to improve the robustness of CT models when applied to images originating from different sensors and dates. A total of 965 ground-truth samples of aquatic vegetation types were obtained in 2009 and 2010 in Taihu Lake, China. Using relevant spectral indices (SI) as classifiers, we manually developed a stable CT model structure and then applied a standard CT algorithm to obtain quantitative (optimal) thresholds from 2009 ground-truth data and images from Landsat7-ETM+, HJ-1B-CCD, Landsat5-TM and ALOS-AVNIR-2 sensors. Optimal CT thresholds produced average classification accuracies of 78.1%, 84.7% and 74.0% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. However, the optimal CT thresholds for different sensor images differed from each other, with an average relative variation (RV) of 6.40%. We developed and evaluated three new approaches to normalizing the images. The best-performing method (Method of 0.1% index scaling) normalized the SI images using tailored percentages of extreme pixel values. Using the images normalized by Method of 0.1% index scaling, CT models for a particular sensor in which thresholds were replaced by those from the models developed for images originating from other sensors provided average classification accuracies of 76.0%, 82.8% and 68.9% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. Applying the CT models developed for normalized 2009 images to 2010 images resulted in high classification (78.0%–93.3%) and overall (92.0%–93.1%) accuracies. Our results suggest that Method of 0.1% index scaling provides a feasible way to apply CT models directly to images from sensors or time periods that differ from those of the images used to develop the original models.
80DCAAC2	This research used classification tree analysis and logistic regression models to identify risk factors related to short- and long-term abstinence. Baseline and cessation outcome data from two smoking cessation trials, conducted from 2001 to 2002 in two Midwestern urban areas, were analyzed. There were 928 participants (53.1% women, 81.8% White) with complete data. Both analyses suggest that relapse risk is produced by interactions of risk factors and that early and late cessation outcomes reflect different vulnerability factors. The results illustrate the dynamic nature of relapse risk and suggest the importance of efficient modeling of interactions in relapse prediction.
78F9553F	Besides serving as prediction models, classification trees are useful for finding important predictor variables and identifying interesting subgroups in the data. These functions can be compromised by weak split selection algorithms that have variable selection biases or that fail to search beyond local main effects at each node of the tree. The resulting models may include many irrelevant variables or select too few of the important ones. Either eventuality can lead to erroneous conclusions. Four techniques to improve the precision of the models are proposed and their effectiveness compared with that of other algorithms, including tree ensembles, on real and simulated data sets. 
7A88004A	The literature on microbial source tracking (MST) suggests that DNA analysis of fecal samples leads to more reliable determinations of bacterial sources of surface water contamination than antibiotic resistance analysis (ARA). Our goal is to determine whether the increased reliability, if any, in library-based MST developed with DNA data is sufficient to justify its higher cost, where the bacteria source predictions are used in TMDL surface water management programs. We describe an application of classification trees for MST applied to ARA and DNA data from samples collected in the Potomac River Watershed in Maryland. Conclusions concerning the comparison of ARA and DNA data, although preliminary at the current time, suggest that the added cost of obtaining DNA data in comparison to the cost of ARA data may not be justified, where MST is applied in TMDL surface water management programs.
7E5348E7	Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.
813A1DD2	Feature subset selection is an important preprocessing step for classification. In biology, where structures or processes are described by a large number of features, the elimination of irrelevant and redundant information in a reasonable amount of time has a number of advantages. It enables the classification system to achieve good or even better solutions with a restricted subset of features, allows for a faster classification, and it helps the human expert focus on a relevant subset of features, hence providing useful biological knowledge.We present a heuristic method based on Estimation of Distribution Algorithms to select relevant subsets of features for splice site prediction in Arabidopsis thaliana. We show that this method performs a fast detection of relevant feature subsets using the technique of constrained feature subsets. Compared to the traditional greedy methods the gain in speed can be up to one order of magnitude, with results being comparable or even better than the greedy methods. This makes it a very practical solution for classification tasks that can be solved using a relatively small amount of discriminative features (or feature dependencies), but where the initial set of potential discriminative features is rather large.
7E2EEE71	Biomarker discovery is an important topic in biomedical applications of computational biology, including applications such as gene and SNP selection from high-dimensional data. Surprisingly, the stability with respect to sampling variation or robustness of such selection processes has received attention only recently. However, robustness of biomarkers is an important issue, as it may greatly influence subsequent biological validations. In addition, a more robust set of markers may strengthen the confidence of an expert in the results of a selection method.Our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm. Secondly, we conducted a large-scale analysis of the recently introduced concept of ensemble feature selection, where multiple feature selections are combined in order to increase the robustness of the final set of selected features. We focus on selection methods that are embedded in the estimation of support vector machines (SVMs). SVMs are powerful classification models that have shown state-of-the-art performance on several diagnosis and prognosis tasks on biological data. Their feature selection extensions also offered good results for gene selection tasks. We show that the robustness of SVMs for biomarker discovery can be substantially increased by using ensemble feature selection techniques, while at the same time improving upon classification performances. The proposed methodology is evaluated on four microarray datasets showing increases of up to almost 30% in robustness of the selected biomarkers, along with an improvement of approximately 15% in classification performance. The stability improvement with ensemble methods is particularly noticeable for small signature sizes (a few tens of genes), which is most relevant for the design of a diagnosis or prognosis model from a gene signature.
21A9A6DF	In clinical medicine, multidimensional time series data can be used to find the rules of disease progress by data mining technology, such as classification and prediction. However, in multidimensional time series data mining problems, the excessive data dimension causes the inaccuracy of probability density distribution to increase the computational complexity. Besides, information redundancy and irrelevant features may lead to high computational complexity and over-fitting problems. The combination of these two factors can reduce the classification performance. To reduce computational complexity and to eliminate information redundancies and irrelevant features, we improved upon a multidimensional time series feature selection method to achieve dimension reduction. The improved method selects features through the combination of the Kozachenko–Leonenko (K–L) information entropy estimation method for feature extraction based on mutual information and the feature selection algorithm based on class separability. We performed experiments on the Electroencephalogram (EEG) dataset for verification and the non-small cell lung cancer (NSCLC) clinical dataset for application. The results show that with the comparison of CLeVer, Corona and AGV, respectively, the improved method can effectively reduce the dimensions of multidimensional time series for clinical data.
7DBCCFE0	The identification of relevant biological features in large and complex datasets is an important step towards gaining insight in the processes underlying the data. Other advantages of feature selection include the ability of the classification system to attain good or even better solutions using a restricted subset of features, and a faster classification. Thus, robust methods for fast feature selection are of key importance in extracting knowledge from complex biological data.In this paper we present a novel method for feature subset selection applied to splice site prediction, based on estimation of distribution algorithms, a more general framework of genetic algorithms. From the estimated distribution of the algorithm, a feature ranking is derived. Afterwards this ranking is used to iteratively discard features. We apply this technique to the problem of splice site prediction, and show how it can be used to gain insight into the underlying biological process of splicing.We show that this technique proves to be more robust than the traditional use of estimation of distribution algorithms for feature selection: instead of returning a single best subset of features (as they normally do) this method provides a dynamical view of the feature selection process, like the traditional sequential wrapper methods. However, the method is faster than the traditional techniques, and scales better to datasets described by a large number of features.
7B1ED9C9	We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. 
7A94F317	We present several methods for full, partial, and practical adaptation. Selector statistics that are measures of skewness, peakedness, and tailweight are used, primarily in estimating loca-tion in some single-sample situations. We note several practical adaptive techniques in current use, including illustrations in-volving stepwise regression, analysis of variance, ridge regres-sion, and splines. We suggest some areas in which future develop-ment of adaptive methods is needed:density estimation; M, R, and L estimation in regression; and dependent data. There is also a need to develop better selector statistics. 
73831E7D	Prostate cancer is complicated by a high level of unexplained variability in the aggressiveness of newly diagnosed disease. Given that this is one of the most prevalent cancers worldwide, finding biomarkers to effectively stratify high risk patient populations is a vital next step in improving survival rates and quality of life after treatment. Materials and Methods: In this study, we selected a dataset consisting of 106 prostate cancer samples, which represent various stages of prostate cancer and developed by RNA-Seq technology. Our objective is to identify differentially expressed transcripts associated with prostate cancer progression using pair-wise stage comparisons. Results: Using machine learning techniques, we identified 44 transcripts that are correlated to different stages of progression. Expression of an identified transcript, USP13, is reduced in stage T3 in comparison with stage T2c, a pattern also observed in breast cancer tumourigenesis. We also identified another differentially expressed transcript, PTGFR, which has also been reported to be involved in prostate cancer progression and has also been linked to breast, ovarian and renal cancers. Conclusions: The results support the use of RNA-Seq along with machine learning techniques as an essential tool in identifying potential biomarkers for prostate cancer progression. Further studies elucidating the biochemical role of identified transcripts in vitro are crucial in validating the use of these biomarkers in the prediction of disease progression and development of effective therapeutic strategies.
7BB1F2A4	In this research, I proposed Emotion Classification of Thai Text based Using Term weighting and Machine Learning Techniques focusing on the comparison of various common term weighting schemes. I found Boolean weighting with Support Vector Machine is most effective in our experiments. I also discovered that the Boolean weighting is suitable for combination with the Information gain feature selection method. The Boolean weighting with Support Vector Machine algorithm yielded the best performance with the accuracy over all algorithms. Based on our experiments, the Support Vector Machine algorithm with the Information gain feature selection yielded the best performance with the accuracy of 77.86%. Our experimental results also reveal that feature weighting methods have a positive effect on the Thai Emotion Classification Framework.
7D6A21F8	In this paper, we address the problems of deformable object matching (alignment) and segmentation with cluttered background. We propose a novel hierarchical log-linear model (HLLM) which represents both shape and appearance features at multiple levels of a hierarchy. This model enables us to combine appearance cues at multiple scales directly into the hierarchy and to model shape deformations at short-range, medium range, and long-range. We introduce the structure-perceptron algorithm to estimate the parameters of the HLLM in a discriminative way. The learning is able to estimate the appearance and shape parameters simultaneously in a global manner. Moreover, the structure-perceptron learning has a feature selection aspect (similar to AdaBoost) which enables us to specify a class of appearance/shape features and allow the algorithm to select which features to use and weight their importance. This method was applied to the tasks of deformable object localization, segmentation, matching (alignment), and parsing. We demonstrate that the algorithm achieves the state of the art performance by evaluation on public dataset (horse and multi-view face).
775B2060	Aquatic weed control through chemical products has attracted much attention in the last years, mainly because of the ecological disorder caused by such plants, and also the consequences to the economical activities. However, this kind of control has been carried out in a non-automatic way by technicians, and may be a not healthy policy, since each species may react differently to the same herbicide. Thus, this work proposes the automatic identification of some species by means of supervised pattern recognition techniques and shape descriptors in order to compose a nearby future expert system for automatic application of the correct herbicide. Experiments using some state-of-the-art techniques have shown the robustness of the employed pattern recognition techniques.
80F1EA90	The current search engines usually return a large number of irrelevant documents for a certain query. As a result, accessing such information and filtering out these documents can cause frustration and often result in waste of time and effort for the users while surfing the web. This is mainly because of the underlying techniques used in these engines. These techniques are mostly based in the frequency of the keywords of the query in the HTML code. In addition, issues such as dealing with classifying the pages found for a query according to previous visits along with features needed to make intelligent decisions regarding the access patterns of the users are not considered. This work presents an intelligent search engine, called ORCA that returns the most relevant documents for user's queries. This search engine analyses the queries and builds themes (models) to be used when the engine is confronted with similar queries. The intelligent component is used for constructing a model of the user behavior and using that model to fetch and even prefetch information and documents considered of interest to the user. It uses both latent semantic analysis and web page feature selection for clustering web pages. Latent semantic analysis is used to find the semantic relations between keywords, and between documents.
5D3D3DF8	Naive Bayes is a well known and studied algorithm both in statistics and machine learning. Bayesian learning algorithms represent each concept with a single probabilistic summary. In this paper we present an iterative approach to naive Bayes. The iterative Bayes begins with the distribution tables built by the naive Bayes. Those tables are iteratively updated in order to improve the probability class distribution associated with each training example. Experimental evaluation of Iterative Bayes on 25 benchmark datasets shows consistent gains in accuracy. An interesting side effect of our algorithm is that it shows to be robust to attribute dependencies
77654CEF	We consider a linear regression problem in a high dimensional setting where the number of covariates p can be much larger than the sample size n. In such a situation, one often assumes sparsity of the regression vector, i.e., the regression vector contains many zero components. We propose a Lasso-type estimator β̂Quad (where ‘Quad’ stands for quadratic) which is based on two penalty terms. The first one is the ℓ1 norm of the regression coefficients used to exploit the sparsity of the regression as done by the Lasso estimator, whereas the second is a quadratic penalty term introduced to capture some additional information on the setting of the problem. We detail two special cases: the Elastic-Net β̂EN introduced in [42], which deals with sparse problems where correlations between variables may exist; and the Smooth-Lasso β̂SL, which responds to sparse problems where successive regression coefficients are known to vary slowly (in some situations, this can also be interpreted in terms of correlations between successive variables). From a theoretical point of view, we establish variable selection consistency results and show that β̂Quad achieves a Sparsity Inequality, i.e., a bound in terms of the number of non-zero components of the ‘true’ regression vector. These results are provided under a weaker assumption on the Gram matrix than the one used by the Lasso. In some situations this guarantees a significant improvement over the Lasso. Furthermore, a simulation study is conducted and shows that the S-Lasso β̂SL performs better than known methods as the Lasso, the Elastic-Net β̂EN, and the Fused-Lasso (introduced in [30]) with respect to the estimation accuracy. This is especially the case when the regression vector is ‘smooth’, i.e., when the variations between successive coefficients of the unknown parameter of the regression are small. The study also reveals that the theoretical calibration of the tuning parameters and the one based on 10 fold cross validation imply two S-Lasso solutions with close performance. 
810F181F	Robustness or stability of feature selection techniques is a topic of recent interest, and is an important issue when selected feature subsets are subsequently analysed by domain experts to gain more insight into the problem modelled. In this work, we investigate the use of ensemble feature selection techniques, where multiple feature selection methods are combined to yield more robust results. We show that these techniques show great promise for high-dimensional domains with small sample sizes, and provide more robust feature subsets than a single feature selection technique. In addition, we also investigate the effect of ensemble feature selection techniques on classification performance, giving rise to a new model selection strategy.
5A30BF03	Identification of individualized feature combinations for survival prediction in breast cancer: a comparison of machine learning techniques"
78848AB6	We present a novel method for detecting near-duplicates from a large collection of documents. Three major parts are involved in our method, feature selection, similarity measure, and discriminant derivation. To find near-duplicates to an input document, each sentence of the input document is fetched and preprocessed, the weight of each term is calculated, and the heavily weighted terms are selected to be the feature of the sentence. As a result, the input document is turned into a set of such features. A similarity measure is then applied and the similarity degree between the input document and each document in the given collection is computed. A support vector machine (SVM) is adopted to learn a discriminant function from a training pattern set, which is then employed to determine whether a document is a near-duplicate to the input document based on the similarity degree between them. The sentence-level features we adopt can better reveal the characteristics of a document. Besides, learning the discriminant function by SVM can avoid trial-and-error efforts required in conventional methods. Experimental results show that our method is effective in near-duplicate document detection.
750A9F88	In the past two decades, the dimensionality of datasets involved in machine learning and data mining applications has increased explosively. Therefore, feature selection has become a necessary step to make the analysis more manageable and to extract useful knowledge about a given domain. A large variety of feature selection techniques are available in literature, and their comparative analysis is a very difficult task. So far, few studies have investigated, from a theoretical and/or experimental point of view, the degree of similarity/dissimilarity among the available techniques, namely the extent to which they tend to produce similar results within specific application contexts. This kind of similarity analysis is of crucial importance when two or more methods are combined in an ensemble fashion: indeed the ensemble paradigm is beneficial only if the involved methods are capable of giving different and complementary representations of the considered domain. This paper gives a contribution in this direction by proposing an empirical approach to evaluate the degree of consistency among the outputs of different selection algorithms in the context of high dimensional classification tasks. Leveraging on a proper similarity index, we systematically compared the feature subsets selected by eight popular selection methods, representatives of different selection approaches, and derived a similarity trend for feature subsets of increasing size. Through an extensive experimentation involving sixteen datasets from three challenging domains (Internet advertisements, text categorization and micro-array data classification), we obtained useful insight into the pattern of agreement of the considered methods. In particular, our results revealed how multivariate selection approaches systematically produce feature subsets that overlap to a small extent with those selected by the other methods.
764B4F14	The web services, a novel paradigm in software technology, have innovative mechanism for rendering services over diversified environment. They promise to allow businesses to adapt rapidly to changes in the business environment and the needs of different customers. The rapid introduction of new web services into a dynamic business environment can adversely affect the service quality and user satisfaction. Consequently, assessment of the quality of web services is of paramount importance in selecting a web service for an application. In this paper, we employed well-known classification models viz., back propagation neural network (BPNN), probabilistic neural network (PNN), group method of data handling (GMDH), classification and regression trees (CART), TreeNet, support vector machine (SVM) and ID3 decision tree (J48) to predict the quality of a web service based on a set of quality attributes. The experiments are carried out on the QWS dataset. We applied 10-fold cross-validation to test the efficacy of the models. The J48 and TreeNet techniques outperformed all other techniques by yielding an average accuracy of 99.72%. We also performed feature selection and found that web-services relevance function (WSRF) is the most significant attribute in determining the quality of a web service. Later, we performed feature selection without WSRF and found that Reliability, Throughput, Successability, Documentation and Response Time are the most important attributes in that order. Moreover, the set of ‘if–then’ rules yielded by J48 and CART can be used as an expert system for web-services classification.
80895228	A filter method of feature selection based on mutual information, called normalized mutual information feature selection (NMIFS), is presented. NMIFS is an enhancement over Battiti's MIFS, MIFS-U, and mRMR methods. The average normalized mutual information is proposed as a measure of redundancy among features. NMIFS outperformed MIFS, MIFS-U, and mRMR on several artificial and benchmark data sets without requiring a user-defined parameter. In addition, NMIFS is combined with a genetic algorithm to form a hybrid filter/wrapper method called GAMIFS. This includes an initialization procedure and a mutation operator based on NMIFS to speed up the convergence of the genetic algorithm. GAMIFS overcomes the limitations of incremental search algorithms that are unable to find dependencies between groups of features.
80643FBF	Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.
7F1C2084	The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.
7FCEC94E	Data mining is the study of how to determine underlying patterns in the data to help make optimal decisions on computers when the database involved is voluminous, hard to characterize accurately and constantly changing. It deploys techniques based on machine learning alongside more conventional methods. These techniques can generate decision or prediction models based on actual historical data. Therefore, they represent true evidence-based decision support. Rainfall prediction is a good problem to solve by data mining techniques. This paper proposes an improved naive Bayes classifier (INCB) technique and explores the use of genetic algorithms (GAs) for the selection of a subset of input features in classification problems. It then carries out a comparison with several other techniques. It compares the following algorithms on real meteorological data in Hong Kong: (1) genetic algorithms with average classification or general classification (GA-AC and GA-C), (2) C4.5 with pruning, and (3) INBC with relative frequency or initial probability density (INBC-RF and INBC-IPD). Two simple schemes are proposed to construct a suitable data set for improving their performance. Scheme I uses all the basic input parameters for rainfall prediction. Scheme II uses the optimal subset of input variables which are selected by a GA. The results show that, among the methods we compared, INBC achieved about a 90% accuracy rate on the rain/no-rain classification problems. This method also attained reasonable performance on rainfall prediction with three-level depth and five-level depth, which are around 65%-70%.
7510E7BD	A plenitude of feature selection (FS) methods is available in the literature, most of them rising as a need to analyze data of very high dimension, usually hundreds or thousands of variables. Such data sets are now available in various application areas like combinatorial chemistry, text mining, multivariate imaging, or bioinformatics. As a general accepted rule, these methods are grouped in filters, wrappers, and embedded methods. More recently, a new group of methods has been added in the general framework of FS: ensemble techniques. The focus in this survey is on filter feature selection methods for informative feature discovery in gene expression microarray (GEM) analysis, which is also known as differentially expressed genes (DEGs) discovery, gene prioritization, or biomarker discovery. We present them in a unified framework, using standardized notations in order to reveal their technical details and to highlight their common characteristics as well as their particularities.
7FB56746	Microarray data classification is a difficult challenge for machine learning researchers due to its high number of features and the small sample sizes. Feature selection has been soon considered a de facto standard in this field since its introduction, and a huge number of feature selection methods were utilized trying to reduce the input dimensionality while improving the classification performance. This paper is devoted to reviewing the most up-to-date feature selection methods developed in this field and the microarray databases most frequently used in the literature. We also make the interested reader aware of the problematic of data characteristics in this domain, such as the imbalance of the data, their complexity, or the so-called dataset shift. Finally, an experimental evaluation on the most representative datasets using well-known feature selection methods is presented, bearing in mind that the aim is not to provide the best feature selection method, but to facilitate their comparative study by the research community.
78D5F25A	Multi-instance learning concerns about building learning models from a number of labeled instance bags, where each bag consists of instances with unknown labels. A bag is labeled positive if one or more multiple instances inside the bag is positive, and negative otherwise. For all existing multi-instance learning algorithms, they are only applicable to the setting where instances in each bag are represented by a set of well defined feature values. In this paper, we advance the problem to a multi-instance multi-graph setting, where a bag contains a number of instances and graphs in pairs, and the learning objective is to derive classification models from labeled bags, containing both instances and graphs, to predict previously unseen bags with maximum accuracy. To achieve the goal, the main challenge is to properly represent graphs inside each bag and further take advantage of complementary information between instance and graph pairs for learning. In the paper, we propose a Dual Embedding Multi-Instance Multi-Graph Learning (DE-MIMG) algorithm, which employs a dual embedding learning approach to (1) embed instance distributions into the informative sub graphs discovery process, and (2) embed discovered sub graphs into the instance feature selection process. The dual embedding process results in an optimal representation for each bag to provide combined instance and graph information for learning. Experiments and comparisons on real-world multi-instance multi-graph learning tasks demonstrate the algorithm performance.
7119C80A	Diagnosing depression in the early curable stages is very important and may even save the life of a patient. In this paper, we study nonlinear analysis of EEG signal for discriminating depression patients and normal controls. Forty-five unmedicated depressed patients and 45 normal subjects were participated in this study. Power of four EEG bands and four nonlinear features including detrended fluctuation analysis (DFA), higuchi fractal, correlation dimension and lyapunov exponent were extracted from EEG signal. For discriminating the two groups, k-nearest neighbor, linear discriminant analysis and logistic regression as the classifiers are then used. Highest classification accuracy of 83.3% is obtained by correlation dimension and LR classifier among other nonlinear features. For further improvement, all nonlinear features are combined and applied to classifiers. A classification accuracy of 90% is achieved by all nonlinear features and LR classifier. In all experiments, genetic algorithm is employed to select the most important features. The proposed technique is compared and contrasted with the other reported methods and it is demonstrated that by combining nonlinear features, the performance is enhanced. This study shows that nonlinear analysis of EEG can be a useful method for discriminating depressed patients and normal subjects. It is suggested that this analysis may be a complementary tool to help psychiatrists for diagnosing depressed patients.
812F24DC	We address the problem of efficiently learning Naive Bayes classifiers under classconditional classification noise (CCCN). Naive Bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn Naive Bayes classifiers under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled. 1.
584A9F54	In this paper, we show how using the Dirichlet Process mixture model as a generative model of data sets provides a simple and effective method for transfer learning. In particular, we present a hierarchical extension of the classic Naive Bayes classifier that couples multiple Naive Bayes classifiers by placing a Dirichlet Process prior over their parameters and show how recent advances in approximate inference in the Dirichlet Process mixture model enable efficient inference. We evaluate the resulting model in a meeting domain, in which the system decides, based on a learned model of the user's behavior, whether to accept or reject the request on his or her behalf. The extended model outperforms the standard Naive Bayes model by using data from other users to influence its predictions.
75CFD48F	In supervised learning it is assumed that it is straightforward to obtain labeled data. However, in reality labeled data can be scarce or expensive to obtain. Active learning (AL) is a way to deal with the above problem by asking for the labels of the most “informative” data points. We propose an AL method based on a metric of classification confidence computed on a feature subset of the original feature space which pertains especially to the large number of dimensions (i.e. examined genes) of microarray experiments. DNA microarray expression experiments permit the systematic study of the correlation of the expression of thousands of genes.Feature selection is critical in the algorithm because it enables faster and more robust retraining of the classifier. The approach that is followed for feature selection is a combination of a variance measure and a genetic algorithm.We have applied the proposed method on DNA microarray data sets with encouraging results. In particular we studied data sets concerning: small round blue cell tumours (4 types), Leukemia (2 types), lung cancer (2 types) and prostate cancer (healthy, unhealthy)
7508EFF3	Considerable effort has been made by researchers in the area of network traffic classification, since the Internet is constantly changing. This characteristic makes the task of traffic identification not a straightforward process. Besides that, encrypted data is being widely used by applications and protocols. There are several methods for classifying network traffic such as known ports and Deep Packet Inspection (DPI), but they are not effective since many applications constantly randomize their ports and the payload could be encrypted. This paper proposes a hybrid model that makes use of a classifier based on computational intelligence, the Extreme Learning Machine (ELM), along with Feature Selection (FS) and Multi-objective Genetic Algorithms (MOGA) to classify computer network traffic without making use of the payload or port information. The proposed model presented good results when evaluated against the UNIBS data set, using four performance metrics: Recall, Precision, Flow Accuracy and Byte Accuracy, with most rates exceeding 90%. Besides that, presented the best features and feature selection algorithm for the given problem along with the best ELM parameters.
76E67A3F	As concurrent and distributive applications are becoming more common and debugging such applications is very difficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instrumentation, timing "noise" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small programs.In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program locations that are in the vicinity of the bugs on a number of industrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with different instrumentations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using instrumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evaluation of the feature but the derivative of that value along the program execution path.As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging algorithms for concurrent applications, based on noise injection based concurrent testing using instrumentation. The infrastructure is described in this paper.
7B5B0392	Age-related macular ARM degeneration is an eye disease, that gradually degrades the macula, a part of the retina, which is responsible for central vision. It occurs in one of the two types, dry and wet age-related macular degeneration. The purpose of this paper is to diagnose the retinal disease age-related macular degeneration. An automated approach is proposed to help in the early detection of age-related macular degeneration using three models and their performances are compared. The amount of the disease spread in the retina can be identified by extracting the features of the retina. Detection of age-related macular degeneration disease has been done using probabilistic neural network PNN, Bayesian classification and support vector machine SVM and the two types of age-related macular degeneration are classified and diagnosed successfully. The results show that SVM achieves a higher performance measure than probabilistic neural network and Bayes classification.
77BEDFB0	With the development and popularization of the remote-sensing imaging technology, there are more and more applications of hyperspectral image classification tasks, such as target detection and land cover investigation. It is a very challenging issue of urgent importance to select a minimal and effective subset from those mass of bands. This paper proposed a hybrid feature selection strategy based on genetic algorithm and support vector machine (GA–SVM), which formed a wrapper to search for the best combination of bands with higher classification accuracy. In addition, band grouping based on conditional mutual information between adjacent bands was utilized to counter for the high correlation between the bands and further reduced the computational cost of the genetic algorithm. During the post-processing phase, the branch and bound algorithm was employed to filter out those irrelevant band groups. Experimental results on two benchmark data sets have shown that the proposed approach is very competitive and effective.
0A088B47	We present a framework for characterizing Bayesian classification methods. This framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the Naive Bayes algorithm at the most restrictive end and the learning of full Bayesian networks at the most general extreme. While much work has been carried out along the two ends of this spectrum, there has been surprising little done along the middle. We analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains. We then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties.
5FC1ECAC	this paper is to investigate the applicability of these techniques to high dimensional problems of Feature Selection. The aim is to establish whether the properties inferred for these techniques from medium scale experiments involving up to a few tens of dimensions extend to dimensionalities of one order of magnitude higher. Further, relative merits of these techniques vis-a-vis such high dimensional problems are explored and the possibility of exploiting the best aspects of these methods to create a composite feature selection procedure with superior properties is considered
7BAB122C	In this study, we developed new computational DNA adduct prediction models by using significantly more diverse training data-set of 217 DNA adducts and 1024 non-DNA adducts, and applying five machine learning methods which include support vector machine (SVM), k-nearest neighbour, artificial neural networks, logistic regression and continuous kernel discrimination. The molecular descriptors used for DNA adduct prediction were selected from a pool of 548 descriptors by using a multi-step hybrid feature selection method combining Fischer-score and Monte Carlo simulated annealing method. Some of the selected descriptors are consistent with the structural and physicochemical properties reported to be important for DNA adduct formation. The y-scrambling method was used to test whether there is a chance correlation in the developed SVM model. In the meantime, fivefold cross-validation of these machine learning methods results in the prediction accuracies of 64.1–82.5% for DNA adducts and 95.1–97.6% for non-DNA adducts, and the prediction accuracies for external test set are 78.2–100% for DNA adducts and 92.6–98.4% for non-DNA adducts. Our study suggested that the tested machine learning methods are potentially useful for DNA adducts identification.
7FFEDC6D	Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approacheswe single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from...
77630BE7	This paper presents a new hybrid genetic algorithm (HGA) for feature selection (FS), called as HGAFS. The vital aspect of this algorithm is the selection of salient feature subset within a reduced size. HGAFS incorporates a new local search operation that is devised and embedded in HGA to fine-tune the search in FS process. The local search technique works on basis of the distinct and informative nature of input features that is computed by their correlation information. The aim is to guide the search process so that the newly generated offsprings can be adjusted by the less correlated (distinct) features consisting of general and special characteristics of a given dataset. Thus, the proposed HGAFS receives the reduced redundancy of information among the selected features. On the other hand, HGAFS emphasizes on selecting a subset of salient features with reduced number using a subset size determination scheme. We have tested our HGAFS on 11 real-world classification datasets having dimensions varying from 8 to 7129. The performances of HGAFS have been compared with the results of other existing ten well-known FS algorithms. It is found that, HGAFS produces consistently better performances on selecting the subsets of salient features with resulting better classification accuracies.
76C82C3C	Feature selection is very important in the mining of multivariate time series data, which is represented in matrix. We propose a novel filter method termed as class separability feature selection (CSFS) for feature selection from multivariate time series with the trace-based class separability criterion. The mutual information matrix between variables is used as the features for classification. And the feature selection algorithm CSFS selects features according to the scores of class separability and variable separability. The proposed method is compared with CLeVer, Corona and AGV on the UCI EEG data sets, and the simulation results substantiate the good performance of CSFS.
5D3348BE	Generic ensemble methods can achieve excellent learning performance, but are not good candidates for active learning because of their different design purposes. We investigate how to use diversity of the member classifiers of an ensemble for efficient active learning. We empirically show, using benchmark data sets, that (1) to achieve a good (stable) ensemble, the number of classifiers needed in the ensemble varies for different data sets; (2) feature selection can be applied for classifier selection from ensembles to construct compact ensembles with high performance. Benchmark data sets and a real-world application are used to demonstrate the effectiveness of the proposed approach.
5E5CE080	We propose a novel feature selection filter for supervised learning, which relies on the efficient estimation of the mutual information between a high-dimensional set of features and the classes. We bypass the estimation of the probability density function with the aid of the entropic-graphs approximation of Rényi entropy, and the subsequent approximation of the Shannon entropy. Thus, the complexity does not depend on the number of dimensions but on the number of patterns/samples, and the curse of dimensionality is circumvented. We show that it is then possible to outperform algorithms which individually rank features, as well as a greedy algorithm based on the maximal relevance and minimal redundancy criterion. We successfully test our method both in the contexts of image classification and microarray data classification. For most of the tested data sets, we obtain better classification results than those reported in the literature.
7B157682	A new suboptimal search strategy for feature selection is presented. It represents a more sophisticated version of “classical” floating search algorithms (Pudil et al., 1994), attempts to remove some of their potential deficiencies and facilitates finding a solution even closer to the optimal one.
75695754	We propose expected attainable discrimination (EAD) as a measure to select discrete valued features for reliable discrimination between two classes of data. EAD is an average of the area under the ROC curves obtained when a simple histogram probability density model is trained and tested on many random partitions of a data set. EAD can be incorporated into various stepwise search methods to determine promising subsets of features, particularly when misclassification costs are difficult or impossible to specify. Experimental application to the problem of risk prediction in pregnancy is described.
78192D7B	Sequential search methods characterized by a dynamically changing number of features included or eliminated at each step, henceforth “floating” methods, are presented. They are shown to give very good results and to be computationally more effective than the branch and bound method.
7FF4A547	Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare An external file that holds a picture, illustration, etc. Object name is pone.0028210.e001.jpg feature selection methods on An external file that holds a picture, illustration, etc. Object name is pone.0028210.e002.jpg public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.
7D0157AF	It is difficult to find the optimal sparse solution of a manifold learning based dimensionality reduction algorithm. The lasso or the elastic net penalized manifold learning based dimensionality reduction is not directly a lasso penalized least square problem and thus the least angle regression (LARS) (Efron et al., Ann Stat 32(2):407–499, 2004), one of the most popular algorithms in sparse learning, cannot be applied. Therefore, most current approaches take indirect ways or have strict settings, which can be inconvenient for applications. In this paper, we proposed the manifold elastic net or MEN for short. MEN incorporates the merits of both the manifold learning based dimensionality reduction and the sparse learning based dimensionality reduction. By using a series of equivalent transformations, we show MEN is equivalent to the lasso penalized least square problem and thus LARS is adopted to obtain the optimal sparse solution of MEN. In particular, MEN has the following advantages for subsequent classification: (1) the local geometry of samples is well preserved for low dimensional data representation, (2) both the margin maximization and the classification error minimization are considered for sparse projection calculation, (3) the projection matrix of MEN improves the parsimony in computation, (4) the elastic net penalty reduces the over-fitting problem, and (5) the projection matrix of MEN can be interpreted psychologically and physiologically. Experimental evidence on face recognition over various popular datasets suggests that MEN is superior to top level dimensionality reduction algorithms.
6BDF01D4	Image spam is a new trend in the family of email spams. The new image spams employ a variety of image processing technologies to create random noises. In this paper, we propose a semi-supervised approach, regularized discriminant EM algorithm (RDEM), to detect image spam emails, which leverages small amount of labeled data and large amount of unlabeled data for identifying spams and training a classification model simultaneously. Compared with fully supervised learning algorithms, the semi-supervised learning algorithm is more suitedin adversary classification problems, because the spammers are actively protecting their work by constantly making changes to circumvent the spam detection. It makes the cost too high for fully supervised learning to frequently collect sufficient labeled data for training. Experimental results demonstrate that our approach achieves 91.66% high detection rate with less than 2.96% false positive rate, meanwhile it significantly reduces the labeling cost.
002A1815	In this paper we propose an automatic salient object extraction method for nature scene. The proposed method first utilizes an algorithm based on visual attention model to obtain a prior knowledge for Graph Cut, and then constructs the weighted graph of Graph Cut based on super-pixels pre-segmented by the improved watershed algorithm in order to accelerate the speed of proposed method. In this framework, Visual saliency map is obtained using chrominance and intensity features in HSV color space, which provides the approximate region that contains salient object to be segmented. Then the salient object region after extension is cropped as input image, and pre-segmented by the improved watershed algorithm into several regions to construct weighted graph. Finally the salient object is obtained by Graph Cut algorithm. Experiment results show that our algorithm can automatically get salient object without human interactions, and speed up the segmentation without decreasing segmentation accuracy.
80F19426	This paper presents a novel way to learn Chinese polarity lexicons by using both external relations and internal formation of Chinese words, i.e. by integrating two kinds of different but complementary models: graph models and morphological feature-based models. The polarity detection is first treated as a semi-supervised learning in a graph, and then machine learning is used based on morphological features of Chinese words. The results show that the the integration of morphological feature-based models and graph models significantly outperforms the baselines.
7E0D8880	This paper proposes a novel method to apply the standard graph cut technique to segmenting multimodal tensor valued images. The Riemannian nature of the tensor space is explicitly taken into account by first mapping the data to a Euclidean space where non-parametric kernel density estimates of the regional distributions may be calculated from user initialized regions. These distributions are then used as regional priors in calculating graph edge weights. Hence this approach utilizes the true variation of the tensor data by respecting its Riemannian structure in calculating distances when forming probability distributions. Further, the non-parametric model generalizes to arbitrary tensor distribution unlike the Gaussian assumption made in previous works. Casting the segmentation problem in a graph cut framework yields a segmentation robust with respect to initialization on the data tested.
7FEDD688	This paper adds a number of novel concepts into global s/t cut methods improving their efficiency and making them relevant for a wider class of applications in vision where algorithms should ideally run in real-time. Our new Active Cuts (AC) method can effectively use a good approximate solution (initial cut) that is often available in dynamic, hierarchical, and multi-label optimization problems in vision. In many problems AC works faster than the state-of-the-art max-flow methods [2] even if initial cut is far from the optimal one. Moreover, empirical speed improves several folds when initial cut is spatially close to the optima. Before converging to a global minima, Active Cuts outputs a multitude of intermediate solutions (intermediate cuts) that, for example, can be used be accelerate iterative learning-based methods or to improve visual perception of graph cuts realtime performance when large volumetric data is segmented. Finally, it can also be combined with many previous methods for accelerating graph cuts.
7DEB0ED6	Voxel occupancy is one approach for reconstructing the 3-dimensional shape of an object from multiple views. In voxel occupancy, the task is to produce a binary labeling of a set of voxels, that determines which voxels are filled and which are empty. In this paper, we give an energy minimization formulation of the voxel occupancy problem. The global minimum of this energy can be rapidly computed with a single graph cut, using a result due to D. Greig et al. (1989). The energy function we minimize contains a data term and a smoothness term. The data term is a sum over the individual voxels, where the penalty for a voxel is based on the observed intensities of the pixels that intersect it. The smoothness term is the number of empty voxels adjacent to filled ones. Our formulation can be viewed as a generalization of silhouette intersection, with two advantages: we do not compute silhouettes, which are a major source of errors; and we can naturally incorporate spatial smoothness. We give experimental results showing reconstructions from both real and synthetic imagery. Reconstruction using this smoothed energy function is not much more time consuming than simple silhouette intersection; it takes about 10 seconds to reconstruct a one million voxel volume.
8062DE9D	In this paper we present a graph cuts based active contours (GCBAC) approach to object segmentation problems. Our method is a combination of active contours and the optimization tool of graph cuts and differs fundamentally from traditional active contours in that it uses graph cuts to iteratively deform the contour. Consequently, it has the following advantages. (1) It has the ability to jump over local minima and provide a more global result. (2) Graph cuts guarantee continuity and lead to smooth contours free of self-crossing and uneven spacing problems. Therefore, the internal force, which is commonly used in traditional energy functions to control the smoothness, is no longer needed, and hence the number of parameters is greatly reduced. (3). Our approach easily extends to the segmentation of three and higher dimensional objects. In addition, the algorithm is suitable for interactive correction and is shown to always converge. Experimental results and analyses are provided.
092E6612	Semi-supervised learning works on utilizing both labeled and unlabeled data to improve learning performance, which has been receiving increasing attention in many applications such as clustering and classification. In this paper, we focus on the semi-supervised learning methods developed on data graph whose edge weights are measured by low-rank representation (LRR) coefficients. Specifically, we impose two constraints on LRR when constructing the graph: local affinity and distant repulsion, to preserve the data manifold information. The proposed model, termed structure preserving LRR (SPLRR), can preserve the local geometrical structure and without distorting the distant repulsion property. Using the augmented Lagrange multiplier (ALM) method framework, we derive an efficient approach to optimizing the SPLRR model. Experiments are conducted on four widely used data sets to validate the effectiveness of our proposed SPLRR model and the results demonstrate that SPLRR is an excellent model for graph based semi-supervised learning in comparison with the state-of-the-art methods.
7F7034C9	In this paper, we describe the structure of separable self-complementary graphs
6A2D0EE9	We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known viewpoints. Multi-camera scene reconstruction is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility. In this paper, we take an approach that has yielded excellent results for stereo, namely energy minimization via graph cuts. We first give an energy minimization formulation of the multi-camera scene reconstruction problem. The energy that we minimize treats the input images symmetrically, handles visibility properly, and imposes spatial smoothness while preserving discontinuities. As the energy function is NP-hard to minimize exactly, we give a graph cut algorithm that computes a local minimum in a strong sense. We handle all camera configurations where voxel coloring can be used, which is a large and natural class. Experimental data demonstrates the effectiveness of our approach.
7E5B1A47	In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available.
7FFB7F08	In this paper, we propose an interactive color natural image segmentation method. The method integrates color feature with multiscale nonlinear structure tensor texture (MSNST) feature and then uses GrabCut method to obtain the segmentations. The MSNST feature is used to describe the texture feature of an image and integrated into GrabCut framework to overcome the problem of the scale difference of textured images. In addition, we extend the Gaussian Mixture Model (GMM) to MSNST feature and GMM based on MSNST is constructed to describe the energy function so that the texture feature can be suitably integrated into GrabCut framework and fused with the color feature to achieve the more superior image segmentation performance than the original GrabCut method. For easier implementation and more efficient computation, the symmetric KL divergence is chosen to produce the estimates of the tensor statistics instead of the Riemannian structure of the space of tensor. The Conjugate norm was employed using Locality Preserving Projections (LPP) technique as the distance measure in the color space for more discriminating power. An adaptive fusing strategy is presented to effectively adjust the mixing factor so that the color and MSNST texture features are efficiently integrated to achieve more robust segmentation performance. Last, an iteration convergence criterion is proposed to reduce the time of the iteration of GrabCut algorithm dramatically with satisfied segmentation accuracy. Experiments using synthesis texture images and real natural scene images demonstrate the superior performance of our proposed method.
7F3B7CA7	he problem of multilabel classification has attracted great interest in the last decade, where each instance can be assigned with a set of multiple class labels simultaneously. It has a wide variety of real-world applications, e.g., automatic image annotations and gene function analysis. Current research on multilabel classification focuses on supervised settings which assume existence of large amounts of labeled training data. However, in many applications, the labeling of multilabeled data is extremely expensive and time consuming, while there are often abundant unlabeled data available. In this paper, we study the problem of transductive multilabel learning and propose a novel solution, called Trasductive Multilabel Classification (TraM), to effectively assign a set of multiple labels to each instance. Different from supervised multilabel learning methods, we estimate the label sets of the unlabeled instances effectively by utilizing the information from both labeled and unlabeled data. We first formulate the transductive multilabel learning as an optimization problem of estimating label concept compositions. Then, we derive a closed-form solution to this optimization problem and propose an effective algorithm to assign label sets to the unlabeled instances. Empirical studies on several real-world multilabel learning tasks demonstrate that our TraM method can effectively boost the performance of multilabel classification by using both labeled and unlabeled data.
7FC37865	Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.
7D3F70C2	In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available.
7CA1F170	Given an edge-weighted graph , a subset , an integer and a real , the minimum subpartition problem asks to find a family of k nonempty disjoint subsets with , , so as to minimize , where denotes the total weight of edges between X and . In this paper, we show that the minimum subpartition problem can be solved in time. The result is then applied to the minimum k-way cut problem and the graph strength problem to improve the previous best time bounds of 2-approximation algorithms for these problems to .
7CEC1AD4	One of the main difficulties in machine learning is how to solve large-scale problems effectively, and the labeled data are limited and fairly expensive to obtain. In this paper a new semi-supervised SVM algorithm is proposed. It applies tri-training to improve SVM. The semi-supervised SVM makes use of the large number of unlabeled data to modify the classifiers iteratively. Although tri-training doesn't put any constraints on the classifier, the proposed method uses three different SVMs as the classification algorithm. Experiments on UCI datasets show that tri-training can improve the classification accuracy of SVM and can increase the difference of classifiers, the accuracy of final classifier will be higher. Theoretical analysis and experiments show that the proposed method has excellent accuracy and classification speed.
7DE594CD	In image classification and other learning-based object recognition tasks, it is often tedious and expensive to label large training data sets. Discriminant-EM (DEM), proposed as a semi-supervised learning framework, takes both labeled and unlabeled data to learn classifiers. The paper extends the linear DEM to a nonlinear kernel algorithm, KDEM, and evaluates KDEM on both benchmark image databases and synthetic data. Various comparisons with other state-of-the-art learning techniques are investigated.
7D78459A	In semi-supervised classification boosting, a similarity measure is demanded in order to measure the distance between samples (both labeled and unlabeled). However, most of the existing methods employed a simple metric, such as Euclidian distance, which may not be able to truly reflect the actual similarity/distance. This paper presents a novel similarity learning method based on the geodesic distance. It incorporates the manifold, margin and the density information of the data which is important in semi-supervised classification. The proposed similarity measure is then applied to a semi-supervised multi-class boosting (SSMB) algorithm. In turn, the three semi-supervised assumptions, namely smoothness, low density separation and manifold assumption, are all satisfied. We evaluate the proposed method on UCI databases. Experimental results show that the SSMB algorithm with proposed similarity measure outperforms the SSMB algorithm with Euclidian distance.
814FEDA0	In the short time since publication of Boykov and Jolly's seminal paper [2001], graph cuts have become well established as a leading method in 2D and 3D semi-automated image segmentation. Although this approach is computationally feasible for many tasks, the memory overhead and supralinear time complexity of leading algorithms results in an excessive computational burden for high-resolution data. In this paper, we introduce a multilevel banded heuristic for computation of graph cuts that is motivated by the well-known narrow band algorithm in level set computation. We perform a number of numerical experiments to show that this heuristic drastically reduces both the running time and the memory consumption of graph cuts while producing nearly the same segmentation result as the conventional graph cuts. Additionally, we are able to characterize the type of segmentation target for which our multilevel banded heuristic yields different results from the conventional graph cuts. The proposed method has been applied to both 2D and 3D images with promising results.
7D7D5508	Geodesic active contours and graph cuts are two standard image segmentation techniques. We introduce a new segmentation method combining some of their benefits. Our main intuition is that any cut on a graph embedded in some continuous space can be interpreted as a contour (in 2D) or a surface (in 3D). We show how to build a grid graph and set its edge weights so that the cost of cuts is arbitrarily close to the length (area) of the corresponding contours (surfaces) for any anisotropic Riemannian metric. There are two interesting consequences of this technical result. First, graph cut algorithms can be used to find globally minimum geodesic contours (minimal surfaces in 3D) under arbitrary Riemannian metric for a given set of boundary conditions. Second, we show how to minimize metrication artifacts in existing graph-cut based methods in vision. Theoretically speaking, our work provides an interesting link between several branches of mathematics -differential geometry, integral geometry, and combinatorial optimization. The main technical problem is solved using Cauchy-Crofton formula from integral geometry.
7E77CFA2	Several new algorithms for visual correspondence based on graph cuts have recently been developed. While these methods give very strong results in practice, they do not handle occlusions properly. Specifically, they treat the two input images asymmetrically, and they do not ensure that a pixel corresponds to at most one pixel in the other image. In this paper, we present a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms. We give experimental results for stereo as well as motion, which demonstrate that our method performs well both at detecting occlusions and computing disparities.
7E68E6DC	In this paper, we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP estimates for dynamically changing MRF models of labeling problems in computer vision, such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, we show how to efficiently compute the maximum flow in a modified version of the graph. Our experiments showed that the time taken by our algorithm is roughly proportional to the number of edges whose weights were different in the two graphs. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video and compare it with the best known st-mincut algorithm. The results show that the dynamic graph cut algorithm is much faster than its static counterpart and enables real time image segmentation. It should be noted that our method is generic and can be used to yield similar improvements in many other cases that involve dynamic change in the graph
8014E25E	Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.
8150BAE6	In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as "object" or "background" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both "object" and "background" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.
807F0055	Most existing semi-supervised methods implemented either the cluster assumption or the manifold assumption. The performance will degrade if the assumption was not proper for the data. A method was proposed by combining both the cluster assumption and the manifold assumption. A semi-supervised kernel which reflected geometric information of the samples was constructed through warping the Reproducing Kernel Hilbert Space. Then the semi-supervised kernel was used in SVM which was based on cluster assumption, and a progressive learning procedure was used in the proposed method. Experiments had been took on synthetic and real data sets, and the results showed that, compared with the progressive SVM with common kernel and the standard SVM with semi supervised kernel, the proposed method using semi-supervised kernel in progressive SVM had competitive performance.
796A74A9	This paper proposes a novel unsupervised cosegmentation method which automatically segments the common objects in multiple images. It designs a simple superpixel matching algorithm to explore the inter-image similarity. It then constructs the object mask for each image using the matched superpixels. This object mask is a convex hull potentially containing the common objects and some backgrounds. Finally, it applies a new FastGrabCut algorithm, an improved GrabCut algorithm, on the object mask to simultaneously improve the segmentation efficiency and maintain the segmentation accuracy. This FastGrabcut algorithm introduces preliminary classification to accelerate convergence. It uses Expectation Maximization (EM) algorithm to estimate optimal Gaussian Mixture Model(GMM) parameters of the object and background and then applies Graph Cuts to minimize the energy function for each image. Experimental results on the iCoseg dataset demonstrate the accuracy and robustness of our cosegmentation method.
8010A489	A modified version for semi-supervised learning algorithm with local and global consistency was proposed in this paper. The new method adds the label information, and adopts the geodesic distance rather than Euclidean distance as the measure of the difference between two data points when conducting calculation. In addition we add class prior knowledge. It was found that the effect of class prior knowledge was different between under high label rate and low label rate. The experimental results show that the changes attain the satisfying classification performance better than the original algorithms.
08451859	An inescapable bottleneck with learning from large data sets is the high cost of labeling training data. Unsupervised learning methods have promised to lower the cost of tagging by leveraging notions of similarity among data points to assign tags. However, unsupervised and semi-supervised learning techniques often provide poor results due to errors in estimation. We look at methods that guide the allocation of human effort for labeling data so as to get the greatest boosts in discriminatory power with increasing amounts of work. We focus on the application of value of information to Gaussian Process classifiers and explore the effectiveness of the method on the task of classifying voice messages.
7E51F583	Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We present motivation and detailed technical description of the basic combinatorial optimization framework for image segmentation via s/t graph cuts. After the general concept of using binary graph cut algorithms for object segmentation was first proposed and tested in Boykov and Jolly (2001), this idea was widely studied in computer vision and graphics communities. We provide links to a large number of known extensions based on iterative parameter re-estimation and learning, multi-scale or hierarchical approaches, narrow bands, and other techniques for demanding photo, video, and medical applications.
8053771F	Graph matching is an essential problem in computer vision that has been successfully applied to 2D and 3D feature matching and object recognition. Despite its importance, little has been published on learning the parameters that control graph matching, even though learning has been shown to be vital for improving the matching rate. In this paper we show how to perform parameter learning in an unsupervised fashion, that is when no correct correspondences between graphs are given during training. Our experiments reveal that unsupervised learning compares favorably to the supervised case, both in terms of efficiency and quality, while avoiding the tedious manual labeling of ground truth correspondences. We verify experimentally that our learning method can improve the performance of several state-of-the art graph matching algorithms. We also show that a similar method can be successfully applied to parameter learning for graphical models and demonstrate its effectiveness empirically.
75DC2DD5	To understand text contents better, many research efforts have been made exploring detection and classification of the semantic relation between a concept pair. As described herein, we present our study of a semantic relation classification task as a graph-based multi-view learning task. Semantic relation can be naturally represented from two views: entity pair view and context view. Then we construct a weighted complete graph for each view and a bipartite graph to combine information of different views. An instance’s label score is propagated on each intra-view graph and inter-view graph. The proposed algorithm is evaluated using the concept description language for natural language (CDL) corpus and SemEval-2007 Task 04 dataset. The experimental results validate its effectiveness.
789986D9	We propose a graph cut based automatic method for prostate segmentation using image feature, context information and semantic knowledge. A volume of interest (VOI) is first identified using supervoxel oversegmentation and their subsequent classification of the supervoxels. All voxels within the VOI are labeled prostate or background using graph cuts. Semantic information obtained from Random forest (RF) classifiers is used to formulate the smoothness cost. Use of context and semantic information contributes to higher segmentation accuracy than competing methods.
7E0AE1E4	Segmentation of tree-like structure within medical imaging modalities, such as x-ray, MRI, ultrasound, etc., is an important step for analyzing branching patterns involved in many anatomic structures. However, images acquired using these different acquisition techniques frequently have features of poor contrast, blurring and noise, and therefore the segmentation result of traditional image segmentation methods may not be satisfactory. In this paper, we propose a framework for accurate segmentation of the ductal network in x-ray galactograms. Our approach is based on the graph cut algorithm and texture analysis to extract features of skewness, coarseness, contrast, energy and fractal dimension. The features are chosen to capture not only architectural variability of the enhanced ductal tree, but also spatial variations among pixels. The proposed approach was applied to a dataset of 20 galactographic images. We performed receiver operating characteristic (ROC) curve analysis to assess the accuracy. The area under the ROC curve observed was 0.76, indicating that our approach may potentially assist clinicians in the interpretation of breast images and facilitate the investigation of relationships among structure and texture of the branching patterns.
5FC2C7FC	Hierarchical prosody structure generation is a key component for a speech synthesis system. One major feature of the prosody of Mandarin Chinese speech flow is prosodic phrase grouping. In this paper we proposed an approach for prediction of Chinese prosodic phrase boundaries from a limited amount of labeled training examples and some amount of unlabeled data using conditional random fields. Some useful unlabeled data are chosen based on the assigned labels and the prediction probabilities of the current learned model. The useful unlabeled data is then exploited to improve the learning. Experiments show that the approach improves overall performance. The precision and recall ratio are improved.
6D5FAEC4	Recommending phrases from web pages for advertisers to bid on against search engine queries is an important research problem with direct commercial impact. Most approaches have found it infeasible to determine the relevance of all possible queries to a given ad landing page and have focussed on making recommendations from a small set of phrases extracted (and expanded) from the page using NLP and ranking based techniques. In this paper, we eschew this paradigm, and demonstrate that it is possible to efficiently predict the relevant subset of queries from a large set of monetizable ones by posing the problem as a multi-label learning task with each query being represented by a separate label.We develop Multi-label Random Forests to tackle problems with millions of labels. Our proposed classifier has prediction costs that are logarithmic in the number of labels and can make predictions in a few milliseconds using 10 Gb of RAM. We demonstrate that it is possible to generate training data for our classifier automatically from click logs without any human annotation or intervention. We train our classifier on tens of millions of labels, features and training points in less than two days on a thousand node cluster. We develop a sparse semi-supervised multi-label learning formulation to deal with training set biases and noisy labels harvested automatically from the click logs. This formulation is used to infer a belief in the state of each label for each training ad and the random forest classifier is extended to train on these beliefs rather than the given labels. Experiments reveal significant gains over ranking and NLP based techniques on a large test set of 5 million ads using multiple metrics.
7D656EF1	In this paper, we propose a method of object recognition and segmentation using Scale-Invariant Feature Transform (SIFT) and Graph Cuts. SIFT feature is invariant for rotations, scale changes, and illumination changes and it is often used for object recognition. However, in previous object recognition work using SIFT, the object region is simply presumed by the affine-transformation and the accurate object region was not segmented. On the other hand, Graph Cuts is proposed as a segmentation method of a detail object region. But it was necessary to give seeds manually. By combing SIFT and Graph Cuts, in our method, the existence of objects is recognized first by vote processing of SIFT keypoints. After that, the object region is cut out by Graph Cuts using SIFT keypoints as seeds. Thanks to this combination, both recognition and segmentation are performed automatically under cluttered backgrounds including occlusion.
622CA6D3	The graph cut model has been widely used in image segmentation, in which both the region and boundary information play important roles for accurate segmentation. However, how to effectively model and combine these two information is still a challenge. In this paper, we improve the conventional graph cut methods by combining the region and boundary information with an effective and straightforward way. When modeling the region information, the component-wise expectation–maximization for Gaussian mixtures algorithm is used to learn the parameters of the prior knowledge. When modeling the boundary information, the weighting patch is used to represent the similarities of the neighboring pixels. Then the region and boundary information are combined by a weighting parameter, where the weight is small for boundary pixels and is large for non-boundary pixels. Finally, experiments on various images from the Berkeley and MSRC data sets were conducted to demonstrate the effectiveness of the proposed method.
811B19DA	Data describing networks—such as communication networks, transaction networks, disease transmission networks, collaboration networks, etc.—are becoming increasingly available. While observational data can be useful, it often only hints at the actual underlying process that governs interactions and attributes. For example, an email communication network provides insight into its users and their relationships, but is not the same as the “real” underlying social network. In this article, we introduce the problem of graph identification, i.e., discovering the latent graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence. This entails solving several canonical problems in network analysis: entity resolution (determining when two observations correspond to the same entity), link prediction (inferring the existence of links), and node labeling (inferring hidden attributes). While each of these subproblems has been well studied in isolation, here we consider them as a single, collective task. We present a simple, yet novel, approach to address all three subproblems simultaneously. Our approach, which we refer to as C3, consists of a collection of Coupled Collective Classifiers that are applied iteratively to propagate inferred information among the subproblems. We consider variants of C3 using different learning and inference techniques and empirically demonstrate that C3 is superior, both in terms of predictive accuracy and running time, to state-of-the-art probabilistic approaches on four real problems.
5E800C58	An N-dimensional image is divided into “object” and “background” segments using a graph cut approach. A graph is formed by connecting all pairs of neighboring image pixels (voxels) by weighted edges. Certain pixels (voxels) have to be a priori identified as object or background seeds providing necessary clues about the image content. Our objective is to find the cheapest way to cut the edges in the graph so that the object seeds are completely separated from the background seeds. If the edge cost is a decreasing function of the local intensity gradient then the minimum cost cut should produce an object/background segmentation with compact boundaries along the high intensity gradient values in the image. An efficient, globally optimal solution is possible via standard min-cut/max-flow algorithms for graphs with two terminals. We applied this technique to interactively segment organs in various 2D and 3D medical images.
76D5AD7F	In this letter, a modified algorithm is proposed to extend 2-class semi-supervised learning on Laplacian eigenmaps to multi-class learning problems. The modified algorithm significantly increases its learning speed, and at the same time attains a satisfactory classification performance that is not lower than the original algorithm.
765578E2	The fuzzy c-partition entropy has been widely adopted as a global optimization technique for finding the optimized thresholds for multilevel image segmentation. However, it involves expensive computation as the number of thresholds increases and often yields noisy segmentation results since spatial coherence is not enforced. In this paper, an iterative calculation scheme is presented for reducing redundant computations in entropy evaluation. The efficiency of threshold selection is further improved through utilizing the artificial bee colony algorithm as the optimization technique. Finally, instead of performing thresholding for each pixel independently, the presented algorithm oversegments the input image into small regions and uses the probabilities of fuzzy events to define the costs of different label assignments for each region. The final segmentation results is computed using graph cut, which produces smooth segmentation results. The experimental results demonstrate the presented iterative calculation scheme can greatly reduce the running time and keep it stable as the number of required thresholds increases. Quantitative evaluations over 20 classic images also show that the presented algorithm outperforms existing multilevel segmentation approaches.
756C1A4A	The problem of segmenting a foreground object out from its complex background is of great interest in image processing and computer vision. Many interactive segmentation algorithms such as graph cut have been successfully developed. In this paper, we present four technical components to improve graph cut based algorithms, which are combining both color and texture information for graph cut, including structure tensors in the graph cut model, incorporating active contours into the segmentation process, and using a “softbrush” tool to impose soft constraints to refine problematic boundaries. The integration of these components provides an interactive segmentation method that overcomes the difficulties of previous segmentation algorithms in handling images containing textures or low contrast boundaries and producing a smooth and accurate segmentation boundary. Experiments on various images from the Brodatz, Berkeley and MSRC data sets are conducted and the experimental results demonstrate the high effectiveness of the proposed method to a wide range of images.
7AFB42E2	This paper proposes a novel texture segmentation approach using independent-scale component-wise Riemannian-covariance Gaussian mixture model (ICRGMM) in Kullback–Leibler (KL) measure based multi-scale nonlinear structure tensor (MSNST) space. We use the independent-scale distribution and full-covariance structure to replace the covariant-scale distribution and 1D-variance structure used in our previous research. To construct the optimal full-covariance structure, we define the full-covariance on KL, Euclidean, log-Euclidean, and Riemannian gradient mappings, and compare their performances. The comparison experiments demonstrate that the Riemannian gradient mapping leads to its optimum properties over other choices when constructing the full-covariance. To estimate and update the statistical parameters more accurately, the component-wise expectation-maximization for mixtures (CEM2) algorithm is proposed instead of the originally used K-means algorithm. The superiority of the proposed ICRGMM has been demonstrated based on texture clustering and Graph Cuts based texture segmentation using a large number of synthesis texture images and real natural scene textured images, and further analyzed in terms of error ratio and modified F-measure, respectively.
7FED3742	This paper proposes a constrained clustering method that is based on a graph-cut problem formalized by SDP (Semi-Definite Programming). Our SDP approach has the advantage of convenient constraint utilization compared with conventional spectral clustering methods. The algorithm starts from a single cluster of a complete dataset and repeatedly selects the largest cluster, which it then divides into two clusters by swapping rows and columns of a relational label matrix obtained by solving the maximum graph-cut problem. This swapping procedure is effective because we can create clusters without any computationally heavy matrix decomposition process to obtain a cluster label for each data. The results of experiments using a Web document dataset demonstrated that our method outperformed other conventional and the state of the art clustering methods in many cases. Hence we consider our clustering provides a promising basic method to interactive Web clustering.
76D55405	The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?
793E2B82	Techniques for automatic query expansion from top retrieved documents have shown promise for improving retrieval effectiveness on large collections; however, they often rely on an empirical ground, and there is a shortage of cross-system comparisons. Using ideas from Information Theory, we present a computationally simple and theoretically justified method for assigning scores to candidate expansion terms. Such scores are used to select and weight expansion terms within Rocchio's framework for query reweigthing. We compare ranking with information-theoretic query expansion versus ranking with other query expansion techniques, showing that the former achieves better retrieval effectiveness on several performance measures. We also discuss the effect on retrieval effectiveness of the main parameters involved in automatic query expansion, such as data sparseness, query difficulty, number of selected documents, and number of selected terms, pointing out interesting relationships.
7BB25BBA	In this paper, we investigate medical students medical search behavior on a medical domain. We use two behavioral signals: detailed query analysis (qualitative and quantitative) and task completion time to understand how medical students perform medical searches based on varying task complexity. We also investigate how task complexity and topic familiarity affect search behavior. We gathered 80 interactive search sessions from an exploratory survey with 20 medical students. We observe information searching behavior using 3 simulated work task scenarios and 1 personal scenario. We present quantitative results from two perspectives: overall and user perceived task complexity. We also analyze query properties from a qualitative aspect. Our results show task complexity and topic familiarity affect search behavior of medical students. In some cases, medical students demonstrate different search traits on a personal task in comparison to the simulated work task scenarios. These findings help us better understand medical search behavior. Medical search engines can use these findings to detect and adapt to medical students' search behavior to enhance a student's search experience.
59F41337	In information retrieval, queries can fail to find documents due to mismatch in terminology. Query expansion is a well-known technique addressing this problem, where additional query terms are automatically chosen from highly ranked documents, and it has been shown to be effective at improving query performance. However, current techniques for query expansion use fixed values for key parameters, determined by tuning on test collections. In this paper we show that these parameters may not be generally applicable, and more significantly that the assumption that the same parameter settings can be used for all queries is invalid. Using detailed experiments with two test collections, we demonstrate that new methods for choosing parameters must be found. However, our experiments also demonstrate that there is considerable further scope for improvement to effectiveness through better query expansion.
7F448138	With the population of information technique and Web application, Web information retrieval has became an essential part of work, study and life. As the problems of retrieval increasing, many researches pay more attention on retrieval technique. The study presented in this paper proposes the query expansion technique based on ontology. It uses the rich semantic knowledge of ontology to upgrade the retrieval based on keywords to concepts, and combines it with the specialized engine to improve retrieval effect and efficiency. The paper also takes patent information for example to explain its application at the end.
5CE38F5A	This paper presents an ongoing research on the comparison of ontological query expansion methods. Query Expansion is a technique that aims to enhance the results of a search by adding terms to the search query; today, it is a very important research topic in the semantic web and information retrieval areas. Although many efforts have been made from the theoretical point of view to implement effective and general methods for expanding queries, based on both statistical and ontological approaches, the practical applicability of them is nowadays restricted to few and very specific domains. The aim of this paper is the definition of a platform for the implementation of a subset of such methods, in order to make comparisons among them and try to define how and when use ontological QE. This work is part of JUMAS, a research project funded by European Community where query expansion is used to support the retrieval of significant information from audio–video transcriptions in the legal domain.
7CD3D243	Enterprise search is important, and the search quality has a direct impact on the productivity of an enterprise. Many information needs of enterprise search center around entities. Intuitively, information related to the entities mentioned in the query, such as related entities, would be useful to reformulate the query and improve the retrieval performance. However, most existing studies on query expansion are term-centric. In this paper, we propose a novel entity-centric query expansion framework for enterprise search. Specifically, given a query containing entities, we first utilize both unstructured and structured information to find entities that are related to the ones in the query. We then discuss how to adapt existing feedback methods to use the related entities to improve search quality. Experiment results show that the proposed entity-centric query expansion strategy is more effective to improve the search performance than the state-of-the-art pseudo feedback methods on longer, natural language-like queries with entities.
79904BF4	Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26%--29%, and 18%--20% over an optimised, conventional expansion approach.
80BCD8D0	We introduce a new theoretical derivation, evaluation methods, and extensive empirical analysis for an automatic query expansion framework in which model estimation is cast as a robust constrained optimization problem. This framework provides a powerful method for modeling and solving complex expansion problems, by allowing multiple sources of domain knowledge or evidence to be encoded as simultaneous optimization constraints. Our robust optimization approach provides a clean theoretical way to model not only expansion benefit, but also expansion risk, by optimizing over uncertainty sets for the data. In addition, we introduce risk-reward curves to visualize expansion algorithm performance and analyze parameter sensitivity. We show that a robust approach significantly reduces the number and magnitude of expansion failures for a strong baseline algorithm, with no loss in average gain. Our approach is implemented as a highly efficient post-processing step that assumes little about the baseline expansion method used as input, making it easy to apply to existing expansion methods. We provide analysis showing that this approach is a natural and effective way to do selective expansion, automatically reducing or avoiding expansion in risky scenarios, and successfully attenuating noise in poor baseline methods.
69FD3F3F	This paper describes our work at CLEF 2007 Robust Task. We have applied local query expansion using windows of terms, but considering different measures of robustness during the training phase in order to optimize the performance: MAP, GMAP, MMR, GS@10, P@10, number of failed topics, number of topics below 0.1 MAP, and number of topics with P@10=0. The results were not disappointing, but no settings were found that simultaneously improved all measures. A key issue for us was to decide which set of measures we had to select for optimization.This year all our runs also gave good rankings, both base runs and expanded ones. However, our expansion technique does not improve significantly the retrieval performance. At TREC and CLEF Robust Tasks other expansion techniques have been used to improve robustness, but results were not uniform. In conclusion, regarding robustness the objective must be to make good information retrieval systems, rather than to tune some query expansion techniques.
5A10CFD2	In this paper we explain experiments in the medical information retrieval task (ImageCLEFmed). We experimented with query expansion and the amount of textual information obtained from the collection. For expansion, we carried out experiments using MeSH ontology and UMLS separately. With respect to textual collection, we produced three different collections, the first one with caption and title, the second one with caption, title and the text of the section where the image appears, and the third one with the full text article. Moreover, we experimented with textual and visual search, along with the combination of these two results. For image retrieval we used the results generated by the FIRE software. The best results were obtained using MeSH query expansion on shortest textual collection (only caption and title) merging with the FIRE results.
5C1D7D00	Query expansion techniques are used to find the desired set of query terms to improve retrieval performance. One of the limitations with the query expansion techniques is that a query is often expanded only by the linguistic features of terms. This paper presents a novel semantic query expansion technique that combines association rules with ontologies and information retrieval techniques. We propose to use the association rule discovery to find good candidate terms to improve the retrieval performance. These candidate terms are automatically derived from collections and added to the original query. Our method is differentiated from others in that 1) it utilizes the semantics as well as linguistic properties of unstructured text corpus and 2) it makes use of contextual properties of important terms discovered by association rules. Experiments conducted on a subset of TREC collections give quite encouraging results. We achieve from 15.49% to 20.98% improvement in term of P@20 with TREC5 ad hoc queries.
7AA55F5D	Novice users often do not have enough domain knowledge to create good queries for searching information on-line. To help alleviate the situation, exploration techniques have been used to increase the diversity of the search results so that not only those explicitly asked will be returned, but also those potentially relevant ones will be returned too. Most existing approaches, such as collaborative filtering, do not allow the level of exploration to be controlled. Consequently, the search results can be very different from what is expected. We propose an exploration strategy that performs intelligent query processing by first searching usable old queries, and then utilising them to adapt the current query, with the hope that the adapted query will be more relevant to the user's areas of interest. We applied the proposed strategy to the implementation of a personal information assistant (PIA) set up for user evaluation for 3 months. The experimental results showed that the proposed exploration method outperformed collaborative filtering, and mutation and crossover methods by around 25% in terms of the elimination of off-topic results. 
5FB22737	There is increasing interest in improving the robustness of IR systems, i.e. their effectiveness on difficult queries. A system is robust when it achieves both a high Mean Average Precision (MAP) value for the entire set of topics and a significant MAP value over its worst X topics (MAP(X)). It is a well known fact that Query Expansion (QE) increases global MAP but hurts the performance on the worst topics. A selective application of QE would thus be a natural answer to obtain a more robust retrieval system.We define two information theoretic functions which are shown to be correlated respectively with the average precision and with the increase of average precision under the application of QE. The second measure is used to selectively apply QE. This method achieves a performance similar to that with unexpanded method on the worst topics, and better performance than full QE on the whole set of topics.
016F5BB5	The use of temporal data extracted from text, to improve the effectiveness of Information Retrieval systems, has recently been the focus of important research work. Our research hypothesis is that the usage of the temporal relationship between words improves the Information Retrieval results. For this purpose, the texts are temporally segmented to establish a relationship between words and dates found in texts. This approach was applied in Query Expansion systems, using a collection with Portuguese newspaper texts. The results showed that the use of the temporality of words can enhance retrieval effectiveness. In particular for time-sensitive queries, we achieved 9.5% improvement in Precision@10. To our knowledge, this is the first work using temporal text segmentation to improve retrieval results.
772E4A41	This paper introduces a future and past search engine, ChronoSeeker, which can help users to develop long-term strategies for their organizations. To provide on-demand searches, we tackled two technical issues: (1) organizing efficient event searches and (2) filtering out noises from search results. Our system employed query expansion with typical expressions related to event information such as year expressions, temporal modifiers, and context terms for efficient event searches. We utilized a machine-learning technique of filtering noise to classify candidates into information or non-event information, using heuristic features and lexical patterns derived from a text-mining approach. Our experiment revealed that filtering achieved an 85% F-measure, and that query expansion could collect dozens more events than those without expansion. 
811EA444	Tourism information is dynamic and travel routes and decisions are dependent on highly varying factors such as perceived attractive sites, weather conditions, prices, transportation, accommodation, holidays, economic changes and so on. Travel guidebooks and present search engines completely lack this potential for dialog. Travel information search activities involved planning, decision making, anticipation of the trip with other people. The community (network of people) may act as a gateway to the information repository, when a tourist is not able to find the right information himself/herself or does not know about his/her information need. This leads to the collaboration for searching tourism information. Considering the following issues we are motivated to study this topic.
7B145537	This paper examines the meaning of context in relation to ontology based query expansion and contains a review of query expansion approaches. The various query expansion approaches include relevance feedback, corpus dependent knowledge models and corpus independent knowledge models. Case studies detailing query expansion using domain-specific and domain-independent ontologies are also included. The penultimate section attempts to synthesise the information obtained from the review and provide success factors in using an ontology for query expansion. Finally the area of further research in applying context from an ontology to query expansion within a newswire domain is described.
05137BD4	This paper evaluates the retrieval effectiveness of query expansion strategies on a MEDLINE test collection using Cornell University's SMART retrieval system. Three expansion strategies are tested on their ability to identify appropriate McSH terms for user queries: expansion using an inter-field statistical thesaurus, expansion via retrieval feedback and expansion using a combined approach. These expansion strategies do not require prior relevance decisions. The study compares retrieval effectiveness using the original unexpanded and the alternative expanded user queries on a collection of 75 queries and 2334 MEDLINE citations. Retrieval effectiveness is assessed using eleven point average precision scores (11-AvgP). The combination of expansion using the thesaurus followed by retrieval feedback gives the best improvement of 17% over a baseline performance of 0.5169 11-AvgP. However this improvement is almost identical to that achieved by expansion via retrieval feedback (16.4%). Query expansion using the inter-field thesaurus gives a significant but lower performance improvement (9.9%) over the same baseline. This study recommends query expansion using retrieval feedback for adding McSH search terms to a user's initial query.
7C2981BC	This paper proposes a novel query expansion method to improve accuracy of text retrieval systems. Our method makes use of a minimal relevance feedback to expand the initial query with a structured representation composed of weighted pairs of words. Such a structure is obtained from the relevance feedback through a method for pairs of words selection based on the Probabilistic Topic Model. We compared our method with other baseline query expansion schemes and methods. Evaluations performed on TREC-8 demonstrated the effectiveness of the proposed method with respect to the baseline.
77808CC5	An evaluation of graph theoretical clusters of index terms which can be extracted from an automatically indexed document collection, and the effects of employing such clusters in automatic document retrieval is described. The graph theoretical clusters which were developed from six data bases under two different cluster definitions were analyzed for average size and related data. The clusters were also used to expand the queries in each of six data bases to determine the effect of the expansions on the document retrieval results.Although a large variety of clusters and associated query expansions were obtained, no significant improvements in the document retrieval performance were achieved. In some cases, however, significant degradations in the retrieval performance occurred. Although seemingly meaningful clusters can be obtained, the results indicate that the effort involved in finding clusters and adding the clustered terms to queries is far too great to warrant their use in an operational system. The data bases employed were relatively small, and the authors caution against generalizing these results to larger data bases or other situations.
5CEC0571	This paper describes a new method to automatically obtain a new thesaurus which exploits previously collected information. Our method relies on different resources, such as a text collection, a set of source thesauri and other linguistic resources. We have applied different techniques in the different phases of the process. By applying indexing techniques, the text collection provides the set of initial terms of interest for the new thesaurus. Then, these terms are searched in the source thesauri, providing the initial structure of the new thesaurus. Finally, the new thesaurus is enriched by searching for new relationships among its terms. These relationships are first detected using similarity measures and then are characterized with a type (equivalence, hierarchy or associativity) by using different linguistic resources. We have based the system evaluation on the results obtained with and without the thesaurus in an information retrieval task proposed by the Cross-Language Evaluation Forum (CLEF). The results of these experiments have revealed a clear improvement of the performance.
7C318F32	The evaluation of 6 ranking algorithms for the ranking of terms for query expansion is discussed within the context of an investigation of interactive query expansion and relevance feedback in a real operational environment. The yardstick for the evaluation was provided by the user relevance judgements on the lists of the candidate terms for query expansion. The evaluation focuses on the similarities in the performance of the different algorithms and how the algorithms with similar performance treat terms.
767E699B	Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We address the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness.
795DA831	Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed-back model by resampling a given query's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.
7732CCC0	Most casual users of IR systems type short queries. Recent research has shown that adding new words to these queries via odhoc feedback improves the re- trieval effectiveness of such queries. We investigate ways to improve this query expansion process by refining the set of documents used in feedback. We start by using manually formulated Boolean filters along with proxim- ity constraints. Our approach is similar to the one pro- posed by Hearst(l2). Next, we investigate a completely automatic method that makes use of term cooccurrence information to estimate word correlation. Experimental results show that refining the set of documents used in query expansion often prevents the query drift caused by blind expansion and yields substantial improvements in retrieval effectiveness, both in terms of average preci- sion and precision in the top twenty documents. More importantly, the fully automatic approach developed in this study performs competitively with the best manual approach and requires little computational overhead.
7D79605B	Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.
75FC25F4	Automatic query expansion has long been suggested as a technique for dealing with the fundamental issue of word mismatch in information retrieval. A number of approaches to ezpanrnion have been studied and, more recently, attention has focused on techniques that analyze the corpus to discover word relationship (global techniques) and those that analyze documents retrieved by the initial quer~ ( local feedback). In this paper, we compare the effectiveness of these approaches and show that, although global analysis haa some advantages, local analysia is generally more effective. We also show that using global analysis techniques, such as word contezt and phrase structure, on the local aet of documents produces re- sults that are both more effective and more predictable than simple local feedback
78D3F0A3	Research into both the algorithmic and human approaches to information retrieval is required to improve information retrieval system design and database searching effectiveness. This study uses the human approach to examine the sources and effectiveness of search terms selected during mediated interactive information retrieval. The study focuses on determining the retrieval effectiveness of search terms identified by users and intermediaries from retrieved items during term relevance feedback. Results show that terms selected from particular database fields of retrieved items during term relevance feedback (TRF) were more effective than search terms from the intermediary, database thesauri or users' domain knowledge during the interaction, but not as effective as terms from the users' written question statements. Implications for the design and testing of automatic relevance feedback techniques that place greater emphasis on these sources and the practice of database searching are also discussed.
7AB99E2A	Many queries are submitted to search engines by right-clicking the marked text (i.e., the query) in Web browsers. Because the document being read by the searcher often provides sufficient contextual information for the query, search engine could provide much more relevant search results if the query is augmented by the contextual information captured from the source document. How to extract the right contextual information from the source document is the main focus of this study. To this end, we evaluate 7 text component extraction schemes, and 5 feature extraction schemes. The former determines from which text component (e.g., title, meta-data, or paragraphs containing the selected query) to extract contextual information; the latter determines which words or phrases to extract. In total 35 combinations are evaluated and our evaluation results show that noun phrases extracted from all paragraphs that contain the query word is the best option.
7B2AB066	In an era of online retrieval, it is appropriate to offer guidance to users wishing to improve their initial queries. One form of such guidance could be short lists of suggested terms gathered from feedback, nearest neighbors, and term variants of original query terms. To verify this approach, a series of experiments were run using the Cranfield test collection to discover techniques to select terms for these lists that would be effective for further retrieval. The results show that significant improvement can be expected from this approach to query expansion.
7626898C	The effectiveness of queries in information retrieval can be improved through query expansion. This technique automatically introduces additional query terms that are statistically likely to match documents on the intended topic. However, query expansion techniques rely on fixed parameters. Our investigation of the effect of varying these parameters shows that the strategy of using fixed values is questionable.
75762437	Users normally facing difficulties to expresses their information needs into a query format that the search system can use to process. Therefore users frequently modify their query with intention to retrieve better results. The goal of this experiment is to compare the precision of the retrieved results by employing domain ontology to reformulate the user query. We proposed to reformulate the query (initially in natural language) by replacing it with the terms retrieved from ontology. Then we improve the approach by combining the terms from ontology with keyword/s extracted from the initial user's natural language query. Precision measure had been conducted for six queries and the results showed that the idea of combining ontology terms with keyword from query gave a promising result. It achieved higher precision in comparison when using ontology terms alone. This experimental result indicates that query reformulation is more efficient when ontology term/s combined with key element from the user's query.
7AAD636F	Query expansion (QE) and document expansion (DE) have been proved effective for improving the retrieval performance in language modeling approach. However, the issue that which expansion technique is more effective in information retrieval (IR), has not been well studied and discussed. To address this issue, this paper performs an empirical study on QE and DE to examine their effects. Moreover, since QE and DE exploit different corpus structures, we also examine the potential effectiveness of incorporating QE and DE. Experimental results on several TREC test collections show that both QE and DE significantly outperform the classical language model, but the effectiveness of QE and DE is varied in different settings of retrieval. In addition, incorporating QE with DE does not always bring about the best performance.
7FF6DDDA	The ineffectiveness of information retrieval systems often caused by the inaccurate use of keywords in a query. In order to solve the ineffectiveness problem in information retrieval systems, many solutions have been proposed over the years. The most common techniques are revolving around query modification techniques such as query expansion, query refinement, etc. Due to the high similarity in these query modification techniques, people are often confused about their differences. However, few existing survey papers compare their differences. Hence, in this paper, we first briefly discuss the basic technique of query expansion, query suggestion and query refinement, and then make a detailed comparison between these three techniques. We finally show the promising future research trend in the field of query modification.
7EDCAF6E	We present empirical analysis of human searches for information from mobile devices, focusing on temporal dynamics, semantics, and topics of queries. Our analysis is based on a large scale data of mobile search logs over a week period from a major US mobile service provider. We find that human searches appear in bursts over time with the distribution of the query inter arrival time following a power law decay up to a day and decays exponentially beyond. Interestingly, this finding conforms to some other measures of human activity reported in previous studies. We also provide preliminary characterisation results of the semantics and topics of queries, some of which conform to that of previous studies. The results would be of general interest for understanding the dynamics of human activity and, in particular, may be leveraged for the design of mobile services.
8026925D	Utilizing external collections to improve retrieval performance is challenging research because various test collections are created for different purposes. Improving medical information retrieval has also gained much attention as various types of medical documents have become available to researchers ever since they started storing them in machine processable formats. In this paper, we propose an effective method of utilizing external collections based on the pseudo relevance feedback approach. Our method incorporates the structure of external collections in estimating individual components in the final feedback model. Extensive experiments on three medical collections (TREC CDS, CLEF eHealth, and OHSUMED) were performed, and the results were compared with a representative expansion approach utilizing the external collections to show the superiority of our method.
5C6FC0E8	This article presents a new, hybrid approach that projects an initial query result onto global information, yielding a local conceptual overview. Concepts found this way are candidates for query refinement.We show that the resulting conceptual structure after a typical short query of 2 terms, contains refinements that perform just as well as a most accurate query formulation.Subsequently we illustrate that query by navigation is an effective mechanism which in most cases finds the optimal concept in a small number of steps. When an optimal concept is not found, the navigation process still finds an acceptable sub-optimum.
8065B044	This paper reports an evaluation of three methods for the expansion of natural language queries in ranked-out put retrieval systems. The methods are based on term co-oc currence data, on Soundex codes, and on a string similarity measure. Searches for 110 queries in a database of 26,280 titles and abstracts suggest that there is no significant differ ence in retrieval effectiveness between any of these methods and unexpanded searches.
7D7D8752	Term cooccurrence data has been extensively used in document retrieval systems for the identification of indexing terms that are similar to those that have been specified in a user query: these similar terms can then be used to augment the original query statement. Despite the plausibility of this approach to query expansion, the retrieval effectiveness of the expanded queries is often no greater than, or even less than, the effectiveness of the unexpanded queries. This article demonstrates that the similar terms identified by cooccurrence data in a query expansion system tend to occur very frequently in the database that is being searched. Unfortunately, frequent terms tend to discriminate poorly between relevant and nonrelevant documents, and the general effect of query expansion is thus to add terms that do little or nothing to improve the discriminatory power of the original query. © 1991 John Wiley & Sons, Inc.
7EE1FB33	In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR measures beyond expansion from current word-embeddings models and well-established traditional QE methods.
78BB5638	Searching for knowledge in collaborative networked organisations (CNOs) is an important issue as partners must share and use the available knowledge and information spread over their members in the network. Besides that, partners of such networks work in several contexts (roles, activities, processes) and they have naturally different interests. Based on these observations, the aim of this work is to characterise the user context in a CNO and use such a context for customising knowledge search in CNOs. The basic assumption is that the relevance of the search results in the CNO domain is not only defined by the terms of the query but also by the context of the user performing the search. This paper presents an ontology-based model for CNO context as well as a set of rules for query customisation and these elements are framed in an existing framework for knowledge search. Finally, a prototype implementation of the model and rules is presented and some experiments are discussed.
7F0B1B0F	With the open source code movement, code search with the intent of reuse has become increasingly popular. So much so that researchers have been calling it the new facet of software reuse. Although code search differs from general-purpose document search in essential ways, most tools still rely mainly on keywords matched against source code text. Recently, researchers have proposed more sophisticated ways to perform code search, such as including interface definitions in the queries (e.g., return and parameter types of the desired function, along with keywords; called here Interface-Driven Code Search — IDCS). However, to the best of our knowledge, there are few empirical studies that compare traditional keyword-based code search (KBCS) with more advanced approaches such as IDCS. In this paper we describe an experiment that compares the effectiveness of KBCS with IDCS in the task of large-scale code search of auxiliary functions implemented in Java. We also measure the impact of query expansion based on types and WordNet on both approaches. Our experiment involved 36 subjects that produced real-world queries for 16 different auxiliary functions and a repository with more than 2,000,000 Java methods. Results show that the use of types can improve recall and the number of relevant functions returned (#RFR) when combined with query expansion (∼30% improvement in recall, and ∼43% improvement in #RFR). However, a more detailed analysis suggests that in some situations it is best to use keywords only, in particular when these are sufficient to semantically define the desired function.
7C342AEC	In this paper we present the design and evaluation of our biomedical literature searching approaches using the TREC 2004 ad hoc retrieval task in the genomics track. The main approach taken in our system is to expand queries by exploiting the three widely used strategies -local analysis, global analysis, and ontology-based term re-weighting across various search engines. The experimental results show that (1) ontology-based term re-weighting provides the best results among the three query expansion strategies, (2) expanding the initial query with more precise ontology-based term enhances LSI based local analysis substantially, and (3) including context to term re-weighting and LSI further improves the precision. Experimental results also show that the ontology-based term re-weighting with LUCENE or LEMUR search engines increases the average precision by up to 20.3% or 12.1%, respectively, compared to that of the baseline runs. In addition, the LSI-based local analysis increases the average precision by 9.2% with LEMUR search engine. We believe the approaches of the term re-weighting and LSI-based local analysis may be exploited in other bio-medical domains.
7F7D848F	This paper presents the results of an experimental investigation into the effects that some forms of query expansion by term addition or term deletion, have on the retrieval effectiveness of a document retrieval system. The overall search strategy used by a user is an iterative process whereby a set of user-judged relevant documents at any point in the search is used to refine and improve on the remainder of the user's search. At some point during the search, the set of relevant documents found so far can be used to modify the original query, either by the addition or deletion of search terms. This process is called ‘query modification’ or ‘query expansion’. A number of different types of query modification strategies are tried and the results obtained are presented and analysed.
14AE49C6	We describe the participation of the University of Amsterdam’s ILPS group in the blog track at TREC 2008. We mainly explored different ways of using external corpora to expand the original query. In the blog post retrieval task we did not succeed in improving over a simple baseline (equal weights for both the expanded and original query). Obtaining optimal weights for the original and the expanded query remains a subject of investigation. In the blog distillation task we tried to improve over our (strong) baseline using external expansion, but due to differences in the run setup, comparing these runs is hard. Compared to a simpler baseline, we see an improvement for the run using external expansion on the combination of news, Wikipedia and blog posts. 1
02D8DAF0	The focus of our study is zero-hit queries in keyword subject searches and the effort of increasing recall in these cases by reformulating and, then, expanding the initial queries using an external source of knowledge, namely a thesaurus. To this end, the objectives of this study are twofold. First, we perform the mapping of query terms to the thesaurus terms. Second, we use the matched terms to expand the user’s initial query by taking advantage of the thesaurus relations and implementing natural language processing (NLP) techniques. We report on the overall procedure and elaborate on key points and considerations of each step of the process.
7D4465AC	Chinese name translation is a special case of the problem of named entity translation. It is a very challenging problem because there exist many kinds of Romanization systems and some people like to add additional words into their english names. Translating a scholar's name to its corresponding English name could help find information about his academic achievements. In this paper, we provide a classification for Chinese names, and propose a novel approach to mining Chinese name translations from Web corpora. Our approach is based on three kinds of features, namely the phonetic similarity, the smallest distance, and the number of appearances in the neighborhood, to extract name translation candidates by using a query expansion technique and support vector machine (SVM). Experimental results show that our approach can correctly translate the majority of Chinese names.
7E1FECD9	Wikipedia is an online encyclopedia, available in more than 100 languages and comprising over 1 million articles in its English version. If we consider each Wikipedia article as a node and each hyperlink between articles as an arc we have a "Wikigraph", a graph that represents the link structure of Wikipedia. The Wikigraph differs from other Web graphs studied in the literature by the fact that there are explicit timestamps associated with each node's events. This allows us to do a detailed analysis of the Wikipedia evolution over time. In the first part of this study we characterize this evolution in terms of users, editions and articles; in the second part, we depict the temporal evolution of several topological properties of the Wikigraph. The insights obtained from the Wikigraphs can be applied to large Web graphs from which the temporal data is usually not available