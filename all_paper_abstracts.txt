7E0F0E4A	Graphical models provide a powerful general framework for encoding the structure of large-scale estimation problems. However, the graphs describing typical real-world phenomena contain many cycles, making direct estimation procedures prohibitively costly. In this paper, we develop an iterative inference algorithm for general Gaussian graphical models. It operates by exactly solving a series of modified estimation problems on spanning trees embedded within the original cyclic graph. When these subproblems are suitably chosen, the algorithm converges to the correct conditional means. Moreover, and in contrast to many other iterative methods, the tree-based procedures we propose can also be used to calculate exact error variances. Although the conditional mean iteration is effective for quite densely connected graphical models, the error variance computation is most efficient for sparser graphs. In this context, we present a modeling example suggesting that very sparsely connected graphs with cycles may provide significant advantages relative to their tree-structured counterparts, thanks both to the expressive power of these models and to the efficient inference algorithms developed herein. The convergence properties of the proposed tree-based iterations are characterized both analytically and experimentally. In addition, by using the basic tree-based iteration to precondition the conjugate gradient method, we develop an alternative, accelerated iteration that is finitely convergent. Simulation results are presented that demonstrate this algorithm effectiveness on several inference problems, including a prototype distributed sensing application.
7F9FC5C6	Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse.
6049E046	A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.
7E144371	In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.
7FF9B70B	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion. 
81431585	Traditional binary hypothesis testing relies on the precise knowledge of the probability density of an observed random vector conditioned on each hypothesis. However, for many applications, these densities can only be approximated due to limited training data or dynamic changes affecting the observed signal. A classical approach to handle such scenarios of imprecise knowledge is via minimax robust hypothesis testing (RHT), where a test is designed to minimize the worst case performance for all models in the vicinity of the approximated imprecise density. Despite the promise of RHT for robust classification problems, its applications have remained rather limited because RHT in its native form does not scale gracefully with the dimension of the observed random vector. In this paper, we use approximations via probabilistic graphical models, in particular block-tree graphs, to enable computationally tractable algorithms for realizing RHT on high-dimensional data. We quantify the reductions in computational complexity. Experimental results on simulated data and a target recognition problem show minimal loss over a true RHT.
7CF1D0CA	The use of Bayesian networks for classification problems has received a significant amount of recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal of classification (label prediction accuracy). Recent approaches to optimizing classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present boosted Bayesian network classifiers, a framework to combine discriminative data-weighting with generative training of intermediate models. We show that boosted Bayesian network classifiers encompass the basic generative models in isolation, but improve their classification performance when the model structure is suboptimal. We also demonstrate that structure learning is beneficial in the construction of boosted Bayesian network classifiers. On a large suite of benchmark data-sets, this approach outperforms generative graphical models such as naive Bayes and TAN in classification accuracy. Boosted Bayesian network classifiers have comparable or better performance in comparison to other discriminatively trained graphical models including ELR and BNC. Furthermore, boosted Bayesian networks require significantly less training time than the ELR and BNC algorithms.
7EBA7FB7	The literature review presented discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The article avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples.
7A89250C	Graphical Markov models use undirected graphs (UDGs), acyclic directed graphs (ADGs), or (mixed) chain graphs to represent possible dependencies among random variables in a multivariate distribution. Whereas a UDG is uniquely determined by its associated Markov model, this is not true for ADGs or for general chain graphs (which include both UDGs and ADGs as special cases). This paper addresses three questions regarding the equivalence of graphical Markov models: when is a given chain graph Markov equivalent (1) to some UDG? (2) to some (at least one) ADG? (3) to some decomposable UDG? The answers are obtained by means of an extension of Frydenberg (1990) elegant graph-theoretic characterization of the Markov equivalence of chain graphs.
7E62FD1B	In this paper, we consider the problem of learning Gaussian multiresolution (MR) models in which data are only available at the finest scale, and the coarser, hidden variables serve to capture long-distance dependencies. Tree-structured MR models have limited modeling capabilities, as variables at one scale are forced to be uncorrelated with each other conditioned on other scales. We propose a new class of Gaussian MR models in which variables at each scale have sparse conditional covariance structure conditioned on other scales. Our goal is to learn a tree-structured graphical model connecting variables across scales (which translates into sparsity in inverse covariance), while at the same time learning sparse structure for the conditional covariance (not its inverse) within each scale conditioned on other scales. This model leads to an efficient, new inference algorithm that is similar to multipole methods in computational physics. We demonstrate the modeling and inference advantages of our approach over methods that use MR tree models and single-scale approximation methods that do not use hidden variables.
7A730712	Markov networks are a common class of graphical models used in machine learning. Such models use an undirected graph to capture dependency information among random variables in a joint probability distribution. Once one has chosen to use a Markov network model, one aims to choose the model that best explains the data that has been observed this model can then be used to make predictions about future data. We show that the problem of learning a maximum likelihood Markov network given certain observed data can be reduced to the problem of identifying a maximum weight low-treewidth graph under a given input weight function. We give the first constant factor approximation algorithm for this problem. More precisely, for any fixed treewidth objective k, we find a treewidth-k graph with an f(k) fraction of the maximum possible weight of any treewidth-k graph.
7E25BD71	Sparse graphical models have proven to be a flexible class of multivariate probability models for approximating high-dimensional distributions. In this paper, we propose techniques to exploit this modeling ability for binary classification by discriminatively learning such models from labeled training data, i.e., using both positive and negative samples to optimize for the structures of the two models. We motivate why it is difficult to adapt existing generative methods, and propose an alternative method consisting of two parts. First, we develop a novel method to learn tree-structured graphical models which optimizes an approximation of the log-likelihood ratio. We also formulate a joint objective to learn a nested sequence of optimal forests-structured models. Second, we construct a classifier by using ideas from boosting to learn a set of discriminative trees. The final classifier can interpreted as a likelihood ratio test between two models with a larger set of pairwise features. We use cross-validation to determine the optimal number of edges in the final model. The algorithm presented in this paper also provides a method to identify a subset of the edges that are most salient for discrimination. Experiments show that the proposed procedure outperforms generative methods such as Tree Augmented Naive Bayes and Chow-Liu as well as their boosted counterparts.
638196DE	This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks frorn data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented.The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.
8000315F	Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.
7F3655D3	We show that the class of strongly connected graphical models with treewidth at most k can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for k > 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time.
79B9CC7E	This paper deals with chain graphs under the Andersson Madigan Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score + search learning algorithms under assumptions weaker than faithfulness. We also study the problem of how to represent the result of marginalizing out some nodes in an AMP CG. We introduce a new family of graphical models that solves this problem partially. We name this new family maximal covariance concentration graphs because it includes both covariance and concentration graphs as subfamilies.
797F1FB0	We define and investigate classes of statistical models for the analysis of associations between variables, some of which are qualitative and some quantitative. In the cases where only one kind of variables is present, the models are well-known models for either contingency tables or covariance structures. We characterize the subclass of decomposable models where the statistical theory is especially simple. All models can be represented by a graph with one vertex for each variable. The vertices are possibly connected with arrows or lines corresponding to directional or symmetric associations being present. Pairs of vertices that are not connected are conditionally independent given some of the remaining variables according to specific rules.
7C7FFC7E	We study the problem of projecting a distribution  onto  (or  finding  a  maximum  likelihood distribution  among)  Markov networks  of  bounded tree-width. By casting it as the combinatorial optimization problem of finding a maximum weight hypertree, we prove that it is NP-hard to solve exactly and provide an approximation algorithm with a provable performance guarantee.
805ED9C0	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on $\ell_1$-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an $\ell_1$-constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes $p$ and maximum neighborhood sizes $d$ are allowed to grow as a function of the number of observations $n$. Our main results provide sufficient conditions on the triple $(n, p, d)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as $\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions are imposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$ samples are sufficient.
7BDE06A5	Advances in acoustic sensing have enabled the simultaneous acquisition of multiple measurements of the same physical event via co-located acoustic sensors. We exploit the inherent correlation among such multiple measurements for acoustic signal classification,to identify the launch/impact of munition (i.e., rockets, mortars).Specifically, we propose a prob- abilistic  graphical  model framework that can explicitly learn the class conditional correlations between the cepstral features extracted from these different measurements.Additionally, we employ symbolic dynamic filtering-based features, which offer improvements over the traditional cepstral features in terms of robustness to signal distortions.Experiments on real acoustic data sets show that our proposed algorithm outperforms conventional classifiers as well as the recently proposed joint sparsity models for multisensor acoustic classification.Additionally our proposed algorithm is less sensitive to insufficiency in training samples compared to competing approaches.
803EA8B7	We propose a new iterative, distributed approach for linear minimum mean-square-error (LMMSE) estimation in graphical models with cycles. The embedded subgraphs algorithm (ESA) decomposes a loopy graphical model into a number of linked embedded subgraphs and applies the classical parallel block Jacobi iteration comprising local LMMSE estimation in each subgraph (involving inversion of a small matrix) followed by an information exchange between neighboring nodes and subgraphs. Our primary application is sensor networks, where the model encodes the correlation structure of the sensor measurements, which are assumed to be Gaussian. The resulting LMMSE estimation problem involves a large matrix inverse, which must be solved in-network with distributed computation and minimal intersensor communication. By invoking the theory of asynchronous iterations, we prove that ESA is robust to temporary communication faults such as failing links and sleeping nodes, and enjoys guaranteed convergence under relatively mild conditions. Simulation studies demonstrate that ESA compares favorably with other recently proposed algorithms for distributed estimation. Simulations also indicate that energy consumption for iterative estimation increases substantially as more links fail or nodes sleep. Thus, somewhat surprisingly, sensor network energy conservation strategies such as low-powered transmission and aggressive sleep schedules could actually prove counterproductive.
80D0DD2E	We consider a problem encountered when trying to estimate a Gaussian random field using a distributed estimation approach based on Gaussian graphical models. Because of constraints imposed by estimation tools used in Gaussian graphical models, the a priori covariance of the random field is constrained to embed conditional independence constraints among a significant number of variables. The problem is, then: given the (unconstrained) a priori covariance of the random field, and the conditional independence constraints, how should one select the constrained covariance, optimally representing the (given) a priori covariance, but also satisfying the constraints? In 1972, Dempster provided a solution, optimal in the maximum likelihood sense, to the above problem. Since then, many works have used Dempster's optimal covariance, but none has addressed the issue of suitability of this covariance for Bayesian estimation problems. We prove that Dempster's covariance is not optimal in most minimum mean squared error (MMSE) estimation problems. We also propose a method for finding the MMSE optimal covariance, and study its properties. We then illustrate the analytical results via a numerical example, that demonstrates the estimation performance advantage gained by using the optimal covariance vs Dempster's covariance. The numerical example also shows that, for the particular estimation scenario examined, Dempster's covariance violates the necessary conditions for optimality.
7C1A282A	Gaussian Markov random fields (GMRFs) or Gaussian graphical models have been widely used in many applications. Efficiently drawing samples from GMRFs has been an important research problem. In this paper, we introduce the subgraph perturbation sampling algorithm, which makes use of any pre-existing tractable inference algorithm for a subgraph by perturbing this algorithm so as to yield asymptotically exact samples for the intended distribution. We study the stationary version where a single fixed subgraph is used in all iterations, as well as the non-stationary version where tractable subgraphs are adaptively selected. The subgraphs used can have any structure for which efficient inference algorithms exist: for example, tree-structured, low tree-width, or having a small feedback vertex set. We present new theoretical results that give convergence guarantees for both stationary and non-stationary graphical splittings. Our experiments using both simulated models and large-scale real models demonstrate that this subgraph perturbation algorithm efficiently yields accurate samples for many graph topologies.
7EF55ED4	Graphical models provide a powerful formalism for statistical signal processing. Due to their sophisticated modeling capabilities,  they have found applications in a variety of fields such as computer vision,image processing, and distributed sensor networks. In this paper, we present a general class of algorithms for estimation in Gaussian graphical models with arbitrary struc- ture. These algorithms involve a sequence of inference problems on tractable subgraphs over subsets of variables.This framework includes parallel iterations such as embedded trees,serial  iterations such as block Gauss Seidel, and hybrid versions of these iterations. We also discuss a method that uses local memory at each node to overcome temporary communication failures that may arise in distributed sensor network applications.We analyze these algorithms based on the recently developed walk-sum interpretation of Gaussian inference.We describe the walks computed by the algorithms using walk-sum diagrams ,and show that for iterations based on a very large and flexible set of sequences of subgraphs, convergence is guaranteed in walk-sum- mable models. Consequently, we are free to choose spanning trees and subsets of variables adaptively at each iteration. This leads to efficient methods for optimizing the next iteration step to achieve maximum reduction in error. Simulation results demonstrate that these nonstationary algorithms provide a significant speedup in convergence over traditional one-tree and two-tree iterations
7F86EE0A	Reviews a significant component of the rich field of statistical multiresolution (MR) modeling and processing. These MR methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. A second goal is to describe how this topic fits into the even larger field of MR methods and concepts-in particular, making ties to topics such as wavelets and multigrid methods. A third goal is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. The principle focus of our presentation is the class of MR Markov processes defined on pyramidally organized trees. The attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability. We show how a variety of methods and models relate to this framework including models for self-similar and 1/f processes. We also illustrate how these methods have been used in practice
008901FD	Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks. This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models. This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
776AFD98	Scalability is another major issue for recommender systems except data sparsity and prediction quality. However, it has still not been well solved while many social recommendation models have been propose to improve the latter two problems. In this paper, we propose a scalable collaborative filtering algorithm based matrix factorization that introduce two common context factors: category and keyword besides social information. In the proposed model, we make prediction together using two preference matrices:user-category and user-keyword instead of only using the user item rating matrix. This has the advantage that for new items, our model can make use of the two factors to make prediction, although they do not exist in the rating matrix. Experimental results on real dataset show that our model has a good scalability for new items, while still performing better than other state of art models.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
5D4FE30E	Given the increasing volume and complexity of network traffic nowadays, network operators often leverage application-layer protocols to differentiate network traffic, so as to improve quality-of-service control, security protection, and resource profiling. We present ProGraph, a tool that accurately infers protocol message formats at both byte-level and bit-level granularities. Unlike existing approaches that mainly exploit statistical features across packets, ProGraph exploits intra-packet dependency among the values of different portions of a packet payload. It systematically constructs a graphical model that captures intra-packet dependency, using various techniques in graph theory and information theory. It also achieves several important design properties for real deployment, including finegrained inference, protocol independence, simple parameterization, robustness to noisy training sets, and fast execution. We show via trace-driven evaluations that ProGraph achieves more accurate inference than existing approaches. We further show how ProGraph can be used for classifying traffic.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
08A0B727	The enormous number of questions needed to acquire a full preference model when the size of the outcome space is large forces us to work with partial models that approximate the users preferences. In this way we must devise elicitation strategies that focus on the most important questions and at the same time do not need to enumerate the outcome space. In this paper we focus on adaptive elicitation of GAI-decomposable preferences for top-k recommendation tasks in large combinatorial domains. We propose a method that interleaves the generation of top-k solutions with a heuristic selection of questions for refining the user preference model. Empirical results for a large combinatorial problem are given.
782AF565	In collaborative environments, members may try to acquire similar information on the Web in order to gain knowledge in one domain. For example, in a company several departments may successively need to buy business intelligence software and employees from these departments may have studied online about different business intelligence tools and their features independently. It will be productive to get them connected and share learned knowledge. We investigate fine-grained knowledge sharing in collaborative environments. We propose to analyze members Web surfing data to summarize the fine-grained knowledge acquired by them. A two-step framework is proposed for mining fine-grained knowledge:Web surfing data is clustered into tasks by a nonparametric generative model;a novel discriminative infinite Hidden Markov Model is developed to mine fine-grained aspects in each task. Finally, the classic expert search method is applied to the mined results to find proper members for knowledge sharing. Experiments on Web surfing data collected from our lab at UCSB and IBM show that the finegrained aspect mining framework works as expected and outperforms baselines. When it is integrated with expert search, the search accuracy improves significantly, in comparison with applying the classic expert search method directly on Web surfing data.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
5B4F822C	Although information extraction and coreference resolution appear together in many applications, most current systems perform them as independent steps. This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models. We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning. On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields.
7A3B2F5E	As social media sites grow in popularity, tagging has naturally emerged as a method of searching, categorizing and filtering online information, especially multimedia content. The unrestricted vocabulary users choose from to annotate content however, has often lead to an explosion of the size of space in which search is performed. This paper is concerned with investigating generative models of social annotations, and testing their efficiency with respect to two information consumption oriented tasks. One task considers recommending new tags (similarly new resources) for new, previously unknown users. We use perplexity as a standard measure for estimating the generalization performance of a probabilistic model. The second task is aimed at recommending new users to connect with. In this task, we examine which users activity is most discriminative in predicting social ties: annotation (i.e. tags), resource usage (i.e. artists), or collective annotation of resources altogether. For the second task, we propose a framework to integrate the modeling of social annotations with network proximity. The proposed approach consists of two steps: (1) discovering salient topics that characterize users, resources and annotations; and (2) enhancing the recommendation power of such models by incorporating social clues from the immediate neighborhood of users. In particular, we propose four classification schemes for social link recommendation, which we evaluate on a real world dataset from Last.fm. Our results demonstrate significant improvements over traditional approaches.
83EF0AC0	The increasing popularity of mobile devices has brought severe challenges to device usability and big data analysis. In this paper we investigate the intellectual recommender system on cell phones by incorporating mobile data analysis. Nowadays with the development of smart phones, more and more applications have emerged on various areas, such as entertainment, education and health care. While these applications have brought great convenience to peoples daily life, they also provide tremendous opportunities for analyzing users interests. In this work we develop an Android background service to collect the user behaviors and analyze their preferences based on their Android application usage. As one of the most intuitive media for visual representation, videos with various types of contents are recommended to users based on a proposed graphical model. The proposed model jointly utilizes the textual descriptions of Android applications and videos, as well as the extracted video content based features. Besides, by analyzing the user habit of application usage we seamlessly integrate the users personal interests during the recommendation. The extensive comparisons to multiple baselines reveal the superiority of the proposed model on the recommendation quality. Furthermore, we conduct experiments on personalized recommendation to demonstrate the capacity of the proposed model in effectively analyzing the users personal interests.
5A7B1DEB	This paper presents a generative graphical model (VC-Aspect) for filtering visual documents such as images. The proposed VC-Aspect extends the well-known Aspect model and combines both content based and collaborative filtering approaches in a unified framework. Instead of considering item indices in the model such as model-based collaborative filtering techniques, we use visual features in describing visual documents. This allows the model to predict ratings for new visual documents with the same set of parameters. Experimental results show the usefulness of such an approach in a real life application such as the content based image retrieval.
7ED83CC3	A fuzzy clustering algorithm is used for the image tree structure vector quantization (TSVQ). First, a digital image is divided into subblocks of fixed size, which consists of 4/spl times/4 blocks of pixels. By performing a 2-D discrete cosine transform (DCT), we select six DCT coefficients to form the feature vector, and use the fuzzy c-means algorithm in constructing the TSVQ codebook. By doing so, the algorithm can preserve the edge of image, make good image quality, and reduce the processing time while constructing the tree structured codebook, and reduce coding and decoding time.
8054BEFC	A convolution-based algorithm for computing the discrete cosine transform (DCT) (with power of two length) that is based on some theorems of number theory is proposed: It computes a length-N DCT (with N a power of two) using only N multiplications.
7B613B01	Video codecs operating at integral multiples of 64 kbps are well-known in visual communications technology as p * 64 systems (p equals 1 to 24). Originally developed as a class of ITU standards, these codecs have served as core technology for videoconferencing, and they have also influenced the MPEG standards for addressable video. Video compression in the above systems is provided by motion compensation followed by discrete cosine transform -- quantization of the residual signal. Notwithstanding the promise of higher bit rates in emerging generations of networks and storage devices, there is a continuing need for facile audiovisual communications over voice band and wireless modems. Consequently, video compression at bit rates lower than 64 kbps is a widely-sought capability. In particular, video codecs operating at rates in the neighborhood of 64, 32, 16, and 8 kbps seem to have great practical value, being matched respectively to the transmission capacities of basic rate ISDN (64 kbps), and voiceband modems that represent high (32 kbps), medium (16 kbps) and low- end (8 kbps) grades in current modem technology. The purpose of this talk is to describe the state of video technology at these transmission rates, without getting too literal about the specific speeds mentioned above. In other words, we expect codecs designed for non- submultiples of 64 kbps, such as 56 kbps or 19.2 kbps, as well as for sub-multiples of 64 kbps, depending on varying constraints on modem rate and the transmission rate needed for the voice-coding part of the audiovisual communications link. The MPEG-4 video standards process is a natural platform on which to examine current capabilities in sub-ISDN rate video coding, and we shall draw appropriately from this process in describing video codec performance. Inherent in this summary is a reinforcement of motion compensation and DCT as viable building blocks of video compression systems, although there is a need for improving signal quality even in the very best of these systems. In a related part of our talk, we discuss the role of preprocessing and postprocessing subsystems which serve to enhance the performance of an otherwise standard codec. Examples of these (sometimes proprietary) subsystems are automatic face-tracking prior to the coding of a head-and-shoulders scene, and adaptive postfiltering after conventional decoding, to reduce generic classes of artifacts in low bit rate video. The talk concludes with a summary of technology targets and research directions. We discuss targets in terms of four fundamental parameters of coder performance: quality, bit rate, delay and complexity; and we emphasize the need for measuring and maximizing the composite quality of the audiovisual signal. In discussing research directions, we examine progress and opportunities in two fundamental approaches for bit rate reduction: removal of statistical redundancy and reduction of perceptual irrelevancy; we speculate on the value of techniques such as analysis-by-synthesis that have proved to be quite valuable in speech coding, and we examine the prospect of integrating speech and image processing for developing next-generation technology for audiovisual communications
7591F8D7	This paper presents an efficient fast algorithm to reduce redundant discrete cosine transform (DCT) and quantization computations for H.264/AVC encoding optimization. A mathematic model is established based on analyzing residual coefficients distribution and considering the properties of DCT coefficientsenergy distribution. The experimental results demonstrate that the proposed approach can achieve the best performance in reducing the DCT and quantization(Q) computations and obtain almost the same video quality as the original encoder in H.264/AVC
7D506DE7	The MPEG committee has recently completed development of a new audio coding standard MPEG-4 Advanced Audio Coding-Enhanced Low Delay (AAC-ELD). AAC-ELD is targeted towards high-quality, full-duplex communication applications such as audio and video conferencing. AAC-ELD uses low delay spectral band replication (LD-SBR) technology together with a low delay AAC core encoder to achieve high coding efficiency and low algorithmic delays. In this paper, we present fast algorithms for computing LD-SBR filterbanks in AAC-ELD. The proposed algorithms map complex exponential modulation portion of the filterbanks to discrete cosine transforms of types IV and II. Our proposed mapping also allows to merge some multiplications with the windowing stage that precedes or succeeds the modulation step. This further reduces computational complexity. Our presentation includes detailed explanation and flow-graphs of the algorithms, complexity analysis, and comparisons with alternative implementations.
7F506E8E	Compressing an image is significantly different than compressing raw binary data. Evidently, general purpose compression algorithms can be used to compress images, but the result is less than optimal. Discrete Cosine Transform (DCT) has been widely used in signal processing of image. Joint Photographic Experts Group (JPEG) is a commonly used standard technique of compression for photographic images and in turn utilizes DCT. Apart from DCT, their also exist a decomposition algorithm well known as Singular Value Decomposition (SVD). The proposed schemes investigate the performance evaluation of variable quantization DCT and variable rank of image matrix SVD based image compression. The numerical analysis of such algorithms is carried out by measuring Peak Signal to Noise Ratio (PSNR), Compression Ratio (CR).
218C8616	 	Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use. 
7B8FFF7E	 2D fast cosine and sine transforms with regular structure are developed for 2n X 2n data points. These algorithms are extended versions of the 1D fast regular algorithms introduced in our recent paper. The rationale for these 2D algorithms for sine/cosine transforms in a 2D decomposition of data sequences into 2D subblocks with reduced dimension, rather than 1D, separable treatments for the columns and rows of the data sets. As a result the number of multiplications is 25 percent less than in row- column approach. Numerous algorithms of these type were proposed previously for discrete Fourier transform (DFT) and discrete cosine transform of type 2 (DCT-II). In DCT-II case the algorithms do not have a regular structure as is the case in DFT algorithms and motivation of this work is to derive 2D algorithms for discrete sine and cosine transforms with regular constant geometry structures. Extension to 2n X 2m data points is straightforward.
7D26715E	The authors present the design and performance evaluation of a robust, DCT-based (discrete-cosine-transform-based) variable-bit-rate (VBR) compression algorithm for use on B-ISDN/ATM networks. The algorithm class under consideration is based on a recent proposal by F. Kishino et al. (1989), intended to provide robust delivery of video under relatively high ATM cell loss conditions. The robust VBR codec is based on separation of subjectively important low-frequency DCT coefficients (for high-priority transport) from the less important high-frequency coefficients (which are sent at a lower priority level). Temporal propagation of error after loss of low-priority ATM cells is avoided by limiting interframe prediction to low-frequency information transmitted in high-priority cells. Several key questions that arise in the design of such an ATM codec are considered, including: (a) the trade-off between total bit-rate and robustness; (b) the influence of the high/low priority boundary parameter on the high-priority and low-priority bit-rates; and (c) performance at the decoder in the presence of ATM channel loss
813D8ACA	Discrete Tchebichef transform (DTT), derived from a discrete class of the popular Chebyshev polynomials, is a novel orthogonal transform that has high energy compaction and de-correlation properties. Therefore, in this paper, DTT is examined and treated for transform coding applications. A framework is laid to derive an approximation-free integer representation of DTT to meet the current application requirements. A fast algorithm is further proposed for multiplier-free computation of DTT. The image compression performance of the 4-point DTT is found to be superior to that of the 4-point discrete cosine transform (DCT) and integer cosine transform (ICT), the integer approximation of DCT. It is shown that the fast DTT is easily derived, has low complexity, does not involve approximations and can be carried out within the same dynamic range. Hence, DTT can be used for image and data compression applications. Since the image compression performance and computational simplicity of DTT are found to be significantly better than that of ICT, the use of DTT in place of ICT for transform coding in the H.264/AVC looks promising.
80D06662	Research of robust and invisible double digital watermark is one of the hot fields currently, and it has received considerable attention. To change the situation that many methods about watermarking are based on embedding one single watermark, a new double digital watermark algorithm on the basis of discrete cosine transformation and image blocks is presented. The algorithm embeds both robust watermark and fragile watermark to one video sequence by using DCT and multiple embedded methods. The later embedded fragile watermark is served for the early robust watermark. The experiment results verify the algorithm achieves better robustness and imperceptibility.
7686BE02	On the basis of the Mobius function, a two-stage algorithm for the discrete cosine transform (DCT) and the inverse DCT (IDCT) is proposed. In this approach, the DCT matrix is factorized into the preprocessing and postprocessing matrices. The preprocessing matrix has elements of values 1 and -1, and the postprocessing matrix is a circular convolution/correlation matrix.
7BCFEA89	In fractal image compression the encoding step is computationally expensive, because every range block must be compared to all domain blocks in the codebook to find the best-matched one during the coding procedure. In this paper, a fast classification algorithm using DCT coefficients is proposed. Simulation results show that the runtime of the proposed algorithm is reduced greatly compared to the existing methods. At the same time, the new algorithm also achieved high PSNR values
6F004731	Applications of bounded error parameter estimation in the field of image compression are described. A bounded error parameter estimator is shown to improve the performance of an adaptive predictive compression scheme. The quantization of discrete cosine transform coefficients is viewed from a parameter bounding perspective and bounds for the coefficient quantization error are derived. They can be used to keep the reconstructed image sample quantization error within bounds.
7D7F7051	We implement a two-dimensional 8/spl times/8 fast discrete cosine transform and its inverse by the Feig's algorithm which is recognized the fastest method so far and this algorithm potentially has very wide applications. All the equations are derived in detail. The verification and evaluation are proved by computer simulation. The result of real application in MPEG1 is also presented.
79046CE5	In this paper, a replacement algorithm for Linear Prediction Coefficients (LPC) along with Hamming Correction Code based Compressor (HCDC) algorithms are investigated for speech compression. We started with an CELP system with order 12 and with Discrete Cosine Transform (DCT) based residual excitation. Forty coefficients with transmission rate of 5.14 kbps were first used. For each frame of the testing signals we applied a multistage HCDC, we tested the compression performance for parities from 2 to 7, we were able to achieve compression only at parity 4. This rate reduction was made with no compromise in the original CELP signal quality since compression is lossless. The compression approach is based on constructing dynamic reflection coefficients codebook, this codebook is constructed and used simultaneously using a certain store/retrieve threshold. The initial linear prediction codec we used is excited by a discrete cosine transform (DCT) residual, the results were tested using the MOS and SSNR, we had acceptable ranges for the MOS (average 3.6), and small variations of the SSNR.
7B8ECF46	In this paper,a novel grayscale watermarking algorithm based on Two-Level Discrete Cosine Transform (DCT) and Singular Value Decomposition (SVD) is proposed. The watermark signal is bit gray image.First,The original image is divided into blocks according to the size of the watermark;each block corresponds to each pixel value of watermark.Second,the DCT is applied in each block twice and form new blocks.Then,perform SVD on each new block to get matrices U,S and V for each block.The pixel value of watermark is embedded into the largest singular value of S matrix of each new block.And the watermark can be detected with the original image.The experimental results show that the algorithm can satisfy the transparence and robustness of the watermarking system very well. Experimental evaluation demonstrates that the proposed scheme is able to withstand a variety of attacks.
5CFE86E1	The DFT is an important tool in digital signal processing. An effective index mapping is introduce by which the odd-length sinusoid-class orthogonal transforms, such as the generalised discrete Fourier transforms (GDFTs), generalised discrete Hartley transforms (GDHTs), discrete cosine transforms (DCTs) and discrete sine transforms (DSTs), can be converted to DFTs, real-valued DFTs (RDFTs) or DHTs of the same length, using primarily permutations and sign changes only. The algorithm proposed is more efficient than others previously published.
7B2F9E2B	this research presents a new algorithm for an image compression consist of three phases; the first phase is using "Discrete Wavelet Transformation (DWT)", to produce low-frequency and high-frequencies sub-bands. The high-frequencies sub-bands are ignored (i.e. not used in this research), in the second phase used "Discrete Cosine Transformation (DCT)" applied on each "2x2" block from "LL" sub-band, then each block stored as a one- dimensional array in the new matrix called "Multi-Array- Matrix (MA-Matrix)". The third phase; MA-Matrix separated into "DC-Column" and "MA2-Matrix", and then applied Minimize-Matrix-Size algorithm on the "MA2- Matrix", to be as a one-dimensional array. Our decompression algorithm phase starts from "Sequential Search Algorithm (SS-Algorithm)" to find the estimated values for the "MA2-Matrix". The SS-Algorithm depends on the three pointers, for decompress MA2-Matrix, and then combined it with DC-Column for reconstructs MA-Matrix. Finally the inverse DCT and the inverse DWT are used for reconstructs approximately original image. Our approach compared with JPEG and JPEG2000 by using PSNR. 
674BBF3E	An effective index mapping is introduced by which the odd-length sinusoid-class orthogonal transforms, such as GDFTs,GDHTs,DCTs and DSTs, can be converted to DFTs,real-valued DFTs (RDFTs) or DHTs of the same length,using primarily permutations and sign changes only.The algorithm proposed here is more efficient than others previously published. 
7EDB4A91	Fast recursive algorithms for the computation of the discrete cosine and sine transforms are developed. An N-point discrete cosine transform (DCT) or discrete sine transform (DST) can be computed from two N/2-point DCTs or DSTs. Compared to the existing algorithms the algorithms have less multiplications by two, and add operations are better positioned, giving rise to faster computation and easier VLSI implementation.
80E6A82C	An efficient direct method for the computation of a length-N discrete cosine transform (DCT) given two adjacent length-(N/2) DCT coefficients, is presented. The computational complexity of the proposed method is lower than the traditional approach for lengths N>8. Savings of N memory locations and 2N data transfers are also achieved.
7A61FB18	In this paper we present a new global method to derive invertible integer-to-integer mappings from given linear mappings . If is given by and Hn is an invertible matrix, then one can always find a suitable factor such that the condition is satisfied. An invertible mapping can now simply be defined by , and obviously, this nonlinear integer mapping is close to the linear mapping . We apply this idea in order to derive a new invertible integer DCT-II transform of radix-2 length and new integer wavelet algorithms. It turns out, that the expansion factors can be chosen very small.
76D04F48	The discrete cosine transform (DCT) is widely applied in various fields, including image data compression, because it operates like the Karhunen-Loeve transform for stationary random data. This paper presents a recursive algorithm for DCT with a structure that allows the generation of the next higher order DCT from two identical lower order DCT's. As a result, the method for implementing this recursive DCT requires fewer multipliers and adders than other DCT algorithms.
7FC2B1C0	Recently, many applications for three-dimensional (3-D) image and video compression have been proposed using 3-D discrete cosine transforms (3-D DCTs). Among different types of DCTs, the type-II DCT (DCT-II) is the most used. In order to use the 3-D DCTs in practical applications, fast 3-D algorithms are essential. Therefore, in this paper, the 3-D vector-radix decimation-in-frequency (3-D VR DIF) algorithm that calculates the 3-D DCT-II directly is introduced. The mathematical analysis and the implementation of the developed algorithm are presented, showing that this algorithm possesses a regular structure, can be implemented in-place for efficient use of memory, and is faster than the conventional row-column-frame (RCF) approach. Furthermore, an application of 3-D video compression-based 3-D DCT-II is implemented using the 3-D new algorithm. This has led to a substantial speed improvement for 3-D DCT-II-based compression systems and proved the validity of the developed algorithm.
781655CB	In this paper, 3-D discrete Hartley transform is applied for the compression of two medical modalities, namely, magnetic resonance images and X-ray angiograms and the performance results are compared with those of 3-D discrete cosine and Fourier transforms using the parameters such as PSNR and bit rate. It is shown that the 3-D discrete Hartley transform is better than the other two transforms for magnetic resonance brain images whereas for the X-ray angiograms, the 3-D discrete cosine transform is found to be superior.
7FE3364C	Recently, a fast radix-q algorithm for an efficient computation of the type-IV discrete cosine transform (DCT-IV) has been proposed in , where q is an odd positive integer. In particular, based on the proposed fast algorithm, optimized efficient 3-, 5-, and 9-point scaled DCT-IV (SDCT-IV) modules have been derived in . As a response, an improved efficient optimized 9-point scaled DCT-IV (SDCT-IV) module in terms of the arithmetic complexity is presented. The improved optimized efficient 9-point SDCT-IV module requires 17 multiplications, 53 additions, and three shifts. Consequently, the arithmetic complexity of extended fast mixed-radix DCT-IV algorithm for composite lengths is also significantly improved.
7FD3F0EB	An index permutation-based fast two-dimensional discrete cosine transform (2-D DCT) algorithm is presented. It is shown that the N/spl times/N 2-D DCT, where N=2/sup m/, can be computed using only N 1-D DCTs and some post additions.
7E68A6E2	The discrete cosine transform (DCT) is often computed from a discrete Fourier transform (DFT) of twice or four times the DCT length. DCT algorithms based on identical-length DFT algorithms generally require additional arithmetic operations to shift the phase of the DCT coefficients. It is shown that a DCT of odd length can be computed by an identical-length DFT algorithm, by simply permuting the input and output sequences. Using this relation, odd-length DCT modules for a prime factor DCT are derived from corresponding DFT modules. The multiplicative complexity of the DCT is then derived in terms of DFT complexities.
77C93D55	The modified discrete cosine transform (MDCT) and modified discrete sine transform (MDST) are employed in subband/transform coding schemes as the analysis/synthesis filter banks based on the concept of time domain aliasing cancellation (TDAC). Princen, Bradley and Johnson defined two types of the MDCT, specifically, for an evenly stacked and oddly stacked analysis/synthesis systems. The MDCT is the basic processing component in the international audio coding standards and commercial products for high-quality audio compression. Almost all existing audio coding systems have used the complex-valued or real-valued FFT algorithms, and the DCT/DST of type IV (DCT-IV/DST-IV) for the fast MDCT computation. New fast and efficient algorithm for a unified forward and inverse MDCT/MDST computation in the oddly stacked system is proposed. It is based on the DCT/DST of types II and III (DCT-II/DST-II, DCT-III/DST-III), and the real arithmetic is used only. Corresponding generalized signal flow graph is regular, structurally simple and enables to compute MDCT/MDST and their inverses in general for any N divisible by 4 (N being length of a data sequence). Consequently, the new fast algorithm can be adopted for the MDCT computation in the current audio coding standards such as MPEG family (MPEG-1, MPEG-2, MPEG-2 Advanced Audio Coding and MPEG-4 audio), and in commercial products (proprietary audio coding algorithms) such as Sony MiniDisc/ATRAC/ATRAC2/SDDS digital audio coding systems, the AT&T Perceptual Audio Coder (PAC) or Lucent Technologies PAC/Enhanced PAC/Multichannel PAC, and Dolby Labs AC-3 digital audio compression algorithm. Besides the new fast algorithm has some interesting properties, it provides an efficient implementation of the forward and inverse MDCT computation for layer III in MPEG audio coding, where the length of data blocks. Especially, for the AC-3 algorithm, it is shown how both the proposed new MDCT/MDST algorithm and existing fast algorithms/computational architectures for the discrete sinusoidal transforms computation of real data sequences such as the DCT-IV/DST-IV, generalized discrete Fourier transform of type IV (DFT-IV) and generalized discrete Hartley transform of type IV (DHT-IV) can be used for the fast alternate or simultaneous (on-line) MDCT/MDST computation by simple pre- and post-processing of data sequences.
78363BC0	Integer DCTs have a wide range of applications in lossless coding, especially in image compression. An integer-to-integer DCT of radix-2-length n is a nonlinear, left-invertible mapping, which acts on /spl Zopf//sup n/ and approximates the classical discrete cosine transform (DCT) of length n. All known integer-to-integer DCT-algorithms of length 8 are based on factorizations of the cosine matrix C/sub 8//sup II/ into a product of sparse matrices and work with lifting steps and rounding off. For fast implementation one replaces floating point numbers by appropriate dyadic rationals. Both rounding and approximation leads to truncation errors. In this paper, we consider an integer-to-integer transform for (2/spl times/2) rotation matrices and give estimates of the truncation errors for arbitrary approximating dyadic rationals. Further, using two known integer-to-integer DCT-algorithms, we show examplarily how to estimate the worst-case truncation error of lifting based integer-to-integer algorithms in fixed-point arithmetic, whose factorizations are based on (2/spl times/2) rotation matrices.
7D10C675	In this paper, we first propose an efficient algorithm for computing one-dimensional (1-D) discrete cosine transform (DCT) for a signal block, given its two adjacent subblocks in the DCT domain and then introduce several algorithms for the fast computation of multidimensional (m-D) DCT with size N/sub 1//spl times/N/sub 2//spl times/.../spl times/N/sub m/ given 2/sup m/ subblocks of DCT coefficients with size N/sub 1//2/spl times/N/sub 2//2/spl times/.../spl times/N/sub m//2, where N/sub i/(i=1,2,...,m) are powers of 2. Obviously, the row-column method, which employs the most efficient algorithms along each dimension, reduces the computational complexity considerably, compared with the traditional method, which employs only the one-dimensional (1-D) fast DCT and inverse DCT (IDCT) algorithms. However, when m/spl ges/2, the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms, has lower computational complexity than the row-column method. Besides, we propose a direct method by dividing the data into 2/sup m/ parts for independent fast computation, in which only two steps of r-dimensional (r=1,2,...,m) IDCT and additional multiplications and additions are required. If all the dimensional sizes are the same, the number of multiplications required for the direct method is only (2/sup m/-1)/m2/sup m-1/ times of that required for the row-column method, and if N/spl ges/2/sup 2m-1/, the computational efficiency of the direct method is surely superior to that of the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms.
7EBEC735	An efficient method for computing the discrete cosine transform (DCT) is proposed. Based on direct decomposition of the DCT, the recursive properties of the DCT for an even length input sequence is derived, which is a generalization of the radix 2 DCT algorithm. Based on the recursive property, a new DCT algorithm for an even length sequence is obtained. The proposed algorithm is very structural and requires fewer computations when compared with others. The regular structure of the proposed algorithm is suitable for fast parallel algorithm and VLSI implementation.
80D1B10B	We present a dynamic graphical model (DGM) for automated multi-instrument musical transcription. By multi-instrument transcription, we mean a system capable of listening to a recording in which two or more instruments are playing, and identifying both the notes that were played and the instruments that played them. Our transcription system models two musical instruments, each capable of playing at most one note at a time. We present results for two-instrument transcription on piano and violin sounds.
7F77E0BA	The discrete cosine transform (DCT) and the discrete sine transform (DST) have found wide applications in speech and image processing, as well as telecommunication signal processing for the purpose of data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new recursive algorithms for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design recursive algorithms for the 2-D DCT and the 2-D DST. These new structured recursive algorithms are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other recursive algorithms. Finally, we propose two parallel algorithms for accelerating the computation.
781849EC	Integer DCTs have important applications in lossless coding. In this paper, an integer DCT of radix-2 length n is understood to be a nonlinear, (left-)invertible mapping which acts on and approximates the classical discrete cosine transform (DCT) of length n. In image compression, the DCT of type II (DCT-II) is of special interest. In this paper we present a new approach to invertible integer DCT-II and integer DCT-IV. Our method is based on a factorization of the cosine matrices of types II and IV into products of sparse, orthogonal matrices. Up to some permutations, each matrix factor is a block-diagonal matrix with blocks being orthogonal matrices of order 2. Hence one has to construct only integer transforms of length 2. We factorize an orthogonal matrix of order 2 into three lifting matrices and work with lifting steps and rounding-off. This allows the construction of new integer DCT algorithms. We give uniform bounds for the worst case difference between the results of exact DCT and the corresponding integer DCT. Finally, we present some numerical experiments for the integer DCT-II of length 8 and for the 2-dimensional integer DCT-II of size 8 8.
7F3438A9	This tutorial paper describes the methods for constructing fast algorithms for the computation of the discrete Fourier transform (DFT) of a real-valued series. The application of these ideas to all the major fast Fourier transform (FFT) algorithms is discussed, and the various algorithms are compared. We present a new implementation of the real-valued split-radix FFT, an algorithm that uses fewer operations than any other real-valued power-of-2-length FFT. We also compare the performance of inherently real-valued transform algorithms such as the fast Hartley transform (FHT) and the fast cosine transform (FCT) to real-valued FFT algorithms for the computation of power spectra and cyclic convolutions. Comparisons of these techniques reveal that the alternative techniques always require more additions than a method based on a real-valued FFT algorithm and result in computer code of equal or greater length and complexity.
7D05A275	As the circuit complexity is increasing in demand for the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera is one of them where realtime image capturing, compression and storage of compressed image data is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image.
7C5BD5F9	A systematic method of sparse matrix factorization is developed for all four versions of the discrete W transform, the discrete cosine transform, and the discrete sine transform, as well as for the discrete Fourier transform. The factorization leads to fast algorithms in which only real arithmetic is involved. A scheme for reducing multiplications and a convenient index system are introduced. This makes new algorithms more efficient than conventional algorithms for the discrete Fourier transform, the discrete cosine transform, and the discrete sine transform.
79411F44	The fast Hartley transform (FHT) is similar to the Cooley-Tukey fast Fourier transform (FFT) but performs much faster because it requires only real arithmetic computations compared to the complex arithmetic computations required by the FFT. Through use of the FHT, discrete cosine transforms (DCT) and discrete Fourier transforms (DFT) can be obtained. The recursive nature of the FHT algorithm derived in this paper enables us to generate the next higher order FHT from two identical lower order FHT's. In practice, this recursive relationship offers flexibility in programming different sizes of transforms, while the orderly structure of its signal flow-graphs indicates an ease of implementation in VLSI.
7576CA59	A new VLSI algorithm and its associated systolic array architecture for a prime length type IV discrete cosine transform is presented. They represent the basis of an efficient design approach for deriving a linear systolic array architecture for type IV DCT. The proposed algorithm uses a regular computational structure called pseudoband correlation structure that is appropriate for a VLSI implementation. The proposed algorithm is then mapped onto a linear systolic array with a small number of I/O channels and low I/O bandwidth. The proposed architecture can be unified with that obtained for type IV DST due to a similar kernel. A highly efficient VLSI chip can be thus obtained with good performance in the architectural topology, computing parallelism, processing speed, hardware complexity and I/O costs similar to those obtained for circular correlation and cyclic convolution computational structures.
7EDD5239	This correspondence presents an odd-factor algorithm for the type-III discrete cosine transform (DCT) for uniform or mixed radix decomposition. By jointly using the old-factor and the existing radix-2 algorithms, a general decomposition method for arbitrarily composite sequence length is developed. A reduction of computational complexity can be achieved compared with that needed by other reported algorithms for M=2/sup m/. The decomposition approach has a simple computational structure and supports a wider range of choices for different sequence lengths.
0BD946B6	Building accurate models from a small amount of available training data can sometimes prove to be a great challenge. Expert domain knowledge can often be used to alleviate this burden. Parameter Sharing is one such important form of domain knowledge. Graphical models like HMMs, DBNs and Module Networks use different forms of Parameter Sharing to reduce the variance in the parameter estimates. The goal of this paper is to present a theoretical approach for learning in presence of several other types of Parameter Related Domain Knowledge that go beyond the ones in the above models. First, we introduce a General Parameter Sharing Framework that describes the models just mentioned, but allows for much finer grained parameter sharing assumptions. In this framework, we present sound procedures for parameter learning from both a Frequentist and a Bayesian point of view, from both complete and incomplete data, in the case where a domain expert specifies in advance the structure of the graphical model, and the subsets of parameters to be shared. Second, we describe a hierarchical extension of this framework based on Parameter Sharing Trees. Finally we present algorithms for using domain knowledge that specifies that certain groups of parameters share certain properties. In particular, we consider two kinds of constraints: first kind states certain groups of parameters share the same aggregate probability mass and second kind states the ratio of the parameters is preserved (shared) in several groups. As an example, we derive a novel form of parameter sharing for Bayesian Multinetworks.
7EBEC735	An efficient method for computing the discrete cosine transform (DCT) is proposed. Based on direct decomposition of the DCT, the recursive properties of the DCT for an even length input sequence is derived, which is a generalization of the radix 2 DCT algorithm. Based on the recursive property, a new DCT algorithm for an even length sequence is obtained. The proposed algorithm is very structural and requires fewer computations when compared with others. The regular structure of the proposed algorithm is suitable for fast parallel algorithm and VLSI implementation.
59E5C607	In this paper I explore the psychology of ritual performance and present a simple graphical model that clarifies several issues in William Irons’s theory of religion as a “hard-to-fake” sign of commitment. Irons posits that religious behaviors or rituals serve as costly signals of an individual’s commitment to a religious group. Increased commitment among members of a religious group may facilitate intra-group cooperation, which is argued to be the primary adaptive benefit of religion. Here I propose a proximate explanation for how individuals are able to pay the short-term costs of ritual performance to achieve the long-term fitness benefits offered by religious groups. The model addresses three significant problems raised by Irons’s theory. First, the model explains why potential free-riders do not join religious groups even when there are significant net benefits that members of religious groups can achieve. Second, the model clarifies how costly a ritual must be to achieve stability and prevent potential free-riders from joining the religious group. Third, the model suggests why religious groups may require adherents to perform private rituals that are not observed by others. Several hypotheses generated from the model are also discussed.
7B13384D	 This paper reviews some of the key statistical ideas that are encountered when trying to find empirical support to causal interpretations and conclusions, by applying statistical methods on experimental or observational longitudinal data. In such data, typically a collection of individuals are followed over time, then each one has registered a sequence of covariate measurements along with values of control variables that in the analysis are to be interpreted as causes, and finally the individual outcomes or responses are reported. Particular attention is given to the potentially important problem of confounding. We provide conditions under which, at least in principle, unconfounded estimation of the causal effects can be accomplished. Our approach for dealing with causal problems is entirely probabilistic, and we apply Bayesian ideas and techniques to deal with the corresponding statistical inference. In particular, we use the general framework of marked point processes for setting up the probability models, and consider posterior predictive distributions as providing the natural summary measures for assessing the causal effects. We also draw connections to relevant recent work in this area, notably to Judea Pearl's formulations based on graphical models and his calculus of so-called do-probabilities. Two examples illustrating different aspects of causal reasoning are discussed in detail.
7E19344D	A novel algorithm to convert the discrete cosine transform (DCT) to skew-circular convolutions is presented. The motivation for developing such an algorithm is the fact that VLSI implementation of distributed arithmetic is very efficient for computing convolutions. It is also shown that the inverse DCT (IDCT) can be computed using the same building blocks which are used for computing the DCT. A DCT/IDCT processor can be designed to compute either the DCT or the IDCT depending on a 1-b control signal.
7CE629CE	Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.
772CE3A1	Most interactive graphical applications that use direct manip- ulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high de- sign, development and maintenance costs and encourages the development of stereotyped applications based on buttons, menus and dialogue boxes instead of direct manipulation of the applications objects. There have been several attempts to provide high level tools for building such applications, including popular toolkits such as Garnet [26], Unidraw [33], Fresco [21, 32] and Open- Inventor [28]. Unfortunately, these tools are not adapted to the development of sophisticated graphical editors because of their lack of extensibility: In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction. We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi- Layer Model and we show how it can take advantage of soft- ware and hardware graphic extensions to provide good per- formance. We also show how it supports multiple input de- vices and simplifies the description of a wide variety of in- teraction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional an- imation system. 
7FA005E7	This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.
77C93D55	The modified discrete cosine transform (MDCT) and modified discrete sine transform (MDST) are employed in subband/transform coding schemes as the analysis/synthesis filter banks based on the concept of time domain aliasing cancellation (TDAC). Princen, Bradley and Johnson defined two types of the MDCT, specifically, for an evenly stacked and oddly stacked analysis/synthesis systems. The MDCT is the basic processing component in the international audio coding standards and commercial products for high-quality audio compression. Almost all existing audio coding systems have used the complex-valued or real-valued FFT algorithms, and the DCT/DST of type IV (DCT-IV/DST-IV) for the fast MDCT computation. New fast and efficient algorithm for a unified forward and inverse MDCT/MDST computation in the oddly stacked system is proposed. It is based on the DCT/DST of types II and III (DCT-II/DST-II, DCT-III/DST-III), and the real arithmetic is used only. Corresponding generalized signal flow graph is regular, structurally simple and enables to compute MDCT/MDST and their inverses in general for any N divisible by 4 (N being length of a data sequence). Consequently, the new fast algorithm can be adopted for the MDCT computation in the current audio coding standards such as MPEG family (MPEG-1, MPEG-2, MPEG-2 Advanced Audio Coding and MPEG-4 audio), and in commercial products (proprietary audio coding algorithms) such as Sony MiniDisc/ATRAC/ATRAC2/SDDS digital audio coding systems, the AT&T Perceptual Audio Coder (PAC) or Lucent Technologies PAC/Enhanced PAC/Multichannel PAC, and Dolby Labs AC-3 digital audio compression algorithm. Besides the new fast algorithm has some interesting properties, it provides an efficient implementation of the forward and inverse MDCT computation for layer III in MPEG audio coding, where the length of data blocks. Especially, for the AC-3 algorithm, it is shown how both the proposed new MDCT/MDST algorithm and existing fast algorithms/computational architectures for the discrete sinusoidal transforms computation of real data sequences such as the DCT-IV/DST-IV, generalized discrete Fourier transform of type IV (DFT-IV) and generalized discrete Hartley transform of type IV (DHT-IV) can be used for the fast alternate or simultaneous (on-line) MDCT/MDST computation by simple pre- and post-processing of data sequences.
80E6A82C	An efficient direct method for the computation of a length-N discrete cosine transform (DCT) given two adjacent length-(N/2) DCT coefficients, is presented. The computational complexity of the proposed method is lower than the traditional approach for lengths N>8. Savings of N memory locations and 2N data transfers are also achieved.
7F9A8948	Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
7E68A6E2	The discrete cosine transform (DCT) is often computed from a discrete Fourier transform (DFT) of twice or four times the DCT length. DCT algorithms based on identical-length DFT algorithms generally require additional arithmetic operations to shift the phase of the DCT coefficients. It is shown that a DCT of odd length can be computed by an identical-length DFT algorithm, by simply permuting the input and output sequences. Using this relation, odd-length DCT modules for a prime factor DCT are derived from corresponding DFT modules. The multiplicative complexity of the DCT is then derived in terms of DFT complexities.
7F77E0BA	The discrete cosine transform (DCT) and the discrete sine transform (DST) have found wide applications in speech and image processing, as well as telecommunication signal processing for the purpose of data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new recursive algorithms for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design recursive algorithms for the 2-D DCT and the 2-D DST. These new structured recursive algorithms are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other recursive algorithms. Finally, we propose two parallel algorithms for accelerating the computation.
7D10C675	In this paper, we first propose an efficient algorithm for computing one-dimensional (1-D) discrete cosine transform (DCT) for a signal block, given its two adjacent subblocks in the DCT domain and then introduce several algorithms for the fast computation of multidimensional (m-D) DCT with size N/sub 1//spl times/N/sub 2//spl times/.../spl times/N/sub m/ given 2/sup m/ subblocks of DCT coefficients with size N/sub 1//2/spl times/N/sub 2//2/spl times/.../spl times/N/sub m//2, where N/sub i/(i=1,2,...,m) are powers of 2. Obviously, the row-column method, which employs the most efficient algorithms along each dimension, reduces the computational complexity considerably, compared with the traditional method, which employs only the one-dimensional (1-D) fast DCT and inverse DCT (IDCT) algorithms. However, when m/spl ges/2, the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms, has lower computational complexity than the row-column method. Besides, we propose a direct method by dividing the data into 2/sup m/ parts for independent fast computation, in which only two steps of r-dimensional (r=1,2,...,m) IDCT and additional multiplications and additions are required. If all the dimensional sizes are the same, the number of multiplications required for the direct method is only (2/sup m/-1)/m2/sup m-1/ times of that required for the row-column method, and if N/spl ges/2/sup 2m-1/, the computational efficiency of the direct method is surely superior to that of the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms.
7FE3364C	Recently, a fast radix-q algorithm for an efficient computation of the type-IV discrete cosine transform (DCT-IV) has been proposed in , where q is an odd positive integer. In particular, based on the proposed fast algorithm, optimized efficient 3-, 5-, and 9-point scaled DCT-IV (SDCT-IV) modules have been derived in . As a response, an improved efficient optimized 9-point scaled DCT-IV (SDCT-IV) module in terms of the arithmetic complexity is presented. The improved optimized efficient 9-point SDCT-IV module requires 17 multiplications, 53 additions, and three shifts. Consequently, the arithmetic complexity of extended fast mixed-radix DCT-IV algorithm for composite lengths is also significantly improved.
7C5AAA1F	Therapy processes are complex dynamical systems where several variables are constantly interacting with each other. In general, the underlying mechanisms are difficult to assess. Our approach is to identify the dependency structure of relevant variables within the therapy process using interaction graphs. These are instruments for multivariate time series which are based on the analysis of partial spectral coherences. We used interaction graphs in order to investigate the therapy process of a multimodal therapy concept for fibromyalgia patients. Our main hypothesis was that self-efficacy plays a central role in the therapy process. Methods: Patients kept an electronic diary for 13 weeks. Pain intensity, depression, sleep quality, anxiety and self-efficacy were assessed via visual analogue scales. The resulting multivariate time series were aggregated over individuals, and partial spectral coherences between each pair of the variables were calculated. From the partial coherences, interaction graphs were plotted. Results: Within the resulting graphical model, self-efficacy was strongly related to pain intensity, depression and sleep quality. All other relations were substantially weaker. There was no direct relationship between pain intensity and sleep quality. Conclusions: The relations between two variables within the therapy process are mainly induced by self-efficacy. Interaction graphs can be used to pool time series data of several patients and thus to assess the common underlying dependency structure of a group of patients. The graphical representation is easily comprehensible and allows to distinguish between direct and indirect relationships.
80D10329	Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.
7D843341	A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.
7963E765	The forensic investigation of the origin and cause of a fire incident is a particularly demanding area of expertise. As the available evidence is often incomplete or vague, uncertainty is a key element. The present study is an attempt to approach this through the use of Bayesian networks, which have been found useful in assisting human reasoning in a variety of disciplines in which uncertainty plays a central role. The present paper describes the construction of a Bayesian network (BN) and its use for drawing inferences about propositions of interest, based upon a single, possibly non replicable item of evidence: detected residual quantities of a flammable liquid in fire debris.
7A1904AD	A new recursive algorithm and two types of circuit architectures are presented for the computation of the two-dimensional discrete cosine transform (2D DCT). The new algorithm permits to compute the 2D DCT by a simple procedure of the 1D recursive calculations involving only cosine coefficients. The recursive kernel for the proposed algorithm contains a small number of operations. Also, it requires a smaller number of pre-computed data compared with many of existing algorithms in the same category. The kernel can be easily implemented in a simple circuit block with a short critical delay path. In order to evaluate the performance improvement resulting from the new algorithm, an architecture for the 2D DCT designed by direct mapping from the computation structure of the proposed algorithm has been implemented in an FPGA board. The results show that the reduction of the hardware consumption can easily reach 25 and the clock frequency can increase 17 compared with a system implementing a recently reported 2D DCT recursive algorithm. For a further reduction of the hardware, another architecture has been proposed for the same 2D DCT computation. Using one recursive computation block to perform different functions, this architecture needs only approximately one-half of the hardware that is required in the first architecture, which has been confirmed by an FPGA implementation.
7B64D19B	The discrete cosine transform of type IV (DCT-IV) and corresponding discrete sine transform of type IV (DST-IV) have played key role in the efficient implementation of orthogonal lapped transforms and perfect reconstruction cosine-modulated filter banks such as the oddly stacked modified discrete cosine transform (MDCT) or equivalently, the modulated lapped transform (MLT). However, the DCT-IV and DST-IV of double sizes are related to two variants of filter banks defined by Dolby Labs AC-3 digital audio compression algorithm. Since these two variants of filter banks are efficiently computed by recently proposed new fast algorithm for the oddly stacked MDCT (Signal Processing 82 (2002) 433), it is shown that the efficient DCT-IV and DST-IV computation can be realized via the MDCT of double size. The careful analysis of regular structure of the new fast MDCT algorithm allows to extract a new DCT-IV/DST-IV computational structure and to suggest a new sparse matrix factorization of the DCT-IV matrix. Finally, the new DCT-IV/DST-IV computational structure provides an alternative efficient implementation of the forward and inverse MDCT in layer III of MPEG (MP3) audio coding.
7A8EB39B	Recommender system provides users with personalized suggestions of product or information. Typically, recommender systems rely on a bipartite graph model to capture user interest. As an extension, some boosted methods analyze content information to further improve the quality of personalized recommendation. However, due to the prevalence of short and sparse messages in online social media, traditional content-boosted methods do not guarantee to capture user preference accurately especially for web contents. In this paper, we propose a novel graphical model to extract hidden topics from web contents, cluster web contents, and detect users' interests on each cluster. In addition, we introduce two reranking models which utilize the detected user interest to further boost the quality of personalized recommendation. Experiment results on a public dataset demonstrated the limitation of a traditional content-boosted approach, and also showed the validity of our proposed techniques.
77988EDA	Two types of efficient algorithms for fast implementation of the 2-D discrete cosine transform (2-D DCT) are developed. One involves recursive structure which implies that the algorithm for (M/2 X N/2) block be extended to (M X N/2) (M/2 X M) and (M X N) blocks (M and N are integer powers of two). The second algorithm is nonrecursive and therefore it has to be tailored for each block size. Both algorithms involve real arithmetic and they reduce the number of multiplications significantly compared to the fast algorithm developed by Chen et al. [8], while the number of additions remain unchanged.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7E271971	We propose a Bayesian Network (BN) model to integrate multiple contextual information and the image measurements for image segmentation. The BN model systematically encodes the contextual relationships between regions, edges and vertices, as well as their image measurements with uncertainties. It allows a principled probabilistic inference to be performed so that image segmentation can be achieved through a most probable explanation (MPE) inference in the BN model. We have achieved encouraging results on the horse images from the Weizmann dataset. We have also demonstrated the possible ways to extend the BN model so as to incorporate other contextual information such as the global object shape and human intervention for improving image segmentation. Human intervention is encoded as new evidence in the BN model. Its impact is propagated through belief propagation to update the states of the whole model. From the updated BN model, new image segmentation is produced.
7D05A275	As the circuit complexity is increasing in demand for the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera is one of them where realtime image capturing, compression and storage of compressed image data is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image.
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
5E453122	In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al.
7F4AA6EA	Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
7B368C93	Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in more highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties. 
81262D6D	A Complex Adaptive System (CAS) is a network of self-organizing, intelligent agents that share knowledge and adapt their operations in order to achieve overall system goals. Three things are needed to understand, design, and evaluate CAS. First, a mathematical model or way-of-thinking about CAS, called Context-Sensitive Systems (CSS) theory, is required to provide a solid foundation upon which to represent and describe the kinds of interactions that occur among the CAS agents during system operation. Second, a graphical modeling language is required that implements CSS theory in a way that enhances visualization and understanding of CAS. Third, a systems design and evaluation tool is required that makes it easy to apply CSS theory, expressed using a graphical modeling language, to understand, design, and evaluate CAS. As an example, an OpEMCSS model of two intelligent agents is discussed that learn rules and maximize their average reward in the prisoner's dilemma game.
80D1B10B	We present a dynamic graphical model (DGM) for automated multi-instrument musical transcription. By multi-instrument transcription, we mean a system capable of listening to a recording in which two or more instruments are playing, and identifying both the notes that were played and the instruments that played them. Our transcription system models two musical instruments, each capable of playing at most one note at a time. We present results for two-instrument transcription on piano and violin sounds.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
761D5749	We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
7576CA59	A new VLSI algorithm and its associated systolic array architecture for a prime length type IV discrete cosine transform is presented. They represent the basis of an efficient design approach for deriving a linear systolic array architecture for type IV DCT. The proposed algorithm uses a regular computational structure called pseudoband correlation structure that is appropriate for a VLSI implementation. The proposed algorithm is then mapped onto a linear systolic array with a small number of I/O channels and low I/O bandwidth. The proposed architecture can be unified with that obtained for type IV DST due to a similar kernel. A highly efficient VLSI chip can be thus obtained with good performance in the architectural topology, computing parallelism, processing speed, hardware complexity and I/O costs similar to those obtained for circular correlation and cyclic convolution computational structures.
7D3C2818	AnN-point discrete Fourier transform (DFT) algorithm can be used to evaluate a discrete cosine transform by a simple rearrangement of the input data. This method is about two times faster compared to the conventional method which uses a2N-point DFT.
5937B17A	By combining the polynomial transform and radix-q decomposition, the paper presents a new algorithm for the type-III r-dimensional discrete Cosine transform (rD-DCT-III) with size ql1 ql2  ...  qlr, where q is an odd prime number. The number of multiplications for computing an rD-DCT-III is approximately 1/r times that needed by the row-column method while the number of additions increase slightly. The total number of operations (additions plus multiplications) is also reduced. The proposed algorithm has a simple computational structure because it needs only 1D-DCT-III and the polynomial transform.
75538016	Discrete cosine transforms (DCT) are essential tools in numerical analysis and digital signal processing. Processors in digital signal processing often use fixed point arithmetic. In this paper, we consider the numerical stability of fast DCT algorithms in fixed point arithmetic. The fast DCT algorithms are based on known factorizations of the corresponding cosine matrices into products of sparse, orthogonal matrices of simple structure. These algorithms are completely recursive, are easy to implement and use only permutations, scaling, butterfly operations, and plane rotations/rotation-reflections. In comparison with other fast DCT algorithms, these algorithms have low arithmetic costs. Using von Neumann Goldstine model of fixed point arithmetic, we present a detailed roundoff error analysis for fast DCT algorithms in fixed point arithmetic. Numerical tests demonstrate the performance of our results.
81736770	A new algorithm for the type-II multidimensional discrete cosine transform (MD-DCT) is proposed. Based on the polynomial transform, the rD-DCT with size N/sub 1//spl times/N/sub 2//spl times//spl middot//spl middot//spl middot//spl times/N/sub r/, where N/sub i/ is a power of 2, can be converted into a series of one dimensional (1-D) discrete cosine transforms (DCTs). The algorithm achieves considerable savings on the number of operations compared with the row-column method. For example, the number of multiplications for computing an r-dimensional DCT is only 1/r times that needed by the row-column method, and the number of additions is also reduced. Compared with other known polynomial transform algorithms for MD-DCT and the most recently presented algorithm for MD-DCT, the proposed one uses about the same number of operations. However, advantages such as better computational structure and flexibility in the choice of dimensional sizes can be achieved.
7D843341	A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.
7B64D19B	The discrete cosine transform of type IV (DCT-IV) and corresponding discrete sine transform of type IV (DST-IV) have played key role in the efficient implementation of orthogonal lapped transforms and perfect reconstruction cosine-modulated filter banks such as the oddly stacked modified discrete cosine transform (MDCT) or equivalently, the modulated lapped transform (MLT). However, the DCT-IV and DST-IV of double sizes are related to two variants of filter banks defined by Dolby Labs AC-3 digital audio compression algorithm. Since these two variants of filter banks are efficiently computed by recently proposed new fast algorithm for the oddly stacked MDCT (Signal Processing 82 (2002) 433), it is shown that the efficient DCT-IV and DST-IV computation can be realized via the MDCT of double size. The careful analysis of regular structure of the new fast MDCT algorithm allows to extract a new DCT-IV/DST-IV computational structure and to suggest a new sparse matrix factorization of the DCT-IV matrix. Finally, the new DCT-IV/DST-IV computational structure provides an alternative efficient implementation of the forward and inverse MDCT in layer III of MPEG (MP3) audio coding.
7A1904AD	A new recursive algorithm and two types of circuit architectures are presented for the computation of the two-dimensional discrete cosine transform (2D DCT). The new algorithm permits to compute the 2D DCT by a simple procedure of the 1D recursive calculations involving only cosine coefficients. The recursive kernel for the proposed algorithm contains a small number of operations. Also, it requires a smaller number of pre-computed data compared with many of existing algorithms in the same category. The kernel can be easily implemented in a simple circuit block with a short critical delay path. In order to evaluate the performance improvement resulting from the new algorithm, an architecture for the 2D DCT designed by direct mapping from the computation structure of the proposed algorithm has been implemented in an FPGA board. The results show that the reduction of the hardware consumption can easily reach 25 and the clock frequency can increase 17 compared with a system implementing a recently reported 2D DCT recursive algorithm. For a further reduction of the hardware, another architecture has been proposed for the same 2D DCT computation. Using one recursive computation block to perform different functions, this architecture needs only approximately one-half of the hardware that is required in the first architecture, which has been confirmed by an FPGA implementation.
7FA005E7	This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.
801D5F6A	A method is proposed to factor the type-II discrete cosine transform (DCT-II) into lifting steps and additions. After approximating the lifting matrices, we get a new type-II integer discrete cosine transform (IntDCT-II) that is float-point multiplication free. Based on the relationships among the various types of DCTs, we can generally factor any DCTs into lifting steps and additions and then get four types of integer DCTs, which need no float-point multiplications. By combining the polynomial transform and the one-dimensional (1-D) integer cosine transform, a two-dimensional (2-D) integer discrete cosine transform is proposed. The proposed transform needs only integer operations and shifts. Furthermore, it is nonseparable and requires a far fewer number of operations than that used by the corresponding row-column 2-D integer discrete cosine transform.
7E83E2D3	In this paper, a generalized fast computational algorithm for the n-dimensional discrete cosine transform (DCT) of length N=2/sup m/ (m/spl ges/2) is presented. The developed algorithm is proved and its efficiency is evaluated theoretically. The theoretical results show that compared with the conventional method of computing the one-dimensional along n directions, the number of multiplications needed by our algorithm is only 1/n of that required by the conventional method; for the total number of additions, the latter is a bit more when N/spl les/8 and much fewer when N/spl ges/16 than the former. To validate the proposed algorithm, we take the case when n=3 as an example and apply it to motion-picture coding. The results show that our method is superior to MPEG-2 in speed and coding performance. The algorithm is clearly described and it is easy to make a computer program for implementation.
7D419CF0	New algorithms are proposed for the type-III multidimensional discrete cosine transform (MD-DCT-III). The polynomial transform is used to convert the type-III MD-DCT into a series of one-dimensional type-III discrete cosine transforms (1-D-DCT-III). The algorithms achieve considerable savings on the number of operations compared to the row column method. For computing an r-dimensional DCT-III, the number of multiplications required by the proposed algorithm is only 1//spl tau/ times that needed by the row-column method, and the number of additions is also reduced. Compared to other known fast algorithms for two-dimensional- and MD-DCTs, the proposed method uses about the same number of operations. However, advantages such as better computational structure and flexibility on the choice of dimensional sizes can be achieved.
77E8F1CA	Due to technology scaling trends, the accurate and efficient calculations of the temperature distribution corresponding to a specific circuit layout and power density distribution will become indispensable in the design of high-performance very large scale integrated circuits. In this paper, we present three highly efficient thermal simulation algorithms for calculating the on-chip temperature distribution in a multilayered substrate structure. All three algorithms are based on the concept of the Green function and utilize the technique of discrete cosine transform. However, the application areas of the algorithms are different. The first algorithm is suitable for localized analysis in thermal problems, whereas the second algorithm targets full-chip temperature profiling. The third algorithm, which combines the advantages of the first two algorithms, can be used to perform thermal simulations where the accuracy requirement differs from place to place over the same chip. Experimental results show that all three algorithms can achieve relative errors of around 1% compared with that of a commercial computational fluid dynamic software package for thermal analysis, whereas their efficiencies are orders of magnitude higher than that of the direct application of the Green function method.
7D9FC910	Advances in wavelet transforms and quantization methods have produced algorithms capable of surpassing the existing image compression standards like the Joint Photographic Experts Group (JPEG) algorithm. The existing compression methods for JPEG standards are using DCT with arithmetic coding and DWT with Huffman coding. The DCT uses a single kernel where as wavelet offers more number of filters depends on the applications. The wavelet based Set Partitioning In Hierarchical Trees (SPIHT) algorithm gives better compression. For best performance in image compression, wavelet transforms require filters that combine a number of desirable properties, such as orthogonality and symmetry, but they cannot simultaneously possess all of these properties. The relatively new field of multiwavelets offer more design options and can combine all desirable transform features. But there are some limitations in using the SPIHT algorithm for multiwavelet coefficients. This paper presents a new method for encoding the multiwavelet decomposed images by defining coefficients suitable for SPIHT algorithm which gives better compression performance over the existing methods in many cases.
7E0F3E15	The Discrete Tchebichef Transform (DTT) which based on discrete orthogonal Tchebichef polynomials can be an alternative to the Discrete Cosine Transform (DCT) for image processing such as image compression and image recognition as the properties of the DTT are similar to that of the DCT. The DTT not only has higher energy compactness than the DCT in images that have high illumination value variations such as artificial diagrams but also has the advantage of easily computation using a set of recurrence relations. In this paper, the DTT will be introduced with explanation of its similarity to DCT, also a new fast and efficient 4x4 algorithm for computing the DTT coefficients which can be used in image compression will be proposed. This algorithm reduces computational complexity as measured in terms of the number of arithmetic operations while keeping the accuracy of the reconstructed images. 
8112BD3C	Recently,  with  the  advances  in  digital  signal  processing,  compression  of  biomedical  signals  has  received  great attention for telemedicine applications. In this paper, an adaptive transform  coding-based  method  for  compression  of  respiratory and  swallowing  sounds  is  proposed.  Using  special  characteristics  of  respiratory  sounds,  the  recorded  signals  are  divided  into stationary and nonstationary portions, and two different bit allocation methods (BAMs) are designed for each portion. The method was applied to the data of 12 subjects and its performance in terms of overall signal-to-noise ratio (SNR) values was calculated at different bit rates. The performance of different quantizers was also considered  and  the  sensitivity  of  the  quantizers  to  initial  conditions has been alleviated. In addition, the fuzzy clustering method was examined for classifying the signal into different numbers of clusters and investigating the performance of the adaptive BAM with increasing the number of classes. Furthermore, the effects of assigning  different  numbers  of  bits  for  encoding  stationary  and nonstationary  portions  of  the  signal  were  studied.  The  adaptive BAM with variable number of bits was found to improve the SNR values of the fixed BAM by 5 dB. Last, the possibility of removing the  training  part  for  finding  the  parameters  of  adaptive  BAMs for  each individual was  investigated. The results indicate that it is  possible  to  use  a  predefined  set  of  BAMs  for  all  subjects  and remove the training part completely. Moreover, the method is fast enough to be implemented for real-time application.
76FFC627	In this paper introduce new idea for image compression based on the two levels DWT. The low-frequency sub-band is minimized by using DCT with the Minimize-Matrix-Size-Algorithm, which is converting the AC-coefficients into array contains stream of real values, and then store the DC-coefficients are stored in a column called DC-Column. DC-Column is transformed by one-dimensional DCT to be converted into T-Matrix, then T-Matrix compressed by RLE and arithmetic coding. While the high frequency sub-bands are compressed by the technique; Eliminate Zeros and Store Data (EZSD). This technique eliminates each 8 × 8 sub-matrix contains zeros from the high frequencies sub-bands, in another hands store nonzero  data in an array.The results of our compression algorithm compared with JPEG2000 by using four different gray level images. 
80A84DE7	This paper presents a robust image watermarking method in discrete cosine transform (DCT) domain based on chaotic sequences encryption. Exploiting some characteristics of Human Visual System (HVS) and DC(Direct Current) components having much larger perceptual capacity than any AC (Alternating Current)components, watermark is embedded into the DC components of the host image. First, we scramble the watermark image to avoid the block effect. Then we split the host image and the scrambled watermark image into 8×8block respectively, and the scrambled watermark is embedded into the DC components of the host image. The experimental results show that the embedded watermark is invisible and robust against noise and commonly used image processing methods such as Gaussian, JPEG compression, Median filtering etc
792E12D0	A modified discrete Fourier-cosine transform (DFCT) algorithm and its VLSI implementation on a high speed VLSI chip are presented. The proposed DFCT algorithm achieves a considerably higher throughput rate when compared to other implementations by exploiting the inherent parallelism of the new flowgraph, proposed for the DFCT algorithm, to the full. DFCTs of greater length and two dimensional DFCTs can be performed by a set of such chips, at board level. 1.2 DLM CMOS technology was used for the implementation of the chip and the chip die size is 112.54 mm2. The throughput rate for the DFCT is 750 Mbitss.
5BE8D5F7	Digital image and video in their raw form require an enormous amount of storage capacity. Considering the important role played by digital imaging and video in medical and health science, it is necessary to develop a system that produces high degree of compression while preserving critical image/video information. In this paper, we present a hybrid algorithm that performs the discrete cosine transform on the discrete wavelet transform coefficients. Simulation has been carried out on several medical and endoscopic images and videos. The results show that the proposed hybrid algorithm performs much better in term of peak-signal-to-noise-ratio with a higher compression ratio compared to standalone DCT and DWT algorithms. The scheme is intended to be used as the image/video compressor engine in medical imaging and video applications, such as, telemedicine and wireless capsule endoscopy
60C2371C	Small length Discrete Cosine Transforms (DCT's) are used for image data compression. In that case, length 8 or 16 DCT's are needed to be performed at video rate. We propose two new implementation of DCT's which have several interesting features, as far as VLSI implementation is concerned. A first one, using modulo-arithmetic, needs only one multiplication per input point, so that a single multiplier is needed on-chip. A second one, based on a decomposition of the DCT into polynomial products, and evaluation of these polynomial products by distributed arithmetic, results in a very small chip, with a great regularity and testability. Furthermore, the same structure can be used for FFT computation by changing only the ROM-part of the chip. Both new architectures are mainly based on a new formulation of a length-2nDCT as a cyclic convolution, which is explained in the first section of the paper.
7E6FD3E0	An adaptive DCT-based image compression algorithm for radar images is proposed, tested and compared to JPEG and to classical coding algorithms for remote sensing imagery. The Modified Adaptive Discrete Cosine Transform (MADCT) scheme is proposed, which allows one to classify each image block by means of a threshold criterion based on AC and DC activity. The strategy of transmission of the DCT coefficients, the recovering process of blocks incorrectly discarded, and the bit-allocation phase have been properly designed to provide high compression of two classes of images: X-band real-aperture radar images for ship traffic control, and SAR images for browsing applications. The experimental results, in terms of PSNR and compression ratio, prove the superiority of the novel scheme with respect to standard coding techniques.
7A6D7FA4	Digital images in their uncompressed form require an enormous amount of storage capacity. Such uncompressed data needs large transmission bandwidth for the transmission over the network. Discrete Cosine Transform (DCT) is one of the widely used image compression method and the Discrete Wavelet Transform (DWT) provides substantial improvements in the quality of picture because of multi resolution nature. Image compression reduces the storage space of image and also maintains the quality information of the image. In this research study the performance of three most widely used techniques namely DCT, DWT and Hybrid DCT-DWT are discussed for image compression and their performance is evaluated in terms of Peak Signal to Noise Ratio (PSNR), Mean Square Error (MSE) and Compression Ratio (CR). The experimental results obtained from the study shows that the Hybrid DCT- DWT technique for image compression has in general a better performance than individual DCT or DWT. 
816929C1	A novel block matching algorithm for motion estimation in a video frame sequence, well suited for a high performance FPGA implementation is presented in this paper. The algorithm is up to 40% faster when compared to one of the fastest existing algorithms, viz., one-at-a-time step search algorithm without compromising either in the image quality or in the compression effected. The speed advantage is preserved even in the event of a sudden scene change in a video sequence. The proposed algorithm is also capable of dynamically detecting the direction of motion of image blocks. The FPGA implementation of the algorithm is capable of processing color pictures of sizes up to 1024x768 pixels at the real time video rate of 25 frames/second and conforms to MPEG-2 standards.
76022FC0	This paper assesses the arithmetic benefits provided by the Residue Number System (RNS) for building Digital Signal Processing (DSP) systems with Field-Programmable Logic (FPL) technology. The quantifiable benefits of this approach are studied in the context of a new Fast Cosine Transform (FCT) architecture enhanced by using the Quadratic Residue Number System (QRNS). The system reduces the number of adders and multipliers required for the N-point Discrete Cosine Transform (DCT) and provides high throughput. For an FPL-based implementation, the proposed design gets significant improvements over an equivalent 2C structure. By using up to 6-bit moduli, an overall increase in the system performance of about 140% is achieved. If this speed increase is considered along with the penalty in device resources, the presented QRNS-based FCT system provides an improvement in the area-delay figure factor of about 20%. Finally, the conversion overhead was carefully studied and it was found that the quantifiable benefits of the proposed design are not affected when converters are included 
80FFB575	In this paper, we propose a system-level error tolerance scheme for systems where a linear transform is combined with quantization. These are key components in multimedia compression systems, e.g., video and image codecs. Using the concept of acceptable degradation, our scheme classifies hardware faults into acceptable and unacceptable faults. We propose analysis techniques that allow us to estimate the faults' impact on compression performance, and in particular on the quality of decoded images/video. We consider as an example the discrete cosine transform (DCT), which is part of a large number of existing image and video compression systems. We propose methods to establish thresholds of acceptable degradation and corresponding testing algorithms for DCT-based systems. Our results for a JPEG encoder using a typical DCT architecture show that over 50% of single stuck-at interconnection faults in one of its 1D DCT modules lead to imperceptible quality degradation in the decoded images, over the complete range of compression rates at which JPEG can operate.
7F066D3F	Frequency -warped signal processing techniques are attractive to many wideband speech and audio applications since they have a clear connection to the frequency resolution of human hearing. A warped version of the linear predictive coding (LPC) for speech compression is implemented in this paper and an analysis of the application of Set Partitioning In Hierarchical Trees (SPIHT) algorithm to the compression of speech signals is performed. It has been shown that the proposed scheme i.e. Warped LPC with MLT_SPIHT algorithm produces an enhancement in speech quality. The proposed scheme is based on the combination of the Modulated Lapped Transform(MLT) and SPIHT. Comparisons are made with Plain LPC Coder, Voice Excited LPC Coder with the coding of the residual signal with DCT, Voice Excited LPC Coder with the coding of the residual signal with MLT and SPIHT. The performance of the coders described has been assessed by computer simulation in terms of a) Signal -to -noise ratio (SNR) b) Compression ratio c) Informal subjective listening test.
80CB08BD	This paper presents a new adaptive algorithm for speech compression using cosine packet transform. The proposed algorithm uses packet decomposition, which reduces a computational complexity of a system. This paper compare the compression ratio of methods using wavelet transform, cosine transform, wavelet packet transform and proposed adaptive algorithm using cosine packet transform for different speech signal samples. The mean compression ratio is calculated for all the methods and compared. The implemented results show that the proposed compression algorithm gives the better performance for speech signals.
7996F7EA	Unlike classical wired networks and wireless sensor networks, WMSN differs from their predecessor’s scalar network basically in the following points; nature and size of data being transmitted, important memory resources, as well as, power consumed per each node for processing and transmission. The most effective solution to overcome those problems is image compression. As the image contains massive amount of redundancies resulting from high correlation between pixels, many compression algorithms have been developed. The main objective of this survey was to study and analyze relevant research directions and the most recent algorithms of image compression over WMSN. This survey characterizes the benefits and shortcomings of recent efforts of such algorithms. Moreover, it provides an open research issue for each compression method; and its potentials to WMSN. Reducing consumed power thus granting long life time is considered the main performance metric and will be the main target in the investigated solution
77ABB4F5	As data compression plays now an important role in the development of medical PACS, a technique has been developed for medical image sequences storage and transmission in order to obtain very high compression ratio: in dynamic nuclear medicine studies it can achieve a compression ratio as high as 100:1 without significant degradation. The implemented technique combines two methods which multiply their effects. In a first step, a principal component analysis (PCA) of the image series is performed. It extracts a limited number of principal components and their associated images. For data compression it is not necessary to perform an oblique factor analysis to estimate the so-called ‘physiological functions’ and their spatial distributions as in factor analysis of dynamic structures (FADS). In a second step, the principal images are compressed by means of a transform coding procedure: an adaptive block-quantization technique using the 2D discrete cosine transform (DCT) is implemented, followed by a statistical quantization method to encode the DCT coefficients. To reconstruct the principal images, an inverse DCT is applied. Then the original series is computed from the reconstructed images combined with the principal components which have been stored without any modification. The reconstructed series is compared to the original series, as well as the time activity curves generated on different regions of interest (ROI) and the factor estimates obtained using FADS performed on the two series. Method and evaluation are illustrated on an example of first pass radionuclide angiocardiography.
80BAFEB7	This paper presents the results of a questionnaire on user behaviour during searching the Web as part of information gathering tasks. In this study, users' Web activities related to finding information, comparing information, managing information, and re-finding information, using different Web search and navigation tools, used for information gathering tasks are explored. The results indicate that current Web tools lack important functionalities for supporting how users find, re-find, and manage information during Web information gathering tasks. Furthermore, the results indicate that visual characteristics of search results, re-finding tools that bookmark complete and partial search sessions with user annotation, and integrated information management features may be useful in improving how users gather information on the Web.
7C5CBF21	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.
7BC471DD	Due to the rapid growth in the size of the web, web search engines are facing enormous performance challenges. The larger engines in particular have to be able to process tens of thousands of queries per second on tens of billions of documents, making query throughput a critical issue. To satisfy this heavy workload, search engines use a variety of performance optimizations including index compression, caching, and early termination. We focus on two techniques, inverted index compression and in- dex  caching,  which  play  a  crucial  rule  in  web  search  engines  as well as other high-performance information retrieval systems.  We perform a comparison and evaluation of several inverted list compression algorithms, including new variants of existing algorithms that have not been studied before.  We then evaluate different inverted list caching policies on large query traces, and finally study the possible performance benefits of combining compression and caching. The overall goal of this paper is to provide an updated discussion and evaluation of these two techniques, and to show how to select the best set of approaches and settings depending on parameter such as disk speed and main memory cache size.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
5CFC04D2	Commercial web search engines adopt parallel and replicated architecture in order to support high query throughput. In this paper, we investigate the effect of caching on the throughput in such a setting. A simple scheme, called uniform caching, would replicate the cache content to all servers. Unfortunately, it does not exploit the variations among queries, thus wasting memory space on caching the same cache content redundantly on multiple servers. To tackle this limitation, we propose a diversified caching problem, which aims to diversify the types of queries served by different servers, and maximize the sharing of terms among queries assigned to the same server. We show that it is NP-hard to find the optimal diversified caching scheme, and identify intuitive properties to seek good solutions. Then we present a framework with a suite of techniques and heuristics for diversified caching. Finally, we evaluate the proposed solution with competitors by using a real dataset and a real query log.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7EE3F39D	Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines. We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information. With this aim, we conducted a survey with 236 experienced web users. The results showed that this group has frequently used key strategies (e.g., using several browser windows in parallel) that they find important, whereas some of the strategies that have been suggested in previous studies are clearly less important for them (e.g., including URLs on a webpage). In some aspects, such as query formulation, this group resembles less experienced web users. For instance, we found that most of the respondents had misconceptions about how their search engine handles queries, as well as other problems with information search and re-access. In addition to presenting the prevalence of the strategies and rationales for their use, we present concrete designs solutions and ideas for making the key strategies also available to less experienced users.
78A4DC3B	The phenomenon of sponsored search advertising â where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results â is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
7644AE7A	We study the problem of caching query result pages in Web search engines. Popular search engines receive millions of queries per day, and for each query, return a result page to the user who submitted the query. The user may request additional result pages for the same query, submit a new query, or quit searching altogether. An efficient scheme for caching query result pages may enable search engines to lower their response time and reduce their hardware requirements. This work studies query result caching within the framework of the competitive analysis of algorithms. We define a discrete time stochastic model for the manner in which queries are submitted to search engines by multiple user sessions. We then present an adaptation of a known online paging scheme to this model. The expected number of cache misses of the resulting algorithm is no greater than 4 times the expected number of misses that any online caching algorithm will experience under our specific model of query generation.
7E418727	Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.
75F59C14	To inform the design of next-generation Web search tools, researchers must better understand how users find, manage, and refind online information. Synthesizing results from one of their studies with related work, the authors propose a search engine use model based on prior task frequency and familiarity.
7B829984	In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.
7D837E78	When a person issues a query, that person has expectations about the search results that will be returned. These expectations can be based on the current information need, but are also influenced by how the searcher believes the search engine works, where relevant results are expected to be ranked, and any previous searches the individual has run on the topic. This paper looks in depth at how the expectations people develop about search result lists during an initial query affect their perceptions of and interactions with future repeat search result lists. Three studies are presented that give insight into how people recall, recognize, and reuse results. The first study (a study of recall) explores what people recall about previously viewed search result lists. The second study (a study of recognition) builds on the first to reveal that people often recognize a result list as one they have seen before even when it is quite different. As long as those aspects that the searcher remembers about the initial list remain the same, other aspects can change significantly. This is advantageous because, as the third study (a study of reuse) shows, when a result list appears to have changed, people have trouble re-using the previously viewed content in the list. They are less likely to find what they are looking for, less happy with the result quality, more likely to find the task hard, and more likely to take a long time searching. Although apparent consistency is important for reuse, people's inability to recognize change makes consistency without stagnation possible. New relevant results can be presented where old results have been forgotten, making both old and new content easy to find.
766A0371	Assistance technology is undoubtedly one of the important elements in the commercial search engines, and routing the user towards the right direction throughout the search sessions is of great importance for providing a good search experience. Most search assistance methods in the literature that involve query generation, query expansion and other techniques consider each suggestion candidate individually, which implies an independence assumption. We challenge this independence assumption and give a method to maximize the utility of a given set of suggestions. For this, we will define a measure of conditional utility for query pairs using query-URL bipartite graphs based on the session logs (clicked and viewed URLs). Afterwards, we remove the redundant queries from the suggestion set using a greedy algorithm to be able to replace them with more useful ones. Both offline (based on user studies and session log analysis) and online (based on millions of user interactions) evaluations show that modeling the conditional utility and maximizing the utility of the set of queries (by eliminating redundant ones) significantly increases the effectiveness of the search assistance both for the presubmit and postsubmit modes.
80026F92	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
7DCF9C8B	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.
5995BFC1	We study the process in which search engines with segmented indices serve queries. In particular, we investigate the number of result pages which search engines should prepare during the query processing phase.Search engine users have been observed to browse through very few pages of results for queries which they submit. This behavior of users suggests that prefetching many results upon processing an initial query is not efficient, since most of the prefetched results will not be requested by the user who initiated the search. However, a policy which abandons result prefetching in favor of retrieving just the first page of search results might not make optimal use of system resources as well. We argue that for a certain behavior of users, engines should prefetch a constant number of result pages per query. We define a concrete query processing model for search engines with segmented indices, and analyze the cost of such prefetching policies. Based on these costs, we show how to determine the constant which optimizes the prefetching policy. Our results are mostly applicable to local index partitions of the inverted files, but are also applicable to processing of short queries in global index architectures.
8035BDA2	A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, “Can link information result in better rankings?”, “Do longer queries result in better answers?”, and, “Do TREC systems work well on Web data?” The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art.
804E7F29	A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel, otherwise it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th-percentile), which we call extreme tail latency.We propose a prediction framework to reduce the extreme tail latency of search servers. The framework has a unique set of characteristics to predict long-running queries with high recall and improved precision. Specifically, prediction is delayed by a short duration to allow many short-running queries to complete without parallelization, and to allow the predictor to collect a set of dynamic features using runtime information. These features estimate query execution time with high accuracy. We also use them to estimate the prediction errors to override an uncertain prediction by selectively accelerating the query for a higher recall.We evaluate the proposed prediction framework to improve search engine performance in two scenarios using a simulation study: (1) query parallelization on a multicore processor, and (2) query scheduling on a heterogeneous processor. The results show that, for both scenarios, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision.
808285A5	Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called "information need". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
7D08AE3B	Long web search result lists can be hard to browse. We demonstrated experimentally, in a previous study, the usefulness of a categorization algorithm and filtering interface. However, the nature of interaction in real settings is not known from an experiment in laboratory settings. To address this problem, we provided our categorizing web search user interface to 16 users for a two month period. The interactions with the system were logged and the users' opinions were elicited with two questionnaires. The results show that categories are successfully used as part of users' search habits. They are helpful when the result ranking of the search engine fails. In those cases, the users are able to access results that locate far in the rank order list with the categories. Users can also formulate simpler queries and find needed results with the help of the categories. In addition, the categories are beneficial when more than one result is needed like in an exploratory or undirected search task.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
7AB88A91	Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14\% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing algorithms even on HDD-based architecture.
7CF71B8F	The Web holds a great quantity of material that can be used to enhance classroom instruction. However, it is not easy to retrieve this material with the search engines currently available. This study produced a specialized search assistant based on Google that significantly increases the number of instances in which teachers find the desired learning objects as compared to using this popular public search engine directly. Success in finding learning objects by study participants went from 80&percnt; using Google alone to 96&percnt; when using our search assistant in one scenario and, in another scenario, from a 40&percnt; success rate with Google alone to 66&percnt; with our assistant. This specialized search assistant implements features such as bilingual search and term suggestion which were requested by teacher participants to help improve their searches. Study participants evaluated the specialized search assistant and found it significantly easier to use and more useful than the popular search engine for the purpose of finding learning objects. 
7F6257E8	Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.
78E21799	Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!
7A6F5F82	Finding the right information in the World Wide Web is becoming a fundamental problem, since the amount of global information that the WWW contains is growing at an incredible rate. In this paper, we present a novel method to extract from a web object its “hyper” informative content, in contrast with current search engines, which only deal with the “textual” informative content. This method is not only valuable per se, but it is shown to be able to considerably increase the precision of current search engines, Moreover, it integrates smoothly with existing search engines technology since it can be implemented on top of every search engine, acting as a post-processor, thus automatically transforming a search engine into its corresponding “hyper” version. We also show how, interestingly, the hyper information can be usefully employed to face the search engines persuasion problem.
7BB23ED5	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time
816FE39A	This paper presents a modified diary study that investigated how people performed personally motivated searches in their email, in their files, and on the Web. Although earlier studies of directed search focused on keyword search, most of the search behavior we observed did not involve keyword search. Instead of jumping directly to their information target using keywords, our participants navigated to their target with small, local steps using their contextual knowledge as a guide, even when they knew exactly what they were looking for in advance. This stepping behavior was especially common for participants with unstructured information organization. The observed advantages of searching by taking small steps include that it allowed users to specify less of their information need and provided a context in which to understand their results. We discuss the implications of such advantages for the design of personal information management tools.
7F79648F	The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.
7B2E5EC6	In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want
7BC0845F	We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We propose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strategies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Yahoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
77445FD3	Re-finding, a common Web task, is difficult when previously viewed information is modified, moved, or removed. For example, if a person finds a good result using the query "breast cancer treatments", she expects to be able to use the same query to locate the same result again. While re-finding could be supported by caching the original list, caching precludes the discovery of new information, such as, in this case, new treatment options. People often use search engines to simultaneously find and re-find information. The Re:Search Engine is designed to support both behaviors in dynamic environments like the Web by preserving only the memorable aspects of a result list. A study of result list memory shows that people forget a lot. The Re:Search Engine takes advantage of these memory lapses to include new results where old results have been forgotten.
777B5147	For many search settings, distributed/replicated search engines deploy a large number of machines to ensure efficient retrieval. This paper investigates how the power consumption of a replicated search engine can be automatically reduced when the system has low contention, without compromising its efficiency. We propose a novel self-adapting model to analyse the trade-off between latency and power consumption for distributed search engines. When query volumes are high and there is contention for the resources, the model automatically increases the necessary number of active machines in the system to maintain acceptable query response times. On the other hand, when the load of the system is low and the queries can be served easily, the model is able to reduce the number of active machines, leading to power savings. The model bases its decisions on examining the current and historical query loads of the search engine. Our proposal is formulated as a general dynamic decision problem, which can be quickly solved by dynamic programming in response to changing query loads. Thorough experiments are conducted to validate the usefulness of the proposed adaptive model using historical Web search traffic submitted to a commercial search engine. Our results show that our proposed self-adapting model can achieve an energy saving of 33% while only degrading mean query completion time by 10 ms compared to a baseline that provisions replicas based on a previous day's traffic.
7F48E77D	With the proliferation of social media, consumers’ cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers’ search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
80FCD18B	Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
7DB5FDAD	This paper reports our efforts to address the grand challenge of the Digital Earth vision in terms of intelligent data discovery from vast quantities of geo-referenced data. We propose an algorithm combining LSA and a Two-Tier Ranking (LSATTR) algorithm based on revised cosine similarity to build a more efficient search engine – Semantic Indexing and Ranking (SIR) – for a semantic-enabled, more effective data discovery. In addition to its ability to handle subject-based search, we propose a mechanism to combine geospatial taxonomy and Yahoo! GeoPlanet for automatic identification of location information from a spatial query and automatic filtering of datasets that are not spatially related. The metadata set, in the format of ISO19115, from NASA's SEDAC (Socio-Economic Data Application Center) is used as the corpus of SIR. Results show that our semantic search engine SIR built on LSATTR methods outperforms existing keyword-matching techniques, such as Lucene, in terms of both recall and precision. Moreover, the semantic associations among all existing words in the corpus are discovered. These associations provide substantial support for automating the population of spatial ontologies. We expect this work to support the operationalization of the Digital Earth vision by advancing the semantic-based geospatial data discovery.
777B5147	For many search settings, distributed/replicated search engines deploy a large number of machines to ensure efficient retrieval. This paper investigates how the power consumption of a replicated search engine can be automatically reduced when the system has low contention, without compromising its efficiency. We propose a novel self-adapting model to analyse the trade-off between latency and power consumption for distributed search engines. When query volumes are high and there is contention for the resources, the model automatically increases the necessary number of active machines in the system to maintain acceptable query response times. On the other hand, when the load of the system is low and the queries can be served easily, the model is able to reduce the number of active machines, leading to power savings. The model bases its decisions on examining the current and historical query loads of the search engine. Our proposal is formulated as a general dynamic decision problem, which can be quickly solved by dynamic programming in response to changing query loads. Thorough experiments are conducted to validate the usefulness of the proposed adaptive model using historical Web search traffic submitted to a commercial search engine. Our results show that our proposed self-adapting model can achieve an energy saving of 33% while only degrading mean query completion time by 10 ms compared to a baseline that provisions replicas based on a previous day's traffic.
7CF24ED6	We investigate how people interact with Web search engine result pages using eye-tracking. While previous research has focused on the visual attention devoted to the 10 organic search results, this paper examines other components of contemporary search engines, such as ads and related searches. We systematically varied the type of task (informational or navigational), the quality of the ads (relevant or irrelevant to the query), and the sequence in which ads of different quality were presented. We measured the effects of these variables on the distribution of visual attention and on task performance. Our results show significant effects of each variable. The amount of visual attention that people devote to organic results depends on both task type and ad quality. The amount of visual attention that people devote to ads depends on their quality, but not the type of task. Interestingly, the sequence and predictability of ad quality is also an important factor in determining how much people attend to ads. When the quality of ads varied randomly from task to task, people paid little attention to the ads, even when they were good. These results further our understanding of how attention devoted to search results is influenced by other page elements, and how previous search experiences influence how people attend to the current page.
7DF6AF1C	In modern commercial search engines, the pay-per-click (PPC) advertising model is widely used in sponsored search. The search engines try to deliver ads which can produce greater click yields (the total number of clicks for the list of ads per impression). Therefore, predicting user clicks plays a critical role in sponsored search. The current ad-delivery strategy is a two-step approach which first predicts individual ad CTR for the given query and then selects the ads with higher predicted CTR. However, this strategy is naturally suboptimal and correlation between ads is often ignored under this strategy. The learning problem is focused on predicting individual performance rather than group performance which is the more important measurement.In this paper, we study click yield measurement in sponsored search and focus on the problem---predicting group performance (click yields) in sponsored search. To tackle all challenges in this problem---depth effects, interactive influence, cold start and sparseness of ad textual information---we first investigate several effects and propose a novel framework that could directly predict group performance for lists of ads. Our extensive experiments on a large-scale real-world dataset from a commercial search engine show that we achieve significant improvement by solving the sponsored search problem from the new perspective. Our methods noticeably outperform existing state-of-the-art approaches.
80026F92	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
80650A24	This paper studies the impact of the tail of the query distribution on caches of Web search engines, and proposes a technique for achieving higher hit ratios compared to traditional heuristics such as LRU. The main problem we solve is the one of identifying infrequent queries, which cause a reduction on hit ratio because caching them often does not lead to hits. To mitigate this problem, we introduce a cache management policy that employs an admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. The admission policy uses either stateless features, which depend only on the query, or stateful features based on usage information. The proposed management policy is more general than existing policies for caching of search engine results, and it is fully dynamic. The evaluation results on two different query logs show that our policy achieves higher hit ratios when compared to previously proposed cache management policies.
7D1F7697	As with any application of machine learning, web search ranking requires labeled data. The labels usually come in the form of relevance assessments made by editors. Click logs can also provide an important source of implicit feedback and can be used as a cheap proxy for editorial labels. The main difficulty however comes from the so called position bias - urls appearing in lower positions are less likely to be clicked even if they are relevant. In this paper, we propose a Dynamic Bayesian Network which aims at providing us with unbiased estimation of the relevance from the click logs. Experiments show that the proposed click model outperforms other existing click models in predicting both click-through rate and relevance.
7FA14777	FPGAs (field programmable gate arrays) have appealing features such as customizable internal and external bandwidth and the ability to exploit vast amounts of fine-grain parallelism. In this paper, we explore the applicability of these features in using FPGAs as smart memory engines for search and reorganization computations over spatial pointer-based data structures. The experimental results in this paper suggests that reconfigurable logic, when combined with the data reorganization, can lead to dramatic performance improvements of up to 20x over traditional computer architectures for pointer-based computations, traditionally not viewed as a good match for reconfigurable technologies.
7340259A	In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset. 
80601F25	We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a "Learning to Rank" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
77D497A2	The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user. In this paper we describe an interface which enables users to have more control over the optimization function used to present the results. We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory. We use an algorithm based on bounded greedy selection [5] to construct the result set based on parameters specified by the user.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7FEBCDC1	This paper analyzes the computational complexity of finding relevant documents on the Web. Given a search query that has n significant terms, relevant documents retrieved by search engines will contain at least a number k of the significant terms. The threshold k chosen will depend on the collection of documents and is determined experimentally upon formation of the collection. Algorithms are then provided to compute a similarity ranking. The fundamental analysis is based on combinatorial theory and theorems providing bounds on the runtime complexity of the algorithms are proven.
77B1FCBA	Providing an effective mobile search service is a difficult task given the unique characteristics of the mobile space. Small-screen devices with limited input and interaction capabilities do not make ideal search devices. In addition, mobile content, by its concise nature, offers limited indexing opportunities, which makes it difficult to build high-quality mobile search engines and indexes. In this paper we consider the issue of limited page content by evaluating a heuristic content enrichment framework that uses standard Web resources as a source of additional indexing knowledge. We present an evaluation using a mobile news service that demonstrates significant improvements in search performance compared to a benchmark mobile search engine.
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
02426620	Code Search Engines (CSE) can serve as powerful resources of open source code, as they can search in billions of lines of open source code available on the web. The strength of CSEs can be used for several tasks like searching relevant code samples, identifying hotspots, and finding bugs. However, the major limitations in using CSEs for these tasks are that the returned samples are too many and they are often partial. Our framework addresses the preceding limitations and thereby helps in using CSEs for these tasks. We showed the effectiveness of our framework with two tools developedbased on our framework.
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
7F48E77D	With the proliferation of social media, consumers’ cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers’ search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
58FB967D	A part of a larger user study conducted within the scope of the EU project IRIS was to investigate the preferences of people with disabilities in regard to interface design of Web applications. User requirements of online help and search engines were also in the focus of this study, since the target user group depends heavily upon powerful tools to support them in the process of seeking information. The results showed that user preferences for information presentation vary a great deal, which could be solved by comprehensive user profiling. However, the functionalities of online help and search functions, as presented in the Internet today, need to be enhanced so they can be used more flexibly and be adjusted to the users’ mental representation of problems and information needs to help them fulfill their tasks.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
7BB9B487	The world wide web of today publishes a great number of real-time content, causing the increasing need for a differentiated way of searching. In this paper, three issues related to retrieving real-time content are presented, and their applications are proposed. First, the characteristics of real-time content, as well as the concept of real-time search are introduced. Second, the real-time technologies that enable real-time search are described. Finally, a platform for application services utilizing real-time search is proposed.
75B65C35	We investigate search engines' mechanism for allocating impressions generated from different search terms. This mechanism is equivalent to running an independent GSP auction for each search term only when the number of search terms is small. In practice, the number of search terms is so large that an advertiser cannot possibly communicate to the search engine all the GSP auctions that he wishes to participate in. For example, a travel agency is interested in all search terms pertaining to flight, including "flight to boston", "ticket to SFO", "cheap airfare", etc. Therefore, the search engine introduces broad match keywords as a bidding language that allows an advertiser to submit a bid for multiple GSP auctions at once. However, with broad match keywords, the GSP auctions are no longer independent, i.e. an advertiser's bid in one auction may depend on his bid in another auction.We propose the broad match mechanism as a model that captures this aspect of the multi-keyword sponsored search mechanism. We study the performance of this mechanism under the price of anarchy (POA) framework. We identify two properties of broad match keywords, namely expressiveness and homogeneity, that characterize the POA and we prove almost tight bounds on the POA. The bounds allow us to explore trade-offs between the two properties. We introduce the exact-match-only mechanism whose performance, when compared to that of broad match mechanisms, gives us an insight into the net benefit of broad match keywords. The broad match mechanism can also be viewed as a mechanism that copes with severe communication constraint i.e. the valuation of an advertiser is described by many more numbers than the search engine can solicit.
7FBD8242	The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.
80BE578A	Structured documents (predominantly encoded in XML) utilize markup dialects for several purposes, such as conveying logical structure, or providing rendering instructions. XML structure can also help users to navigate within documents to satisfy their information needs. However, including the user's structural preferences in the ranking of retrieved elements remains a key challenge in XML retrieval. In this paper, we propose an approach for including structural preferences in the ranking of XML elements by improving the structural relevance (SR) of results. SR is an evaluation measure which relies on graphical navigation models to capture the structural preferences of users. We propose several algorithms to post-process search engine output to improve the SR of the output. Experimental results (using data, assessments, and search engines from INEX 2007 and 2008) demonstrate the effect of different combinations of post-processing algorithms and navigation models on the effectiveness of systems.
7B60E99F	Understanding the functional mechanisms of the complex biological system as a whole is drawing more and more attention in global health care management. Traditional Chinese Medicine (TCM), essentially different from Western Medicine (WM), is gaining increasing attention due to its emphasis on individual wellness and natural herbal medicine, which satisfies the goal of integrative medicine. However, with the explosive growth of biomedical data on the Web, biomedical researchers are now confronted with the problem of large-scale data analysis and data query. Besides that, biomedical data also has a wide coverage which usually comes from multiple heterogeneous data sources and has different taxonomies, making it hard to integrate and query the big biomedical data. Embedded with domain knowledge from different disciplines all regarding human biological systems, the heterogeneous data repositories are implicitly connected by human expert knowledge. Traditional search engines cannot provide accurate and comprehensive search results for the semantically associated knowledge since they only support keywords-based searches. In this paper, we present BioTCM-SE, a semantic search engine for the information retrieval of modern biology and TCM, which provides biologists with a comprehensive and accurate associated knowledge query platform to greatly facilitate the implicit knowledge discovery between WM and TCM.
7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
75CC9391	With much information available from open sources on the internet, computer generated databases have become commonplace. The question whether computer generated databases can be protected under the sui generis database right has hitherto received little attention. This article investigates this question and finds that the substantiality of investment, the definition of the rights holder and the interpretation of exclusive rights raise fundamental issues.
7F2BF256	Today, Web browsers can interpret an enormous amount of different file types, including time-continuous data. By consuming an audio or video, however, the hyperlinking functionality of the Web is "left behind" since these files are typically unsearchable, thus not indexed by common text-based search engines. Our XML-based CMML annotation format and the Annodex file format presented in this paper are designed to solve this problem of "dark matter" on the Internet: Continuous media files are annotated and indexed (i.e., Annodexed), enabling hyperlinks to and from the media. Furthermore, the hyperlinks do not typically point to an entire media file, but to and from arbitrary fragments or intervals. The standards proposed in to create the Continuous Media Web have been submitted to the IETF for review.
753A28D6	Document similarity search is to find documents similar to a given query document and return a ranked list of similar documents to users, which is widely used in many text and web systems, such as digital library, search engine, etc. Traditional retrieval models, including the Okapi's BM25 model and the Smart's vector space model with length normalization, could handle this problem to some extent by taking the query document as a long query. In practice, the Cosine measure is considered as the best model for document similarity search because of its good ability to measure similarity between two documents. In this paper, the quantitative performances of the above models are compared using experiments. Because the Cosine measure is not able to reflect the structural similarity between documents, a new retrieval model based on TextTiling is proposed in the paper. The proposed model takes into account the subtopic structures of documents. It first splits the documents into text segments with TextTiling and calculates the similarities for different pairs of text segments in the documents. Lastly the overall similarity between the documents is returned by combining the similarities of different pairs of text segments with optimal matching method. Experiments are performed and results show: 1) the popular retrieval models (the Okapi's BM25 model and the Smart's vector space model with length normalization) do not perform well for document similarity search; 2) the proposed model based on TextTiling is effective and outperforms other models, including the Cosine measure; 3) the methods for the three components in the proposed model are validated to be appropriately employed.
7F561450	The market for Internet search is not only economically and socially important, it is also highly concentrated. Is this a problem? We study the question whether "competition is only a free click away". We argue that the market for Internet search is characterized by indirect network externalities and construct a simple model of search engine competition, which produces a market share development that fits the empirically observed development since 2003 well. We find that there is a strong tendency towards market tipping and, subsequently, monopolization, with negative consequences on economic welfare. Therefore, we propose to require search engines to share their data on previous searches. We compare the resulting "competitive oligopoly" market structure with the less competitive current situation and show that our proposal would spur innovation, search quality, consumer surplus, and total welfare. We also discuss the practical feasibility of our policy proposal and sketch the legal issues involved.
75F71181	One of the enabling technologies of the World Wide Web, along with browsers, domain name servers, and hypertext markup language, is the search engine. Although the Web contains over 100 million pages of information, those millions of pages are useless if you cannot find the pages you need. All major Web search engines operate the same way: a gathering program explores the hyperlinked documents of the Web, foraging for Web pages to index. These pages are stockpiled by storing them in some kind of database or repository. Finally, a retrieval program takes a user query and creates a list of links to Web documents matching the words, phrases, or concepts in the query. Although the retrieval program itself is correctly called a search engine, by popular usage the term now means a database combined with a retrieval program. For example, the Lycos search engine comprises the Lycos Catalog of the Internet and the Pursuit retrieval program. This paper describes the Lycos system for collecting, storing, and retrieving information about pages on the Web. After outlining the history and precursors of the Lycos system, the paper discusses some of the design choices made in building this Web indexer and touches briefly on the economic issues involved in working with very large retrieval systems.
7D600956	Web search engines are powerful tools used to satisfy specific information needs on the web. Their purpose is to maximize user satisfaction when performing this task. Although there are other sources of evidence, besides text, to characterize document relevance for a specific need, especially for HTML documents, current search engines do not allow users to explore these features when posing a query. Search engine queries are based almost exclusively on keywords. We believe that it is possible to improve user satisfaction if HTML tags and document metadata are available to users at query time. In this paper we present Xearch, a meta-search system that wraps public search engines in a framework that improves both the expressiveness of the language available for the user to specify information needs and the control over the answer format. Xearch converts HTML pages to a specific XML Schema, covering text and metadata derived from HTML. User queries are then submitted on this schema and can be specified through keywords but also explore documents’ HTML tags and metadata. Results from our experimental evaluation confirm that it is possible to improve the answer quality with this framework.
77C4F21A	In  this  work,  we  investigate  how  to  propagate  annotated labels for a given single image from the image-level to their corresponding semantic regions, namely Label-to-Region  (L2R),  by  utilizing  the  auxiliary  knowledge  from Internet image search with the annotated image labels as queries.  A nonparametric solution is proposed to perform L2R for single image with complete labels.  First, each la- bel of the image is used as query for online image search engines to obtain a set of semantically related and visually similar images, which along with the input image are encoded as Bags-of-Hierarchical-Patches.   Then,  an efficient  two-stage feature  mining procedure is  presented  to discover  those  input-image  specific,  salient  and  descriptive features for each label from the proposed Interpolation SIFT (iSIFT) feature pool. These features consequently constitute a patch-level representation, and the continuity biased sparse coding is proposed to select few patches from the online images with preference to larger patches to reconstruct a candidate region, which randomly merges the spatially connected patches of the input image.  Such candidate regions are further ranked according to the reconstruction errors, and the top regions are used to derive the label confidence vector for each patch of the input image. Finally, a patch clustering procedure is performed as postprocessing to finalize L2R for the input image.  Extensive experiments on three public databases demonstrate the encouraging performance of the proposed nonparametric L2R solution.
7E48A49D	People often want to know expected future events related to given real world entities. For supporting users in the process of future scenario analysis, we propose several methods that enable to retrieve and analyze future-related opinions from large text collections. In particular, we focus on time-unreferenced predictions, which do not contain any explicit future time reference and hence are more difficult to be retrieved. As a second contribution, we propose estimating validity of predictions by automatically searching for real world events corresponding to the predictions. This kind of analysis aims to help detect predictions that are no longer valid as well as help estimating prediction accuracy of information sources.
5F6157DD	The ability to rapidly locate useful on-line services (e.g. software applications, software components, process models, or service organizations), as opposed to simply useful documents, is becoming increasingly critical in many domains. Current service retrieval technology is, however, notoriously prone to low precision. This paper describes a novel service retrieval approached based on the sophisticated use of process ontologies. Our preliminary evaluations suggest that this approach offers qualitatively higher retrieval precision than existing (keyword and table-based) approaches without sacrificing recall and computational tractability/scalability.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
00B17298	he search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
5A9D2B23	We present an approach to increasing the effectiveness of ranked-output retrieval systems that relies on graphical display and user manipulation of “views” of retrieval results, where a view is the subset of retrieved documents that contain a specified subset of query terms. This approach has been implemented in a system named VIEWER (VIEwing WEb Results), acting as an interface to available search engines. An experimental evaluation of the performance of VIEWER in contrast to AltaVista is the major focus of the paper. We first report the results of an experiment on single, short query searches where VIEWER, used as an interactive ranking system, markedly outperformed AltaVista. We then concentrate on a more realistic searching scenario, involving free query formulation, unconstrained selection of retrieval results, and possibility of query reformulation. We report the results of an experiment where the use of VIEWER, compared to AltaVista, seemed to shift the user effort from inspection to evaluation of results, increasing retrieval effectiveness, and user satisfaction. In particular, we found that the VIEWER users retrieved half as many nonrelevant documents as the AltaVista users while retrieving a comparable number of relevant documents.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
80BF1EF4	The information explosion on the Internet makes it hard for users to obtain required information from the Web searched results in a more personalized way. For the same input word, most search engines return the same result to each user without taking into consideration user preference. For many users, it is no longer sufficient to get non-customized results. It is crucial to analyze users' search and browsing behaviors based on searching keywords entered by users, the clicking rate of each link in the result and the time they spend on each site. To this end, we have proposed a method to derive user searching profiles. We have also proposed a mechanism to derive document profiles, based on similarity score of documents. In this paper, we discuss how to use our model to combine the user searching profiles and the document profile, with a view to presenting customized search results to the users.
76244FC6	We conduct a broad survey of query-adaptive search strategies in a variety of application domains, where the internal retrieval mechanisms used for search are adapted in response to the anticipated needs for each individual query experienced by the system. While these query-adaptive approaches can range from meta-search over text collections to multimodal search over video databases, we propose that all such systems can be framed and discussed in the context of a single, unified framework. In our paper, we keep an eye towards the domain of video search, where search cues are available from a rich set of modalities, including textual speech transcripts, low-level visual features, and high-level semantic concept detectors. The relative efficacy of each of the modalities is highly variant between many types of queries. We observe that the state of the art in query-adaptive retrieval frameworks for video collections is highly dependent upon the definition of classes of queries, which are groups of queries that share similar optimal search strategies, while many applications in text and Web retrieval have included many advanced strategies, such as direct prediction of search method performance and inclusion of contextual cues from the searcher. We conclude that such advanced strategies previously developed for text retrieval have a broad range of possible applications in future research in multimodal video search.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
803D2737	In the current information age, the dominant method for information search is by providing few keywords to a search engine. Keyword search is currently one of the most important operations in search engines and numerous other applications. In this paper we propose a new text indexing technique for improving the performance of keyword search. Our proposed technique not only speeds up searching operations but also the operations for inserting and for deleting keywords, which are particularly important for the ever increasing and dynamic changing databases such as that for search engines. We propose to partition all keywords into search trees based on the first character and the length of the keywords. Our partitioning scheme creates a much more even distribution of keywords and results in a 32% speedup in the worst cases and a 1% speedup in the average cases in comparing to one of the leading text indexing techniques called burst tries. In addition, our proposed technique stores document indexes only at the leaf nodes of the search trees and results in efficient algorithms for searching, insertion, and deletion of keywords. We successfully integrated the technique into our Information Classification and Search Engine system and showed its potential and feasibility.
6E411207	This study examines existent and new methods for evaluating the success of information retrieval systems. The theory underlying current methods is not robust enough to allow testing retrieval using different meta-tagging schemas. Traditional measures rely on judgments of whether a document is relevant to a particular question. A good system returns all the relevant documents and no extraneous documents. There is a rich literature questioning the efficacy of relevance judgments. Such questions as, Relevant to whom? When? and To what purpose? are not well-answered in traditional theory. In this study, two new measures (Spink's Information Need and Cooper's Utility) are used in evaluating two search tools (tag-based and text-based), comparing these new measures with traditional measures and each other. The open-source Swish text-based search engine and a self-constructed tag-based search tool were used. Thirty-four educators searched for information using both search engines and evaluated the information retrieved by each. Construct measures, derived by multiplying each of the three measures (traditional, information need, and utility) by a rating of satisfaction were compared using two way analysis of variance. This study specifically analyzes small information systems. The design concepts would be untenable for large systems. Results indicated that there was a significant correlation between the three measures, indicating that the new measures provide an equivalent method of evaluating systems and have some significant advantages, which include not requiring relevance judgments and the ability to use the measures in situ
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7EE419B7	Information Retrieval (IR) approaches for semantic web search engines have become very populars in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
78A4DC3B	The phenomenon of sponsored search advertising where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7DEA9D12	An information retrieval (IR) process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In IR a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.An object is an entity which keeps or stores information in a database. User queries are matched to objects stored in the database. Depending on the application the data objects may be, for example, text documents, images or videos. The documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates.Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.In this paper we try to explain IR methods and asses them from two view points and finally propose a simple method for ranking terms and documents on IR and implement the method and check the result.
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
72030340	Users often express confusion and frustration in trying to locate the full-text availability of a journal and having to check multiple resources and interfaces on a library's Web site. The challenge was to seek an interim practical solution which would address this need effectively. The Social Science Journals Database and Search Engine (Soc-dbase) project demonstrates a low-cost one-stop search solution that can be easily and quickly adopted and implemented. The project involves creating a single Web interface to search a database of selected social science and sociology journal titles that include full-text online availability information. This paper presents the design and creation of the social sciences journals database that can be searched to find a journal's full-text availability at the Rutgers University Libraries or on the Web.
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the document side(type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
01D30712	Personalized search engines must be able to cope with various user preferences, retrieving the best matches to a query. For SQL and XML applications new methods for such preference-based searches have been implemented recently. Here we adopt this approach to keyword search in full-text search engines. We propose to augment the vector space model (VSM) by preference constructors having an intuitive partial order semantics: Pareto-accumulation and prioritization. We show that prioritization can be interpreted as subspace preference in the VSM. Using a preprocessor approach we succeed to map prioritization onto the VSM. A first query benchmark, using the standard Time-collection, revealed promising results. The retrieval quality, measured by average expected search length, could be slightly improved. Using the proposed meta engine approach, this gain in retrieval quality is accompanied by a substantial speed up of query runtimes. Thus subspace preferences can be integrated efficiently into existing full-text search engines. 
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.
7A6F5F82	Finding the right information in the World Wide Web is becoming a fundamental problem, since the amount of global information that the WWW contains is growing at an incredible rate. In this paper, we present a novel method to extract from a web object its “hyper” informative content, in contrast with current search engines, which only deal with the “textual” informative content. This method is not only valuable per se, but it is shown to be able to considerably increase the precision of current search engines, Moreover, it integrates smoothly with existing search engines technology since it can be implemented on top of every search engine, acting as a post-processor, thus automatically transforming a search engine into its corresponding “hyper” version. We also show how, interestingly, the hyper information can be usefully employed to face the search engines persuasion problem.
7DCF9C8B	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time.
808285A5	Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called "information need". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
7E418727	Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7D2009E3	Search advertising is typical heterogeneous multi-dimensional data sets whose click-through rate depends on query words, terms and other factors. Traditional data mining methods, limited to homogenous data source, represent search ads as the vector space model, so they fail to sufficiently consider the search advertisements’ characteristics of heterogeneous data. This paper presents consistent bipartite graph model to describe ads, adopting spectral co-clustering method in data mining. In order to solve the balance partition of the map in clustering, heuristic algorithm is introduced into consistent bipartite graph's co-partition; a more effective subgraph redistribution algorithm is established. Experiments on real ads dataset shows that our approach worked effectively and efficiently in both clustering and prediction.
7A81F96C	Today’s knowledge workers rely increasingly on information to get their job done, and the availability of search engines to locate relevant information is thus essential. Understanding how users interact with search engines is a prerequisite for the successful design of useful systems and a body of knowledge has in recent years begun to compile. However, all previous studies have focused on the public web, not acknowledging the fact that much business-related information seeking occur on corporate internal networks. In this exploratory study, we have collected and analysed intranet search engine log files from three different years – 2000, 2002, and 2004 – enabling us to detect shifting trends in intranet search behaviour. Comparing our data to what has been reported from the public web we conclude that intranet searchers are both similar to and different from searchers on the public web. In sum, it appears that intranet users are more extreme in their behaviour and that qualitative studies are needed to understand the motives and rationales governing their actions.
79A2A305	The Internet is a widely used source of information for patients searching for medical/health care information. While many studies have assessed existing medical/health care information on the Internet, relatively few have examined methods for design and delivery of such websites, particularly those aimed at the general public.This study describes a method of evaluating material for new medical/health care websites, or for assessing those already in existence, which is correlated with higher rankings on Google's Search Engine Results Pages (SERPs).A website quality assessment (WQA) tool was developed using criteria related to the quality of the information to be contained in the website in addition to an assessment of the readability of the text. This was retrospectively applied to assess existing websites that provide information about generic medicines. The reproducibility of the WQA tool and its predictive validity were assessed in this studyThe WQA tool demonstrated very high reproducibility (intraclass correlation coefficient=0.95) between 2 independent users. A moderate to strong correlation was found between WQA scores and rankings on Google SERPs. Analogous correlations were seen between rankings and readability of websites as determined by Flesch Reading Ease and Flesch-Kincaid Grade Level scores.The use of the WQA tool developed in this study is recommended as part of the design phase of a medical or health care information provision website, along with assessment of readability of the material to be used. This may ensure that the website performs better on Google searches. The tool can also be used retrospectively to make improvements to existing websites, thus, potentially enabling better Google search result positions without incurring the costs associated with Search Engine Optimization (SEO) professionals or paid promotion.
048F1814	Recent advances in click modeling have established it as an attractive approach to interpret search click data. These advances characterize users’ search behavior either in advertisement blocks, or within an organic search block through probabilistic models. Yet, when searching for information on a search result page, one is often interacting with the search engine via an entire page instead of a single block. Consequently, previous works that exclusively modeled user behavior in a single block may sacrifice much useful user behavior information embedded in other blocks.To solve this problem, in this paper, we put forward a novel Whole Page Click (WPC) Model to characterize user behavior in multiple blocks. Specifically, WPC uses a Markov chain to learn the user transition probabilities among different blocks in the whole page. To compare our model with the best alternatives in the Web-Search literature, we run a large-scale experiment on a real dataset and demonstrate the advantage of the WPC model in terms of both the whole page and each block in the page. Especially, we find that WPC can achieve significant gain in interpreting the advertisement data, despite of the sparsity of the advertisement click data.
7AB88A91	Caching is a widely used technique to boost the performance of search engines. Based on the observation that the speed gap between the random access of flash-based solid state drive and its sequential access is much inapparent than that of magnetic hard disk drive, we introduce a new static list caching algorithm which takes the block-level access latency into consideration. The experimental results show that the proposed policy can reduce the average disk access latency per query by up to 14\% over the state-of-the-art algorithms in the SSD-based infrastructure. Besides, the results also reveal that our new strategy outperforms other existing algorithms even on HDD-based architecture.
77EB1685	Current automatic wrappers using DOM tree and visual properties of data records to extract the required information from the search engine results pages generally have limitations such as the inability to check the similarity of tree structures accurately. Our study on the properties of data records shows that these data records located in search engine results pages are not only having similar visual properties and tree structures, but they are also related semantically in their contents. In this context, we propose an ontological technique using existing lexical database for English (WordNet) for the extraction of data records. We find that wrappers designed based on ontological technique are able to reduce the number of potential data regions to be extracted, thus they are able to improve the data extraction accuracy. We then use visual cue from the browser rendering engine to locate and extract the relevant data region from the web page by measuring the size of text and image of data records. Experimental results indicate that our technique is robust and performs better than the existing state of the art visual based wrappers.
7F9F2D76	This study measures the frequency with which search engines update their indices. Therefore, 38 websites that are updated on a daily basis were analysed within a time-span of six weeks. The analysed search engines were Google, Yahoo and MSN. We find that Google performs best overall with the most pages updated on a daily basis, but only MSN is able to update all pages within a time-span of less than 20 days. Both other engines have outliers that are older. In terms of indexing patterns, we find different approaches at the different engines. While MSN shows clear update patterns, Google shows some outliers and the update process of the Yahoo index seems to be quite chaotic. Implications are that the quality of different search engine indices varies and more than one engine should be used when searching for current content.
7BC0845F	We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We propose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strategies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Yahoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
809F7B04	To confront with an ever increasing number of published scientific articles, an effective, efficient, and easy-to-use tool is required to support biomedical scientists, while entering a new scientific field and encountering clinical decision, to organize a vast amount of PubMed abstracts into the panorama of specific topics according to their relevance. In brief, the set of associations among frequently co-occurring terms in given a set of PubMed documents forms naturally a simplicial complex. Afterwards each connected component of this simplicial complex represents a concept in the collection. Based on these concepts, documents can be clustered into meaningful classes. This paper presents an alternative search engine that applies a combinatorial topological method to automatically extract semantic clusters from the PubMed database of biomedical literature. We use several qualitative parameters to perform the user study that shows users are able to reduce search time.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
7B829984	In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.
7ECEAEEC	With web image search engines, we face a situation where the results are very noisy, and when we ask for a specific object, we are not ensured that this object is contained in all the images returned by the search engines: about 50% of the images returned are off-topic. In this paper, we explain how knowing the color of an object can help locating the object in images, and we also propose methods to automatically find the color of an object, so that the whole process can be fully automatic. Results reveal that this method allows us to reduce the noise in returned images while providing automatic segmentation so that it can be used for clustering or object learning.
5CFC04D2	Commercial web search engines adopt parallel and replicated architecture in order to support high query throughput. In this paper, we investigate the effect of caching on the throughput in such a setting. A simple scheme, called uniform caching, would replicate the cache content to all servers. Unfortunately, it does not exploit the variations among queries, thus wasting memory space on caching the same cache content redundantly on multiple servers. To tackle this limitation, we propose a diversified caching problem, which aims to diversify the types of queries served by different servers, and maximize the sharing of terms among queries assigned to the same server. We show that it is NP-hard to find the optimal diversified caching scheme, and identify intuitive properties to seek good solutions. Then we present a framework with a suite of techniques and heuristics for diversified caching. Finally, we evaluate the proposed solution with competitors by using a real dataset and a real query log.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
7F6257E8	Query processing is a major cost factor in operating large web search engines. In this paper, we study query result caching, one of the main techniques used to optimize query processing performance. Our first contribution is a study of result caching as a weighted caching problem. Most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. We describe and evaluate several algorithms for weighted result caching, and study the impact of Zipf-based query distributions on result caching. Our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. Finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
78A4DC3B	The phenomenon of sponsored search advertising where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7BB23ED5	Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time
7F427012	We examine sponsored search auctions run by Overture (now part of Yahoo!) and Google and present evidence of strategic bidder behavior in these auctions. Between June 15, 2002, and June 14, 2003, we estimate that Overture's revenue might have been higher if it had been able to prevent this strategic behavior. We present an alternative mechanism that could reduce the amount of strategizing by bidders, raise search engines' revenues, and increase the efficiency of the market. We conclude by showing that strategic behavior has not disappeared over time; it remains present on both search engines.
80359F5F	Among the challenges of searching the vast information source the Web has become, improving Web search efficiency by different strategies using semantics and the user generated data from Web 2.0 applications remains a promising and interesting approach. In this paper, we present the Personal Social Dataset and Ontology-guided Input strategies and couple them together, providing a proofof- concept implementation.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7698D6DC	A constrained L1 minimization method is proposed for estimating a sparse inverse covariance matrix based on a sample of n iid p-variate random variables. The resulting estimator is shown to enjoy a number of desirable properties. In particular, it is shown that the rate of convergence between the estimator and the true s-sparse precision matrix under the spectral norm is slogp/n−−−−−−√ when the population distribution has either exponential-type tails or polynomial-type tails. Convergence rates under the elementwise L∞ norm and Frobenius norm are also presented. In addition, graphical model selection is considered. The procedure is easily implementable by linear programming. Numerical performance of the estimator is investigated using both simulated and real data. In particular, the procedure is applied to analyze a breast cancer dataset. The procedure performs favorably in comparison to existing methods.
797F1FB0	We define and investigate classes of statistical models for the analysis of associations between variables, some of which are qualitative and some quantitative. In the cases where only one kind of variables is present, the models are well-known models for either contingency tables or covariance structures. We characterize the subclass of decomposable models where the statistical theory is especially simple. All models can be represented by a graph with one vertex for each variable. The vertices are possibly connected with arrows or lines corresponding to directional or symmetric associations being present. Pairs of vertices that are not connected are conditionally independent given some of the remaining variables according to specific rules.
0BD946B6	Building accurate models from a small amount of available training data can sometimes prove to be a great challenge. Expert domain knowledge can often be used to alleviate this burden. Parameter Sharing is one such important form of domain knowledge. Graphical models like HMMs, DBNs and Module Networks use different forms of Parameter Sharing to reduce the variance in the parameter estimates. The goal of this paper is to present a theoretical approach for learning in presence of several other types of Parameter Related Domain Knowledge that go beyond the ones in the above models. First, we introduce a General Parameter Sharing Framework that describes the models just mentioned, but allows for much finer grained parameter sharing assumptions. In this framework, we present sound procedures for parameter learning from both a Frequentist and a Bayesian point of view, from both complete and incomplete data, in the case where a domain expert specifies in advance the structure of the graphical model, and the subsets of parameters to be shared. Second, we describe a hierarchical extension of this framework based on Parameter Sharing Trees. Finally we present algorithms for using domain knowledge that specifies that certain groups of parameters share certain properties. In particular, we consider two kinds of constraints: first kind states certain groups of parameters share the same aggregate probability mass and second kind states the ratio of the parameters is preserved (shared) in several groups. As an example, we derive a novel form of parameter sharing for Bayesian Multinetworks.
59E5C607	In this paper I explore the psychology of ritual performance and present a simple graphical model that clarifies several issues in William Irons’s theory of religion as a “hard-to-fake” sign of commitment. Irons posits that religious behaviors or rituals serve as costly signals of an individual’s commitment to a religious group. Increased commitment among members of a religious group may facilitate intra-group cooperation, which is argued to be the primary adaptive benefit of religion. Here I propose a proximate explanation for how individuals are able to pay the short-term costs of ritual performance to achieve the long-term fitness benefits offered by religious groups. The model addresses three significant problems raised by Irons’s theory. First, the model explains why potential free-riders do not join religious groups even when there are significant net benefits that members of religious groups can achieve. Second, the model clarifies how costly a ritual must be to achieve stability and prevent potential free-riders from joining the religious group. Third, the model suggests why religious groups may require adherents to perform private rituals that are not observed by others. Several hypotheses generated from the model are also discussed.
7B13384D	This paper reviews some of the key statistical ideas that are encountered when trying to find empirical support to causal interpretations and conclusions, by applying statistical methods on experimental or observational longitudinal data. In such data, typically a collection of individuals are followed over time, then each one has registered a sequence of covariate measurements along with values of control variables that in the analysis are to be interpreted as causes, and finally the individual outcomes or responses are reported. Particular attention is given to the potentially important problem of confounding. We provide conditions under which, at least in principle, unconfounded estimation of the causal effects can be accomplished. Our approach for dealing with causal problems is entirely probabilistic, and we apply Bayesian ideas and techniques to deal with the corresponding statistical inference. In particular, we use the general framework of marked point processes for setting up the probability models, and consider posterior predictive distributions as providing the natural summary measures for assessing the causal effects. We also draw connections to relevant recent work in this area, notably to Judea Pearl's formulations based on graphical models and his calculus of so-called do-probabilities. Two examples illustrating different aspects of causal reasoning are discussed in detail.
7BCF362F	Multivariate Gaussian graphical models are defined in terms of Markov properties, i.e., conditional independences, corresponding to missing edges in the graph. Thus model selection can be accomplished by testing these independences, which are equivalent to zero values of corresponding partial correlation coefficients. For concentration graphs, acyclic directed graphs, and chain graphs (both LWF and AMP classes), we apply Fisher's z-transform, Šidák's correlation inequality, and Holm's step-down procedure to simultaneously test the multiple hypotheses specified by these zero values. This simple method for model selection controls the overall error rate for incorrect edge inclusion. Prior information about the presence and/or absence of particular edges can be readily incorporated.
772CE3A1	Most interactive graphical applications that use direct manipulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high design, development and maintenance costs and encourages the development of stereotyped  applications based  on buttons,menus and dialogue boxes instead of direct manipulation of the applications objects. In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction . We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi-Layer Model and we show how it can take advantage of software and hardware graphic extensions to provide good performance.  We also show how it supports multiple input devices and simplifies the description of a wide variety of interaction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional animation system.
096AEEA7	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion. 
7F3655D3	We show that the class of strongly connected graphical models with tree-width at most <i>k</i> can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Hoffgen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for <i>k</i> > 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time.
7BDE06A5	Advances in acoustic sensing have enabled the simultaneous acquisition of multiple measurements of the same physical event via co-located acoustic sensors. We exploit the inherent correlation among such multiple measurements for acoustic signal classification, to identify the launch/impact of munition (i.e., rockets, mortars). Specifically, we propose a probabilistic graphical model framework that can explicitly learn the class conditional correlations between the cepstral features extracted from these different measurements. Additionally, we employ symbolic dynamic filtering-based features, which offer improvements over the traditional cepstral features in terms of robustness to signal distortions. Experiments on real acoustic data sets show that our proposed algorithm outperforms conventional classifiers as well as the recently proposed joint sparsity models for multisensor acoustic classification. Additionally our proposed algorithm is less sensitive to insufficiency in training samples compared to competing approaches.
7F9A8948	Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
80379CA8	We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.
008901FD	Recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on Bayesian networks. This paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. Besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. It is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. Based on possibly incomplete data, the score and the observed information are derived for these models. This accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. Throughout the paper the specialization into recursive graphical models is accounted for by a simple example.
76E3283E	We consider the problem of estimating a sparse precision matrix of a multivariate Gaussian distribution, where the dimension may be large. Gaussian graphical models provide an important tool in describing conditional independence through presence or absence of edges in the underlying graph. A popular non-Bayesian method of estimating a graphical structure is given by the graphical lasso. In this paper, we consider a Bayesian approach to the problem. We use priors which put a mixture of a point mass at zero and certain absolutely continuous distribution on off-diagonal elements of the precision matrix. Hence the resulting posterior distribution can be used for graphical structure learning. The posterior convergence rate of the precision matrix is obtained and is shown to match the oracle rate. The posterior distribution on the model space is extremely cumbersome to compute using the commonly used reversible jump Markov chain Monte Carlo methods. However, the posterior mode in each graph can be easily identified as the graphical lasso restricted to each model. We propose a fast computational method for approximating the posterior probabilities of various graphs using the Laplace approximation approach by expanding the posterior density around the posterior mode. We also provide estimates of the accuracy in the approximation.
5A0EE170	Multiscale (multiresolution) graphical models have gained widespread popularity in recent years, since they enjoy rich modeling power as well as efficient inference procedures. Existing approaches to learning multiscale graphical models often leverage the framework of penalized likelihood, and therefore suffer from the issue of regularization selection. In this paper, we propose a novel method to learn multiscale graphical models from the Bayesian perspective. More specifically, the regularization parameters are treated as random variables that follow Gamma distributions. We then derive an efficient variational Bayes algorithm to learn the model, and further demonstrate the advantages of the proposed method through numerical experiments.
761D5749	We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.
7E144371	In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.
7C7FFC7E	We study the problem of projecting a distribution  onto  (or  finding  a  maximum  likelihood distribution  among)  Markov networks  of  bounded tree-width. By casting it as the combinatorial optimization problem of finding a maximum weight hypertree, we prove that it is NP-hard to solve exactly and provide an approximation algorithm with a provable performance guarantee.
79AF77CD	We consider acyclic directed mixed graphs, in which directed edges (x→y) and bi-directed edges (x↔y) may occur. A simple extension of Pearl's d-separation criterion, called m-separation, is applied to these graphs. We introduce a local Markov property which is equivalent to the global property resulting from the m-separation criterion for arbitrary distributions.
7A730712	Markov networks are a common class of graphical models used in machine learning. Such models use an undirected graph to capture dependency information among random variables in a joint probability distribution. Once one has chosen to use a Markov network model, one aims to choose the model that best explains the data that has been observed this model can then be used to make predictions about future data. We show that the problem of learning a maximum likelihood Markov network given certain observed data can be reduced to the problem of identifying a maximum weight low-treewidth graph under a given input weight function. We give the first constant factor approximation algorithm for this problem. More precisely, for any fixed treewidth objective k, we find a treewidth-k graph with an f(k) fraction of the maximum possible weight of any treewidth-k graph.
7E25BD71	Sparse graphical models have proven to be a flexible class of multivariate probability models for approximating high-dimensional distributions. In this paper, we propose techniques to exploit this modeling ability for binary classification by discriminatively learning such models from labeled training data, i.e., using both positive and negative samples to optimize for the structures of the two models. We motivate why it is difficult to adapt existing generative methods, and propose an alternative method consisting of two parts. First, we develop a novel method to learn tree-structured graphical models which optimizes an approximation of the log-likelihood ratio. We also formulate a joint objective to learn a nested sequence of optimal forests-structured models. Second, we construct a classifier by using ideas from boosting to learn a set of discriminative trees. The final classifier can interpreted as a likelihood ratio test between two models with a larger set of pairwise features. We use cross-validation to determine the optimal number of edges in the final model. The algorithm presented in this paper also provides a method to identify a subset of the edges that are most salient for discrimination. Experiments show that the proposed procedure outperforms generative methods such as Tree Augmented Naive Bayes and Chow-Liu as well as their boosted counterparts.
805ED9C0	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on $\ell_1$-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an $\ell_1$-constraint. Our framework applies to the high-dimensional setting, in which both the number of nodes $p$ and maximum neighborhood sizes $d$ are allowed to grow as a function of the number of observations $n$. Our main results provide sufficient conditions on the triple $(n, p, d)$ for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as $\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions are imposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$ samples are sufficient.
7F6DA698	Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula---or "nonparanormal"---for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples. 
7C5AAA1F	Therapy processes are complex dynamical systems where several variables are constantly interacting with each other. In general, the underlying mechanisms are difficult to assess. Our approach is to identify the dependency structure of relevant variables within the therapy process using interaction graphs. These are instruments for multivariate time series which are based on the analysis of partial spectral coherences. We used interaction graphs in order to investigate the therapy process of a multimodal therapy concept for fibromyalgia patients. Our main hypothesis was that self-efficacy plays a central role in the therapy process. Methods: Patients kept an electronic diary for 13 weeks. Pain intensity, depression, sleep quality, anxiety and self-efficacy were assessed via visual analogue scales. The resulting multivariate time series were aggregated over individuals, and partial spectral coherences between each pair of the variables were calculated. From the partial coherences, interaction graphs were plotted. Results: Within the resulting graphical model, self-efficacy was strongly related to pain intensity, depression and sleep quality. All other relations were substantially weaker. There was no direct relationship between pain intensity and sleep quality. Conclusions: The relations between two variables within the therapy process are mainly induced by self-efficacy. Interaction graphs can be used to pool time series data of several patients and thus to assess the common underlying dependency structure of a group of patients. The graphical representation is easily comprehensible and allows to distinguish between direct and indirect relationships.
80D10329	Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.
7963E765	The forensic investigation of the origin and cause of a fire incident is a particularly demanding area of expertise. As the available evidence is often incomplete or vague, uncertainty is a key element. The present study is an attempt to approach this through the use of Bayesian networks, which have been found useful in assisting human reasoning in a variety of disciplines in which uncertainty plays a central role. The present paper describes the construction of a Bayesian network (BN) and its use for drawing inferences about propositions of interest, based upon a single, possibly non replicable item of evidence: detected residual quantities of a flammable liquid in fire debris.
77757110	Motivated by the analysis of genetical genomic data, we consider the problem of estimating high-dimensional sparse precision matrix adjusting for possibly a large number of covariates, where the covariates can affect the mean value of the random vector. We develop a two-stage estimation procedure to first identify the relevant covariates that affect the means by a joint ℓ1 penalization. The estimated regression coefficients are then used to estimate the mean values in a multivariate sub-Gaussian model in order to estimate the sparse precision matrix through a ℓ1-penalized log-determinant Bregman divergence. Under the multivariate normal assumption, the precision matrix has the interpretation of a conditional Gaussian graphical model. We show that under some regularity conditions, the estimates of the regression coefficients are consistent in element-wise ℓ∞ norm, Frobenius norm and also spectral norm even when p≫n and q≫n. We also show that with probability converging to one, the estimate of the precision matrix correctly specifies the zero pattern of the true precision matrix. We illustrate our theoretical results via simulations and demonstrate that the method can lead to improved estimate of the precision matrix. We apply the method to an analysis of a yeast genetical genomic data.
801D527C	Using domain/expert knowledge when learning Bayesian networks from data has been considered a promising idea since the very beginning of the field. However, in most of the previously proposed approaches, human experts do not play an active role in the learning process. Once their knowledge is elicited, they do not participate any more. The interactive approach for integrating domain/expert knowledge we propose in this work aims to be more efficient and effective. In contrast to previous approaches, our method performs an active interaction with the expert in order to guide the search based learning process. This method relies on identifying the edges of the graph structure which are more unreliable considering the information present in the learning data. Another contribution of our approach is the integration of domain/expert knowledge at different stages of the learning process of a Bayesian network: while learning the skeleton and when directing the edges of the directed acyclic graph structure.
7ED9E3E7	We consider the problem of estimating the graph structure associated with a discrete Markov random field. We describe a method based on ℓ1-regularized logistic regression,in which the neighborhood of any given node is estimated by performing logistic regression subject to an ℓ1-constraint. Our framework applies to the high-dimensional setting,in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. Under certain assumptions on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n= Ω(d3logp), with the error decaying as O(exp(−Cn/d3)) for some constant C. If these same assumptions are imposed directly on the sample matrices, we show that n= Ω(d2logp) samples are sufficient.
756B4845	Automatically learning the graph structure of a single Bayesian network (BN) which accurately represents the underlying multivariate probability distribution of a collection of random variables is a challenging task. But obtaining a Bayesian solution to this problem based on computing the posterior probability of the presence of any edge or any directed path between two variables or any other structural feature is a much more involved problem, since it requires averaging over all the possible graph structures. For the former problem, recent advances have shown that search + score approaches find much more accurate structures if the search is constrained by a previously inferred skeleton (i.e. a relaxed structure with undirected edges which can be inferred using local search based methods). Based on similar ideas, we propose two novel skeleton-based approaches to approximate a Bayesian solution to the BN learning problem: a new stochastic search which tries to find directed acyclic graph (DAG) structures with a non-negligible score; and a new Markov chain Monte Carlo method over the DAG space. These two approaches are based on the same idea. In a first step, both employ a previously given skeleton and build a Bayesian solution constrained by this skeleton. In a second step, using the preliminary solution, they try to obtain a new Bayesian approximation but this time in an unconstrained graph space, which is the final outcome of the methods. As shown in the experimental evaluation, this new approach strongly boosts the performance of these two standard techniques proving that the idea of employing a skeleton to constrain the model space is also a successful strategy for performing Bayesian structure learning of BNs.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
80A025FA	Many statistical methods gain robustness and flexibility by sacrificing convenient computational structures. In this article, we illustrate this fundamental tradeoff by studying a semiparametric graph estimation problem in high dimensions. We explain how novel computational techniques help to solve this type of problem. In particular, we propose a nonparanormal neighborhood pursuit algorithm to estimate high-dimensional semiparametric graphical models with theoretical guarantees. Moreover, we provide an alternative view to analyze the tradeoff between computational efficiency and statistical error under a smoothing optimization framework. Though this article focuses on the problem of graph estimation, the proposed methodology is widely applicable to other problems with similar structures. We also report thorough experimental results on text, stock, and genomic datasets.	
79B9CC7E	This paper deals with chain graphs under the Andersson–Madigan–Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score + search learning algorithms under assumptions weaker than faithfulness.We also study the problem of how to represent the result of marginalizing out some nodes in an AMP CG. We introduce a new family of graphical models that solves this problem partially. We name this new family maximal covariance–concentration graphs because it includes both covariance and concentration graphs as subfamilies.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7E271971	We propose a Bayesian Network (BN) model to integrate multiple contextual information and the image measurements for image segmentation. The BN model systematically encodes the contextual relationships between regions, edges and vertices, as well as their image measurements with uncertainties. It allows a principled probabilistic inference to be performed so that image segmentation can be achieved through a most probable explanation (MPE) inference in the BN model. We have achieved encouraging results on the horse images from the Weizmann dataset. We have also demonstrated the possible ways to extend the BN model so as to incorporate other contextual information such as the global object shape and human intervention for improving image segmentation. Human intervention is encoded as new evidence in the BN model. Its impact is propagated through belief propagation to update the states of the whole model. From the updated BN model, new image segmentation is produced.
766DF9F3	G protein coupled receptors (GPCRs) are seven helical transmembrane proteins that function as signal transducers. They bind ligands in their extracellular and transmembrane regions and activate cognate G proteins at their intracellular surface at the other side of the membrane. The relay of allosteric communication between the ligand binding site and the distant G protein binding site is poorly understood. In this study, GREMLIN 1, a recently developed method that identifies networks of co-evolving residues from multiple sequence alignments, was used to identify those that may be involved in communicating the activation signal across the membrane. The GREMLIN-predicted long-range interactions between amino acids were analyzed with respect to the seven GPCR structures that have been crystallized at the time this study was undertaken. GREMLIN significantly enriches the edges containing residues that are part of the ligand binding pocket, when compared to a control distribution of edges drawn from a random graph. An analysis of these edges reveals a minimal GPCR binding pocket containing four residues (T1183.33, M2075.42, Y2686.51 and A2927.39). Additionally, of the ten residues predicted to have the most long-range interactions (A1173.32, A2726.55, E1133.28, H2115.46, S186EC2, A2927.39, E1223.37, G902.57, G1143.29 and M2075.42), nine are part of the ligand binding pocket. We demonstrate the use of GREMLIN to reveal a network of statistically correlated and functionally important residues in class A GPCRs. GREMLIN identified that ligand binding pocket residues are extensively correlated with distal residues. An analysis of the GREMLIN edges across multiple structures suggests that there may be a minimal binding pocket common to the seven known GPCRs. Further, the activation of rhodopsin involves these long-range interactions between extracellular and intracellular domain residues mediated by the retinal domain. A minimal ligand binding pocket within a network of correlated mutations identified by multiple sequence and structural analysis of G protein coupled receptors.
80DC1B7C	Pearl's d-separation concept and the ensuing Markov property is applied to graphs which may have, between each two different vertices i and j, any subset of {i←j, i→j, i↔j} as edges. The class of graphs so obtained is closed under marginalization. Furthermore, the approach permits a direct proof of this theorem: “The distribution of a multivariate normal random vector satisfying a system of linear simultaneous equations is Markov w.r.t. the path diagram of the linear system.
7E66E464	Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using l1-penalization methods. We propose and study the following method. We combine a multiple regression approach with ideas of thresholding and refitting: first we infer a sparse undirected graphical model structure via thresholding of each among many l1-norm penalized regression functions; we then estimate the covariance matrix and its inverse using the maximum likelihood estimator. We show that under suitable conditions, this approach yields consistent estimation in terms of graphical structure and fast convergence rates with respect to the operator and Frobenius norm for the covariance matrix and its inverse. We also derive an explicit bound for the Kullback Leibler divergence. 
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
5E453122	In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al.
7F4AA6EA	Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.
7A8EB39B	Recommender system provides users with personalized suggestions of product or information. Typically, recommender systems rely on a bipartite graph model to capture user interest. As an extension, some boosted methods analyze content information to further improve the quality of personalized recommendation. However, due to the prevalence of short and sparse messages in online social media, traditional content-boosted methods do not guarantee to capture user preference accurately especially for web contents. In this paper, we propose a novel graphical model to extract hidden topics from web contents, cluster web contents, and detect users' interests on each cluster. In addition, we introduce two reranking models which utilize the detected user interest to further boost the quality of personalized recommendation. Experiment results on a public dataset demonstrated the limitation of a traditional content-boosted approach, and also showed the validity of our proposed techniques.
7CE629CE	Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
7FF9B70B	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.
7CF1D0CA	The use of Bayesian networks for classification problems has received a significant amount of recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal of classification (label prediction accuracy). Recent approaches to optimizing classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present boosted Bayesian network classifiers, a framework to combine discriminative data-weighting with generative training of intermediate models. We show that boosted Bayesian network classifiers encompass the basic generative models in isolation, but improve their classification performance when the model structure is suboptimal. We also demonstrate that structure learning is beneficial in the construction of boosted Bayesian network classifiers. On a large suite of benchmark data-sets, this approach outperforms generative graphical models such as naive Bayes and TAN in classification accuracy. Boosted Bayesian network classifiers have comparable or better performance in comparison to other discriminatively trained graphical models including ELR and BNC. Furthermore, boosted Bayesian networks require significantly less training time than the ELR and BNC algorithms.
7CA63E85	Undernutrition is one of the most important health problems in developing countries. Examining its determinants implies the investigation of a complex association structure including a large number of potential influence variables and different types of influences. A recently developed statistical technique to cope with such situations are graphical chain models. In this paper, this approach is used to investigate the determinants of undernutrition in Benin (West Africa). Since this method also reveals indirect influences, interesting insight is gained into the association structure of all variables incorporated. The analysis identifies mother's education, socioeconomic status, and religion as three variables with particularly strong direct and indirect linkages to undernutrition.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
7B368C93	Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in more highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties. 
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
814F9EFD	We introduce a new approach to learning statistical models from multiple sequence alignments (MSA) of proteins. Our method, called GREMLIN (Generative REgularized ModeLs of proteINs), learns an undirected probabilistic graphical model of the amino acid composition within the MSA. The resulting model encodes both the position-specific conservation statistics and the correlated mutation statistics between sequential and long-range pairs of residues. Existing techniques for learning graphical models from MSA either make strong, and often inappropriate assumptions about the conditional independencies within the MSA (e.g., Hidden Markov Models), or else use suboptimal algorithms to learn the parameters of the model. In contrast, GREMLIN makes no a priori assumptions about the conditional independencies within the MSA. We formulate and solve a convex optimization problem, thus guaranteeing that we find a globally optimal model at convergence. The resulting model is also generative, allowing for the design of new protein sequences that have the same statistical properties as those in the MSA. We perform a detailed analysis of covariation statistics on the extensively studied WW and PDZ domains and show that our method out-performs an existing algorithm for learning undirected probabilistic graphical models from MSA. We then apply our approach to 71 additional families from the PFAM database and demonstrate that the resulting models significantly out-perform Hidden Markov Models in terms of predictive accuracy. 
80F77A3D	A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.
7F9FC5C6	Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse.
0A7B0D45	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning, in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is founded on an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a typical Smart Environment – the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
75C9738A	The binding of regulatory proteins to their specific DNA targets determines the accurate expression of the neighboring genes. The in silico prediction of new binding sites in completely sequenced genomes is a key aspect in the deeper understanding of gene regulatory networks. Several algorithms have been described to discriminate against false-positives in the prediction of new binding targets; however none of them has been implemented so far to assist the detection of binding sites at the genomic scale.FITBAR (Fast Investigation Tool for Bacterial and Archaeal Regulons) is a web service designed to identify new protein binding sites on fully sequenced prokaryotic genomes. This tool consists in a workbench where the significance of the predictions can be compared using different statistical methods, a feature not found in existing resources. The Local Markov Model and the Compound Importance Sampling algorithms have been implemented to compute the P-value of newly discovered binding sites. In addition, FITBAR provides two optimized genomic scanning algorithms using either log-odds or entropy-weighted position-specific scoring matrices. Other significant features include the production of a detailed genomic context map for each detected binding site and the export of the search results in spreadsheet and portable document formats. FITBAR discovery of a high affinity Escherichia coli NagC binding site was validated experimentally in vitro as well as in vivo and publishedFITBAR was developed in order to allow fast, accurate and statistically robust predictions of prokaryotic regulons. This feature constitutes the main advantage of this web tool over other matrix search programs and does not impair its performance.
7E8549A3	Markov models are often employed to represent stochastic processes, that is, random processes that evolve over time. In a healthcare context, Markov models are particularly suited to modelling chronic disease. In this article, we describe the use of Markov models for economic evaluation of healthcare interventions. The intuitive way in which Markov models can handle both costs and outcomes make them a powerful tool for economic evaluation modelling. The time component of Markov models can offer advantages of standard decision tree models, particularly with respect to discounting. This paper gives a comprehensive description of Markov modelling for economic evaluation, including a discussion of the assumptions on which the type of model is based, most notably the memoryless quality of Markov models often termed the 'Markovian assumption'. A hypothetical example of a drug intervention to slow the progression of a chronic disease is employed to demonstrate the modelling technique and the possible methods of analysing Markov models are explored. Analysts should be aware of the limitations of Markov models, particularly the Markovian assumption, although the adept modeller will often find ways around this problem.
7D5544BA	An intelligent home is likely in the near future. A n important ingredient in an intelligent environment such as a home is prediction - of the next action, the next l ocation, and the next task that an inhabitant is likely to p erform. In this paper we describe our approach to solving the problem of predicting inhabitant behavior in a smart home. We model the inhabitant actions as states in a simple Markov model, then improve the model by supplying it with data from discovered high-level inhabitant tasks. For si mulated data we achieved good accuracy, whereas on real dat a we had marginal performance. We also investigate clust ering of actions and subsequently predict the next action an d the task with hidden Markov models created using the clusters.
5C2535CD	In this paper, we present a new search algorithm for sequential labeling tasks based on the conditional Markov models (CMMs) frameworks. Unlike conventional beam search, our method traverses all possible incoming arcs and also considers the “local best” so-far of each previous node. Furthermore, we propose two heuristics to fit the efficiency requirement. To demonstrate the effect of our method, six variant and large-scale sequential labeling tasks were conducted in the experiment. In addition, we compare our method to Viterbi and Beam search approaches. The experimental results show that our method yields not only substantial improvement in runtime efficiency, but also slightly better accuracy. In short, our method achieves 94.49 F(β) rate in the well-known CoNLL-2000 chunking task.
7D89C765	Web based learning systems provides huge volume of educational content to learners. However, a single learner might not be interested in learning all the contents delivered. To encourage learners of varying skill sets and to develop learning interests web recommendation system is needed for web based learning. This paper focuses on providing recommendations to learners as well as web masters to improve overall effectiveness of web based teaching and learning. This work deals with analysis of web log data and development of recommendation framework using web usage mining techniques like upper approximation based rough set clustering using k nearest neighbors, dynamic support pruned all k-th order Markov model and all k-th order association rule mining by dynamic frequent (k+1) item set generation using Apriori. The goal of this integrated approach is to make accurate recommendations for learning management systems with reduced state space complexity.
07027B17	Markov models have been a keystone in Artificial Intelligence for many decades. However, they remain unsatisfactory when the environment modelled is partially observable. There are pathological examples where no history of fixed length is sufficient for accurate prediction or decision making. On the other hand, working with a hidden state (like in Hidden Markov Models or Partially Observable Markov Decision Processes) has a high computational cost. In order to circumvent this problem, we suggest the use of a context-based model. Our approach replaces strict transition probabilities by influences on transitions. The method proposed provides a trade-off between a fully and partially observable model. We also discuss the capacity of our framework to model hierarchical knowledge and abstraction. Simple examples are given in order to show the advantages of the algorithm.
7E5073A1	Efficient intra prediction is an important aspect of video coding with high compression efficiency. H.264/AVC applies directional prediction from neighboring pixels on an adjustable block size for local decorrelation. In this paper, we present an extended prediction scheme in the context of H.264/AVC that comprises two additional prediction methods exploiting self-similar properties of the encoded texture. A new macroblock type is implemented, allowing for flexible selection of the available prediction methods for sub-partitions of the macroblock. Depending on the content of the encoded video sequence, substantial gains in rate-distortion performance are achieved. The results may indicate directions towards an enhanced intra coding scheme with improved rate-distortion performance.
7E714964	In this paper, we present a stream-based mining algorithm for online anomaly prediction. Many real-world applications such as data stream analysis requires continuous cluster operation. Unfortunately, today's large-scale cluster systems are still vulnerable to various software and hardware problems. System administrators are often overwhelmed by the tasks of correcting various system anomalies such as processing bottlenecks (i.e., full stream buffers), resource hot spots, and service level objective (SLO) violations. Our anomaly prediction scheme raises early alerts for impending system anomalies and suggests possible anomaly causes. Specifically, we employ Bayesian classification methods to capture different anomaly symptoms and infer anomaly causes. Markov models are introduced to capture the changing patterns of different measurement metrics. More importantly, our scheme combines Markov models and Bayesian classification methods to predict when a system anomaly will appear in the foreseeable future and what are the possible anomaly causes. To the best of our knowledge, our work provides the first stream-based mining algorithm for predicting system anomalies. We have implemented our approach within the IBM System S distributed stream processing cluster, and conducted case study experiments using fully implemented distributed data analysis applications processing real application workloads. Our experiments show that our approach efficiently predicts and diagnoses several bottleneck anomalies with high accuracy while imposing low overhead to the cluster system.
769C66A6	Understanding and modeling user online behavior, as well as predicting future requests remain an open challenge for researchers, analysts and marketers. In this paper, we propose an efficient prediction schema based on the extraction of sequential navigation patterns from server log files, combined with web site topology. Traversed paths are monitored, internally recorded and cleaned before being completed with cashed page views. After session and episode identification follows the construction of n-grams. Prediction is based upon a 5 + n-gram schema with all lower level n-grams participating, a procedure that resembles the construction of an All 5th-order Markov Model. The schema achieves full coverage while maintaining competitive prediction precision.
7E17B5B5	Most epidemiological studies of major depression report period prevalence estimates. These are of limited utility in characterizing the longitudinal epidemiology of this condition. Markov models provide a methodological framework for increasing the utility of epidemiological data. Markov models relating incidence and recovery to major depression prevalence have been described in a series of prior papers. In this paper, the models are extended to describe the longitudinal course of the disorder. Data from three national surveys conducted by the Canadian national statistical agency (Statistics Canada) were used in this analysis. These data were integrated using a Markov model. Incidence, recurrence and recovery were represented as weekly transition probabilities. Model parameters were calibrated to the survey estimates. The population was divided into three categories: low, moderate and high recurrence groups. The size of each category was approximated using lifetime data from a study using the WHO Mental Health Composite International Diagnostic Interview (WMH-CIDI). Consistent with previous work, transition probabilities reflecting recovery were high in the initial weeks of the episodes, and declined by a fixed proportion with each passing week. Markov models provide a framework for integrating psychiatric epidemiological data. Previous studies have illustrated the utility of Markov models for decomposing prevalence into its various determinants: incidence, recovery and mortality. This study extends the Markov approach by distinguishing several recurrence categories.
80846A31	Conditional Random Fields (CRFs) are undirected probabilistic graphical models that were introduced for solving sequence labeling and segmenting problems. CRFs have several advantages compared to other well understood and widely used techniques such as Hidden Markov Models (HMMs) or Maximum Entropy Markov Models (MEMMs). Being a conditional model, it does not explicitly model the input data sequences but uses feature functions (features) to incorporate the arbitrary interactions and inter-dependencies that exist in the observation sequences. The number of all possible features is extremely large, up to millions, and is usually specified and designed in advance or according to a feature-generating scheme based on domain knowledge. This paper introduces a feature subset selection method for CRFs based on genetic algorithms, in which a population of candidate feature function subsets is evolved to achieve a maximal CRF performance. The method was experimentally validated on the well known bioinformatics problem of protein phosphorylation site prediction, phosphorylation being one of the most important protein modification mechanisms.
5D7D351E	Intuitively, any ‘bag of words’ approach in IR should benefit from taking term dependencies into account. Unfortunately, for years the results of exploiting such dependencies have been mixed or inconclusive. To improve the situation, this paper shows how the natural language properties of the target documents can be used to transform and enrich the term dependencies to more useful statistics. This is done in three steps. The term co-occurrence statistics of queries and documents are each represented by a Markov chain. The paper proves that such a chain is ergodic, and therefore its asymptotic behavior is unique, stationary, and independent of the initial state. Next, the stationary distribution is taken to model queries and documents, rather than their initial distributions. Finally, ranking is achieved following the customary language modeling paradigm. The main contribution of this paper is to argue why the asymptotic behavior of the document model is a better representation then just the document’s initial distribution. A secondary contribution is to investigate the practical application of this representation in case the queries become increasingly verbose. In the experiments (based on Lemur’s search engine substrate) the default query model was replaced by the stable distribution of the query. Just modeling the query this way already resulted in significant improvements over a standard language model baseline. The results were on a par or better than more sophisticated algorithms that use fine-tuned parameters or extensive training. Moreover, the more verbose the query, the more effective the approach seems to become.
772EDB8B	A probabilistic graphical model is proposed in order to detect the coevolution between different sites in biological sequences. The model extends the continuous-time Markov process of sequence substitution for single nucleic or amino acids and imposes general constraints regarding simultaneous changes on the substitution rate matrix. Given a multiple sequence alignment for each molecule of interest and a phylogenetic tree, the model can predict potential interactions within or between nucleic acids and proteins. Initial validation of the model is carried out using tRNA and 16S rRNA sequence data. The model accurately identifies the secondary interactions of tRNA as well as several known tertiary interactions. In addition, results on 16S rRNA data indicate this general and simple coevolutionary model outperforms several other parametric and nonparametric methods in predicting secondary interactions. Furthermore, the majority of the putative predictions exhibit either direct contact or proximity of the nucleotide pairs in the 3-dimensional structure of the Thermus thermophilus ribosomal small subunit. The results on RNA data suggest a general model of coevolution might be applied to other types of interactions between protein, DNA, and RNA molecules.
7F8D3B50	In recent years there has been an increased interest in the modelling and recognition of human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approach is presented for automatically acquiring stochastic models of the high-level structure of an activity without the assumption of any prior knowledge. The process involves temporal segmentation intoplausible atomic behaviour com-ponents and the use of variable length Markov models for the efficient rep-resentation of behaviours. Experimental results are presented which demon-strate the generation of realistic sample behaviours and evaluate the perfor-mance of models for long-term temporal prediction.
5B14A4B4	Proactive User Interfaces (PUIs) aim at facili- tating the interaction with a user interface, e.g., by highlighting fields or adapting the interface. For that purpose, they need to be able to pre- dict the next user action from the interaction his- tory. In this paper, we give an overview of se- quence prediction algorithms (SPAs) that are ap- plied in this domain, and build upon them to de- velop two new algorithms that base on combin- ing different order Markov models. We iden- tify the special requirements that PUIs pose on these algorithms, and evaluate the performance of the SPAs in this regard. For that purpose, we use three datasets with real usage-data and syn- thesize further data with specific characteristics. Our relatively simple yet efficient algorithm FxL performs extremely well in the domain of SPAs which make it a prime candidate for integration in a PUI. To facilitate further research in this field, we provide a Perl library that contains all presented algorithms and tools for the evaluation.
7FDEABB6	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.
00BA8AB5	MUSART is a research project developing and studying new techniques for music information retrieval.The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling,melodic contour, and phonetic streams for music retrieval.To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music.Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.
78B9FFFA	Various software packages are commonly used for the implementation and calculation of decision-analytic models for health economic evaluations. However, comparison of these programs with regard to ease of implementing a model is lacking.(i) to compare the assets and drawbacks of three commonly used software packages for Markov models with regard to ease of implementation; and (ii) to investigate how a technical model validation can be conducted by comparing the results of the three implementations.A Markov model on chronic obstructive pulmonary disease was implemented in TreeAge, Microsoft Excel and Arena with the same assumptions on model structure, transition probabilities and costs. A hypothetical smoking cessation programme for patients in stage 1 was evaluated against usual care. The packages were compared with respect to time and effort for implementation, run-time, features for the presentation of results, and flexibility. Agreement between the packages on average costs and life-years gained and on the incremental cost-effectiveness ratio was considered for technical validation in the form of expected values (between TreeAge and Excel only) and Monte Carlo simulations.Ease of implementation was best in TreeAge, whereas Arena offered the highest flexibility. Deterministic results were in agreement between TreeAge and Excel, as were simulated values between all three packages.Excel offers an intuitive spreadsheet interface, but the acquisition of and the training in TreeAge or Arena is worthwhile for more complex models. Double implementation is a practicable validation technique that should be conducted to ensure correct model implementation.
7E61B32A	Depression is among the major contributors to worldwide disease burden and adequate modelling requires a framework designed to depict real world disease progression as well as its economic implications as closely as possible.In light of the specific characteristics associated with depression (multiple episodes at varying intervals, impact of disease history on course of illness, sociodemographic factors), our aim was to clarify to what extent "Discrete Event Simulation" (DES) models provide methodological benefits in depicting disease evolution.We conducted a comprehensive review of published Markov models in depression and identified potential limits to their methodology. A model based on DES principles was developed to investigate the benefits and drawbacks of this simulation method compared with Markov modelling techniques.The major drawback to Markov models is that they may not be suitable to tracking patients' disease history properly, unless the analyst defines multiple health states, which may lead to intractable situations. They are also too rigid to take into consideration multiple patient-specific sociodemographic characteristics in a single model. To do so would also require defining multiple health states which would render the analysis entirely too complex. We show that DES resolve these weaknesses and that its flexibility allow patients with differing attributes to move from one event to another in sequential order while simultaneously taking into account important risk factors such as age, gender, disease history and patients attitude towards treatment, together with any disease-related events (adverse events, suicide attempt etc.).DES modelling appears to be an accurate, flexible and comprehensive means of depicting disease progression compared with conventional simulation methodologies. Its use in analysing recurrent and chronic diseases appears particularly useful compared with Markov processes.
7DC8E808	A research area that has become increasingly important in recent years is that of on-board mobile communication, where users on a vehicle are connected to a local network that attaches to the Internet via a mobile router and a wireless link. In this architecture, link disruptions (e.g., due to signal degradation) may have an immediate impact on a potentially large number of connections. We argue that the advance knowledge of public transport routes, and their repetitive nature, allows a certain degree of prediction of impending link disruptions, which can be used to offset their catastrophic impact. Focusing on the transmission control protocol (TCP) and its extension known as Freeze-TCP, we present a detailed analysis of the performance improvement of TCP connections in the presence of disruption prediction. In particular, we propose a Markov model of Freeze-TCP that captures both the TCP behavior and the prediction+"freezing" feature and, using simulations, show that it accurately predicts the performance of the protocol. Our results demonstrate the significant throughput improvement that can be gained by disruption prediction, even with random packet losses or imperfect timing of the predicted disruptions
7EAB152E	Tropical Pacific sea surface temperatures (SSTs) and the accompanying El Niño–Southern Oscillation phenomenon are recognized as significant components of climate behavior. The atmospheric and oceanic processes involved display highly complicated variability over both space and time. Researchers have applied both physically derived modeling and statistical approaches to develop long-lead predictions of tropical Pacific SSTs. The comparative successes of these two approaches are a subject of substantial inquiry and some controversy. Presented in this article is a new procedure for long-lead forecasting of tropical Pacific SST fields that expresses qualitative aspects of scientific paradigms for SST dynamics in a statistical manner. Through this combining of substantial physical understanding and statistical modeling and learning, this procedure acquires considerable predictive skill. Specifically, a Markov model, applied to a low-order (empirical orthogonal function–based) dynamical system of tropical Pacific SST, with stochastic regime transition, is considered. The approach accounts explicitly for uncertainty in the formulation of the model, which leads to realistic error bounds on forecasts. The methodology that makes this possible is hierarchical Bayesian dynamical modeling.
7A42DD4C	Recently, attention has been focused on spatial databases which combine conventional and spatial data. The need for spatial query languages has been identified in several different application domain as Geographic Information Systems (GISs) and Image Databases. Several extensions to the relational database query language SQL have been proposed to serve as a spatial query language. The large availability on the market place of the relational database technology is the major reason why an SQL-based spatial query language is welcome both from the GIS vendors and the GIS user community. Recent SQL extensions convinced us that it is the right time for working on the standardization of SQL-based spatial query languages. This paper sets a kernel of basic features for the creation of a standard and compares a large number of SQL spatial extensions according to such features.
7F5443CD	In this paper, we study the correlation properties of the fading mobile radio channel. Based on these studies, we model the channel as a one-step Markov process whose transition probabilities are a function of the channel characteristics. Then we present the throughput performance of the Go-Back-N and selective-repeat automatic repeat request (ARQ) protocols with timer control, using the Markov model for both forward and feedback channels. This approximation is found to be very good, as confirmed by simulation results.
7E4DC75A	We propose a new and effective method of predicting tracking failures and apply it to the robust analysis of gait and human motion. We define a tracking failure as an event and describe its temporal characteristics using a hidden Markov model (HMM). We represent the human body using a three-dimensional, multicomponent structural model, where each component is designed to independently allow the extraction of certain gait variables. To enable a fault-tolerant tracking and feature extraction system, we introduce a single HMM for each element of the structural model, trained on previous examples of tracking failures. The algorithm derives vector observations for each Markov model using the time-varying noise covariance matrices of the structural model parameters. When transformed with a logarithmic function, the conditional output probability of each HMM is shown to have a causal relationship with imminent tracking failures. We demonstrate the effectiveness of the proposed approach on a variety of multiview video sequences of complex human motion.
7E33249D	Smoking cessation is the only strategy that has shown a lasting reduction in the decline of lung function in patients with chronic obstructive pulmonary disease. This study aims to evaluate the cost-effectiveness of smoking cessation interventions in patients with chronic obstructive pulmonary disease, to assess the quality of the Markov models and to estimate the consequences of model structure and input data on cost–effectiveness. A systematic literature search was conducted in PubMed, Embase, BusinessSourceComplete and Econlit on June 11, 2014. Data were extracted, and costs were inflated. Model quality was evaluated by a quality appraisal, and results were interpreted. Ten studies met the inclusion criteria. The results varied widely from cost savings to additional costs of €17,004 per quality adjusted life year. The models scored best in the category structure, followed by data and consistency. The quality of the models seems to rise over time, and regarding the results there is no economic reason to refuse the reimbursement of any smoking cessation intervention 
7F603AAF	A method of dynamically constructing Markov chain models that describe the characteristics of binary messages is developed. Such models can be used to predict future message characters and can therefore be used as a basis for data compression. To this end, the Markov modelling technique is combined with Guazzo's arithmetic coding scheme to produce a powerful method of data compression. The method has the advantage of being adaptive: messages may be encoded or decoded with just a single pass through the data. Experimental results reported here indicate that.the Markov modelling approach generally achieves much better data compression than that observed with competing methods on typical computer data.
7F5E056C	Motivated by applications such as automated visual surveillance and video monitoring and annotation, there has been a lot of interest in constructing cognitive vision systems capable of interpreting the high level semantics of dynamic scenes. In this paper we present a novel approach for automatically inferring models of object interactions that can be used to interpret observed behaviour within a scene. A real-time low-level computer vision system, together with an attentional control mechanism, are used to identify incidents or events that occur in the scene. A data driven approach has been taken in order to automatically infer discrete and abstract representations (symbols) of primitive object interactions; effectively the system learns a set of qualitative spatial relations relevant to the dynamic behaviour of the domain. These symbols then form the alphabet of a VLMM which automatically infers the high level structure of typical interactive behaviour. The learnt behaviour model has generative capabilities and is also capable of recognizing typical or atypical activities within a scene. Experiments have been performed within the traffic monitoring domain; however the proposed method is applicable to the general automatic surveillance task since it does not assume a priori knowledge of a specific domain.
7E030AE4	A foremost objective in wireless networks is to facilitate the communication of mobile users and the widespread tracking and prediction of their mobility regardless of their point of attachment to the network. In indoor environments the effective users' motion prediction system and wireless localization technology play an important role in all aspects of people's daily lives, including e.g. living assistant, navigation, emergency detection, surveillance/tracking of target-of-interest, evacuation purposes, and many other location-based services. Prediction techniques that are currently used do not consider the motivation behind the movement of mobile nodes and incur huge overheads to manage and manipulate the information required to make predictions. In this paper we propose an activity-based continuous-time Markov model to define and predict the human movement patterns. Then we demonstrate the utility of Nonparametric Belief Propagation (NBP) technique in particle filtering, for both estimating the node locations and representing location uncertainties, and for prediction of the areas that would be visited and those that would not in the future. NBP method admits a wide variety of statistical models, and can represent multi-modal uncertainty. This prediction system may be used as an additional input into intelligent building automation systems
7ABB00FC	Markov modelling offers the possibility of extracting long-term results from dynamic simulations with a significantly reduced execution time over that which would be necessary with an equivalent time-series simulation. The discretization of the problem variables which is necessary for Markov modelling introduces an inaccuracy into the simulation process which means that a balance must be struck between the accuracy required and the time reduction that is possible. The paper describes an approach to the quantification of the errors introduced by discretization of the simulation variables. Two error expressions were tested by running a representative building/HVAC system dynamic model in both time-series and Markov modes with varying degrees of discretization of the variables. A simplified error bound was found useful in identifying near-optimal discretization schemes: further refinement of this approach led to an expression that was found to give a good prediction of the error in Markov simulations of this type of system.
81507EC1	In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.
802AF8D6	We investigate the behavior of block errors which arise in data transmission on fading channels. Our approach takes into account the details of the specific coding/modulation scheme and tracks the fading process symbol by symbol. It is shown that a Markov approximation for the block error process (possibly degenerating into an identically distributed (i.i.d.) process for sufficiently fast fading) is a good model for a broad range of parameters. Also, it is observed that the relationship between the marginal error rate and the transition probability is largely insensitive to parameters such as block length, degree of forward error correction and modulation format, and depends essentially on an appropriately normalized version of the Doppler frequency. This relationship can therefore be computed in the simple case of a threshold model and then used more generally as an accurate approximation. This observation leads to a unified approach for the channel modeling, and to a simplified performance analysis of upper layer protocols.
78744637	In this paper, we introduce a 3-D human-body tracker capable of handling fast and complex motions in real-time. The parameter space, augmented with first order derivatives, is automatically partitioned into Gaussian clusters each representing an elementary motion: hypothesis propagation inside each cluster is therefore accurate and efficient.  The transitions between clusters use the predictions of a Variable Length Markov Model which can explain high level behaviours over a long history. Using Monte-Carlo methods, evaluation of model candidates is critical for both speed and robustness.  We present a new evaluation scheme based on volumetric reconstruction and blobs-fitting, where appearance models and image evidences are represented by Gaussian mixtures.  We demonstrate the application of our tracker to long video sequences exhibiting rapid and diverse movements.
80105513	With the introduction of dynamic image processing, such as in image analysis, the computational complexity has become data dependent and memory usage irregular. Therefore, the possibility of runtime estimation of resource usage would be highly attractive and would enable Quality-of-Service (QoS) control for dynamic image-processing applications with shared resources. A possible solution to this problem is to characterize the application execution using model descriptions of the resource usage. In this paper, we attempt to predict resource usage for groups of dynamic image-processing tasks based on Markov-chain modeling. As a typical application, we explore a medical imaging application to enhance a wire mesh tube (stent) under X-ray fluoroscopy imaging during angioplasty. Simulations show that Markov modeling can be successfully applied to describe the resource usage function even if the flow graph dynamically switches between groups of tasks. For the evaluated sequences, an average prediction accuracy of 97% is reached with sporadic excursions of the prediction error up to 20–30%.
793EC21A	We consider the prediction of new observations in a general Gauss–Markov model. We state the fundamental equations of the best linear unbiased prediction, BLUP, and consider some properties of the BLUP. Particularly, we focus on such linear statistics, which preserve enough information for obtaining the BLUP of new observations as a linear function of them. We call such statistics linearly prediction sufficient for new observations, and introduce some equivalent characterizations for this new concept.
80739C5A	Spectrum sensing is one of the key functionalities in cognitive radios which enables opportunistic spectrum access. In a cognitive radio system, secondary users need to detect the emergence of primary users as soon as possible to avoid harmful interference. In particular, sensing performance can be evaluated by detection delay and sensing overhead. Sequential detection techniques such as quickest detection can achieve minimum detection delay, while MAC layer sensing scheduling of periodic energy detection has demonstrated its high sensing efficiency. These motivate us to propose a joint PHY-MAC spectrum sensing algorithm in this letter, which employs sequential probability ratio test in the PHY layer and a probability-based sensing scheduling mechanism in the MAC layer. This algorithm can minimize detection delay with limited sensing overhead. Simulation results reveal that it has remarkable performance improvement compared with periodic energy detection.
80B4014C	The SAM-T04 method for predicting protein structures uses a single protocol across the entire range of targets, from comparative modeling to new folds. This protocol is similar to the SAM-T02 protocol used in CASP5, but has improvements in the iterative search for similar sequences in finding and aligning templates, in creating fragment libraries, in generating protein conformations, and in scoring the conformations. The automatic procedure made some improvements over simply selecting an alignment to the highest-scoring template, and human intervention made substantial improvements over the automatic procedure. The main improvements made by human intervention were from adding constraints to build (or retain) beta-sheets and from splitting multidomain proteins into separate domains. The uniform protocol was moderately successful across the entire range of target difficulty, but was somewhat less successful than other approaches in CASP6 on the comparative modeling targets.
7D9B3729	Background. Serial period prevalence estimates for recurrent diseases such as major depression are available more frequently than fully detailed longitudinal data, but it is difficult to estimate incidence and episode duration from such data. Incidence and episode duration are critical decision modeling parameters for recurrent diseases. Objectives. To reduce bias that would otherwise occur in national incidence and duration-of-episode estimates for major depressive episodes deriving from studies using serial period prevalence data and to illustrate amethodological approach for the estimation of incidence from such studies. Methods. Monte Carlo simulation was applied to a Markov process describingincidence and recovery from major depressive episodes. Results. The annual incidence and episode duration were found to be 3.1% and 17.1 weeks, respectively. These estimates are expected to be less subject to bias than those generated without modeling. Conclusions. These results highlight the usefulness of Markov models for analysis of longitudinal data. The methods described here may be useful for decision modeling andmay be generalizable to other chronic diseases.
810600AC	Markov models are useful when a decision problem involves risk that is continuous over time, when the timing of events is important, and when important events may happen more than once. Representing such clinical settings with conventional decision trees is difficult and may require unrealistic simplifying assumptions. Markov models assume that a patient is always in one of a finite number of discrete health states, called Markov states. All events are represented as transitions from one state to another. A Markov model may be evaluated by matrix algebra, as a cohort simulation, or as a Monte Carlo simulation. A newer representation of Markov models, the Markov-cycle tree, uses a tree representation of clinical events and may be evaluated either as a cohort simulation or as a Monte Carlo simulation. The ability of the Markov model to represent repetitive events and the time dependence of both probabilities and utilities allows for more accurate representation of clinical settings that involve these issues.
7CBB23DF	Chronic obstructive pulmonary disease (COPD) is currently the fourth leading cause of death worldwide. It has serious health effects and causes substantial costs for society.The aim of the present paper was to develop a state-of-the-art decision-analytic model of COPD whereby the cost effectiveness of interventions in Germany can be estimated. To demonstrate the applicability of the model, a smoking cessation programme was evaluated against usual care. A seven-stage Markov model (disease stages I to IV according to the GOLD [Global Initiative for Chronic Obstructive Lung Disease] classification, states after lung-volume reduction surgery and lung transplantation, death) was developed to conduct a cost-utility analysis from the societal perspective over a time horizon of 10, 40 and 60 years. Patients entered the cohort model at the age of 45 with mild COPD. Exacerbations were classified into three levels: mild, moderate and severe. Estimation of stage-specific probabilities (for smokers and quitters), utilities and costs was based on German data where possible. Data on effectiveness of the intervention was retrieved from the literature. A discount rate of 3% was applied to costs and effects. Probabilistic sensitivity analysis was used to assess the robustness of the results.The smoking cessation programme was the dominant strategy compared with usual care, and the intervention resulted in an increase in health effects of 0.54 QALYs and a cost reduction of €1115 per patient (year 2007 prices) after 60 years. In the probabilistic analysis, the intervention dominated in about 95% of the simulations. Sensitivity analyses showed that uncertainty primarily originated from data on disease progression and treatment cost in the early stages of disease.e model developed allows the long-term cost effectiveness of interventions to be estimated, and has been adapted to Germany. The model suggests that the smoking cessation programme evaluated was more effective than usual care as well as being cost-saving. Most patients had mild or moderate COPD, stages for which parameter uncertainty was found to be high. This raises the need to improve data on the early stages of COPD.
80D7D606	The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
78AD17F6	Using a finite state Markov channel model, we develop an analytical method for evaluation of the packet error structure in multiple-input multiple-output (MIMO) systems based on singular value decomposition (SVD). We consider dual-branch MIMO systems, with either two transmit and arbitrary number of receive antennas, or arbitrary number of transmit and two receive antennas. The corresponding Markov model parameters are obtained using a novel closed-form expressions for probability density function and level crossing rate of the signal-to-noise ratio at the output of eigenchannels in a MIMO system, derived for a case of Rayleigh propagation, imperfect channel state information and any fixed power allocation. The exact bit error rate for the transmission of quadrature amplitude modulated (QAM) symbols through the eigenchannels is derived in polynomial closed form. Furthermore, by using the developed Markov model, the packet error statistics in the corresponding eigenchannels are determined, and the closed-form analytical expression for the system throughput is derived when ‘go-back-N’ automatic repeat request procedure is applied in time-varying eigenchannels. The analytical results are validated by using Monte Carlo simulations.
7EA1B380	A formula for the go-back-N ARQ (automatic repeat request) scheme applicable to Markov error patterns is derived. It is a generalization of the well-known efficiency formula p/(p+m(1-p)) (where m is the round trip delay in number of block durations and p is the block transmission success probability), and it has been successfully validated against simulation measurements. It is found that for a given error rate, error patterns having zero correlation between successive transmission generally fare better than those with negative correlation, and that error patterns with positive correlation fare better still. It is shown that the present analysis can be extended in a straightforward manner to cope with error patterns of a more complex nature. Simple procedures for numerical evaluation of efficiency under quite general error structures are presented.
81018323	We use several approaches to demonstrate that neural networks can detect precursors to failure. That is, they can detect subtle changes in the process signals. In some cases these subtle changes are early warnings that a subsystem failure is imminent. The results on detection of precursors and faults with various types of time-delay neural networks are discussed. We also measure the noise inherent in our database and place bounds on neural network prediction in the presence of noise. We observe that the noise level can be as high as 40% for detection of failures and can be at 30% to still detect precursors to failure. We note that although self-organizing networks for classification of faults seems like a good idea, in fact they do not perform well in the presence of noise. Lastly, we show that neural networks can induce, or self-build, Markov models from process data and these models can be used to predict system state to a significant distance in the future (e.g., 100 wafers).
7D1020E8	An automatic-repeat-request (ARQ) Go-Back-N (GBN) protocol with unreliable feedback and time-out mechanism is studied, using renewal theory. Transmissions on both the forward and the reverse channels are assumed to experience Markovian errors. The exact throughput of the protocol is evaluated, and simulation results, that confirm the analysis, are presented. A detailed comparison of the proposed method and the commonly used transfer function method reveals that the proposed approach is simple and potentially more powerful.
80567ABE	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning,in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events  that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is  founded  on  an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a  typical Smart Environment–the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of  this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
762DB3E0	The measurement of individual single-channel events arising from the gating of ion channels provides a detailed data set from which the kinetic mechanism of a channel can be deduced. In many cases, the pattern of dwells in the open and closed states is very complex, and the kinetic mechanism and parameters are not easily determined. Assuming a Markov model for channel kinetics, the probability density function for open and closed time dwells should consist of a sum of decaying exponentials. One method of approaching the kinetic analysis of such a system is to determine the number of exponentials and the corresponding parameters which comprise the open and closed dwell time distributions. These can then be compared to the relaxations predicted from the kinetic model to determine, where possible, the kinetic constants. We report here the use of a linear technique, linear prediction/singular value decomposition, to determine the number of exponentials and the exponential parameters. Using simulated distributions and comparing with standard maximum-likelihood analysis, the singular value decomposition techniques provide advantages in some situations and are a useful adjunct to other single-channel analysis techniques.
7FD11145	Block-error processes in transmissions over slow-fading channels can be accurately modeled by a two-state Markov chain [the block Markov model (BMM)]. Another line of research has focused on the use of a channel-state Markov model (CSMM) to analyze block transmissions. Although both techniques provide results that agree well with observations, the relationship between both Markov models has not been recognized in the previous literature. In this letter, we show that the BMM for slow-fading channels can be directly derived from the CSMM. In addition, we introduce a greatly simplified channel-modeling methodology. In the new methodology, the BMM is the primary channel characterization tool, and the CSMM becomes essentially an estimation technique that provides parameters for the BMM. Results of packet transmissions in slow-fading channels show that our approach provides significant improvements in both accuracy and simplicity over previously proposed techniques.
799E3C70	This paper presents a software simulator applicable to multipath fading channels in urban environments of mobile communication networks. The simulator is constructed by a two-state Markov model and several statistical models for simulating the characterizations of different environments. A core idea of the simulator is to construct a Rice distribution-based multipath fading module produced by a modified Gans Doppler power spectrum, and in combination with a Markov model to predict the time-dependent characteristics of packet in different radio circumstances. It can simply predict the packet performance of the future channel and evaluate the relations between the radio channel and the modulation schemes, error control protocols and channel coding. Simulation results demonstrate that it is a reliable and efficient method.
7852BB29	A novel multi-layer perceptrons (MLP)-based speech recognition method is proposed in this study. In this method, the dynamic time warping capability of hidden Markov models (HMM) is directly combined with the discriminant based learning of MLP for the sake of employing a sequence of MLPs (SMLP) as a word recognizer. Each MLP is regarded as a state recognizer to distinguish an acoustic event. Next, the word recognizer is formed by serially cascading all state recognizers. Advantages of both HMM and MLP methods are attained in this system through training the SMLP with an algorithm which combines a dynamic programming (DP) procedure with a generalized probabilistic descent (GPD) algorithm. Additionally, two sub-syllable SMLP-based schemes are studied through application of this method toward the recognition of isolated Mandarin digits. Simulation results confirm that the performance of the method is comparable to a well modeled continuous Gaussian mixture density HMM trained with the minimum error criterion. Not only does the SMLP require less trainable parameters than the HMM system, but the former is more convenient for analysing internal features. With the aid of internal feature selection, discarding the least useful parameters of SMLP without affecting its performance is relatively easy.
7559C9A4	This paper proposes an algorithm, called sequence prediction via enhanced episode discovery (SPEED), to predict inhabitant activity in smart homes. SPEED is a variant of the sequence prediction algorithm. It works with the episodes of smart home events that have been extracted based on the on -off states of home appliances. An episode is a set of sequential user activities that periodically occur in smart homes. The extracted episodes are processed and arranged in a finite-order Markov model. A method based on prediction by partial matching (PPM) algorithm is applied to predict the next activity from the previous history. The result shows that SPEED achieves an 88.3% prediction accuracy, which is better than LeZi Update, Active LeZi, IPAM, and C4.5.
7C7C2B02	To date, decision trees and Markov models have been the most common methods used in pharmacoeconomic evaluations. Both of these techniques lack the flexibility required to appropriately represent clinical reality. In this paper an alternative, more natural, way to model clinical reality — discrete event simulation — is presented and its application is illustrated with a real world example.A discrete event simulation represents the course of disease very naturally, with few restrictions. Neither mutually exclusive branches nor states are required, nor is a fixed cycle. All relevant aspects can be incorporated explicitly and efficiently. Flexibility in handling perspectives and carrying out sensitivity analyses, including structural variations, is incorporated and the entire model can be presented very transparently. The main limitations are imposed by lack of data to fit realistic models.Discrete event simulation, though rarely employed in pharmacoeconomics today, should be strongly considered when carrying out economic evaluations, particularly those aimed at informing policy makers and at estimating the budget impact of a pharmaceutical intervention.
7F79AB85	This paper investigates the properties of a method for obtaining Markov models of unspecified order to be applied to narrow-band fading channels with additive white Gaussian noise. The models are obtained by applying the context tree pruning algorithm to experimental or simulated sequences. Fading environments are identified in which the extension from first-order to higher order models is justified. The paper presents, as examples, the evaluation of the covariance function and the packet error distribution.
5BD5188C	This paper develops a service for ontology evolution based on crowdsourcing. The approach is demonstrated using OntoAssist, a specially designed semantic search service that is capable of capturing and disambiguating user’s search intent as well as automatically enabling ontology evolution. Successful and consistent ontology evolution often requires large amount of input data to specify new terms or changes in relationships. These inputs typically come mainly from domain experts or ontology professionals, which makes it hard to keep up with the change of open, dynamic World Wide Web environment. By integrating OntoAssist with an existing search engine, we show that users’ search intent can be disambiguated and aggregated to help to evolve underlying ontology. The disambiguation feature helps the users to find desirable search results. OntoAssist has been implemented and tested by Turkers from Amazon Mechanical Turk in a live demonstration site. Promising results and analysis are reported.
0319453A	Search advertising is typical heterogeneous multi-dimensional data sets whose click-through rate depends on query words, terms and other factors. Traditional data mining methods, limited to homogenous data source, represent search ads as the vector space model, so they fail to sufficiently consider the search advertisements’ characteristics of heterogeneous data. This paper presents consistent bipartite graph model to describe ads, adopting spectral co-clustering method in data mining. In order to solve the balance partition of the map in clustering, heuristic algorithm is introduced into consistent bipartite graph's co-partition; a more effective subgraph redistribution algorithm is established. Experiments on real ads dataset shows that our approach worked effectively and efficiently in both clustering and prediction.
8035BDA2	A frozen 18.5 million page snapshot of part of the Web has been created to enable and encourage meaningful and reproducible evaluation of Web search systems and techniques. This collection is being used in an evaluation framework within the Text Retrieval Conference (TREC) and will hopefully provide convincing answers to questions such as, “Can link information result in better rankings?”, “Do longer queries result in better answers?”, and, “Do TREC systems work well on Web data?” The snapshot and associated evaluation methods are described and an invitation is extended to participate. Preliminary results are presented for an effectivess comparison of six TREC systems working on the snapshot collection against five well-known Web search systems working over the current Web. These suggest that the standard of document rankings produced by public Web search engines is by no means state-of-the-art.
79E123BB	This paper investigates the composition of search engine results pages. We define what elements the most popular web search engines use on their results pages (e.g., organic results, advertisements, shortcuts) and to which degree they are used for popular vs. rare queries. Therefore, we send 500 queries of both types to the major search engines Google, Yahoo, Live.com and Ask. We count how often the different elements are used by the individual engines. In total, our study is based on 42,758 elements. Findings include that search engines use quite different approaches to results pages composition and therefore, the user gets to see quite different results sets depending on the search engine and search query used. Organic results still play the major role in the results pages, but different shortcuts are of some importance, too. Regarding the frequency of certain host within the results sets, we find that all search engines show Wikipedia results quite often, while other hosts shown depend on the search engine used. Both Google and Yahoo prefer results from their own offerings (such as YouTube or Yahoo Answers). Since we used the .com interfaces of the search engines, results may not be valid for other country-specific interfaces.
7918FE86	Influenza epidemics detection is critically important in recent years because there is a significant economic and public health impact associated with the influenza epidemic. Influenza epidemics detection attracts much attention from governments, organizations, and research institutes, and recently, a novel method using search engine query data to detect influenza activities was presented by Google. In this paper, a data mining based framework using web data is introduced for influenza epidemics detection. Under the framework, a neural network based approach using search engine query data is developed to detect influenza activities. In the proposed method, an automated feature selection model is firstly constructed to reduce the dimension of the query data. Secondly, various neural networks are employed to model the relationship between influenza-like illness data and query data. Thirdly, an optimal neural network is selected as the detector by using the cross-validation method. Finally, the selective neural network detector with the best feature subset is used to detect influenza activities. Experimental results show that the proposed method can outperform traditional statistical models and other models used in the experiments in terms of accuracy. These findings imply that data mining, such as neural network method, can be used as a promising tool to detect influenza activities.
78D19500	Internet search engines and comparison shopping have recently begun implementing a paid placement strategy, where some content providers are given prominent positioning in return for a placement fee. This bias generates placement revenues but creates a disutility to users, thus reducing user-based revenues. We formulate the search engine design problem as a tradeoff between these two types of revenues. We demonstrate that the optimal placement strategy depends on the relative benefits (to providers) and disutilities (to users) of paid placement. We compute the optimal placement fee, characterize the optimal bias level, and analyze sensitivity of the placement strategy to various factors. In the optimal paid placement strategy, the placement revenues are set below the monopoly level due to its negative impact on advertising revenues. An increase in the search engine's quality of service allows it to improve profits from paid placement, moving it closer to the ideal. However, an increase in the value-per-user motivates the gatekeeper to increase market share by reducing further its reliance on paid placement and fraction of paying providers.
815859E9	Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.
80E58704	This paper deals with one aspect of the index quality of search engines: index freshness. The purpose is to analyse the update strategies of the major web search engines Google, Yahoo, and MSN/Live.com. We conducted a test of the updates of 40 daily updated pages and 30 irregularly updated pages. We used data from a time span of six weeks in the years 2005, 2006 and 2007. We found that the best search engine in terms of up-to-dateness changes over the years and that none of the engines has an ideal solution for index freshness. Indexing patterns are often irregular, and there seems to be no clear policy regarding when to revisit Web pages. A major problem identified in our research is the delay in making crawled pages available for searching, which differs from one engine to another.
84D0666F	In recent years, the search engine results pages (SERP's) have been augmented with new markup elements that introduce seamlessly additional semantic information. Examples of such elements are the aggregated results disseminated by vertical portals, and the enriched snippets that display meta-information from the landing pages. In this paper, we investigate the gaze behaviour of web users who inter- act with SERP's that contain plain and rich snippets, and observe the impact of both types of snippets on the web search experience. For our study, we consider a wide range of snippet types, such as multimedia elements (Google Images, Google Videos), recommendation snippets (Author, Google Plus, Reviews, Google Shopping Product), and geo-location snippets (Google Places). We conduct two controlled user studies that employ eye tracking and mouse tracking, and analyse the search interactions of 213 participants, focusing on three factors: noticeability, interest, and conversion. Our findings indicate that ranking remains the most critical factor in relevance perception, although in certain cases the richness of snippets can capture user attention.
7CE1777E	HyPursuit is a new hierarchical network search engine that clusters hypertext documents to structure a given information space for browsing and search act ivities. Our content-link clustering algorithm is based on the semantic information embedded in hyperlink structures and document contents. HyPursuit admits multiple, coexisting cluster hierarchies based on different prin- ciples for grouping documents, such as the Library of Congress catalog scheme and automatically created hy- pertext clusters. HyPursuit’s abstraction functions summarize cluster con- tents to support scalable query processing. The abstrac- tion functions satisfy system resource limitations with controlled information 10SS. The result of query pro- cessing operations on a cluster summary approximates the result of performing the operations on the entire in- formation space. We constructed a prototype system comprising 100 leaf World- Wide Web sites and a hier- archy of 42 servers that route queries to the leaf sites. Experience with our system suggests that abstraction functions based on hypertext clustering can be used to construct meaningful and scalable cluster hierarchies. We are also encouraged by preliminary results on clus- tering based on both document contents and hyperlink structures
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
7CA05884	Search engine advertising has become a significant element of the Web browsing experience. Choosing the right ads for the query and the order in which they are displayed greatly affects the probability that a user will see and click on each ad. This ranking has a strong impact on the revenue the search engine receives from the ads. Further, showing the user an ad that they prefer to click on improves user satisfaction. For these reasons, it is important to be able to accurately estimate the click-through rate of ads in the system. For ads that have been displayed repeatedly, this is empirically measurable, but for new ads, other means must be used. We show that we can use features of ads, terms, and advertisers to learn a model that accurately predicts the click-though rate for new ads. We also show that using our model improves the convergence and performance of an advertising system. As a result, our model increases both revenue and user satisfaction.
7C5CBF21	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.
7E6A3532	Understanding how people interact with search engines is important in improving search quality. Web search engines typically analyze queries and clicked results, but these actions provide limited signals regarding search interaction. Laboratory studies often use richer methods such as gaze tracking, but this is impractical at Web scale. In this paper, we examine mouse cursor behavior on search engine results pages (SERPs), including not only clicks but also cursor movements and hovers over different page regions. We: (i) report an eye-tracking study showing that cursor position is closely related to eye gaze, especially on SERPs; (ii) present a scalable approach to capture cursor movements, and an analysis of search result examination behavior evident in these large-scale cursor data; and (iii) describe two applications (estimating search result relevance and distinguishing good from bad abandonment) that demonstrate the value of capturing cursor data. Our findings help us better understand how searchers use cursors on SERPs and can help design more effective search systems. Our scalable cursor tracking method may also be useful in non-search settings.
7E0964F5	PageRank algorithm is used to re-rank the search results according to relations of Web links and to capture the relative Web pages dependent on particular search query terms. According to the features of web links of structural model, a new adaptive T-Rank algorithm for specific topics was proposed in order to yield more accurate results, deal with a problem of topic-drift exists in links among Web pages and to decrease the re-ranked time after new pages being crawled. Analysis and simulation results show that the improved T-Rank algorithm can efficiently save CPU resources, reduce the calculating time, and better resolve the problem of topic-drift.
06B1E08B	Search engines have changed the way we see the Internet. The ability to find the information by just typing in keywords was a big contribution to the overall web experience. While the conventional search engine methodology worked well for textual documents, locating scientific data remains a problem since they are stored in databases not readily accessible by search engine bots. Considering different temporal, spatial and thematic coverage of different databases, especially for interdisciplinary research it is typically necessary to work with multiple data sources. These sources can be federal agencies which generally offer national coverage or regional sources which cover a smaller area with higher detail. However for a given geographic area of interest there often exists more than one database with relevant data. Thus being able to query multiple databases simultaneously is a desirable feature that would be tremendously useful for scientists. Development of such a search engine requires dealing with various heterogeneity issues. In scientific databases, systems often impose controlled vocabularies which ensure that they are generally homogeneous within themselves but are semantically heterogeneous when moving between different databases. This defines the boundaries of possible semantic related problems making it easier to solve than with the conventional search engines that deal with free text. We have developed a search engine that enables querying multiple data sources simultaneously and returns data in a standardized output despite the aforementioned heterogeneity issues between the underlying systems. This application relies mainly on metadata catalogs or indexing databases, ontologies and webservices with virtual globe and AJAX technologies for the graphical user interface. Users can trigger a search of dozens of different parameters over hundreds of thousands of stations from multiple agencies by providing a keyword, a spatial extent, i.e. a bounding box, and a temporal bracket. As part of this development we have also added an environment that allows users to do some of the semantic tagging, i.e. the linkage of a variable name (which can be anything they desire) to defined concepts in the ontology structure which in turn provides the backbone of the search engine. 
815D5340	Modern computer vision research consumes labelled data in quantity, and building datasets has become an important activity. The Internet has become a tremendous resource for computer vision researchers. By seeing the Internet as a vast, slightly disorganized collection of visual data, we can build datasets. The key point is that visual data are surrounded by contextual information like text and HTML tags, which is a strong, if noisy, cue to what the visual data means. In a series of case studies, we illustrate how useful this contextual information is. It can be used to build a large and challenging labelled face dataset with no manual intervention. With very small amounts of manual labor, contextual data can be used together with image data to identify pictures of animals. In fact, these contextual data are sufficiently reliable that a very large pool of noisily tagged images can be used as a resource to build image features, which reliably improve on conventional visual features. By seeing the Internet as a marketplace that can connect sellers of annotation services to researchers, we can obtain accurately annotated datasets quickly and cheaply. We describe methods to prepare data, check quality, and set prices for work for this annotation process. The problems posed by attempting to collect very big research datasets are fertile for researchers because collecting datasets requires us to focus on two important questions: What makes a good picture? What is the meaning of a picture?
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
804E7F29	A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel, otherwise it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th-percentile), which we call extreme tail latency.We propose a prediction framework to reduce the extreme tail latency of search servers. The framework has a unique set of characteristics to predict long-running queries with high recall and improved precision. Specifically, prediction is delayed by a short duration to allow many short-running queries to complete without parallelization, and to allow the predictor to collect a set of dynamic features using runtime information. These features estimate query execution time with high accuracy. We also use them to estimate the prediction errors to override an uncertain prediction by selectively accelerating the query for a higher recall.We evaluate the proposed prediction framework to improve search engine performance in two scenarios using a simulation study: (1) query parallelization on a multicore processor, and (2) query scheduling on a heterogeneous processor. The results show that, for both scenarios, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision.
78E21799	Commercial Web search engines have to process user queries over huge Web indexes under tight latency constraints. In practice, to achieve low latency, large result caches are employed and a portion of the query traffic is served using previously computed results. Moreover, search engines need to update their indexes frequently to incorporate changes to the Web. After every index update, however, the content of cache entries may become stale, thus decreasing the freshness of served results. In this work, we first argue that the real problem in today's caching for large-scale search engines is not eviction policies, but the ability to cope with changes to the index, i.e., cache freshness. We then introduce a novel algorithm that uses a time-to-live value to set cache entries to expire and selectively refreshes cached results by issuing refresh queries to back-end search clusters. The algorithm prioritizes the entries to refresh according to a heuristic that combines the frequency of access with the age of an entry in the cache. In addition, for setting the rate at which refresh queries are issued, we present a mechanism that takes into account idle cycles of back-end servers. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages. An implementation of this algorithm is currently in production use at Yahoo!
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
7D2BE772	The overall goal of an information retrieval process is to retrieve the information relevant to the given request. The information retrieval techniques commonly used are based on keywords. These techniques use keyword listed to describe the content of information, but one problem with such list is that they do not say anything about the semantic relationships between keywords, nor do they take into account the meaning of words and phrases. To overcome these limitations, from the recent literature it is identified that it is necessary to analyze and determine the semantic features of both the content in document and query. Hence in this paper it is proposed to develop ontology and a comparison is made between the normal search and ontology based semantic search. Various experimental results are carried on, which shows the increase in document retrieval recall and precision rates, thereby demonstrating the effectiveness of the model.
8117FEA8	Web search behaviour studies, including eye-tracking studies of search result examination, have resulted in numerous insights to improve search result quality and presentation. Yet, eye tracking studies have been restricted in scale, due to the expense and the effort required. Furthermore, as the reach of the Web expands, it becomes increasingly important to understand how searchers around the world see and interact with the search results. To address both challenges, we introduce ViewSer, a novel methodology for performing web search examination studies remotely, at scale, and without requiring eye-tracking equipment. ViewSer operates by automatically modifying the appearance of a search engine result page, to clearly show one search result at a time as if through a "viewport", while partially blurring the rest and allowing the participant to move the viewport naturally with a computer mouse or trackpad. Remarkably, the resulting result viewing and clickthrough patterns agree closely with unrestricted viewing of results, as measured by eye-tracking equipment, validated by a study with over 100 participants. We also explore applications of ViewSer to practical search tasks, such as analyzing the search result summary (snip- pet) attractiveness, result re-ranking, and evaluating snippet quality. These experiments could have only be done previously by tracking the eye movements for a small number of subjects in the lab. In contrast, our study was performed with over 100 participants, allowing us to reproduce and extend previous findings, establishing ViewSer as a valuable tool for large-scale search behavior experiments.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
76974C30	Web search engines rank potentially relevant pages/sites for a user query. Ranking documents for user queries has also been at the heart of the Text REtrieval Conference (TREC in short) under the label ad-hoc retrieval. The TREC community has developed document ranking algorithms that are known to be the best for searching the document collections used in TREC, which are mainly comprised of newswire text. However, the web search community has developed its own methods to rank web pages/sites, many of which use link structure on the web, and are quite different from the algorithms developed at TREC. This study evaluates the performance of a state-of-the-art keyword-based document ranking algorithm (coming out of TREC) on a popular web search task: finding the web page/site of an entity, e.g. companies, universities, organizations, individuals, etc. This form of querying is quite prevalent on the web. The results from the TREC algorithms are compared to four commercial web search engines. Results show that for finding the web page/site of an entity, commercial web search engines are notably better than a state-of-the-art TREC algorithm. These results are in sharp contrast to results from several previous studies. 
812F4683	Nowadays machine learning algorithms are intensively used to improve the relevance of search engines by training on Internet search data, while their software implementations on commodity computers are not efficient (in terms of computation speed, power consumption, etc). Therefore FPGA-based accelerators have been proposed to process these large scale data. Data compression/decompression technology plays an important role for it could significantly increase those accelerators’ performance. Based on the statistics on datasets and the analysis of conventional data compression algorithms, we propose a bit mapping compression/decompression method to provide non-blocking streaming data to accelerators. Experiments indicate that the performance of hardware accelerator is increased up to 140% after adding the bit mapping modules. This method could also be extended to other data-intensive hardware accelerators.
80FCD18B	Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
80103D90	Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
7B2E5EC6	In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want
80846772	We propose a model that leverages the millions of clicks received by web search engines to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgments between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.
5E342F1A	Search sessions consist of a person presenting a query to a search engine, followed by that person examining the search results, selecting some of those search results for further review, possibly following some series of hyperlinks, and perhaps backtracking to previously viewed pages in the session. The series of pages selected for viewing in a search session, sometimes called the click data, is intuitively a source of relevance feedback information to the search engine. We are interested in how that relevance feedback can be used to improve the search results quality for all users, not just the current user. For example, the search engine could learn which documents are frequently visited when certain search queries are given. In this article, we address three issues related to using click data as implicit relevance feedback: (1) How click data beyond the search results page might be more reliable than just the clicks from the search results page; (2) Whether we can further subselect from this click data to get even more reliable relevance feedback; and (3) How the reliability of click data for relevance feedback changes when the goal becomes finding one document for the user that completely meets their information needs (if possible). We refer to these documents as the ones that are strictly relevant to the query. Our conclusions are based on empirical data from a live website with manual assessment of relevance. We found that considering all of the click data in a search session as relevance feedback has the potential to increase both precision and recall of the feedback data. We further found that, when the goal is identifying strictly relevant documents, that it could be useful to focus on last visited documents rather than all documents visited in a search session. 
7AEA750B	Unemployment rate prediction has become critically important, because it can help government to make decision and design policies. In recent years, forecast of unemployment rate attracts much attention from governments, organizations, and research institutes, and researchers. Recently, a novel method using search engine query data to forecast unemployment was proposed by scholars. In this paper, a data mining based framework using web information is introduced for unemployment rate prediction. Under the framework, a neural network method, as one of the most effective data mining tools, is developed to forecast unemployment trend using search engine query data. In the proposed method, search engine query data related with employment activities is firstly found. Secondly, feature selection models including correlation coefficient method and genetic algorithm are constructed to reduce the dimension of the query data. Thirdly, various neural networks are employed to model the relationship between unemployment rate data and query data. Fourthly, an optimal neural network is selected as the selective predictor by using the cross-validation method. Finally, the selective neural network predictor with the best feature subset is used to forecast unemployment trend. The empirical results show that the proposed method clearly outperforms the classical forecasting approaches for the unemployment rate prediction. These findings imply that data mining method, such as neural networks, together with web information, can be used as an alternative tool to forecast social/economic hotspot.
7918FE86	Influenza epidemics detection is critically important in recent years because there is a significant economic and public health impact associated with the influenza epidemic. Influenza epidemics detection attracts much attention from governments, organizations, and research institutes, and recently, a novel method using search engine query data to detect influenza activities was presented by Google. In this paper, a data mining based framework using web data is introduced for influenza epidemics detection. Under the framework, a neural network based approach using search engine query data is developed to detect influenza activities. In the proposed method, an automated feature selection model is firstly constructed to reduce the dimension of the query data. Secondly, various neural networks are employed to model the relationship between influenza-like illness data and query data. Thirdly, an optimal neural network is selected as the detector by using the cross-validation method. Finally, the selective neural network detector with the best feature subset is used to detect influenza activities. Experimental results show that the proposed method can outperform traditional statistical models and other models used in the experiments in terms of accuracy. These findings imply that data mining, such as neural network method, can be used as a promising tool to detect influenza activities.
815D5340	Modern computer vision research consumes labelled data in quantity, and building datasets has become an important activity. The Internet has become a tremendous resource for computer vision researchers. By seeing the Internet as a vast, slightly disorganized collection of visual data, we can build datasets. The key point is that visual data are surrounded by contextual information like text and HTML tags, which is a strong, if noisy, cue to what the visual data means. In a series of case studies, we illustrate how useful this contextual information is. It can be used to build a large and challenging labelled face dataset with no manual intervention. With very small amounts of manual labor, contextual data can be used together with image data to identify pictures of animals. In fact, these contextual data are sufficiently reliable that a very large pool of noisily tagged images can be used as a resource to build image features, which reliably improve on conventional visual features. By seeing the Internet as a marketplace that can connect sellers of annotation services to researchers, we can obtain accurately annotated datasets quickly and cheaply. We describe methods to prepare data, check quality, and set prices for work for this annotation process. The problems posed by attempting to collect very big research datasets are fertile for researchers because collecting datasets requires us to focus on two important questions: What makes a good picture? What is the meaning of a picture?
776647D6	   The  World Wide Web provides an immense source of information.Accessing information of interest presents a challenge to scientists and analysts, particularly if the desired information is structural in nature. Our goal is to design a structural search engine that uses the hyperlink structure of the Web, in addition to textual information, to search for sites of interest.Our structural search engine, called WebSUBDUE, searches not only for particular words or topics but also for a desired hyperlink structure.Enhanced by WordNet text functions, our search engine retrieves sites corresponding to structures formed by graph-based userqueries. We hypothesize that this system can form the heart  of a structural query engine,and demonstrate the approach on a number of structural web queries.
06B1E08B	Search engines have changed the way we see the Internet. The ability to find the information by just typing in keywords was a big contribution to the overall web experience. While the conventional search engine methodology worked well for textual documents, locating scientific data remains a problem since they are stored in databases not readily accessible by search engine bots. Considering different temporal, spatial and thematic coverage of different databases, especially for interdisciplinary research it is typically necessary to work with multiple data sources. These sources can be federal agencies which generally offer national coverage or regional sources which cover a smaller area with higher detail. However for a given geographic area of interest there often exists more than one database with relevant data. Thus being able to query multiple databases simultaneously is a desirable feature that would be tremendously useful for scientists. Development of such a search engine requires dealing with various heterogeneity issues. In scientific databases, systems often impose controlled vocabularies which ensure that they are generally homogeneous within themselves but are semantically heterogeneous when moving between different databases. This defines the boundaries of possible semantic related problems making it easier to solve than with the conventional search engines that deal with free text. We have developed a search engine that enables querying multiple data sources simultaneously and returns data in a standardized output despite the aforementioned heterogeneity issues between the underlying systems. This application relies mainly on metadata catalogs or indexing databases, ontologies and webservices with virtual globe and AJAX technologies for the graphical user interface. Users can trigger a search of dozens of different parameters over hundreds of thousands of stations from multiple agencies by providing a keyword, a spatial extent, i.e. a bounding box, and a temporal bracket. As part of this development we have also added an environment that allows users to do some of the semantic tagging, i.e. the linkage of a variable name (which can be anything they desire) to defined concepts in the ontology structure which in turn provides the backbone of the search engine. 
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
77EB1685	Current automatic wrappers using DOM tree and visual properties of data records to extract the required information from the search engine results pages generally have limitations such as the inability to check the similarity of tree structures accurately. Our study on the properties of data records shows that these data records located in search engine results pages are not only having similar visual properties and tree structures, but they are also related semantically in their contents. In this context, we propose an ontological technique using existing lexical database for English (WordNet) for the extraction of data records. We find that wrappers designed based on ontological technique are able to reduce the number of potential data regions to be extracted, thus they are able to improve the data extraction accuracy. We then use visual cue from the browser rendering engine to locate and extract the relevant data region from the web page by measuring the size of text and image of data records. Experimental results indicate that our technique is robust and performs better than the existing state of the art visual based wrappers.
7FA14777	FPGAs (field programmable gate arrays) have appealing features such as customizable internal and external bandwidth and the ability to exploit vast amounts of fine-grain parallelism. In this paper, we explore the applicability of these features in using FPGAs as smart memory engines for search and reorganization computations over spatial pointer-based data structures. The experimental results in this paper suggests that reconfigurable logic, when combined with the data reorganization, can lead to dramatic performance improvements of up to 20x over traditional computer architectures for pointer-based computations, traditionally not viewed as a good match for reconfigurable technologies.
75D95FB3	Unlabeled document collections are becoming increasingly common and mining such databases becomes a major challenge. It is a major issue to retrieve good websites from the larger collections of websites. As the number of available Web pages grows, it is become more difficult for users finding documents relevant to their interests. Clustering is the classification of a data set into subsets (clusters), so that the data in each subset share some common trait - often proximity according to some defined distance measure. By clustering we improve the quality of websites by grouping similar websites in groups. This paper addresses the applications of data mining tool Weka by applying k means clustering to find clusters from huge data sets and find the attributes that govern optimization of search engines.
7BD8C7FA	In an web based application, different users may have different search goals when they submit it to a search engine. For a broad-topic and ambiguous query it is difficult. Here we Propose a novel approach to infer user search goals by analyzing search engine query logs. A major deficiency of generic search engines is that they follow the “one size fits all” model and are not adaptable to individual users. This is typically shown in cases such as these: Different users have different backgrounds and interests. However, effective personalization cannot be achieved without accurate user profiles. We address the problem of learning the user profile within the user's ongoing behaviors by using the user search. We propose a framework that enables large-scale evaluation of personalized search. User interest is employed in the clustering process to achieve personalization effect. The goal of personalized IR (information retrieval) is to return search results that better match the user intent. First, we propose a framework to discover different user search goals for a query by clustering the proposed feedback sessions. Feedback sessions are get constructed from user click-through logs and can efficiently reflect the information needs of users. Second, we propose an approach to generate pseudo-documents to better represent the feedback sessions for clustering. Most document-based methods focus on analyzing users' clicking and browsing behaviors recorded at the users' clickthrough data. In the Web search engines, clickthrough data are important implicit feedback mechanism from users. An example of clickthrough data for the query "apple," which contains a list of ranked search results presented to the user, which contains identification on the results that was previously clicked by the user. The bolded documents that have been clicked by the user have been ranked. Several personalized systems that employ clickthrough data to capture users' interest have been proposed.
802A5C28	Web page classification is a major issue for categorising web documents to facilitate indexing, search and retrieval of web pages for search engine. Different crawling techniques have been utilised to accumulate web pages of different domains under separate databases depending on practical scenario. Downloaded web pages are being parsed for further processing. A classifier is designed dynamically using single cycle multiple attractor cellular automata for mapping downloaded web pages of different domains into specific structure. This paper proposes alternate technique for automatic categorisation of web pages into different domains. Retrieved web pages have been ranked automatically at the time of classifier formation. Typically, our system consists of crawling, ranking and storage parts created in a different way. Hierarchical concept has been used over parallel crawler. GF(2P) concept is introduced in ranking. The concept of SMACA has been utilised in indexing storage. Overall, a search engine module has been created using agent-based method.
58B0DC1D	Open-ended questions can be a nightmare for statistical processing. Any mistake in spelling can result in a mismatch during merging, or multiple counting of the same object. For example, the answers to the "place-of-birth" question might be "Chicago" and "San Francisco", but in practice they are often "Chicaga" and "SanFrancisko". Manual correction of hundreds of answers is tedious, and becomes infeasible with a larger dataset. For a long time, algorithms like SOUNDEX remained the only alternative for researchers. A new Stata command allows taking advantage of Internet search engines, like Google or Yahoo to find proper substitutes for an unclear word or multiple words. The distinctive feature of the search engines is that they rely not only on the spelling similarity, but are also context driven: other words may affect the suggestion, such as including "city" into the query. This will hint to the search engine to give more priority to the names of cities. This presentation will demonstrate this new command and explain the main steps necessary to programmatically acquire information available on the Internet and convert it into Stata-usable format.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you’ll see a vision for Web search’s future that's more sophisticated, individual, and portable.
7E0964F5	PageRank algorithm is used to re-rank the search results according to relations of Web links and to capture the relative Web pages dependent on particular search query terms. According to the features of web links of structural model, a new adaptive T-Rank algorithm for specific topics was proposed in order to yield more accurate results, deal with a problem of topic-drift exists in links among Web pages and to decrease the re-ranked time after new pages being crawled. Analysis and simulation results show that the improved T-Rank algorithm can efficiently save CPU resources, reduce the calculating time, and better resolve the problem of topic-drift.
7D2BE772	The overall goal of an information retrieval process is to retrieve the information relevant to the given request. The information retrieval techniques commonly used are based on keywords. These techniques use keyword listed to describe the content of information, but one problem with such list is that they do not say anything about the semantic relationships between keywords, nor do they take into account the meaning of words and phrases. To overcome these limitations, from the recent literature it is identified that it is necessary to analyze and determine the semantic features of both the content in document and query. Hence in this paper it is proposed to develop ontology and a comparison is made between the normal search and ontology based semantic search. Various experimental results are carried on, which shows the increase in document retrieval recall and precision rates, thereby demonstrating the effectiveness of the model.
7A81F96C	Today’s knowledge workers rely increasingly on information to get their job done, and the availability of search engines to locate relevant information is thus essential. Understanding how users interact with search engines is a prerequisite for the successful design of useful systems and a body of knowledge has in recent years begun to compile. However, all previous studies have focused on the public web, not acknowledging the fact that much business-related information seeking occur on corporate internal networks. In this exploratory study, we have collected and analysed intranet search engine log files from three different years – 2000, 2002, and 2004 – enabling us to detect shifting trends in intranet search behaviour. Comparing our data to what has been reported from the public web we conclude that intranet searchers are both similar to and different from searchers on the public web. In sum, it appears that intranet users are more extreme in their behaviour and that qualitative studies are needed to understand the motives and rationales governing their actions.
812F4683	Nowadays machine learning algorithms are intensively used to improve the relevance of search engines by training on Internet search data, while their software implementations on commodity computers are not efficient (in terms of computation speed, power consumption, etc). Therefore FPGA-based accelerators have been proposed to process these large scale data. Data compression/decompression technology plays an important role for it could significantly increase those accelerators’ performance. Based on the statistics on datasets and the analysis of conventional data compression algorithms, we propose a bit mapping compression/decompression method to provide non-blocking streaming data to accelerators. Experiments indicate that the performance of hardware accelerator is increased up to 140% after adding the bit mapping modules. This method could also be extended to other data-intensive hardware accelerators.
809F7B04	To confront with an ever increasing number of published scientific articles, an effective, efficient, and easy-to-use tool is required to support biomedical scientists, while entering a new scientific field and encountering clinical decision, to organize a vast amount of PubMed abstracts into the panorama of specific topics according to their relevance. In brief, the set of associations among frequently co-occurring terms in given a set of PubMed documents forms naturally a simplicial complex. Afterwards each connected component of this simplicial complex represents a concept in the collection. Based on these concepts, documents can be clustered into meaningful classes. This paper presents an alternative search engine that applies a combinatorial topological method to automatically extract semantic clusters from the PubMed database of biomedical literature. We use several qualitative parameters to perform the user study that shows users are able to reduce search time. This clustering search engine is publicly available at http://ginni. bme.ntu. edu. tw/
7DB5FDAD	This paper reports our efforts to address the grand challenge of the Digital Earth vision in terms of intelligent data discovery from vast quantities of geo-referenced data. We propose an algorithm combining LSA and a Two-Tier Ranking (LSATTR) algorithm based on revised cosine similarity to build a more efficient search engine – Semantic Indexing and Ranking (SIR) – for a semantic-enabled, more effective data discovery. In addition to its ability to handle subject-based search, we propose a mechanism to combine geospatial taxonomy and Yahoo! GeoPlanet for automatic identification of location information from a spatial query and automatic filtering of datasets that are not spatially related. The metadata set, in the format of ISO19115, from NASA's SEDAC (Socio-Economic Data Application Center) is used as the corpus of SIR. Results show that our semantic search engine SIR built on LSATTR methods outperforms existing keyword-matching techniques, such as Lucene, in terms of both recall and precision. Moreover, the semantic associations among all existing words in the corpus are discovered. These associations provide substantial support for automating the population of spatial ontologies. We expect this work to support the operationalization of the Digital Earth vision by advancing the semantic-based geospatial data discovery.
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
5BD5188C	This paper develops a service for ontology evolution based on crowdsourcing. The approach is demonstrated using OntoAssist, a specially designed semantic search service that is capable of capturing and disambiguating user’s search intent as well as automatically enabling ontology evolution. Successful and consistent ontology evolution often requires large amount of input data to specify new terms or changes in relationships. These inputs typically come mainly from domain experts or ontology professionals, which makes it hard to keep up with the change of open, dynamic World Wide Web environment. By integrating OntoAssist with an existing search engine, we show that users’ search intent can be disambiguated and aggregated to help to evolve underlying ontology. The disambiguation feature helps the users to find desirable search results. OntoAssist has been implemented and tested by Turkers from Amazon Mechanical Turk in a live demonstration site. Promising results and analysis are reported.
7E054376	We describe an ensemble approach to learning salient spatial regions from arbitrarily partitioned simulation data. Ensemble approaches for anomaly detection are also explored. The partitioning comes from the distributed processing requirements of large-scale simulations. The volume of the data is such that classifiers can train only on data local to a given partition. Since the data partition reflects the needs of the simulation, the class statistics can vary from partition to partition.Some classes will likely be missing from some or even most partitions. We combine a fast ensemble learning algorithm with scaled probabilistic majority voting in order to learn an accurate classifier from such data. Since some simulations are difficult to model without a considerable number of false positive errors, and since we are essentially building a search engine for simulation data, we order predicted regions to increase the likelihood that most of the top-ranked predictions are correct(salient). Results from simulation runs of a canister being torn and from a casing being dropped show that regions of interest are successfully identified in spite of the class imbalance in the individual training sets. Lift curve analysis shows that the use of data driven ordering methods provides a statistically significant improvement over the use of the default, natural time step ordering. Significant time is saved for the end user by allowing an improved focus on areas of interest without the need to conventionally search all of the data. We have also found that using random forests weighted and distance-based outlier ensemble methods for supervised learning of anomaly detection provide significant accuracy improvements when compared to existing methods on the same dataset. Further, distance-based outlier and local outlier factor ensemble methods for unsupervised learning of anomaly detection also compare favorably to existing methods.
7D4DA857	This work describes a Web search approach taking into account the semantic content of Web pages. Eliminating irrelevant Web pages, the time-consuming task of revise the obtained results from actual search engines is reduced. The proposed approach is focused on Web pages that are not defined with semantic Web structure (most of the actual Web pages are in this format). The challenge is extract the semantic content from heterogeneous and human oriented Web pages. The approach integrates structures of ontologies, WordNet, and a hierarchical similarity measure to determine the relevance of a Web page.
7F83374A	Local business voice search is a popular application for mobile phones, where hands-free interaction and speed are critical to users. However, speech recognition accuracy is still not satisfactory when the number of businesses and locations is extended nationwide. For mobile users, searching a local business directory is often related to the fulfillment of specific tasks "on-the-move", such as finding a restaurant, a movie theater, or a retailer chain. Restricting the local search to specific domains improves the quality of search results. In this paper, we present a new approach to data selection for bootstrapping and optimizing language models for vertical business sectors by exploiting semantic knowledge encoded in the business database and in the business category taxonomy. We demonstrate that, in the case of queries in the restaurant domain and without collecting new data, speech recognition word accuracy improves by 9.5% relative when compared with a generic local business language model.
80359F5F	Among the challenges of searching the vast information source the Web has become, improving Web search efficiency by different strategies using semantics and the user generated data from Web 2.0 applications remains a promising and interesting approach. In this paper, we present the Personal Social Dataset and Ontology-guided Input strategies and couple them together, providing a proofof- concept implementation.
7D6DFDA0	Contextual retrieval is a critical technique for todaypsilas search engines in terms of facilitating queries and returning relevant information. This paper reports on the development and evaluation of a system designed to tackle some of the challenges associated with contextual information retrieval from the World Wide Web (WWW). The developed system has been designed with a view to capturing both implicit and explicit user data which is used to develop a personal contextual profile. Such profiles can be shared across multiple users to create a shared contextual knowledge base. These are used to refine search queries and improve both the search results for a user as well as their search experience. An empirical study has been undertaken to evaluate the system against a number of hypotheses. In this paper, results related to one are presented that support the claim that users can find information more readily using the contextual search system	
5D1A2FAD	The main purpose of analyzing the social network data is to observe the behaviors and trends that are followed by people. How people interact with each other, what they usually share, what are their interests on social networks, so that analysts can focus new trends for the provision of those things which are of great interest for people so in this paper an easy approach of gathering and analyzing data through keyword based search in social networks is examined using NodeXL and data is gathered from twitter in which political trends have been analyzed. As a result it will be analyzed that, what people are focusing most in politics.
79A2A305	The Internet is a widely used source of information for patients searching for medical/health care information. While many studies have assessed existing medical/health care information on the Internet, relatively few have examined methods for design and delivery of such websites, particularly those aimed at the general public.This study describes a method of evaluating material for new medical/health care websites, or for assessing those already in existence, which is correlated with higher rankings on Google's Search Engine Results Pages (SERPs).A website quality assessment (WQA) tool was developed using criteria related to the quality of the information to be contained in the website in addition to an assessment of the readability of the text. This was retrospectively applied to assess existing websites that provide information about generic medicines. The reproducibility of the WQA tool and its predictive validity were assessed in this studyThe WQA tool demonstrated very high reproducibility (intraclass correlation coefficient=0.95) between 2 independent users. A moderate to strong correlation was found between WQA scores and rankings on Google SERPs. Analogous correlations were seen between rankings and readability of websites as determined by Flesch Reading Ease and Flesch-Kincaid Grade Level scores.The use of the WQA tool developed in this study is recommended as part of the design phase of a medical or health care information provision website, along with assessment of readability of the material to be used. This may ensure that the website performs better on Google searches. The tool can also be used retrospectively to make improvements to existing websites, thus, potentially enabling better Google search result positions without incurring the costs associated with Search Engine Optimization (SEO) professionals or paid promotion.
7ECEAEEC	With web image search engines, we face a situation where the results are very noisy, and when we ask for a specific object, we are not ensured that this object is contained in all the images returned by the search engines: about 50% of the images returned are off-topic. In this paper, we explain how knowing the color of an object can help locating the object in images, and we also propose methods to automatically find the color of an object, so that the whole process can be fully automatic. Results reveal that this method allows us to reduce the noise in returned images while providing automatic segmentation so that it can be used for clustering or object learning.
7340259A	In this paper we consider the problem of anonymizing datasets in which each individual is associated with a set of items that constitute private information about the individual. Illustrative datasets include market-basket datasets and search engine query logs. We formalize the notion of k-anonymity for set-valued data as a variant of the k-anonymity model for traditional relational datasets. We define an optimization problem that arises from this definition of anonymity and provide O(klogk) and O(1)-approximation algorithms for the same. We demonstrate applicability of our algorithms to the America Online query log dataset. 
7D55F348	The size of the publicly indexable World Wide Web (WWW) has probably surpassed 14.3 billion documents and as yet growth shows no sign of leveling off. Search engines encounter the problem of ambiguity in words; therefore, search engines use ontology to find pages with words that are syntactically different but semantically similar. The knowledge provided by ontology is extremely useful in defining the structure and scope for mining web content. Context-ontology is a shared vocabulary to share context information in a pervasive computing domain and include machine-interpretable definitions of basic concepts in the domain and relations among them. This paper proposes an architecture for relevant searching of web documents using data mining techniques such as clustering and association rules. These techniques together with context and ontology extract potentially useful documents from the database. Also, an algorithm has been devised which shows the working in sequence of steps. Finally, the results are compared with the prevailing approaches and with the help of an example it has been seen that CODT is better in context of relevancy.
7FC2A67C	The Semantic Web, an extension of the current web,is an emerging field with the aim of building infrastructure,wherein software agents and people can work in cooperation by sharing knowledge.The emerging Semantic Web community has proposed ontology’s to express knowledge in a machine understandable way. Using this ontology, the search engine is proposed which combines content based search techniques with spread  activation techniques applied to a semantic model of a given domain. It is based on a hybrid spread activation algorithm applied to the concept instances graph to find the related concepts in the ontology based on the given query.
78B9FFFA	Various software packages are commonly used for the implementation and calculation of decision-analytic models for health economic evaluations. However, comparison of these programs with regard to ease of implementing a model is lacking.(i) to compare the assets and drawbacks of three commonly used software packages for Markov models with regard to ease of implementation; and (ii) to investigate how a technical model validation can be conducted by comparing the results of the three implementations.A Markov model on chronic obstructive pulmonary disease was implemented in TreeAge, Microsoft Excel and Arena with the same assumptions on model structure, transition probabilities and costs. A hypothetical smoking cessation programme for patients in stage 1 was evaluated against usual care. The packages were compared with respect to time and effort for implementation, run-time, features for the presentation of results, and flexibility. Agreement between the packages on average costs and life-years gained and on the incremental cost-effectiveness ratio was considered for technical validation in the form of expected values (between TreeAge and Excel only) and Monte Carlo simulations.Ease of implementation was best in TreeAge, whereas Arena offered the highest flexibility. Deterministic results were in agreement between TreeAge and Excel, as were simulated values between all three packages.Excel offers an intuitive spreadsheet interface, but the acquisition of and the training in TreeAge or Arena is worthwhile for more complex models. Double implementation is a practicable validation technique that should be conducted to ensure correct model implementation.
7F5443CD	In this paper, we study the correlation properties of the fading mobile radio channel. Based on these studies, we model the channel as a one-step Markov process whose transition probabilities are a function of the channel characteristics. Then we present the throughput performance of the Go-Back-N and selective-repeat automatic repeat request (ARQ) protocols with timer control, using the Markov model for both forward and feedback channels. This approximation is found to be very good, as confirmed by simulation results.
78744637	In this paper, we introduce a 3-D human-body tracker capable of handling fast and complex motions in real-time. The parameter space, augmented with first order derivatives, is automatically partitioned into Gaussian clusters each representing an elementary motion: hypothesis propagation inside each cluster is therefore accurate and efficient.  The transitions between clusters use the predictions of a Variable Length Markov Model which can explain high level behaviours over a long history. Using Monte-Carlo methods, evaluation of model candidates is critical for both speed and robustness.  We present a new evaluation scheme based on volumetric reconstruction and blobs-fitting, where appearance models and image evidences are represented by Gaussian mixtures.  We demonstrate the application of our tracker to long video sequences exhibiting rapid and diverse movements.
7D1020E8	An automatic-repeat-request (ARQ) Go-Back-N (GBN) protocol with unreliable feedback and time-out mechanism is studied, using renewal theory. Transmissions on both the forward and the reverse channels are assumed to experience Markovian errors. The exact throughput of the protocol is evaluated, and simulation results, that confirm the analysis, are presented. A detailed comparison of the proposed method and the commonly used transfer function method reveals that the proposed approach is simple and potentially more powerful.
80D7D606	The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model's statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
78AD17F6	Using a finite state Markov channel model, we develop an analytical method for evaluation of the packet error structure in multiple-input multiple-output (MIMO) systems based on singular value decomposition (SVD). We consider dual-branch MIMO systems, with either two transmit and arbitrary number of receive antennas, or arbitrary number of transmit and two receive antennas. The corresponding Markov model parameters are obtained using a novel closed-form expressions for probability density function and level crossing rate of the signal-to-noise ratio at the output of eigenchannels in a MIMO system, derived for a case of Rayleigh propagation, imperfect channel state information and any fixed power allocation. The exact bit error rate for the transmission of quadrature amplitude modulated (QAM) symbols through the eigenchannels is derived in polynomial closed form. Furthermore, by using the developed Markov model, the packet error statistics in the corresponding eigenchannels are determined, and the closed-form analytical expression for the system throughput is derived when ‘go-back-N’ automatic repeat request procedure is applied in time-varying eigenchannels. The analytical results are validated by using Monte Carlo simulations.
7D9B3729	Background. Serial period prevalence estimates for recurrent diseases such as major depression are available more frequently than fully detailed longitudinal data, but it is difficult to estimate incidence and episode duration from such data. Incidence and episode duration are critical decision modeling parameters for recurrent diseases. Objectives. To reduce bias that would otherwise occur in national incidence and duration-of-episode estimates for major depressive episodes deriving from studies using serial period prevalence data and to illustrate amethodological approach for the estimation of incidence from such studies. Methods. Monte Carlo simulation was applied to a Markov process describingincidence and recovery from major depressive episodes. Results. The annual incidence and episode duration were found to be 3.1% and 17.1 weeks, respectively. These estimates are expected to be less subject to bias than those generated without modeling. Conclusions. These results highlight the usefulness of Markov models for analysis of longitudinal data. The methods described here may be useful for decision modeling andmay be generalizable to other chronic diseases.
7E17B5B5	Most epidemiological studies of major depression report period prevalence estimates. These are of limited utility in characterizing the longitudinal epidemiology of this condition. Markov models provide a methodological framework for increasing the utility of epidemiological data. Markov models relating incidence and recovery to major depression prevalence have been described in a series of prior papers. In this paper, the models are extended to describe the longitudinal course of the disorder. Data from three national surveys conducted by the Canadian national statistical agency (Statistics Canada) were used in this analysis. These data were integrated using a Markov model. Incidence, recurrence and recovery were represented as weekly transition probabilities. Model parameters were calibrated to the survey estimates. The population was divided into three categories: low, moderate and high recurrence groups. The size of each category was approximated using lifetime data from a study using the WHO Mental Health Composite International Diagnostic Interview (WMH-CIDI). Consistent with previous work, transition probabilities reflecting recovery were high in the initial weeks of the episodes, and declined by a fixed proportion with each passing week. Markov models provide a framework for integrating psychiatric epidemiological data. Previous studies have illustrated the utility of Markov models for decomposing prevalence into its various determinants: incidence, recovery and mortality. This study extends the Markov approach by distinguishing several recurrence categories. 
7E61B32A	Depression is among the major contributors to worldwide disease burden and adequate modelling requires a framework designed to depict real world disease progression as well as its economic implications as closely as possible.In light of the specific characteristics associated with depression (multiple episodes at varying intervals, impact of disease history on course of illness, sociodemographic factors), our aim was to clarify to what extent "Discrete Event Simulation" (DES) models provide methodological benefits in depicting disease evolution.We conducted a comprehensive review of published Markov models in depression and identified potential limits to their methodology. A model based on DES principles was developed to investigate the benefits and drawbacks of this simulation method compared with Markov modelling techniques.The major drawback to Markov models is that they may not be suitable to tracking patients' disease history properly, unless the analyst defines multiple health states, which may lead to intractable situations. They are also too rigid to take into consideration multiple patient-specific sociodemographic characteristics in a single model. To do so would also require defining multiple health states which would render the analysis entirely too complex. We show that DES resolve these weaknesses and that its flexibility allow patients with differing attributes to move from one event to another in sequential order while simultaneously taking into account important risk factors such as age, gender, disease history and patients attitude towards treatment, together with any disease-related events (adverse events, suicide attempt etc.).DES modelling appears to be an accurate, flexible and comprehensive means of depicting disease progression compared with conventional simulation methodologies. Its use in analysing recurrent and chronic diseases appears particularly useful compared with Markov processes.
802AF8D6	We investigate the behavior of block errors which arise in data transmission on fading channels. Our approach takes into account the details of the specific coding/modulation scheme and tracks the fading process symbol by symbol. It is shown that a Markov approximation for the block error process (possibly degenerating into an identically distributed (i.i.d.) process for sufficiently fast fading) is a good model for a broad range of parameters. Also, it is observed that the relationship between the marginal error rate and the transition probability is largely insensitive to parameters such as block length, degree of forward error correction and modulation format, and depends essentially on an appropriately normalized version of the Doppler frequency. This relationship can therefore be computed in the simple case of a threshold model and then used more generally as an accurate approximation. This observation leads to a unified approach for the channel modeling, and to a simplified performance analysis of upper layer protocols.
7FD11145	Block-error processes in transmissions over slow-fading channels can be accurately modeled by a two-state Markov chain [the block Markov model (BMM)]. Another line of research has focused on the use of a channel-state Markov model (CSMM) to analyze block transmissions. Although both techniques provide results that agree well with observations, the relationship between both Markov models has not been recognized in the previous literature. In this letter, we show that the BMM for slow-fading channels can be directly derived from the CSMM. In addition, we introduce a greatly simplified channel-modeling methodology. In the new methodology, the BMM is the primary channel characterization tool, and the CSMM becomes essentially an estimation technique that provides parameters for the BMM. Results of packet transmissions in slow-fading channels show that our approach provides significant improvements in both accuracy and simplicity over previously proposed techniques.
7CBB23DF	Chronic obstructive pulmonary disease (COPD) is currently the fourth leading cause of death worldwide. It has serious health effects and causes substantial costs for society.The aim of the present paper was to develop a state-of-the-art decision-analytic model of COPD whereby the cost effectiveness of interventions in Germany can be estimated. To demonstrate the applicability of the model, a smoking cessation programme was evaluated against usual care. A seven-stage Markov model (disease stages I to IV according to the GOLD [Global Initiative for Chronic Obstructive Lung Disease] classification, states after lung-volume reduction surgery and lung transplantation, death) was developed to conduct a cost-utility analysis from the societal perspective over a time horizon of 10, 40 and 60 years. Patients entered the cohort model at the age of 45 with mild COPD. Exacerbations were classified into three levels: mild, moderate and severe. Estimation of stage-specific probabilities (for smokers and quitters), utilities and costs was based on German data where possible. Data on effectiveness of the intervention was retrieved from the literature. A discount rate of 3% was applied to costs and effects. Probabilistic sensitivity analysis was used to assess the robustness of the results.The smoking cessation programme was the dominant strategy compared with usual care, and the intervention resulted in an increase in health effects of 0.54 QALYs and a cost reduction of €1115 per patient (year 2007 prices) after 60 years. In the probabilistic analysis, the intervention dominated in about 95% of the simulations. Sensitivity analyses showed that uncertainty primarily originated from data on disease progression and treatment cost in the early stages of disease.e model developed allows the long-term cost effectiveness of interventions to be estimated, and has been adapted to Germany. The model suggests that the smoking cessation programme evaluated was more effective than usual care as well as being cost-saving. Most patients had mild or moderate COPD, stages for which parameter uncertainty was found to be high. This raises the need to improve data on the early stages of COPD.
7F603AAF	A method of dynamically constructing Markov chain models that describe the characteristics of binary messages is developed. Such models can be used to predict future message characters and can therefore be used as a basis for data compression. To this end, the Markov modelling technique is combined with Guazzo's arithmetic coding scheme to produce a powerful method of data compression. The method has the advantage of being adaptive: messages may be encoded or decoded with just a single pass through the data. Experimental results reported here indicate that.the Markov modelling approach generally achieves much better data compression than that observed with competing methods on typical computer data.
7F5E056C	Motivated by applications such as automated visual surveillance and video monitoring and annotation, there has been a lot of interest in constructing cognitive vision systems capable of interpreting the high level semantics of dynamic scenes. In this paper we present a novel approach for automatically inferring models of object interactions that can be used to interpret observed behaviour within a scene. A real-time low-level computer vision system, together with an attentional control mechanism, are used to identify incidents or events that occur in the scene. A data driven approach has been taken in order to automatically infer discrete and abstract representations (symbols) of primitive object interactions; effectively the system learns a set of qualitative spatial relations relevant to the dynamic behaviour of the domain. These symbols then form the alphabet of a VLMM which automatically infers the high level structure of typical interactive behaviour. The learnt behaviour model has generative capabilities and is also capable of recognizing typical or atypical activities within a scene. Experiments have been performed within the traffic monitoring domain; however the proposed method is applicable to the general automatic surveillance task since it does not assume a priori knowledge of a specific domain.
810600AC	Markov models are useful when a decision problem involves risk that is continuous over time, when the timing of events is important, and when important events may happen more than once. Representing such clinical settings with conventional decision trees is difficult and may require unrealistic simplifying assumptions. Markov models assume that a patient is always in one of a finite number of discrete health states, called Markov states. All events are represented as transitions from one state to another. A Markov model may be evaluated by matrix algebra, as a cohort simulation, or as a Monte Carlo simulation. A newer representation of Markov models, the Markov-cycle tree, uses a tree representation of clinical events and may be evaluated either as a cohort simulation or as a Monte Carlo simulation. The ability of the Markov model to represent repetitive events and the time dependence of both probabilities and utilities allows for more accurate representation of clinical settings that involve these issues. 
7E33249D	Smoking cessation is the only strategy that has shown a lasting reduction in the decline of lung function in patients with chronic obstructive pulmonary disease. This study aims to evaluate the cost-effectiveness of smoking cessation interventions in patients with chronic obstructive pulmonary disease, to assess the quality of the Markov models and to estimate the consequences of model structure and input data on cost–effectiveness. A systematic literature search was conducted in PubMed, Embase, BusinessSourceComplete and Econlit on June 11, 2014. Data were extracted, and costs were inflated. Model quality was evaluated by a quality appraisal, and results were interpreted. Ten studies met the inclusion criteria. The results varied widely from cost savings to additional costs of €17,004 per quality adjusted life year. The models scored best in the category structure, followed by data and consistency. The quality of the models seems to rise over time, and regarding the results there is no economic reason to refuse the reimbursement of any smoking cessation intervention 
81507EC1	In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.
804AD709	Most epidemiological studies of major depression report estimates of period prevalence. Such estimates are useful for public health applications, but are not very helpful for informing clinical practice. Period prevalence is determined predominantly by incidence and episode duration, but it is difficult to connect these epidemiological concepts to clinical issues such as risk and prognosis. Incidence is important for primary and secondary prevention, and prognostic information is useful for clinical decision-making. The objective of this study was to decompose period prevalence data for major depression into its constituent elements, thereby enhancing the value of these estimates for clinical practice. Data from a series of population-based Canadian studies were used in the analysis. Markov models depicting incidence, prevalence and recovery from major depressive episodes were developed. Monte Carlo simulation was used to constrain model parameters to the epidemiological data.The association of sex with major depression was found to be due to a higher incidence in women. In distinction, the higher prevalence in unmarried subjects was mostly due to a different prognosis. Age-related changes in prevalence were influenced by both factors. Education, which was not found to be associated with major depression in the survey data, had no impact either on risk or prognosis.The period prevalence of major depression is influenced both by incidence (risk) and episode duration (prognosis). Mathematical modeling of the underlying epidemiological relationships can make such data more readily interpretable in relation to clinical practice.
7EA1B380	A formula for the go-back-N ARQ (automatic repeat request) scheme applicable to Markov error patterns is derived. It is a generalization of the well-known efficiency formula p/(p+m(1-p)) (where m is the round trip delay in number of block durations and p is the block transmission success probability), and it has been successfully validated against simulation measurements. It is found that for a given error rate, error patterns having zero correlation between successive transmission generally fare better than those with negative correlation, and that error patterns with positive correlation fare better still. It is shown that the present analysis can be extended in a straightforward manner to cope with error patterns of a more complex nature. Simple procedures for numerical evaluation of efficiency under quite general error structures are presented.
7E8549A3	Markov models are often employed to represent stochastic processes, that is, random processes that evolve over time. In a healthcare context, Markov models are particularly suited to modelling chronic disease. In this article, we describe the use of Markov models for economic evaluation of healthcare interventions. The intuitive way in which Markov models can handle both costs and outcomes make them a powerful tool for economic evaluation modelling. The time component of Markov models can offer advantages of standard decision tree models, particularly with respect to discounting. This paper gives a comprehensive description of Markov modelling for economic evaluation, including a discussion of the assumptions on which the type of model is based, most notably the memoryless quality of Markov models often termed the 'Markovian assumption'. A hypothetical example of a drug intervention to slow the progression of a chronic disease is employed to demonstrate the modelling technique and the possible methods of analysing Markov models are explored. Analysts should be aware of the limitations of Markov models, particularly the Markovian assumption, although the adept modeller will often find ways around this problem.
7F8D3B50	In recent years there has been an increased interest in the modelling and recognition of human activities involving highly structured and semantically rich behaviour such as dance, aerobics, and sign language. A novel approach is presented for automatically acquiring stochastic models of the high-level structure of an activity without the assumption of any prior knowledge. The process involves temporal segmentation intoplausible atomic behaviour com-ponents and the use of variable length Markov models for the efficient rep-resentation of behaviours. Experimental results are presented which demon-strate the generation of realistic sample behaviours and evaluate the perfor-mance of models for long-term temporal prediction.
7F79AB85	This paper investigates the properties of a method for obtaining Markov models of unspecified order to be applied to narrow-band fading channels with additive white Gaussian noise. The models are obtained by applying the context tree pruning algorithm to experimental or simulated sequences. Fading environments are identified in which the extension from first-order to higher order models is justified. The paper presents, as examples, the evaluation of the covariance function and the packet error distribution.
7C7C2B02	To date, decision trees and Markov models have been the most common methods used in pharmacoeconomic evaluations. Both of these techniques lack the flexibility required to appropriately represent clinical reality. In this paper an alternative, more natural, way to model clinical reality — discrete event simulation — is presented and its application is illustrated with a real world example.A discrete event simulation represents the course of disease very naturally, with few restrictions. Neither mutually exclusive branches nor states are required, nor is a fixed cycle. All relevant aspects can be incorporated explicitly and efficiently. Flexibility in handling perspectives and carrying out sensitivity analyses, including structural variations, is incorporated and the entire model can be presented very transparently. The main limitations are imposed by lack of data to fit realistic models.Discrete event simulation, though rarely employed in pharmacoeconomics today, should be strongly considered when carrying out economic evaluations, particularly those aimed at informing policy makers and at estimating the budget impact of a pharmaceutical intervention.
0A7B0D45	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning, in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is founded on an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a typical Smart Environment – the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.
5D7D351E	Intuitively, any ‘bag of words’ approach in IR should benefit from taking term dependencies into account. Unfortunately, for years the results of exploiting such dependencies have been mixed or inconclusive. To improve the situation, this paper shows how the natural language properties of the target documents can be used to transform and enrich the term dependencies to more useful statistics. This is done in three steps. The term co-occurrence statistics of queries and documents are each represented by a Markov chain. The paper proves that such a chain is ergodic, and therefore its asymptotic behavior is unique, stationary, and independent of the initial state. Next, the stationary distribution is taken to model queries and documents, rather than their initial distributions. Finally, ranking is achieved following the customary language modeling paradigm. The main contribution of this paper is to argue why the asymptotic behavior of the document model is a better representation then just the document’s initial distribution. A secondary contribution is to investigate the practical application of this representation in case the queries become increasingly verbose. In the experiments (based on Lemur’s search engine substrate) the default query model was replaced by the stable distribution of the query. Just modeling the query this way already resulted in significant improvements over a standard language model baseline. The results were on a par or better than more sophisticated algorithms that use fine-tuned parameters or extensive training. Moreover, the more verbose the query, the more effective the approach seems to become.
7A42DD4C	Recently, attention has been focused on spatial databases which combine conventional and spatial data. The need for spatial query languages has been identified in several different application domain as Geographic Information Systems (GISs) and Image Databases. Several extensions to the relational database query language SQL have been proposed to serve as a spatial query language. The large availability on the market place of the relational database technology is the major reason why an SQL-based spatial query language is welcome both from the GIS vendors and the GIS user community. Recent SQL extensions convinced us that it is the right time for working on the standardization of SQL-based spatial query languages. This paper sets a kernel of basic features for the creation of a standard and compares a large number of SQL spatial extensions according to such features.
7559C9A4	This paper proposes an algorithm, called sequence prediction via enhanced episode discovery (SPEED), to predict inhabitant activity in smart homes. SPEED is a variant of the sequence prediction algorithm. It works with the episodes of smart home events that have been extracted based on the on -off states of home appliances. An episode is a set of sequential user activities that periodically occur in smart homes. The extracted episodes are processed and arranged in a finite-order Markov model. A method based on prediction by partial matching (PPM) algorithm is applied to predict the next activity from the previous history. The result shows that SPEED achieves an 88.3% prediction accuracy, which is better than LeZi Update, Active LeZi, IPAM, and C4.5.
7FDEABB6	Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.
7D5544BA	An intelligent home is likely in the near future. A n important ingredient in an intelligent environment such as a home is prediction - of the next action, the next l ocation, and the next task that an inhabitant is likely to p erform. In this paper we describe our approach to solving the problem of predicting inhabitant behavior in a smart home. We model the inhabitant actions as states in a simple Markov model, then improve the model by supplying it with data from discovered high-level inhabitant tasks. For si mulated data we achieved good accuracy, whereas on real dat a we had marginal performance. We also investigate clust ering of actions and subsequently predict the next action an d the task with hidden Markov models created using the clusters.
00BA8AB5	MUSART is a research project developing and studying new techniques for music information retrieval.The MUSART architecture uses a variety of representations to support multiple search modes. Progress is reported on the use of Markov modeling,melodic contour, and phonetic streams for music retrieval.To enable large-scale databases and more advanced searches, musical abstraction is studied. The MME subsystem performs theme extraction, and two other analysis systems are described that discover structure in audio representations of music.Theme extraction and structure analysis promise to improve search quality and support better browsing and “audio thumbnailing.” Integration of these components within a single architecture will enable scientific comparison of different techniques and, ultimately, their use in combination for improved performance and functionality.
797DF42C	This paper proposes a new approach to combined spatial (Intra) prediction and adaptive transform coding in block-based video and image compression. Context-adaptive spatial prediction from available, previously decoded boundaries of the block, is followed by optimal transform coding of the prediction residual. The derivation of both the prediction and the adaptive transform for the prediction error, assumes a separable first-order Gauss-Markov model for the image signal. The resulting optimal transform is shown to be a close relative of the sine transform with phase and frequencies such that basis vectors tend to vanish at known boundaries and maximize energy at unknown boundaries. The overall scheme switches between the above sine-like transform and discrete cosine transform (per direction, horizontal or vertical) depending on the prediction and boundary information. It is implemented within the H.264/AVC intra mode, is shown in experiments to significantly outperform the standard intra mode, and achieve significant reduction of the blocking effect.
799E3C70	This paper presents a software simulator applicable to multipath fading channels in urban environments of mobile communication networks. The simulator is constructed by a two-state Markov model and several statistical models for simulating the characterizations of different environments. A core idea of the simulator is to construct a Rice distribution-based multipath fading module produced by a modified Gans Doppler power spectrum, and in combination with a Markov model to predict the time-dependent characteristics of packet in different radio circumstances. It can simply predict the packet performance of the future channel and evaluate the relations between the radio channel and the modulation schemes, error control protocols and channel coding. Simulation results demonstrate that it is a reliable and efficient method.
80567ABE	Prediction is an important component in a variety of domains in Artificial Intelligence and Machine Learning,in order that Intelligent Systems may make more informed and reliable decisions. Certain domains require that prediction be performed on sequences of events  that can typically be modeled as stochastic processes. This work presents Active LeZi, a sequential prediction algorithm that is  founded  on  an Information Theoretic approach, and is based on the acclaimed LZ78 family of data compression algorithms. The efficacy of this algorithm in a  typical Smart Environment–the Smart Home, is demonstrated by employing this algorithm to predict device usage in the home. The performance of  this algorithm is tested on synthetic data sets that are representative of typical interactions between a Smart Home and the inhabitant.  
762DB3E0	The measurement of individual single-channel events arising from the gating of ion channels provides a detailed data set from which the kinetic mechanism of a channel can be deduced. In many cases, the pattern of dwells in the open and closed states is very complex, and the kinetic mechanism and parameters are not easily determined. Assuming a Markov model for channel kinetics, the probability density function for open and closed time dwells should consist of a sum of decaying exponentials. One method of approaching the kinetic analysis of such a system is to determine the number of exponentials and the corresponding parameters which comprise the open and closed dwell time distributions. These can then be compared to the relaxations predicted from the kinetic model to determine, where possible, the kinetic constants. We report here the use of a linear technique, linear prediction/singular value decomposition, to determine the number of exponentials and the exponential parameters. Using simulated distributions and comparing with standard maximum-likelihood analysis, the singular value decomposition techniques provide advantages in some situations and are a useful adjunct to other single-channel analysis techniques.
75C9738A	The binding of regulatory proteins to their specific DNA targets determines the accurate expression of the neighboring genes. The in silico prediction of new binding sites in completely sequenced genomes is a key aspect in the deeper understanding of gene regulatory networks. Several algorithms have been described to discriminate against false-positives in the prediction of new binding targets; however none of them has been implemented so far to assist the detection of binding sites at the genomic scale.FITBAR (Fast Investigation Tool for Bacterial and Archaeal Regulons) is a web service designed to identify new protein binding sites on fully sequenced prokaryotic genomes. This tool consists in a workbench where the significance of the predictions can be compared using different statistical methods, a feature not found in existing resources. The Local Markov Model and the Compound Importance Sampling algorithms have been implemented to compute the P-value of newly discovered binding sites. In addition, FITBAR provides two optimized genomic scanning algorithms using either log-odds or entropy-weighted position-specific scoring matrices. Other significant features include the production of a detailed genomic context map for each detected binding site and the export of the search results in spreadsheet and portable document formats. FITBAR discovery of a high affinity Escherichia coli NagC binding site was validated experimentally in vitro as well as in vivo and publishedFITBAR was developed in order to allow fast, accurate and statistically robust predictions of prokaryotic regulons. This feature constitutes the main advantage of this web tool over other matrix search programs and does not impair its performance.
5B14A4B4	Proactive User Interfaces (PUIs) aim at facili- tating the interaction with a user interface, e.g., by highlighting fields or adapting the interface. For that purpose, they need to be able to pre- dict the next user action from the interaction his- tory. In this paper, we give an overview of se- quence prediction algorithms (SPAs) that are ap- plied in this domain, and build upon them to de- velop two new algorithms that base on combin- ing different order Markov models. We iden- tify the special requirements that PUIs pose on these algorithms, and evaluate the performance of the SPAs in this regard. For that purpose, we use three datasets with real usage-data and syn- thesize further data with specific characteristics. Our relatively simple yet efficient algorithm FxL performs extremely well in the domain of SPAs which make it a prime candidate for integration in a PUI. To facilitate further research in this field, we provide a Perl library that contains all presented algorithms and tools for the evaluation. 
80B4014C	The SAM-T04 method for predicting protein structures uses a single protocol across the entire range of targets, from comparative modeling to new folds. This protocol is similar to the SAM-T02 protocol used in CASP5, but has improvements in the iterative search for similar sequences in finding and aligning templates, in creating fragment libraries, in generating protein conformations, and in scoring the conformations. The automatic procedure made some improvements over simply selecting an alignment to the highest-scoring template, and human intervention made substantial improvements over the automatic procedure. The main improvements made by human intervention were from adding constraints to build (or retain) beta-sheets and from splitting multidomain proteins into separate domains. The uniform protocol was moderately successful across the entire range of target difficulty, but was somewhat less successful than other approaches in CASP6 on the comparative modeling targets.
7BCB90E0	Augmenting accurate prediction of channel attenuations can be of immense value in improving the quality of signals at high frequency for satellite communication networks. Such prediction of weather related attenuation factors for the impending weather conditions based on the weather data and the Markovian theory are the main object of this paper. The paper also describes an intelligent weather aware control system (IWACS) that is used to employ the predictions made from Markov model to maintain the quality of service (QoS) in channels that are impacted by rain, gaseous, cloud, fog, and scintillation attenuations. Based on that, a three dimensional relationship is proposed among estimated atmospheric attenuations, propagation angle, and predicted rainfall rate at a given location and operational frequency. This novel method of predicting weather characteristics supplies valuable data for mitigation planning, and subsequently for developing an algorithm to iteratively tune the IWACS by adaptively selecting appropriate channel frequency, modulation, coding, propagation angle, transmission power level, and data transmission rate to improve the satellite's system performance. Some simulation results are presented to show the effectiveness of the proposed scheme. 
7E5073A1	Efficient intra prediction is an important aspect of video coding with high compression efficiency. H.264/AVC applies directional prediction from neighboring pixels on an adjustable block size for local decorrelation. In this paper, we present an extended prediction scheme in the context of H.264/AVC that comprises two additional prediction methods exploiting self-similar properties of the encoded texture. A new macroblock type is implemented, allowing for flexible selection of the available prediction methods for sub-partitions of the macroblock. Depending on the content of the encoded video sequence, substantial gains in rate-distortion performance are achieved. The results may indicate directions towards an enhanced intra coding scheme with improved rate-distortion performance.
7EAB152E	Tropical Pacific sea surface temperatures (SSTs) and the accompanying El Niño–Southern Oscillation phenomenon are recognized as significant components of climate behavior. The atmospheric and oceanic processes involved display highly complicated variability over both space and time. Researchers have applied both physically derived modeling and statistical approaches to develop long-lead predictions of tropical Pacific SSTs. The comparative successes of these two approaches are a subject of substantial inquiry and some controversy. Presented in this article is a new procedure for long-lead forecasting of tropical Pacific SST fields that expresses qualitative aspects of scientific paradigms for SST dynamics in a statistical manner. Through this combining of substantial physical understanding and statistical modeling and learning, this procedure acquires considerable predictive skill. Specifically, a Markov model, applied to a low-order (empirical orthogonal function–based) dynamical system of tropical Pacific SST, with stochastic regime transition, is considered. The approach accounts explicitly for uncertainty in the formulation of the model, which leads to realistic error bounds on forecasts. The methodology that makes this possible is hierarchical Bayesian dynamical modeling.
7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
7F48E77D	With the proliferation of social media, consumers  cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers  search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
00B17298	The search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
595640C9	In order to retrieve an item of information on the Web, many search engines have been proposed. They are rarely efficient at the first attempt: the display of results “forces” the user to navigate. In parallel, Web query languages have been developed to avoid these two sequential phases: research then navigation. In this context, the QIRI@D experimental platform, based on the functional programming language SgmlQL, enables both information retrieval and manipulation of distributed semi-structured documents published on a sub-network made up of the sites where QIRI@D is running. It is possible in an unique query to specify criteria to find a document, to filter it, to extract parts of it for building the result. An automatical enrichment of any published document is used to improve the search efficiency.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the documentside (type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.
58CF4DA7	Clustering of data has numerous applications and has been studied extensively. Though most of the algorithms in the literature are sequential, many parallel algorithms have also been designed. In this paper, we present parallel algorithms with better performance than known algorithms. We consider algorithms that work well in the worst case as well as algorithms with good expected performance.
6FA7B132	Clustering of data has numerous applications and has been studied extensively. It is very important in Bioinformatics and data mining. Though many parallel algorithms have been designed, most of algorithms use the CRCW-PRAM or CREW-PRAM models of computing. This paper proposed a parallel EREW deterministic algorithm for hierarchical clustering. Based on algorithms of complete graph and Euclidean minimum spanning tree, the proposed algorithms can cluster n objects with O(p) processors in O(n2/p) time where 1≤ p ≤ nlogn. Performance comparisons show that our algorithm is the first algorithm that is both without memory conflicts and adaptive.
7CE77E1C	Hierarchical agglomerative clustering (HAC) is a clustering method widely used in various disciplines from astronomy to zoology. HAC is useful for discovering hierarchical structure embedded in input data. The cost of executing HAC on large data is typically high, due to the need for maintaining global inter-cluster distance information throughout the execution. To address this issue, we propose a new parallelization scheme for multi-threaded shared-memory machines based on the concept of nearest-neighbor (NN) chains. The proposed multi-threaded algorithm allocates available threads into two groups, one for managing NN chains and the other for updating distance information. In-depth analysis of our approach gives insight into the ideal configuration of threads and theoretical performance bounds. We evaluate our proposed method by testing it with multiple public datasets and comparing its performance with that of several alternatives. In our test, the proposed method completes hierarchical clustering 3.09-51.79 times faster than the alternatives. Our test results also reveal the effects of performance-limiting factors such as starvation in chain growing, overhead incurred from using synchronization locks, and hardware aspects including memory-bandwidth saturation. According to our evaluation, the proposed scheme is effective in improving the HAC algorithm, achieving significant gains over the alternatives in terms of runtime and scalability.
79AFD2A7	We introduce information retrieval strategies which are based on automatic hierarchic clustering of documents. We discuss the evaluation of retrieval strategies and show, using a subset of the Cranfield Aeronautics document collection, that cluster-based retrieval strategies can be devised which are as effective as linear associative retrieval strategies and much more efficient. Finally, we outline how cluster-based retrieval may be extended to large growing document collections and indicate some ways in which the effectiveness of cluster-based retrieval strategies may be improved.
7E45BA76	Two years after the first edition, a new Fingerprint Verification Competition (FVC2002) was organized by the authors, with the aim of determining the state-of-the-art in this challenging pattern recognition application. The experience and the feedback received from FVC2000 allowed the authors to improve the organization of FVC2002 and to capture the attention of a significantly higher number of academic and commercial organizations (33 algorithms were submitted). This paper discusses the FVC2002 database, the test protocol and the main differences between FVC2000 and FVC2002. The algorithm performance evaluation will be presented at the 16/sup th/ ICPR.
7FA45194	In style-constrained classification often there are onlya few samples of each style and class, and the correspondencesbetween styles in the training set and the test setare unknown. To avoid gross misestimates of the classifierparameters it is therefore important to model the patterndistributions accurately. We offer empirical evidence for intuitivelyappealing assumptions, in feature spaces appropriatefor symbolic patterns, for (1) tetrahedral configurationsof class means that suggests linear style-adaptive classification,(2) improved estimates of classification boundariesby taking into account the asymmetric configuration of thepatterns with respect to the directions toward other classes,and (3) pattern-correlated style variability.
759199FF	Fingerprint enhancement is a key issue in fingerprint minutiae extraction due to various image qualities. Most of exist fingerprint enhancement algorithms need to estimate the ridge orient and ridge distance, which are used to design the enhancement filter. However ridge distance estimation is a rather dffIcult task due to the fingerprint image quality and singularity, therefbre, the ridge distance estimate usually fails in those case and leads to enhancement algorithm failed. In this paper, we propose an AM-FM basedfingerprint image enhancement algorithm, which uses a novel Dominate Component Analysis (DGA,) technique to estimate the dominate component, and uses band pass filter to enhance those component rather than directly estimate the ridge distance, ridge orientation. Experiment results show that our enhancement algorithm leads to signflcant image quality improvement as well as the system efficient. 
7E7B912E	Fingerprint identification is based on two basic premises: (1) persistence and (2) individuality. We address the problem of fingerprint individuality by quantifying the amount of information available in minutiae features to establish a correspondence between two fingerprint images. We derive an expression which estimates the probability of a false correspondence between minutiae-based representations from two arbitrary fingerprints belonging to different fingers. Our results show that (1) contrary to the popular belief, fingerprint matching is not infallible and leads to some false associations, (2) while there is an overwhelming amount of discriminatory information present in the fingerprints, the strength of the evidence degrades drastically with noise in the sensed fingerprint images, (3) the performance of the state-of-the-art automatic fingerprint matchers is not even close to the theoretical limit, and (4) because automatic fingerprint verification systems based on minutia use only a part of the discriminatory information present in the fingerprints, it may be desirable to explore additional complementary representations of fingerprints for automatic matching.
80EC24E2	As a global feature of fingerprints, the orientation field is very important for automatic fingerprint recognition. Many algorithms have been proposed for orientation field estimation, but their results are unsatisfactory, especially for poor quality fingerprint images. In this paper, a model-based method for the computation of orientation field is proposed. First a combination model is established for the representation of the orientation field by considering its smoothness except for several singular points, in which a polynomial model is used to describe the orientation field globally and a point-charge model is taken to improve the accuracy locally at each singular point. When the coarse field is computed by using the gradient-based algorithm, a further result can be gained by using the model for a weighted approximation. Due to the global approximation, this model-based orientation field estimation algorithm has a robust performance on different fingerprint images. A further experiment shows that the performance of a whole fingerprint recognition system can be improved by applying this algorithm instead of previous orientation estimation methods.
77610135	Algorithms are identified which are best suited for an automatic fingerprint recognition system operating on low quality images. New preprocessing algorithms for noise removal and binarization are described. Three approaches to classification are investigated: a correlation classifier, and two feature-based classification schemes. The best results on a database of 80 fingerprints are obtained with spatial-frequency features. Three classifiers (neural net, linear classifier and nearest neighbour) using these features are successful in identifying an independent test set. Details of the results are shown. In conclusion suggestions are made concerning the most suitable algorithms in each of the processing steps.
77FB0890	Fingerprint matching is the most important part in the field of fingerprint recognition. In this paper, a novel fingerprint matching algorithm based on the probabilistic graphical model and 3-tree model is proposed. First, minutiae matching problems are considered as a special point-set matching. Fingerprint minutiae are viewed as random variables. Each minutia pairs have some probability to be matched. Second, an algorithm is proposed to generate the graphical model and choose "signal points", which dynamically have corresponding points in other point set. We choose three base minutiae pairs as signal pairs. Third, the model is converted into a Junction Tree. A 3-tree model is built and the potentials of other minutiae pairs are calculated through Junction Tree (J.T.) algorithm. Then we translate the matching problem into the best matching problem of a weighted bipartite graph. Finally, the number of common matching pairs can be got through maximum flow algorithm. The similarity of two fingerprints is evaluated using the number of common matching pairs and the maximal posteriori probability. In order to deal with part-matching problems, we use the smallest convex hull which contains all the matched minutiae. Experiments evaluated on FVC 2004 show both effectiveness and efficiency of our methods.
855DC306	Fingerprint recognition has found a reliable application for verification or identification of people in biometrics. Globally, fingerprints can be viewed as valuable traits due to several perceptions observed by the experts; such as the distinctiveness and the permanence on humans and the performance in real applications. Among the main stages of fingerprint recognition, the automated matching phase has received much attention from the early years up to nowadays. This paper is devoted to review and categorize the vast number of fingerprint matching methods proposed in the specialized literature. In particular, we focus on local minutiae-based matching algorithms, which provide good performance with an excellent trade-off between efficacy and efficiency. We identify the main properties and differences of existing methods. Then, we include an experimental evaluation involving the most representative local minutiae-based matching models in both verification and evaluation tasks. The results obtained will be discussed in detail, supporting the description of future directions.
811D3336	Since there are many possible types of nonlinear charge-flux constitutive relations of ideal memristors, such elements can manifest various behavior, and the identification of typical memristor fingerprints from the measured data or simulator outputs can be difficult in some cases. The aim of this paper is to reveal several fingerprints of ideal memristors, which extend the repertoire of hitherto published and currently well-known memristor fingerprints. These results can be useful for a clear and fast identification of a system behavior that violates the principles of the operation of ideal memristors.
7E655DA1	In automatic fingerprint identification systems poor quality impressions and latents often require expert interpretation if usable encodings are to be obtained. When this is necessary there are basically two different approaches taken for data entry "direct" encoding of the minutiae and "tracing" the ridges. The minutiae identified using these two methods on a test set of 49 latents were compared. The results indicate that these two methods are not just alternative ways of entering information but are radically different approaches which often lead to substantially different interpretations of the impression. The results of running searches on the two sets of minutiae using the Home Office matching algorithm do not indicate that either method is to be preferred from the point of view of accuracy. This conclusion is reinforced when the encodings are compared directly by eye with the matching impressions. Interpreting and encoding an impression viewed in isolation, by whatever method, is a fundamentally different task to the iterative process used by fingerprint experts to determining a match. There is scope for improving the matching accuracy of automatic fingerprint systems. The view is presented that a significant improvement could be achieved if algorithms are developed which are capable of iterative interpretation of the encoded or original data.
78D2A528	Accurate fingerprint orientation is a prerequisite in fingerprint based recognition system. This paper proposes an algorithm for modeling the fingerprint orientation field by using a model based algorithm based on the weighted Legendre basis. Weights required in the modeling are obtained by using symmetric filters, such that: i) high weights should be assigned to the areas near singular points, ii) areas having uniform ridge-valley flow should be given high weights, and iii) areas containing bad quality due to dry/ wet fingerprints, scars, bruises or sensor condition should be given low weights. These conditions ensure accurate reconstruction of fingerprint orientation field for bad quality areas while preserving the true orientation field near singular points. The proposed algorithm has been evaluated on a publicly available database, FVC2004 DB1A. Experimental results reveal that it has better orientation field estimation compared to the various state of the art algorithms.
80AAD859	We study randomized fingerprinting codes that achieve the fundamental capacity limits subject to the so-called Boneh-Shaw marking assumption. Two decoding schemes are studied in particular: the joint decoder is capacity-achieving but computationally intense, while the simple decoder is suboptimal but efficient. We provide tight bounds as well as numerical results for capacities and study the difference between these two schemes. Finally, security strategies for both the fingerprint embedders and the collusive attackers are presented.
80F2D5E3	In this paper, we studied the algorithms of fingerprint image enhancement and put forward a new method to calculate the direction image. Firstly, Radon Transform is used to estimate the direction image, then, the image is filtered by a set directional filters, thus, we get the enhanced image. The experiment results indicate that this method is simple and efficient; which established the firmed foundation to fingerprint image matching.
7A57F0D6	Several years ago the Federal Bureau of Investigation initiated the development of a reader to extract detailed information from single fingerprints. The fine details or minutiae consist of the position and angle of ridge endings and bifurcations, A free standing laboratory reader now exists which accepts an inked fingerprint at its input and outputs this minutia data.
7E9C0F63	In local area network (LAN), the large-scaled peer-to-peer (P2P) application has aroused our attention by its convenience as well as the problems inborn. The network traffic and the resources consumed have severely influenced the usage of other network application, and this makes the study of identification on P2P traffic much more significant. This article advances an identification method based on the host process fingerprint, which can be done on the client computer with P2P application on. Experiments show that method has a high classification capability on the P2P traffic in LAN.
7CC2B662	Fingerprint recognition using a joint transform correlator (JTC) is the most well-known technology among optical fingerprint recognition methods. The JTC method optically compares the reference fingerprint image with the sample fingerprint image then examines match or non-match by acquiring a correlation peak. In contrast to the JTC method, this paper presents a new method to examine fingerprint recognition by producing a computer generated hologram (CGH) of those two fingerprint images and directly comparing them. As a result, we present some parameters to show that fingerprint recognition capability of the CGH direct comparison method is superior to that of the JTC method
7FE858F6	Orientation fields can be used to describe interleaved ridge and valley patterns of fingerprint image, providing features useful for fingerprint recognition. However, for tasks such as fingerprint indexing, additional image alignment is often required to avoid confounding effects caused by pose differences. In this paper, we propose to employ a set of polar complex moments (PCMs) for extraction of rotation invariant fingerprint representation. PCMs are capable of describing fingerprint ridge flow structures, including singular regions, and are tolerant to spurious orientations in noisy fingerprints. From the orientation fields, a set of rotation moment invariants are derived to form a feature vector for comprehensive fingerprint structural description. This feature vector gives a compact and rotation invariant representation that is important for pose-robust fingerprint indexing. A clustering-based fingerprint indexing scheme is employed to facilitate efficient and effective retrieval of the most likely candidates from a fingerprint database. Our experimental results on NIST and FVC fingerprint databases indicate that the proposed invariant representation improves the performance of fingerprint indexing as compared to state-of-the-art methods.
81255706	The uniqueness and randomness of Physical Unclonable Functions (PUFs) allow secret keys to be stored in a tamper-proof package. In this paper, we propose a design for using a 65nm 10T sub-threshold Static Random Access Memory (SRAM) as a PUF. For this PUF, the challenge is the sub-threshold supply voltage and the response is the fingerprint obtained from the initial values of the cells when powered-up. In this design, we achieve significant improvements in power consumption and security over existing designs which makes it promising for low-power security applications.
7E0D9EF7	According to the characteristics of fingerprint images, combined with the human visual properties, this paper presents a de-noising algorithm of fingerprint image processing to decrease the influence of noises. Based on the different characteristics between the noise points and image features, the algorithm decreased the noises of the source images and filtered the corresponding noise coefficients after the wavelet decomposition. As for the coefficient matrices following the wavelet decomposition, after calculating all local gradients of the coefficients, we established the algorithm by using the local gradients of the source images as the judgment basis, and chose the highest Gradient coefficient in the different directions of the source images as the final fusion coefficient. The experimental results indicate that the algorithm can promote the SNR of the source images, protect the details of the images and improve the visual effects.
7812615D	We describe the design and implementation of an online fingerprint verification system which operates in two stages: (i) minutia extraction and (ii) minutia matching. An improved minutia extraction algorithm that is much faster and more accurate than our earlier algorithm has been implemented. For minutia matching, an alignment-based elastic matching algorithm has been developed. This algorithm is capable of finding the correspondences between input minutiae and the stored template without resorting to exhaustive search and has the ability to adaptively compensate for the nonlinear deformations and inexact pose transformations between finger prints. The system has been tested on two sets of finger print images captured with inkless scanners. The verification accuracy is found to be over 99% with a 15% reject rate. Typically, a complete fingerprint verification procedure takes, on an average, about 8 seconds on a SPARC 20 workstation. It meets the response time requirements of on-line verification with high accuracy.
3013FD47	Nowadays, there is a great bunch of applications implementing authentication processes in order to protect user access and users' data. Accessing our mobile device, doing a bank transfer or getting engaged with e-commerce are examples of how the great majority of population is in touch with an authentication process. The most common system to log in web applications is the use of pairs (username, password). It could be said that is rather contended but at the same time could imply problems, normally related to the strength or weakness of the used passwords. On the one hand, if strong passwords are used, users tend to forget them. On the other hand, if weak passwords are used, a non-legitimate user could impersonate a legitimate account attempting against the user's privacy. Bearing this in mind and taking into account that biometrics systems are securer and by far more user-friendly, in this TFG we propose an implementation of a web authentication process based on users' fingerprints. The proposed system is an externalized and standardized solution and, therefore, webApps wishing to use it could be easily integrated. For demonstration purposes, we have implemented three different entities: first, an authentication and authorization server, the authServer; second, a mobile application which stands for the authClient; and finally third, a dummy web application, the webApp. Being the webApp the application that the client will see in its device, the registry and authentication process will be performed by the authClient and authServer in a barely imperceptive way for the user.
7FDE1A1F	Proposes a fingerprint minutia matching technique, which matches the fingerprint minutiae by using both the local and global structures of minutiae. The local structure of a minutia describes a rotation and translation invariant feature of the minutia in its neighborhood. It is used to find the correspondence of two minutiae sets and increase the reliability of the global matching. The global structure of minutiae reliably determines the uniqueness of fingerprint. Therefore, the local and global structures of minutiae together provide a solid basis for reliable and robust minutiae matching. The proposed minutiae matching scheme is suitable for an online processing due to its high processing speed. Experimental results show the performance of the proposed technique.
7576667A	The estimation of fingerprint ridge orientation is an essential step in every automatic fingerprint verification system. The importance of ridge orientation can be deflected from the fact that it is inevitably used for detecting, describing and matching fingerprint features such as minutiae and singular points. In this paper we propose a novel method for fingerprint ridge orientation modelling using Legendre polynomials. One of the main problems it addresses is smoothing orientation data while preserving details in high curvature areas, especially singular points. We show that singular points, which result in a discontinuous orientation field, can be modelled by the zero-poles of Legendre polynomials. The models parameters are obtained in a two staged optimization procedure. Another advantage of the proposed method is a very compact representation of the orientation field, using only 56 coefficients. We have carried out extensive experiments using a state-of-the-art fingerprint matcher and a singular point detector. Moreover, we compared the proposed method with other state-of-the-art fingerprint orientation estimation algorithms. We can report significant improvements in both singular point detection and matching rates.
7213B07D	In this study, two techniques that can improve the authentication process are examined: (i) multiple samples and (ii) multiple biometric sources. We propose the fusion of multiple samples obtained from multiple biometric sources at the score level. By using the average operator, both the theoretical and empirical results show that integrating as many samples and as many biometric sources as possible can improve the overall reliability of the system. This strategy is called the multi-sample multi-source approach. This strategy was tested on a real-life database using neural networks trained in one-versus-all configuration.
812C84C8	Gait is a potential behavioral feature, and many allied studies have demonstrated that it can be served as a useful biometric feature for recognition. This paper described a novel gait recognition technique based on support vector machine fusion of contour projection and skeleton model features. A principal component analysis method was used to lower the dimension of contour projection after segmenting silhouettes from the background in the key frame of gait picture sequence and a skeleton model was built to produce other shape features. The combining features were fused by a support vector machine and tested on the CASIA database at the feature level and decision level based on posterior probability. Experimental results have demonstrated the effectiveness and advantages of the proposed algorithm.
7F20C72C	Modern fingerprint image compression and reconstruction standards used by the US Federal Bureau of Investigation (FBI) are based upon the popular 9/7 discrete wavelet transform. Multiresolution analysis tools have been successfully applied for fingerprint image compression for more than a decade; we propose a novel fingerprint image compression technique based on recently proposed wave atoms decomposition. Wave atoms decomposition has specifically been designed for enhanced representation of oscillatory patterns to convey temporal and spatial information. Our proposed compression scheme is based upon linear vector quantization of decomposed wave atoms representation of fingerprint images. Later quantized information is encoded with arithmetic entropy scheme. The proposed image compression standard outperforms the FBI fingerprint image compression standard, the wavelet scalar quantization (WSQ). Data mining, law enforcement, border security, and forensic applications can potentially benefit from our proposed compression scheme.
7BFF86A8	Noting the advantages of texture-based features over the structural descriptors of vascular trees, we investigated texture-based features from gray level cooccurrence matrix (GLCM) and various wavelet packet energies to classify retinal vasculature for biometric identification. Wavelet packet energy features were generated by Daubechies, Coiflets and Reverse Biorthogonal wavelets. Two different entropy methods, Shannon and logarithm of energy, were used to prune wavelet packet decomposition trees. Next, wrapper methods were used for classification-guided feature selection. Features were ranked based on area under the receiver operating curves, Bhattacharya, and t-test metrics. Using the ranked lists, wrapper methods were used in conjunction with Naïve Bayesian, k-nearest neighbor (k-NN), and Support Vector Machine (SVM) classifiers. Best results were achieved by using features from Reverse Biorthogonal 2.4 wavelet packet decomposition in conjunction with a nearest neighbor classifier, yielding a 3-fold cross validation accuracy of 99.42% with a sensitivity and specificity of 98.33% and 99.47% respectively
76A7D81C	With the development of business activities, the property rights protection for digital content becomes a hot topic. In prior researches, digital fingerprinting techniques are widely used. They find the illegal distributors by traitor tracing techniques. But in the emerging digital wholesale and retail, it is possible that middlemen collude. Then it becomes a development tendency that the fingerprint is added with the group property which improves the group detection performance. Considering the largely existed COX fingerprint which has low computation cost and easy realization, it will save large resources that we classify them to generate the group fingerprint. We utilize four algorithms (k-means, hierarchical clustering, SOM, FCM) to construct the COX fingerprint classifier through which the group fingerprint generating and group traitor tracing algorithms are implemented. The performance of the group fingerprint on the practicability and security are obtained by the colluding attack and multimedia processing experiments. The experiment results show that it is easy to implement the group fingerprinting schemes by the four algorithms. All the classifiers based group fingerprinting schemes withstand the JPEG compressions. The k-means scheme has superior performance than the other three. Even in the averaging attack experiments, in which the other three got the worst performance, k-means obtains acceptable performance.
58596363	A new biometric indicator based on the patterns of conjunctival vasculature is proposed. Conjunctival vessels can be observed on the visible part of the sclera that is exposed to the outside world.These vessels demonstrate rich and specific details invisible light, and can be easily photographed using a regular digital camera. In this paper we discuss methods for conjunctival imaging, preprocessing, and feature extraction in order to derive a suitable conjunctival vascular template for biometric authentication. Commensurate classification methods along  with the observed accuracy are discussed. Experimental results suggest the potential of using conjunctival vasculature as a biometric measure. 
77F03DFE	Fingerprint classification is crucial to reduce the processing time in a large-scale database. In this paper a fingerprint classification based on continuous orientation field and singular points is proposed. The continuous orientation field can not only filter the noises in point directional image,but also represent the basic structural feature of fingerprint more precisely.Singularities are the most important and reliable feature in classification.The reliable and fast classification algorithm is made possible by a simple but effective combination of continuous orientation field and the modified Poincare index in the determination of singular points.The experiment results show the effectiveness of the proposed method in producing good classification result.
7C59D1E3	Biometric systems based on face recognition have been shown unreliable under the presence of face-spoofing images. Hence, automatic solutions for spoofing detection became necessary. In this paper, face-spoofing detection is proposed by searching for Moiré patterns due to the overlap of the digital grids. The conditions under which these patterns arise are first described, and their detection is proposed which is based on peak detection in the frequency domain. Experimental results for the algorithm are presented for an image database of facial shots under several conditions.
7D386E49	In this paper, a fingerprint embedding method better-suited for the AND anti-collusion code (AND-ACC) is proposed. The proposed method embeds both a code and an orthogonal fingerprint using different basis vectors depending on the bit. Although the detection for the embedding method is complex, the performance of the fingerprinting system using proposed embedding method with the AND-ACC against average attack is improved compared with the AND-ACC fingerprinting scheme using code modulation embedding method. The system using the proposed embedding method is robust against the linear combination collusion attack (LCCA) whereas the system using the code modulation is not.
7766147C	Fingerprint recognition has found a reliable application for verification or identification of people in biometrics. Globally, fingerprints can be viewed as valuable traits due to several perceptions observed by the experts; such as the distinctiveness and the permanence on humans and the performance in real applications. Among the main stages of fingerprint recognition, the automated matching phase has received much attention from the early years up to nowadays. This paper is devoted to review and categorize the vast number of fingerprint matching methods proposed in the specialized literature. In particular, we focus on local minutiae-based matching algorithms, which provide good performance with an excellent trade-off between efficacy and efficiency. We identify the main properties and differences of existing methods. Then, we include an experimental evaluation involving the most representative local minutiae-based matching models in both verification and evaluation tasks. The results obtained will be discussed in detail, supporting the description of future directions.
7E3E0BA5	Fingerprint verification is an important biometric technique for personal identification. We describe the design and implementation of a prototype automatic identity-authentication system that uses fingerprints to authenticate the identity of an individual. We have developed an improved minutiae-extraction algorithm that is faster and more accurate than our earlier algorithm (1995). An alignment-based minutiae-matching algorithm has been proposed. This algorithm is capable of finding the correspondences between input minutiae and the stored template without resorting to exhaustive search and has the ability to compensate adaptively for the nonlinear deformations and inexact transformations between an input and a template. To establish an objective assessment of our system, both the Michigan State University and the National Institute of Standards and Technology NIST 9 fingerprint data bases have been used to estimate the performance numbers. The experimental results reveal that our system can achieve a good performance on these data bases. We also have demonstrated that our system satisfies the response-time requirement. A complete authentication procedure, on average, takes about 1.4 seconds on a Sun ULTRA I workstation (it is expected to run as fast or faster on a 200 HMz Pentium).
806C38BC	Most automatic systems for fingerprint comparison are based on minutiae matching. Minutiae are essentially terminations and bifurcations of the ridge lines that constitute a fingerprint pattern. Automatic minutiae detection is an extremely critical process, especially in low-quality fingerprints where noise and contrast deficiency can originate pixel configurations similar to minutiae or hide real minutiae. Several approaches have been proposed in the literature; although rather different from each other, all these methods transform fingerprint images into binary images. In this work we propose an original technique, based on ridge line following, where the minutiae are extracted directly from gray scale images. The results achieved are compared with those obtained through some methods based on image binarization. In spite of a greater conceptual complexity, the method proposed performs better both in terms of efficiency and robustness.
7FB3ABDE	Reliable and accurate fingerprint recognition is a challenging pattern recognition problem, requiring algorithms robust in many contexts. FVC2000 competition attempted to establish the first common benchmark, allowing companies and academic institutions to unambiguously compare performance and track improvements in their fingerprint recognition algorithms. Three databases were created using different state-of-the-art sensors and a fourth database was artificially generated; 11 algorithms were extensively tested on the four data sets. We believe that FVC2000 protocol, databases, and results will be useful to all practitioners in the field not only as a benchmark for improving methods, but also for enabling an unbiased evaluation of algorithms.
7DB6CB3D	Biometrics-based verification, especially fingerprint-based identification, is receiving a lot of attention. There are two major shortcomings of the traditional approaches to fingerprint representation. For a considerable fraction of population, the representations based on explicit detection of complete ridge structures in the fingerprint are difficult to extract automatically. The widely used minutiae-based representation does not utilize a significant component of the rich discriminatory information available in the fingerprints. Local ridge structures cannot be completely characterized by minutiae. Further, minutiae-based matching has difficulty in quickly matching two fingerprint images containing a different number of unregistered minutiae points. The proposed filter-based algorithm uses a bank of Gabor filters to capture both local and global details in a fingerprint as a compact fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the two corresponding FingerCodes and hence is extremely fast. We are able to achieve a verification accuracy which is only marginally inferior to the best results of minutiae-based algorithms published in the open literature. Our system performs better than a state-of-the-art minutiae-based system when the performance requirement of the application system does not demand a very low false acceptance rate. Finally, we show that the matching performance can be improved by combining the decisions of the matchers based on complementary (minutiae-based and filter-based) fingerprint information.
762F072E	A fingerprint classification procedure using a computer is described. It classifies the prints into one of ten defined types. The procedure is implemented using PICAP (picture array processor). The picture processing system includes a TV camera input and a special picture processor. The first part of the procedure is a transformation of the original print to a sampling matrix, where the dominant direction of the ridges for each subpicture is indicated. After smoothing, the lines in this pattern are traced out and converted to strings of symbols. Finally, a syntactic approach is adopted to make the type classification based on this string of symbols.
7B0AA26C	In this paper, we present a device to be a memristor should exhibit three characteristic fingerprints: (a) Pinched hysteresis loop in the voltage v vs. current i plane, namely, the v-i loci always passes through origin for any bipolar periodic input voltage v(t). (b) Beyond a certain critical frequency ω*, the area of the pinched hysteresis lobe decreases monotonically as the frequency of the periodic input signal increases and (c) the shape of the pinched hysteresis loop varies with frequency f and shrinks to a single-valued function through the origin, as the frequency tends to infinity. Examples of memristors exhibiting these three fingerprints are presented.
7AD7D628	A novel method of fault diagnosis is presented based on power grid flow fingerprint identification technology and synchronized phasor measurements. In this paper, the definitions of power grid flow fingerprint and flow fingerprint characteristic point are introduced at first. Then, the proposed method is achieved utilizing the flow fingerprint identification scheme. Meanwhile, two evaluation indexes are developed when the online matching is made between the simulated flow fingerprint and real-time flow fingerprint collected by PMUs. The method also employs improved integer programming technique for optimal PMU placement. The proposed approach is effective to eliminate uncertainty and obtain accurate diagnosis results using less data based on a systematic view. Especially, the application of the proposed method is helpful and beneficial to identify initial fault and accurately capture the nature of cascading failures. Simulation results have proved the validity of this method.
7C22F707	 The joint fingerprinting and decryption (JFD) framework has high efficiency and can provide comprehensive, effective content security protection for remote sensing images. However, the fingerprint is generated by random partial decryption in JFD, which will lead to obvious degradation of image quality after embedding fingerprints and will severely impact the precision and further application of remote sensing images. In order to solve this problem, an improved JFD scheme based on neighborhood similarity is proposed in this paper, where the image is partitioned into different regions, and the regions that affect future application most are excluded from the fingerprint embedding area. Moreover, neighborhood similarity is defined in order to evaluate the change of pixel correlation, and areas with good neighborhood similarity after encryption are chosen to embed fingerprints. The experimental results prove that the fingerprinted image has a good image quality, and it will not affect the following application such as edge extraction and unsupervised classification, etc. Therefore, it is a suitable content security protection method for remote sensing image.
7544EE34	The minimum spanning tree (MST) matching algorithm has been used for searching the partial matching points extracted from fingerprint images. The method, however, has some limitations. To obtain the relationship between the enrolled fingerprints and the input fingerprints the MST is used to generate the tree graph that represents the unique graph for the given minutiae. From the graph, the matching points are estimated. However, the shape of the graph highly depends on the positions of the minutiae. If there are some pseudo minutiae caused by noise, the shape of the graph will be totally different. To overcome the limitations of the MST, we propose the center of rotation method (CRM) that finds true partial matching points. The proposed method is based on the assumption that the input fingerprint minutiae are rigid body. In addition, we employ the polygon wrapping the minutiae for fast matching speed and reliable matching results. In conclusion we have been able to confirm fast performance and high identification ratio by using CRM. (C) 2004 Society of Photo-Optical Instrumentation Engineers.
7AA47C57	Sweat pores and other level 3 features have been proven to provide more discriminatory information about fingerprint characteristics, which is useful for personal identification especially in law enforcement applications. With the advent of high resolution (≥1000 ppi) fingerprint scanning equipment, sweat pores are attracting increasing attention in automatic fingerprint identification system (AFIS), where the extraction of pores is a critical step. This paper presents a scale parameter-estimating method in filtering-based pore extraction procedure. Pores are manually extracted from a 1000 ppi grey-level fingerprint image. The size and orientation of each detected pore are extracted together with local ridge width and orientation. The quantitative relation between the pore parameters (size and orientation) and local image parameters (ridge width and orientation) is statistically obtained. The pores are extracted by filtering fingerprint image with the new pore model, whose parameters are determined by local image parameters and the statistically established relation. Experiments conducted on high resolution fingerprints indicate that the new pore model gives good performance in pore extraction. 
7CA842F6	Biometric based characteristic authentication is an asymmetric authentication technology. This means that the reference biometric data generated during the enrolment process and stored in the biometric database, will never match any freshly offered biometric data exactly (100%). This is commonly accepted due to the nature of the biometric algorithm central to the biometric environment.A password or pin on the other hand, is a symmetric authentication mechanism. This means that an exact match is expected, and if the offered password deviates ever so slightly from the password stored in the password database file, authenticity is rejected.Encryption technologies rely on symmetric authentication to function, as the password or pin is often used as the seed for a random number that will assist in the generation of the cipher. If the password used to encrypt the cipher is not 100% the same as the password supplied to decrypt, the cipher will not unlock.The asymmetric nature of biometrics traditionally renders biometric data unfit to be used as the secret key for an encryption algorithm.This paper introduces a system that allows biometric data to be used as the secret key in an encryption algorithm. This method relies on the BioVault infrastructure. For this reason BioVault will briefly be discussed, followed by a discussion of biometrically based encryption.
7CEAF6EF	In this paper, we have proposed a fingerprint orientation model based on 2D Fourier expansions (FOMFE) in the phase plane. The FOMFE does not require prior knowledge of singular points (SPs). It is able to describe the overall ridge topology seamlessly, including the SP regions, even for noisy fingerprints. Our statistical experiments on a public database show that the proposed FOMFE can significantly improve the accuracy of fingerprint feature extraction and thus that of fingerprint matching. Moreover, the FOMFE has a low-computational cost and can work very efficiently on large fingerprint databases. The FOMFE provides a comprehensive description for orientation features, which has enabled its beneficial use in feature-related applications such as fingerprint indexing. Unlike most indexing schemes using raw orientation data, we exploit FOMFE model coefficients to generate the feature vector. Our indexing experiments show remarkable results using different fingerprint databases
7F0B8455	This paper proposes a new hash-based indexing method to speed up fingerprint identification in large databases. A Locality-Sensitive Hashing (LSH) scheme has been designed relying on Minutiae Cylinder-Code (MCC), which proved to be very effective in mapping a minutiae-based representation (position/angle only) into a set of fixed-length transformation-invariant binary vectors. A novel search algorithm has been designed thanks to the derivation of a numerical approximation for the similarity between MCC vectors. Extensive experimentations have been carried out to compare the proposed approach against 15 existing methods over all the benchmarks typically used for fingerprint indexing. In spite of the smaller set of features used (top performing methods usually combine more features), the new approach outperforms existing ones in almost all of the cases.
7BB6A54C	We have reported the Charge Coupled Device (CCD) fingerprint method for identification of digital still cameras. The CCD fingerprint method utilizes the nonhomogeneous nature of dark currents in CCDs. In this study, we have measured CCD defects patterns of various digital still cameras including professional cameras and cheap ones with various resolution and compression rates. As a result, CCD defect pattern was detected in all cameras except for a low-resolution cheap camera using only one image. Resolution mode change of digital cameras did not affect the position of defect points in general but in some cases, relative pixel intensity varied. Image compression did not affect the pixel position for blank images within normal compression rate, but when there existed light in the background, the pixel position was blurred as the compression rate became high. In conclusion, it is recognized that the CCD fingerprint method can be applied in principle to digital still cameras, that is, individual camera identification can be achieved in principle by using images taken with the camera. 
7A8AB444	Automated fingerprint recognition has received considerable attention over the past decade. Progress has been made on models of the structure of fingerprints, techniques for the acquisition of prints, and the development of commercial automated fingerprint recognition systems. Despite these advances, there remain considerable opportunities for improvement. The speed of retrieval, and the ability to recognize partial or distorted prints are prominent among those areas that require improvement. This study will describe a structural model of fingerprints, based on local structural relations among features, and an associated automated recognition system which addresses the limitations of existing fingerprint models.
7B2093D3	In this paper, the advantages and disadvantages of the gray variance directional image segmentation methods were analyzed. On the base of gray normalization, a new approach to segment the fingerprint images was proposed using the normal distribution model of the fingerprint image gray’s character. This approach makes use of the gray characters of the local and global images. Meanwhile, it has the advantage to choose the threshold automatic without the experience. The experimental results show that the method for the fingerprint image segmentation is effective, adaptive and quick and it can meet the need of fingerprint recognition system.
786AD364	This paper presents a novel method for fingerprint orientation modeling, which executes in two phases. Firstly, the orientation field is reconstructed using a lower order Legendre polynomial to capture the global orientation pattern in the fingerprint structure. Then the preliminary model around the region with presence of fingerprint singularities is dynamically refined using a higher order Legendre polynomial. The singular region is automatically detected through the analysis on the orientation residual field between the original orientation field and the orientation model. The method does not require any prior knowledge on the fingerprint structure. To validate the performance, the method has been applied to fingerprint image enhancement, fingerprint singularity detection and fingerprint recognition using the FVC 2004 data sets. Compared with the recently published Legendre polynomial model, the proposed method attains higher accuracy in fingerprint singularity detection, lower error rates in fingerprint matching.
7BEFDEB7	Fingerprint matching is a key issue in research of an automatic fingerprint identification system. On the basis of triangulation in computational geometry, we develop a kind of method for fingerprint matching based on Delaunay Triangulation net in this paper. Through carrying on Delaunay Triangulation to the topological structure of fingerprint minutiae, minutiae with closer distance link to each other on the space according to the Delaunay criterion and form the Delaunay Triangulation net. Then look for some reference minutiae pairs correctly from the net. According to the reference minutiae pairs, match fingerprint on point pattern. The experimental results on FVC2000 indicate the validity of algorithm.
7DBF4AA0	This correspondence proposes new candidate list reduction criteria for fingerprint indexing approaches. The basic idea is that, given a query fingerprint, the initial set of scores produced by an indexer could contain useful information to reduce the candidate list. Novel reduction criteria have been proposed, and extensive experiments have been carried out over five publicly available benchmarks, using two state-of-the-art fingerprint indexing techniques. Although quite simple, the proposed criteria achieved remarkable results, allowing a substantial reduction of the candidate list: for instance, at 1% error rate, the average penetration rate of a state-of-the-art minutiae-based indexer decreases from 27% to 3.9% on FVC2000 DB2. The new reduction criteria are applicable to any indexing approach, since they only require a list of scores as input.
8055257B	This paper introduces a novel algorithm based on global comprehensive similarity with three steps. To describe the Euclidean space-based relative features among minutiae, we first build a minutia-simplex that contains a pair of minutiae as well as their associated textures, with its transformation-variant and invariant relative features employed for the comprehensive similarity measurement and parameter estimation, respectively. By the second step, we use the ridge-based nearest neighborhood among minutiae to represent the ridge-based relative features among minutiae. With these ridge-based relative features, minutiae are grouped according to their affinity with a ridge. The Euclidean space-based and ridge-based relative features among minutiae reinforce each other in the representation of a fingerprint. Finally, we model the relationship between transformation and the comprehensive similarity between two fingerprints in terms of histogram for initial parameter estimation. Through these steps, our experiment shows that the method mentioned above is both effective and suitable for limited memory AFIS owing to its less than 1k byte template size.
816C3258	Biometric identification is an emerging subject in applications like high-security wireless access and secure transactions across computer networks. Fingerprints are easy to use and provide relatively good performance. Furthermore,fingerprint sensors are cheap and can be integrated easily in wireless hardware.In this paper, methods are presented for the estimation of a high resolution directional field from fingerprints. It is shown how, from the directional field, very accurate detection of the singular points and the orientations of those points can be obtained. These estimates can for instance be used for accurate registration (alignment) of two fingerprints in a fingerprint verification system.
7FCCCBB3	The first subject of the paper is the estimation of a high resolution directional field of fingerprints. Traditional methods are discussed and a method, based on principal component analysis, is proposed. The method not only computes the direction in any pixel location, but its coherence as well. It is proven that this method provides exactly the same results as the "averaged square-gradient method" that is known from literature. Undoubtedly, the existence of a completely different equivalent solution increases the insight into the problem's nature. The second subject of the paper is singular point detection. A very efficient algorithm is proposed that extracts singular points from the high-resolution directional field. The algorithm is based on the Poincare index and provides a consistent binary decision that is not based on postprocessing steps like applying a threshold on a continuous resemblance measure for singular points. Furthermore, a method is presented to estimate the orientation of the extracted singular points. The accuracy of the methods is illustrated by experiments on a live-scanned fingerprint database.
8103D7BE	This paper addresses the design of additive fingerprints that are maximally resilient against Gaussian averaging collusion attacks. The detector performs a binary hypothesis test in order to decide whether a user of interest is among the colluders. The encoder (fingerprint designer) is to imbed additive fingerprints that minimize the probability of error of the test. Both the encoder and the attackers are subject to squared-error distortion constraints. We show that n-simplex fingerprints are optimal in sense of maximizing a geometric figure of merit for the detection test; these fingerprints outperform orthogonal fingerprints. They are also optimal in terms of maximizing the error exponent of the detection test, and maximizing the deflection criteria at the detector when the attacker’s noise is non-Gaussian. Reliable detection is guaranteed provided that the number of colluders K≪N−−√, where N is the length of the host vector.
785052E1	Orientation field represents the topological structure of the interleaved ridge and valley flows in fingerprint images. Although a number of methods have been proposed for orientation estimation, reliable computation of orientation field is still a challenging problem due to the poor quality of some fingerprints. This paper proposes a method to reconstruct fingerprint orientation field by weighted discrete cosine transform (DCT). First, the DCT functions are used to build the basis atoms for linear representation of orientation field. Then, the DCT basis atoms of low and high orders are combined with the weights determined by singularity measurements for orientation reconstruction. The weighted DCT model is further extended for partial fingerprints to gradually and iteratively reconstruct the orientations in noisy or missing parts of fingerprints. The proposed method can perform well in smoothing out the noise while maintaining the orientation details in singular regions. Extensive experiments have been done to compare the proposed method with some existing methods on NIST and FVC fingerprint databases in terms of the reconstruction accuracy of orientation field, fingerprint indexing performance, and fingerprint recognition accuracy. Experimental results illustrate the effectiveness of the proposed method in reconstructing orientation fields, especially for poor quality and partial fingerprints.
80C497D6	In this paper, we introduce the Minutia Cylinder-Code (MCC): a novel representation based on 3D data structures (called cylinders), built from minutiae distances and angles. The cylinders can be created starting from a subset of the mandatory features (minutiae position and direction) defined by standards like ISO/IEC 19794-2 (2005). Thanks to the cylinder invariance, fixed-length, and bit-oriented coding, some simple but very effective metrics can be defined to compute local similarities and to consolidate them into a global score. Extensive experiments over FVC2006 databases prove the superiority of MCC with respect to three well-known techniques and demonstrate the feasibility of obtaining a very effective (and interoperable) fingerprint recognition implementation for light architectures.
27435BF3	Hierarchical clustering is a popular approach in a number of fields with many well known algorithms. However, all existing work to our knowledge implements a greedy heuristic algorithm with no explicit objective function. In this work we formalize hierarchical clustering as an integer linear programming (ILP) problem with a natural objective function and the dendrogram properties enforced as linear constraints. Our experimental work shows that even for small data sets finding the global optimum produces more accurate results. Formalizing hierarchical clustering as an ILP with constraints has several advantages beyond finding the global optima. Relaxing the dendrogram constraints such as transitivity can produce novel problem variations such as finding hierarchies with overlapping clusterings. It is also possible to add constraints to encode guidance such as , , etc. Finally, though exact solvers exist for ILP we show that a simple randomized algorithm and a linear programming (LP) relaxation can be used to provide approximate solutions faster.
805F1CE5	HMM has been largely applied in many fields with great success. To achieve a better performance, an easy way is using more states or more free parameters for a better signal modelling. Thus, state sharing and state clipping methods have been proposed to reduce parameter redundancy and to limit the explosive consummation of system resources. We focus on a simple state sharing method for a hybrid neuro-Markovian on-line handwriting recognition system. At first, a likelihood-based distance is proposed for measuring the similarity between two HMM state models. Afterwards, a minimum quantification error aimed hierarchical clustering algorithm is also proposed to select the most representative models. Here, models are shared to the most under the constraint of the minimum system performance loss. As the result, we maintain about 98% of the system performance while about 60% of the parameters are reduced.
7B2F0A06	Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, clustering algorithms that build meaningful hierarchies out of large document collections are ideal tools for their interactive visualization and exploration as they provide data-views that are consistent, predictable, and at different levels of granularity. This paper focuses on document clustering algorithms that build such hierarchical solutions and (i) presents a comprehensive study of partitional and agglomerative algorithms that use different criterion functions and merging schemes, and (ii) presents a new class of clustering algorithms called constrained agglomerative algorithms, which combine features from both partitional and agglomerative approaches that allows them to reduce the early-stage errors made by agglomerative methods and hence improve the quality of clustering solutions. The experimental evaluation shows that, contrary to the common belief, partitional algorithms always lead to better solutions than agglomerative algorithms; making them ideal for clustering large document collections due to not only their relatively low computational requirements, but also higher clustering quality. Furthermore, the constrained agglomerative methods consistently lead to better solutions than agglomerative methods alone and for many cases they outperform partitional methods, as well.
79B17C36	We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By “categorical data,” we mean tables with fields that cannot be naturally ordered by a metric – e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems.
765DCAF5	A histogram clustering algorithm is suggested, which builds the hierarchy of distributions better in cluster separability. The algorithm optimizes the average cluster separability choosing the system of the data subdomain quantization grid and allows a significant decrease in the number of clusters. Application of the algorithm for uncontrolled Earth’s surface classification by satellite spectral data is shown.
806BFB0C	Mining frequent patterns is a fundamental part of data mining. Most of the previous studies adopt an a priori-like candidate set generation-and-test approach. The a priori is the first algorithm which uses the a priori property to prune the search space. In this paper the AFOPT algorithm is adapted for mining at different levels by using different support. Furthermore, the efficiency of this algorithm is being shown by comparing it to similar algorithms.
791C640A	Self-Organizing Map (SOM) networks have been successfully applied as a clustering method to numeric datasets. However, it is not feasible to directly apply SOM for clustering transactional data. This paper proposes the Transactions Clustering using SOM (TCSOM) algorithm for clustering binary transactional data. In the TCSOM algorithm, a normalized Dot Product norm based dissimilarity measure is utilized for measuring the distance between input vector and output neuron. And a modified weight adaptation function is employed for adjusting weights of the winner and its neighbors. More importantly, TCSOM is a one-pass algorithm, which is extremely suitable for data mining applications. Experimental results on real datasets show that TCSOM algorithm is superior to those state-of-the-art transactional data clustering algorithms with respect to clustering accuracy.
0624B843	Partitioning a large set of objects into homogeneous clusters is a fundamental operation in data mining. The k-means algorithm isbest suited for implementing this operation because of its efficiency in clustering large data sets. However, working only onnumeric values limits its use in data mining because data sets indata mining often contain categorical values. In this paper we present an algorithm, called k-modes, to extend the k-means paradigm to categorical domains. We introduce new dissimilarity measures to deal with categorical objects, replace means of clusters with modes, and use a frequency based method to update modes in the clustering process to minimise the clustering costfunction. Tested with the well known soybean disease data setthe algorithm has demonstrated a very good classification performance. Experiments on a very large health insurance dataset consisting of half a million records and 34 categorical attributes show that the algorithm is scalable in terms of both thenumber of clusters and the number of records.
5CD02299	In this paper, Web mining in a fuzzy environment is devoted. Browsing time staying on a Web page is considered and characterized as a fuzzy variable. Thus, the frequent preferred paths with fuzzy expected values can be gained. With the comparison of the fuzzy expected values, the measure of the interest of people for different Web pages is clear. In order to find more completely frequent fuzzy preferred paths, an efficient algorithm based on the frequent link and access tree(FLAAT) is designed, in which the access tree is traversed using a top-down strategy, and to avoid the loss of useful information, the frequent link is searched to find such nodes that may be neglected, and then the access tree is searched again to find other frequent preferred paths. The gained frequent preferred paths with fuzzy expected values more completely disclose the interest of users. Finally, an example is provided to clearly illustrate the proposed approach. And the results show that our algorithm achieves significant performance improvement over previous work.
7F94DC21	As advances in the technologies of predicting protein interactions, huge data sets portrayed as networks have been available. Identification of functional modules from such networks is crucial for understanding principles of cellular organization and functions. However, protein interaction data produced by high-throughput experiments are generally associated with high false positives, which makes it difficult to identify functional modules accurately. In this paper, we propose a fast hierarchical clustering algorithm HC-PIN based on the local metric of edge clustering value which can be used both in the unweighted network and in the weighted network. The proposed algorithm HC-PIN is applied to the yeast protein interaction network, and the identified modules are validated by all the three types of Gene Ontology (GO) Terms: Biological Process, Molecular Function, and Cellular Component. The experimental results show that HC-PIN is not only robust to false positives, but also can discover the functional modules with low density. The identified modules are statistically significant in terms of three types of GO annotations. Moreover, HC-PIN can uncover the hierarchical organization of functional modules with the variation of its parameter's value, which is approximatively corresponding to the hierarchical structure of GO annotations. Compared to other previous competing algorithms, our algorithm HC-PIN is faster and more accurate.
756A50D3	Johnson has shown that the single linkage and the complete linkage hierarchical clustering algorithms induce a metric on the data known as the ultrametric. Through the use of the Lance and Williams recurrence formula, Johnson's proof is extended to four other common clustering algorithms. It is also noted that two additional methods produce hierarchical structures which can violate the ultrametric inequality.
76B1A066	The location of the author of a social media message is not invariably the same as the location that the author writes about in the message. In applications that mine these messages for information such as tracking news, political events or responding to disasters, it is the geographic content of the message rather than the location of the author that is important. To this end, we present a method to geo-parse the short, informal messages known as microtext. Our preliminary investigation has shown that many microtext messages contain place references that are abbreviated, misspelled, or highly localized. These references are missed by standard geo-parsers. Our geo-parser is built to find such references. It uses Natural Language Processing methods to identify references to streets and addresses, buildings and urban spaces, and toponyms, and place acronyms and abbreviations. It combines heuristics, open-source Named Entity Recognition software, and machine learning techniques. Our primary data consisted of Twitter messages sent immediately following the February 2011 earthquake in Christchurch, New Zealand. The algorithm identified location in the data sample, Twitter messages, giving an F statistic of 0.85 for streets, 0.86 for buildings, 0.96 for toponyms, and 0.88 for place abbreviations, with a combined average F of 0.90 for identifying places. The same data run through a geo-parsing standard, Yahoo! Placemaker, yielded an F statistic of zero for streets and buildings (because Placemaker is designed to find neither streets nor buildings), and an F of 0.67 for toponyms.
7EAD8177	We investigate the effect of weak gravitational lensing in the limit of small angular scales where projected galaxy clustering is strongly nonlinear. This is the regime likely to be probed by future weak lensing surveys. We use well-motivated hierarchical scaling arguments and the plane-parallel approximation to study multi-point statistical properties of the convergence field. These statistics can be used to compute the vertex amplitudes in tree models of hierarchical clustering; these can be compared with similar measurements from galaxy surveys, leading to a powerful probe of galaxy bias. 
7AC42166	A novel algorithm named NB+ which is an extended version of the traditional Naïve Bayesian algorithm has been presented in this paper. An exception occurs when there is an equal probability for the class label value in the Naïve Bayesian algorithm. The approach aims to suggest a solution with the help of a partial matching method. Consequently, the classification accuracy has drastically improved. Experimental evaluation has been done on various databases to show that NB+ algorithm outperforms the traditional Naïve Bayesian algorithm.
82D539EC	Various algorithms have been developed to improve the quantity and quality of information that can be extracted from complex datasets obtained using hyphenated mass spectrometric techniques. While different approaches are possible, the key step often consists in arranging the data into a large series of profiles known as extracted ion profiles. Those profiles, similar to mono-dimensional separation profiles, are then processed to detect potential chromatographic peaks. This allows extracting from the dataset a large number of peaks that are characteristics of the compounds that have been separated. However, with mass spectrometry (MS) detection, the response is usually a complex signal whose pattern depends on the analyte, the MS instrument and the ionization method. When converted to ionic profiles, a single separated analyte will have multiple images at different m/z range. In this manuscript we present a hierarchical agglomerative clustering algorithm to group profiles with very similar feature. Each group aims to contain all profiles that are due to the transport and monitoring of a single analyte. Clustering results are then used to generate a 2 dimensional representation, called clusters plot, which allows an in-depth analysis of the MS dataset including the visualization of poorly separated compounds even when their intensity differs by more than two orders of magnitude. The usefulness of this new approach has been validated with data from capillary electrophoresis time of flight mass spectrometry hyphenated via an electrospray ionization. Using a mixture of 17 low molecular endogenous compounds it was verified that ionic profiles belonging to each compounds were correctly clustered even with very low degree of separation (R below 0.03). The approach was also validated using a urine sample. While with the total ion profile 15 peaks could be distinguished, 70 clusters were obtained allowing a much thorough analysis. In this particular example, the total computing took less than 10min.
7C9D9D1B	Milligan presented the conditions that are required for a hierarchical clustering strategy to be monotonic, based on a formula by Lance and Williams. In the present paper, the statement of the conditions is improved and shown to provide necessary and sufficient conditions.
75EB34A6	Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-ANMI, a new efficient algorithm for clustering categorical data. The k-ANMI algorithm works in a way that is similar to the popular k-means algorithm, and the goodness of clustering in each step is evaluated using a mutual information based criterion (namely, average normalized mutual information – ANMI) borrowed from cluster ensemble. This algorithm is easy to implement, requiring multiple hash tables as the only major data structure. Experimental results on real datasets show that k-ANMI algorithm is competitive with those state-of-the-art categorical data clustering algorithms with respect to clustering accuracy.
7FA715BB	The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints.
7D37CDF7	FP-Growth algorithm recursively generates huge amounts of conditional pattern bases and conditional FP-trees when the dataset is huge. In such a case, both the memory usage and computational cost are expensive, such that, the FP-tree can not meet the memory requirement. In this work, we propose a novel parallel FP-Growth algorithm, which is designed to run on the computer cluster. To avoid memory overflow, this algorithm finds all the conditional pattern bases of frequent items by the projection method without constructing an FP-tree. Hereafter, it splits the mining task into number of independent sub-tasks, executes these sub-tasks in parallel on nodes and then aggregates the results back for the final result. Our algorithm works independently at each node. As a result, it can efficiently reduce the inter-node communication cost. Experiments show that this parallel algorithm not only avoids the memory overflow but accelerate the computational speed. In addition, it achieves much better scalability than that of the FP-Growth algorithm.
7528D3D9	In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on  hierarchical clustering. Our delivery scheme involves cooperation among the participating peers.The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree  structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical  level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.    
674B2B5A	This paper studies an adaptive clustering problem. We focus on re-clustering an object set, previously clustered, when the feature set characterizing the objects increases. We propose an adaptive clustering method based on a hierarchical agglomerative approach, Hierarchical Adaptive Clustering (HAC), that adjusts the partitioning into clusters that was established by applying the hierarchical agglomerative clustering algorithm (HACA) (Han and Kamber, 2001) before the feature set changed. We aim to reach the result more efficiently than running HACA again from scratch on the feature-extended object set. Experiments testing the method's efficiency and a practical distributed systems problem in which the HAC method can be efficiently used (the problem of adaptive horizontal fragmentation in object oriented databases) are also reported.
7592732C	In many applications, data objects are described by both numeric and categorical features. The k-prototype algorithm is one of the most important algorithms for clustering this type of data. However, this method performs hard partition, which may lead to misclassification for the data objects in the boundaries of regions, and the dissimilarity measure only uses the user-given parameter for adjusting the significance of attribute. In this paper, first, we combine mean and fuzzy centroid to represent the prototype of a cluster, and employ a new measure based on co-occurrence of values to evaluate the dissimilarity between data objects and prototypes of clusters. This measure also takes into account the significance of different attributes towards the clustering process. Then we present our algorithm for clustering mixed data. Finally, the performance of the proposed method is demonstrated by a series of experiments on four real world datasets in comparison with that of traditional clustering algorithms.
7E7A888D	This paper presents TreeGNG, a top-down unsupervised learning method that produces hierarchical classification schemes. Tree- GNG extends the Growing Neural Gas algorithm by maintaining a time history of the learned topological mapping. TreeGNG is able to recover from poor decisions made during the construction of the tree, and provides the novel ability to influence the general shape of the hierarchy.
7F0EF70B	Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.
801AA81F	The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large data sets, which is critical to data mining applications.
000069BF	A few years ago, Vanecek (1994) suggested to apply a variant of back-face culling to speed-up collision detection between polyhedral objects. However, Vanecek's method is linear in the number of faces in the object, which is unpractical for large models. This paper suggests to add some geometrical information to hierarchies of bounding volumes, typically used in collision detection, and perform conservative back-face culling at the bounding-volume level in constant time. The method described in this paper can be applied to complement any kind of bounding-volumes hierarchy and allows a trade-off between memory and speed. Preliminary experimental results suggest that the method allows a significant speed-up, especially in close proximity situations.
77245C93	Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally “connected,” while the other forms clusters that are optimally “compact.”
802F0036	This paper presents QuoCast, a resource-aware protocol for reliable stream diffusion in unreliable environments, where processes may crash and communication links may lose messages. QuoCast is resource-aware in the sense that it takes into account memory, CPU, and bandwidth constraints. Memory constraints are captured by the limited knowledge each process has of its neighborhood. CPU and bandwidth constraints are captured by a fixed quota on the number of messages that a process can use for streaming. Both incoming and outgoing traffic are accounted for. QuoCast maximizes the probability that each streamed packet reaches all consumers while respecting their incoming and outgoing quotas. The algorithm is based on a tree-construction technique that dynamically distributes the forwarding load among processes and links, based on their reliabilities and on their available quotas. The evaluation results show that the adaptiveness of QuoCast to several contraints provides better reliability when compared to other adaptive approaches.
7D648CAA	This paper first studies the methods of web documents mining and text clustering, and summaries the fuzzy clustering algorithms and similarity measure functions, then proposes a modified similarity function which can solve the problems of feature selection and feature extraction in high-dimensional space. Finally, this paper puts forward to a dynamic fluzzy clustering algorithm(DCFCM) by combining the proposed similarity function with approximated C-mediods. The experiments show that DCFCM can effectively improve he precision of web documents clustering, the method is feasible in web documents mining.
7DAD351F	Parallel algorithms on SIMD (single-instruction stream multiple-data stream) machines for hierarchical clustering and cluster validity computation are proposed. The machine model uses a parallel memory system and an alignment network to facilitate parallel access to both pattern matrix and proximity matrix. For a problem with N patterns, the number of memory accesses is reduced from O(N/sup 3/) on a sequential machine to O(N/sup 2/) on an SIMD machine with N PEs.
7599B1B2	Data mining is a process that analyzes voluminous digital data in order to discover hidden but useful patterns from digital data. However, the discovering of such hidden patterns has statistical meaning and may often disclose some sensitive information. As a result, privacy becomes one of the prime concerns in the data-mining research community. Since distributed association mining discovers association rules by combining local models from various distributed sites, breaching data privacy happens more often than it does in centralized environments. In this work, we present a methodology that generates association rules without revealing confidential inputs such as statistical properties of individual sites, and yet retains a high level of accuracy in the resultant rules. One of the important outcomes of the proposed technique is that it reduces the overall communication costs. Performance evaluation of our proposed method shows that it reduces the communication cost significantly when we compare it with other well-known, distributed association-rule-mining algorithms. Nevertheless, the global rule model generated by the proposed method is based on the exact global support of each item set and hence diminishes inconsistency, which indeed occurs when global models are generated from partial support count of an item set.
8000EE71	Deng et al. [Deng, S., He, Z., Xu, X.: G-ANMI: A mutual information based genetic clustering algorithm for categorical data, Knowledge-Based Systems 23, 144–149(2010)] proposed a mutual information based genetic clustering algorithm named G-ANMI for categorical data. While G-ANMI is superior or comparable to existing algorithms for clustering categorical data in terms of clustering accuracy, it is very time-consuming due to the low efficiency of genetic algorithm (GA). In this paper, we propose a new initialization method for G-ANMI to improve its efficiency. Experimental results show that the new method greatly improves the efficiency of G-ANMI as well as produces higher clustering accuracy.
75AC0D00	Data objects with mixed numeric and categorical attributes are commonly encountered in real world. The k-prototypes algorithm is one of the principal algorithms for clustering this type of data objects. In this paper, we propose an improved k-prototypes algorithm to cluster mixed data. In our method, we first introduce the concept of the distribution centroid for representing the prototype of categorical attributes in a cluster. Then we combine both mean with distribution centroid to represent the prototype of the cluster with mixed attributes, and thus propose a new measure to calculate the dissimilarity between data objects and prototypes of clusters. This measure takes into account the significance of different attributes towards the clustering process. Finally, we present our algorithm for clustering mixed data, and the performance of our method is demonstrated by a series of experiments on four real-world datasets in comparison with that of traditional clustering algorithms.
7BB625D3	Due to enormous growth in both volume and variety of data, clustering a very large database is a time-consuming process. To speed up clustering process, sampling has been recognized as a very utilitarian approach to reduce the dataset size in which a collection of data points are taken as a sample and then a clustering algorithm is applied to partitioning the data points in that sample into clusters. In this approach, the data points, that are not sampled, do not get their cluster labels. The process of allocating unlabeled data points into proper clusters has been well explored purely in numerical or categorical domain only, but not the both. In this paper, we propose a hybrid similarity coefficient to find the resemblance between an unlabeled data point and a cluster, based on the importance of categorical attribute values and the mean values of numerical attributes. Furthermore, we propose a Hybrid Data Labeling Algorithm (HDLA), based on this similarity coefficient to designate an appropriate cluster label to each unlabeled data point. We analyze its time complexity and perform various experiments using synthetic and real world datasets to demonstrate the efficacy of HDLA.
7547B164	A Multilayered Self-Learning Spiking Neural Network and its Learning Algorithm Based on ‘Winner-Takes-More’ Rule in Hierarchical Clustering This paper introduces architecture of multilayered selflearning spiking neural network for hierarchical data clustering. It consists of the layer of population coding and several layers of spiking neurons. Contrary to originally suggested multilayered spiking neural network, the proposed one does not require a separate learning algorithm for lateral connections. Irregular clusters detecting capability is achieved by improving the temporal Hebbian learning algorithm. It is generalized by replacing ‘Winner-Takes-All’ rule with ‘Winner-Takes-More’ one. It is shown that the layer of receptive neurons can be treated as a fuzzification layer where pool of receptive neurons is a linguistic variable, and receptive neuron within a pool is a linguistic term. The network architecture is designed in terms of control systems theory. Using the Laplace transform notion, spiking neuron synapse is presented as a second-order critically damped response unit. Spiking neuron soma is modeled on the basis of bang-bang control systems theory as a threshold detection system. Simulation experiment confirms that the proposed architecture is effective in detecting irregular clusters. A Multilayered Self-Learning Spiking Neural Network and its Learning Algorithm Based on ‘Winner-Takes-More’ Rule in Hierarchical Clustering
7A132388	Recently, there has been enormous growth in the amount of commercial and scientific data, such as protein sequences, retail transactions, and web-logs. Such datasets consist of sequence data that have an inherent sequential nature. However, few existing clustering algorithms consider sequentiality. In this paper, we study how to cluster these sequence datasets. We propose a new similarity measure to compute the similarity between two sequences. In the proposed measure, subsets of a sequence are considered, and the more identical subsets there are, the more similar the two sequences. In addition, we propose a hierarchical clustering algorithm and an efficient method for measuring similarity. Using a splice dataset and synthetic datasets, we show that the quality of clusters generated by our proposed approach is better than that of clusters produced by traditional clustering algorithms.
590717D5	Chapter Eight begins with a review of the clustering task, and the concept of distance. Good clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation. First, hierarchical clustering methods are examined. In hierarchical clustering, a treelike cluster structure (dendrogram) is created through recursive partitioning (divisive methods) or combining (agglomerative) of existing clusters. Single-linkage, complete-linkage, and average-linkage methods are discussed. The single-linkage and complete-linkage clustering algorithms are walked-through, using a small univariate data set. Differences in the resulting dendrogram structure are discussed. The average-linkage algorithm is shown to produce the same dendrogram as the complete-linkage algorithm, for this data set, though not necessarily in general. Next, we turn to the k-means clustering algorithm, beginning with the definition of the steps involved in the algorithm. Cluster centroids are defined. The k-means algorithm is walked-through, using a tiny bivariate data set, showing graphically how the cluster centers are updated. An application of k-means clustering to the large churn data set is undertaken, using SAS Enterprise Miner. The resulting clusters are profiled. Finally, the methodology of using cluster membership for further analysis downstream is illustrated, with the clusters identified by SAS Enterprise Miner helping to predict churn. The exercises include challenges to readers to construct single-linkage, complete-linkage, and k-means clustering solutions for small univariate and bivariate data sets. The hands-on analysis problems include generating k-means clusters using the cereals data set, and applying these clusters to help predict nutrition rating.
7C73C958	This paper evaluates the performance of different criterion functions in the context of partitional clustering algorithms for document datasets. Our study involves a total of seven different criterion functions, three of which are introduced in this paper and four that have been proposed in the past. We present a comprehensive experimental evaluation involving 15 different datasets, as well as an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce. Our experimental results show that there are a set of criterion functions that consistently outperform the rest, and that some of the newly proposed criterion functions lead to the best overall results. Our theoretical analysis shows that the relative performance of the criterion functions depends on (i) the degree to which they can correctly operate when the clusters are of different tightness, and (ii) the degree to which they can lead to reasonably balanced clusters.
79CA8DD3	The original k-means clustering algorithm is designed to work primarily on numeric data sets. This prohibits the algorithm from being directly applied to categorical data clustering in many data mining applications. The k-modes algorithm [Z. Huang, Clustering large data sets with mixed numeric and categorical value, in: Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference. World Scientific, Singapore, 1997, pp. 21–34] extended the k-means paradigm to cluster categorical data by using a frequency-based method to update the cluster modes versus the k-means fashion of minimizing a numerically valued cost. However, as is the case with most data clustering algorithms, the algorithm requires a pre-setting or random selection of initial points (modes) of the clusters. The differences on the initial points often lead to considerable distinct cluster results. In this paper we present an experimental study on applying Bradley and Fayyad's iterative initial-point refinement algorithm to the k-modes clustering to improve the accurate and repetitiveness of the clustering results [cf. P. Bradley, U. Fayyad, Refining initial points for k-mean clustering, in: Proceedings of the 15th International Conference on Machine Learning, Morgan Kaufmann, Los Altos, CA, 1998]. Experiments show that the k-modes clustering algorithm using refined initial points leads to higher precision results much more reliably than the random selection method without refinement, thus making the refinement process applicable to many data mining applications with categorical data.
7E1C5F27	In the above-titled paper (ibid., vol.12, no.11, p.1088-92, Nov. 1990), parallel implementations of hierarchical clustering algorithms that achieve O(n/sup 2/) computational time complexity and thereby improve on the baseline of sequential implementations are described. The latter are stated to be O(n/sup 3/), with the exception of the single-link method. The commenter points out that state-of-the-art hierarchical clustering algorithms have O(n/sup 2/) time complexity and should be referred to in preference to the O(n/sup 3/) algorithms, which were described in many texts in the 1970s. Some further references in the parallelizing of hierarchic clustering algorithms are provided.
76F0F507	This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method of increasing the height-to-width ratio of the cluster histogram, we develop a novel algorithm -- CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms.
7FA96432	It has often been asserted that since hierarchical clustering algorithms require pairwise interobject proximities, the complexity of these clustering procedures is at least O(N2). Recent work has disproved this by incorporating efficient nearest neighbour searching algorithms into the clustering algorithms. A general framework for hierarchical, agglomerative clustering algorithms is discussed here, which opens up the prospect of much improvement on current, widely-used algorithms. This ‘progress report’ details new algorithmic approaches in this area, and reviews recent results
752C5C0B	Ensemble clustering, also known as consensus clustering, aims to generate a stable and robust clustering through the consolidation of multiple base clusterings. In recent years many ensemble clustering methods have been proposed, most of which treat each clustering and each object as equally important. Some approaches make use of weights associated with clusters, or with clusterings, when assembling the different base clusterings. Boosting algorithms developed for classification have also led to the idea of considering weighted objects during the clustering process. However, not much effort has been put towards incorporating weighted objects into the consensus process. To fill this gap, in this paper we propose an approach called Weighted-Object Ensemble Clustering (WOEC). We first estimate how difficult it is to cluster an object by constructing the co-association matrix that summarizes the base clustering results, and we then embed the corresponding information as weights associated to objects. We propose three different consensus techniques to leverage the weighted objects. All three reduce the ensemble clustering problem to a graph partitioning one. We present extensive experimental results which demonstrate that our WOEC approach outperforms state-of-the-art consensus clustering methods and is robust to parameter settings.
7A2AE3B6	Hierarchical clustering is a common method used to determine clusters of similar data points in multidimensional spaces. O(n2) algorithms are known for this problem [3,4,11,19]. This paper reviews important results for sequential algorithms and describes previous work on parallel algorithms for hierarchical clustering. Parallel algorithms to perform hierarchical clustering using several distance metrics are then described. Optimal PRAM algorithms using n/log n processors are given for the average link, complete link, centroid, median, and minimum variance metrics. Optimal butterfly and tree algorithms using n/log n processors are given for the centroid, median, and minimum variance metrics. Optimal asymptotic speedups are achieved for the best practical algorithm to perform clustering using the single link metric on a n/log n processor PRAM, butterfly, or tree.
5DF1BD46	We explore the use of instance and cluster-level constraints with agglomerative hierarchical clustering. Though previous work has illustrated the benefits of using constraints for non-hierarchical clustering, their application to hierarchical clustering is not straight-forward for two primary reasons. First, some constraint combinations make the feasibility problem (Does there exist a single feasible solution?) NP-complete. Second, some constraint combinations when used with traditional agglomerative algorithms can cause the dendrogram to stop prematurely in a dead-end solution even though there exist other feasible solutions with a significantly smaller number of clusters. When constraints lead to efficiently solvable feasibility problems and standard agglomerative algorithms do not give rise to dead-end solutions, we empirically illustrate the benefits of using constraints to improve cluster purity and average distortion. Furthermore, we introduce the new γ constraint and use it in conjunction with the triangle inequality to considerably improve the efficiency of agglomerative clustering.
7CB6B315	A monotone invariant method of hierarchical clustering based on the Mann-Whitney U-statistic is presented. The effectiveness of the complete-link, single-link, and U-statistic methods in recovering tree structures from error perturbed data are evaluated. The U-statistic method is found to be consistently more effective in recovering the original tree structures than either the single-link or complete-link methods.
7CF2C846	Categorical data clustering (CDC) and cluster ensemble (CE) have long been considered as separate research and application areas. The main focus of this paper is to investigate the commonalities between these two problems and the uses of these commonalities for the creation of new clustering algorithms for categorical data based on cross-fertilization between the two disjoint research fields. More precisely, we formally define the CDC problem as an optimization problem from the viewpoint of CE, and apply CE approach for clustering categorical data. Experimental results on real datasets show that CE based clustering method is competitive with existing CDC algorithms with respect to clustering accuracy.
5EC00903	We explore the use of constraints with divisive hierarchical clustering. We mention some considerations on the effects of the inclusion of constraints into the hierarchical clustering process. Furthermore, we introduce an implementation of a semi-supervised divisive hierarchical clustering algorithm and show the influence of including constraints into the divisive hierarchical clustering process. In this task our main interest lies in building stable dendrograms when clustering with different subsets of data.
756A50D3	Johnson has shown that the single linkage and the complete linkage hierarchical clustering algorithms induce a metric on the data known as the ultrametric. Through the use of the Lance and Williams recurrence formula, Johnson's proof is extended to four other common clustering algorithms. It is also noted that two additional methods produce hierarchical structures which can violate the ultrametric inequality.
778095CE	Clustering techniques are usually used in pattern recognition, image segmentation and object detection. Let N be the number of patterns and M be the number of features of each pattern and . In this paper, we first design two O(1) time basic operations for concentrating all nonempty data of size N and computing the proximity matrix using N × N and N × N × M processors, respectively. Then, based on these two operations, a constant time parallel hierarchical clustering algorithm is proposed on a 3-D processor array with reconfigurable bus system using N4 processors. Then, by reducing the number of processors by a factor of N, an O(log2 N) time algorithm for this problem is also derived. Note that no one had ever obtained a constant time algorithm for this problem on the existing parallel computation models
7C9D9D1B	Milligan presented the conditions that are required for a hierarchical clustering strategy to be monotonic, based on a formula by Lance and Williams. In the present paper, the statement of the conditions is improved and shown to provide necessary and sufficient conditions.
76035A9B	Hierarchical agglomerative clustering (HAC) is very useful but due to high CPU time and memory complexity its practical use is limited. Earlier, we proposed an efficient partitioning – partially overlapping partitioning (POP) – based on the fact that in HAC small and closely placed clusters are agglomerated initially, and only towards the end larger and distant clusters are agglomerated. Here, we present the parallel version of POP, pPOP. Theoretical analysis shows that, compared to the existing algorithms, pPOP achieves CPU time speed-up and memory scale-down of O(c) without compromising accuracy where c is the number of cells in the partition. A shared memory implementation shows that pPOP outperforms existing algorithms significantly.
7FA96432	It has often been asserted that since hierarchical clustering algorithms require pairwise interobject proximities, the complexity of these clustering procedures is at least O(N2). Recent work has disproved this by incorporating efficient nearest neighbour searching algorithms into the clustering algorithms. A general framework for hierarchical, agglomerative clustering algorithms is discussed here, which opens up the prospect of much improvement on current, widely-used algorithms. This ‘progress report’ details new algorithmic approaches in this area, and reviews recent results
7F1704E8	Clustering is a basic operation in image processing and computer vision, and it plays an important role in unsupervised pattern recognition and image segmentation. While there are many methods for clustering, the single-link hierarchical clustering is one of the most popular techniques. In this paper, with the advantages of both optical transmission and electronic computation, we design efficient parallel hierarchical clustering algorithms on the arrays with reconfigurable optical buses (AROB). We first design three efficient basic operations which include the matrix multiplication of two N×N matrices, finding the minimum spanning tree of a graph with N vertices, and identifying the connected component containing a specified vertex. Based on these three data operations, an O(log N) time parallel hierarchical clustering algorithm is proposed using N3 processors. Furthermore, if the connectivity of the AROB with four-port connection is allowed, two constant time clustering algorithms can be also derived using N4 and N3 processors, respectively. These results improve on previously known algorithms developed on various parallel computational models.
85C8C653	This paper proposes a hierarchical clustering algorithm based on Saturated Neighbor Graph -- hi-CLUBS and a new concept, natural nearest neighbor, which adopts a parameter-less algorithm of searching the natural neighbors for each point in a dataset. In the work, the Saturated Neighbor Graph is constructed by the natural nearest neighbor firstly. Then modularity is introduced into graph partitioning algorithm, with which the generated graph is partitioned into small sub-clusters without any parameters. Finally, these initial sub-clusters are repeatedly merged with another cluster according to similarity measurement based on connectivity and closeness, until the desired cluster number is reached. The results show that hi-CLUBS produces a set of final clusters achieves better quality than the traditional clustering algorithms.
59658E71	When performing hierarchical clustering, some metric must be used to determine the similarity between pairs of clusters. Traditional similarity metrics either can only deal with simple shapes or are very sensitive to outliers. We propose two potential-based similarity metrics, APES and AMAPES, inspired by the concept of electric potential in physics. The main features of these metrics are: they have strong anti-jamming capability; and they are capable of finding clusters of complex irregular shapes.
7E6AAED5	Among the microarray data analysis clustering methods, K-means and hierarchical clustering are researchers' favorable tools today. However, each of these traditional clustering methods has its limitations. In this study, we introduce a new method, hierarchical K-means regulating divisive or agglomerative approach. The hierarchical K-means firstly employs K-means' algorithm in each cluster to determine K cluster while operating and then employs it on hierarchical clustering technique to shorten merging clusters time while generating a tree-like dendrogram. We apply this method in two original microarray datasets. The result indicates divisive hierarchical K-means is superior to hierarchical clustering on cluster quality and is superior to K-means clustering on computational speed. Our conclusion is that divisive hierarchical K-means establishes a better clustering algorithm satisfying researchers' demand. 	
71CD6C9A	In this paper we introduce the novel class hierarchy construction algorithm (CHCA) in order to create hierarchical clusterings of Web documents. Unlike most clustering methods, CHCA operates on nominal data (the words occurring in each document) and it differs from other hierarchical clustering techniques in that it uses the object-oriented concept of inheritance to create the parent/child relationship between clusters. A prototype system has been developed using CHCA to create cluster hierarchies from web search results returned by conventional search engines. CHCA, without any guidance, creates term-based clusters from the contents of the retrieved pages and assigns each page to a cluster; the clusters correspond to topics and sub-topics in the investigated domain. The performance of our system is compared with a similar web search clustering system (Vivisimo).
76DDB4CD	Non-hierarchical clustering methods are frequently based on the idea of forming groups around 'objects'. The main exponent of this class of methods is the "k"-means method, where these objects are points. However, clusters in a data set may often be due to certain relationships between the measured variables. For instance, we can find linear structures such as straight lines and planes, around which the observations are grouped in a natural way. These structures are not well represented by points. We present a method that searches for linear groups in the presence of outliers. The method is based on the idea of impartial trimming. We search for the 'best' subsample containing a proportion 1 - "&agr;" of the data and the best "k" affine subspaces fitting to those non-discarded observations by measuring discrepancies through orthogonal distances. The population version of the sample problem is also considered. We prove the existence of solutions for the sample and population problems together with their consistency. A feasible algorithm for solving the sample problem is described as well. Finally, some examples showing how the method proposed works in practice are provided. Copyright (c) 2009 Royal Statistical Society. 
06E566AC	this paper provides a good representation of the clustering structure, thus it can be used as a tool to get insight into the distribution of a data set. In addition, the visualization can also reveal hierarchical cluster with different sizes, densities and shapes. 
79249275	Tens, hundreds and even thousands of cores are to be integrated into a single chip. Network on chip appears to offer efficient communication between cores. However, the increased requirement for larger communication bandwidths and lower power consumption challenges the traditional electrical interconnects. Advances in silicon nanophotonics make optical interconnect a promising candidate for network on chip architectures in future. In this paper, we develop a hierarchical cluster-based optical NoC (HCONoC). It is a hybrid electrical/optical on chip network architecture. It connects the lowest level cluster of IP cores with electrical interconnects while optical interconnects are used for inter-cluster communication to improve the efficiency of the network. Three types of routers including electrical and optical router architectures are carefully designed together with an efficient routing algorithm. Packet switching is employed which helps to avoid the overhead of path setup, alleviating the contention. Finally, we simulate the HCONoC for the 64-core architecture and show the network performance including end-to-end delay and network throughput.
7528D3D9	In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on  hierarchical clustering. Our delivery scheme involves cooperation among the participating peers.The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree  structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical  level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.    
7A989C58	In this paper we propose to combine two clustering approaches, namely fuzzy and possibilistic c-means. While fuzzy c-means algorithm finds suitable clusters for groups of data points, obtained memberships of data, however, encounters a major deficiency caused by misinterpretation of membership values of data points. Therefore, membership values cannot correctly interpret compatibility or degree to which data points belong to clusters. As a result, noisy data will be misinterpreted by incorrect memberships assigned, as sum of memberships of each noisy data to all clusters is constrained to be equal to 1. To overcome this, a possibilistic approach has been proposed which removes this constraint. It has, however, caused another shortcoming as cluster centers converge to an identical point. Therefore, possibilities cannot correctly interpret the degrees of compatibilities. To correct this problem, a number of works have been carried out which all try to change possibilistic objective function proposed by Krishnapuram and James M. Keller. In this work, a hierarchical approach has been proposed based on properties of both fuzzy and possibilistic approaches to overcome this deficiency. Sensitivities of both methods have been studied together with analyzing results obtained by both methods. Superiority of the proposed method as opposed to conventional possibilistic c-means is shown to be conspicuous. 
04208BBF	 Clustering analysis is an important technique in many applications, such as in biology, medicine, psychology, pattern recognition, image processing, marketing, and data engineering. The large number of existing clustering algorithms can be broadly classified into two types: (1) hierarchical and (2) partitional. Depending on the algorithmic approach taken, a hierarchical structure begins with N clusters, one per pattern, and grows a sequence of clusterings until all N patterns are in a single cluster (the agglomerative approach), or begins with one cluster containing all N patterns and successively divides clusters until N clusters are achieved (the divisive approach). Hierarchical clustering is a sequence of nested partitions in the form of a tree diagram or a dendrogram, whereas a partitional clustering is a single partition. Some fuzzy partitional clustering algorithms and their convergence properties have been given in the literature, but no fuzzy hierarchical clustering algorithm has yet been presented. In this study, fuzzy hierarchical clustering algorithms for the agglomerative approach and the divisive approach, respectively, are proposed. A performance comparison among the two proposed algorithms with different parameter values is included. The two proposed algorithms are compared with two existing algorithms. Additionally, to teduce computational time, the corresponding parallel versions of the two proposed algorithms are developed. Some experimental results show the feasibility of the proposed approaches. 
7731DE76	The processing power of parallel coprocessors like the Graphics Processing Unit (GPU) is dramatically increasing. However, until now only a few approaches have been presented to utilize this kind of hardware for mesh clustering purposes. In this paper, we introduce a Multilevel clustering technique designed as a parallel algorithm and solely implemented on the GPU. Our formulation uses the spatial coherence present in the cluster optimization and hierarchical cluster merging to significantly reduce the number of comparisons in both parts. Our approach provides a fast, high-quality, and complete clustering analysis. Furthermore, based on the original concept, we present a generalization of the method to data clustering. All advantages of the mesh-based techniques smoothly carry over to the generalized clustering approach. Additionally, this approach solves the problem of the missing topological information inherent to general data clustering and leads to a Local Neighbors k-means algorithm. We evaluate both techniques by applying them to Centroidal Voronoi Diagram (CVD)-based clustering. Compared to classical approaches, our techniques generate results with at least the same clustering quality. Our technique proves to scale very well, currently being limited only by the available amount of graphics memory.
77560214	Attribute discretization is one of the basis pre-treatment methods for data stream mining. For the reason that the attribute discretization algorithm cannot be applied with the highspeed data stream directly, we firstly build a synopsis structure for data stream, then proposed an attribute discretization algorithm based on a synopsis structure, finally, the simulation experiment results show that the method has achieved the problem of data stream attribute discretization.
7DA4FF3A	This paper focuses on document clustering algorithms that build hierarchical solutions. In this paper is evaluate the performance of different criterion functions for the problem of clustering documents.
798FC174	Hierarchical clustering (HC) is a widely used approach both in pattern recognition and data mining and has rich solutions in the literature. But all these existing solutions have some restrictions when the clustered dataset has complex structure. Spectral clustering is a graph-based, simple and outperforming method with the ability to find complex structure in dataset using spectral properties of the dataset-associated affinity matrix. In this paper, we propose a novel effective HC algorithm called SHC base on the techniques of spectral method. The experiment results both on artificial and real data sets show that our algorithm can hierarchically cluster complex data effectively and naturally.
7D4C763F	The ordering of nodes with respect to destinations of interest by means of spatial information (e.g., distances, path constituency, complete or partial topology) has been a fundamental aspect of all routing protocols in wireless networks. This spatial ordering has also included the use of geographical or virtual coordinates denoting the location of nodes. We propose the use of ordering of nodes based on time rather than space, and without the need to establish any clock synchronization among nodes. We demonstrate for the first time that using the relative times when each node receives and transmits packets is sufficient to establish multiple loop-free paths to destinations, and that such time-based ordering renders more efficient loop-free routing than the spatial ordering of nodes. With the use of self-adjusted delays, nodes can manipulate their ordering so that the resulting routing choices are more robust to failures than routing choices based solely on times driven by the physical topology. Furthermore, we show that the problem of resetting sequence numbers, which is a network-wide operation with traditional spatial ordering, is trivial with temporal ordering. We introduce the Time Ordered Routing Protocol (TORP) and compare it against routing protocols based on spatial ordering to demonstrate that temporal ordering can lead to superior performance in multi-hop wireless networks.
76DB1BAE	Pairwise Broadcast is a novel low-power wireless sensor network clock synchronization method. A subset of sensor nodes are synchronized by overhearing the timing message exchanges of a pair of sensor nodes. Therefore, a group of sensor nodes can be synchronized without sending any extra messages. This paper mainly account for the problem that the original Pairwise Broadcast Synchronization in absence of the clock skew between the pairwise broadcast node, proposed that use MLE(Maximum Likelihood Estimation) for accurate estimation of the clock offset and clock skew of nodes under Gaussian delay model. The simulation results indicate this method to ensure high accuracy simultaneously effectively reduce energy consumption.
5B0E344F	Time Synchronization in wireless networks is extremely important for basic communication, but it also provides the ability to detect movement, location, and proximity. The synchronization problem consists of four parts: send time, access time, propagation time, and receive time. Three current synchronization protocol Reference Broadcast Synchronization, Timing-sync Protocol for Sensor Networks, and Flooding Time Synchronization Protocol are presented and how they attempt solve the synchronization problem is also discussed. Security concerns as well as an industry case are also presented. 
77104C17	Wireless sensor networks are a nascent type of ad-hoc networks that had drawn the interest of the research community in last few years. They require working together for distributed tasks. Accurate and reliable time is one of the needs for some of its applications. The use of traditional time synchronization protocols for wired network is restricted by severe energy constraint in sensor network. The realization of time synchronized network poses many challenges, which are the subject of active research in the fileld. In this paper we argue that clock synchronization provides a lot of overhead, and can be eliminated in some of the cases. We advocate that sensor should be allowed to run unsynchronized. The time when any event of interest occurs, the sensor node records that event in its memory with timestamp from its cluster head's clock, instead of its local clock.
7C1B594D	An improved approach, called receiver only synchronization (ROS), relative to timing synch protocol for sensor networks (TPSN) was devised based on a two-way timing message exchange mechanism between two nodes. In ROS, all the nodes in their transmission range overhear the messages and adjust their clock parameters without having to communicate by themselves, hence saving a lot of energy. In this paper, we are proving that in ROS, one-way timing message transmission from the root node to the neighboring nodes located within its transmission range is superior to the two-way timing message exchange. Also, we are showing that choosing the more accurate model instead of the linear regression model yields further performance gains for the clock parameters estimation.
5A68FCE2	The   possibility   of   establishing   the chronology   of   events   in   a   widely   distributed network,      or      even      stronger,      the timesynchronization  of  all  nodes  in  the  network  is often  needed  for  applications  of  wireless  sensor networks. In  this  paper  we describethe Flooding Time    Synchronization    Protocol    (FTSP) that providestime   synchronization   service   in   such networks. The  protocol was  designed  to utilize low   communication   bandwidth,   scale   well   for medium  ized  multi-hop  networks,  and  be  robust against  topology  changes  and  node  failures.  The FTSP achieves ts robustness by utilizing periodic radio broadcast of synchronization messages and implicit   dynamic   opology   update.   MAC-layer time-stamping,   comprehensive compensation   oferrors  and linear  regression  re  used  to  achieve high accuracy. The resulting time synchronization rror of the FTSP is significantly ower than that  the  existing  RBS  and  TPSN  algorithms.The data  from a  comprehensive  multi-hop  experiment shows the  average  network-wide  synchronization error to   be in   the   microsecond   range.   The protocol  was  further  validated  as  part  of  our countersniper system that was field tested in a US military facility.
7F95D7EE	Time synchronization may play a key role in wireless sensor networks to meet real-time and energy-saving requirements and improve data-fusion and multiplexing efficiency. In this paper, we introduce a clock synchronization algorithm between the base station and sensor node used in wireless check meter system. The algorithm can not only been extended more conveniently, but also less affected by environment. The algorithm is implemented in the platform of s3c2410 solidified OS of Windows CE.Net. So the most important interrupt delay contributor, ISR and IST, can not be ignored, A result for the maximum clock skew of the hardware-based synchronization is presented, which can fulfill the need of wireless sensor networks built for wireless check meter system.
77B2E72B	Time synchronization is extremely important for wireless sensor networks. Since the sensors have very limited energy resources and it is very difficult to change batteries for the sensor nodes, energy efficiency is one of primary importance for a sensor network. To prolong the lifetime of a sensor network, we proposed a cluster based on demand time synchronization in wireless sensor networks(COD). In this protocol many nodes are set to “sleep” modes in most cases, for the formation of clustering and time synchronization procedure is initiated only when the event is detected. Compared to other clock synchronizat- ion protocols, the COD simulation results show that it is more energy efficient than RBS and FTSP.
061A259D	Wireless sensor networks (WSNs) assume a collection of tiny sensing devices connected wirelessly and which are used to observe and monitor a variety of phenomena in the real physical world. Time synchronization is an important issue in wireless sensor networks.  Many applications based on these WSNs assume local clocks need to be synchronized to a common view of clock at each sensor node. Some essential limitations of sensor networks such as limited energy resources, storage, computation, and bandwidth, combined with potentially high density of nodes make traditional synchronization methods incompatible for these networks. Hence, an increasing research focus on designing synchronization schemes is required. This paper reviews existing time synchronization protocols and the need for synchronization in sensor networks and then presents the proposed algorithm to construct adhoc tree structure of sensor network along with the process of clock synchronization. 
79D02E4A	Having access to an accurate time is a vital building block in all networks; in wireless sensor networks even more so, because wireless media access or data fusion may depend on it. Starting out with a novel analysis, we show that orthodox clock synchronization algorithms make fundamental mistakes. The state-of-the-art clock synchronization algorithm FTSP exhibits an error that grows exponentially with the size of the network, for instance. Since the involved parameters are small, the error only becomes visible in midsize networks of about 10--20 nodes. In contrast, we present PulseSync, a new clock synchronization algorithm that is asymptotically optimal. We evaluate PulseSync on a Mica2 testbed, and by simulation on larger networks. On a 20 node network, the prototype implementation of PulseSync outperforms FTSP by a factor of 5. Theory and simulation show that for larger networks, PulseSync offers an accuracy which is several orders of magnitude better than FTSP. To round off the presentation, we investigate several optimization issues, e.g. media access and local skew.
815E7D30	Time synchronization is an important research issue in wireless sensor networks (WSNs). Many applications based on these WSNs assume local clocks at each sensor node that need to be synchronized to a common notion of time. Some inherent properties of sensor networks such as limited energy, storage, computation, and bandwidth resources, combined with potentially high density of nodes make conventional synchronization methods unsuitable for these networks. Therefore, an increasing research focus on designing synchronization algorithms is required. In this paper we explore various time synchronization protocols and present theoretical analysis of protocols based on quantitative and qualitative criteria with the proposed Tree Structured Time Synchronization Protocol. The comparative study shows that it is an excellent conciliation among synchronization accuracy, computational complexity, and convergence time.
7913EB96	Many applications of Wireless Sensor Networks (WSNs) require accurate time synchronization for data consistency and coordination. While the existing protocols for time synchronization provide sufficient accuracy, they consume high energy and poorly synchronize the distant nodes. We propose a Recursive Time Synchronization Protocol (RTSP) which provides global clock synchronization in an accurate and energy-efficient way. It achieves better performance by using a novel blend of techniques such as the MAC-layer time-stamping based on Start of Frame Delimiter (SFD) byte, fewer broadcasts by the reference node, compensation of the propagation delay and adjustment of the timestamps at each hop, estimation of the relative skew and offset using least square linear regression on two data points (2LR), adaptive re-synchronization interval, aggregation of the synchronization requests and energy-awareness. We also analyze the sources of errors and efficiency. Simulation results show an average accuracy of 0.3μs in a large multi-hop network while using only 1/5th of the energy consumed by FTSP in the long run, i.e., the RTSP outperforms all other protocols of its class including RBS, TPSN and FTSP.
76684966	The International Atomic Time TAI is a physically realized time scale which is ultimately used for comparisons between observations and dynamical theories. Its definition should tell unambiguously what an ideal TAI should be. For terrestrial applications, TAI has been defined as a geocentric coordinate time. In Solar System Dynamics, a barycentric coordinate time is needed. In general, it is not possible to convert a coordinate time into another coordinate time. But a specific clock synchronized on TAI in the terrestrial system can be considered as reading a ‘modified’, proper time [TAI]i, which can be converted into a barycentric coordinate time. In this conversion appears a small location dependent term. By this process all the clocks of the TAI system give an unique barycentric time with the same metrological properties as TAI.
7F980C5B	A blind method for jointly estimating and correcting time, carrier frequency and sampling clock frequency synchronization in OFDM based systems is proposed. The algorithm is particularlly suitable for low-cost terminal implementations, since it provides a great efficiency in terms of hardware complexity. Its performance has been tested on a real system, taking 3GPP-LTE as target technology and FPGAs as hardware platform. Results show its good performance while keeping computational complexity unincreased.
7AC21292	This paper addresses the implementation of global time in transputer systems by software synchronization of local processor timers. After an introduction on the synchronization problem, SYNC_WAVE, a synchronization algorithm for multicomputer architectures, is briefly described. The rest of the paper gives details of an existing implementation of SYNC_WAVE for transputer-based systems. Most of the devised code is presented and commented on, showing the methods used to improve synchronization accuracy and to read the global clock from within a user process. The performance results obtained are shown, and the usefulness and portability of the proposed code are discussed.
7D58E42C	A probabilistic method is proposed for reading remote clocks in distributed systems subject to unbounded random communication delays. The method can achieve clock synchronization precisions superior to those attainable by previously published clock synchronization algorithms. Its use is illustrated by presenting a time service which maintains externally (and hence, internally) synchronized clocks in the presence of process, communication and clock failures.
816D7F63	Accurately synchronized clocks are crucial for many applications in sensor networks. Existing time synchronization algorithms provide on average good synchronization between arbitrary nodes, however, as we show in this paper, close-by nodes in a network may be synchronized poorly. We propose the Gradient Time Synchronization Protocol (GTSP) which is designed to provide accurately synchronized clocks between neighbors. GTSP works in a completely decentralized fashion: Every node periodically broadcasts its time information. Synchronization messages received from direct neighbors are used to calibrate the logical clock. The algorithm requires neither a tree topology nor a reference node, which makes it robust against link and node failures. The protocol is implemented on the Mica2 platform using TinyOS. We present an evaluation of GTSP on a 20-node testbed setup and simulations on larger network topologies.
7F1F639A	We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node, such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.Gradient clock synchronization has been widely studied in the static setting. We show that the bounds for the static case also apply to our highly dynamic setting: if two nodes remain at distance d from each other for sufficiently long, it is possible to synchronize their clocks to within O(d log(D/d)), where D is the diameter of the network. This is known to be optimal for static networks, and since a static network is a special case of a dynamic network, it is optimal for dynamic networks as well. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length d appears between two nodes, the time required until the skew between the two nodes is reduced to O(d log(D/d)) is O(D), which we prove is optimal.
76E0C500	We describe a new fault-tolerant algorithm for solving a variant of Lamport's clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtaines its local time. The algorithm solves the problem of maintaining closely synchronized local times, assuming that processes' local times are closely synchronized initially. The algorithm is able to tolerate the failure of just under one-third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time and its uncertainty, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the basic alborithm. A similar style algorithm can also be used to achieve synchronization initially.
79826E19	We present a novel clock synchronization algorithm and prove tight upper and lower bounds on the worst-case clock skew that may occur between any two participants in any given distributed system. More importantly, the worst-case clock skew between neighboring nodes is (asymptotically) at most a factor of two larger than the best possible bound. While previous results solely focused on the dependency of the skew bounds on the network diameter, we prove that our techniques are optimal also with respect to the maximum clock drift, the uncertainty in message delays, and the imposed bounds on the clock rates. The presented results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds. Furthermore, our algorithm exhibits a number of other highly desirable properties. First, the algorithm ensures that the clock values remain in an affine linear envelope of real time. A better worst-case bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. Second, the algorithm minimizes the number and size of messages that need to be exchanged in a given time period. Moreover, only a small number of bits must be stored locally for each neighbor. Finally, our algorithm can easily be adapted for a variety of other prominent synchronization models. 
80081FB9	Large propagation delay and node movement are considered to be two significant attributes that differentiate an underwater wireless sensor network (UWSN) from a ground wireless sensor network (WSN). Considering the effects of both propagation delay and movement, we propose a time synchronization algorithm suitable for a UWSN. An underwater node can move out of and into another node's range frequently. With the proposed algorithm, no time synchronization is necessary if the time stamps of the received data packets are within the tolerance. In this fashion, the network underwater does not need to perform global time synchronizations periodically, which reduces the time used to synchronize clocks among sensor nodes. The simulation results show the time cost for synchronization is linear to the data packets exchanged.
5DDE6ADB	Clock synchronization is a crucial service in many distributed systems, including wireless ad-hoc networks. This paper studies external clock synchronization, in which nodes should bring their clocks close to the value of some external reference time, which is provided in the system by one or more source clocks.Reference broadcast synchronization (RBS) is a known approach that exploits the broadcast nature of wireless networks for a single hop. However, when networks are large in physical extent, additional mechanisms must be employed.Using multi-hop algorithms that re-broadcast time information to short distances reduces the energy consumed for clock synchronization. The reason is that energy costs grow more than linearly with the broadcast distance. On the other hand, the quality of the clock synchronization, as measured in the closeness of the clocks, deteriorates as the number of hops increases.This paper shows how to balance these two contradictory goals, achieving optimal clock synchronization while adhering to an energy budget at each node. In particular, a distributed algorithm is presented that uses multi-hop broadcasting over a shallow infrastructure to synchronize the clocks. The closeness of clock synchronization achieved by the algorithm is proved to be optimal for the given energy constraints.
76367B21	A number of time synchronization protocols for wireless sensor networks (WSNs) have been recently proposed aiming at maximizing the accuracy and minimizing the power efficiency. This paper proposes novel clock skew estimators for the protocols based on two-way timing message exchanges to achieve long term reliability of synchronization. The proposed clock synchronization mechanism is far more power efficient than the conventional ones by significantly increasing the re-synchronization period. Moreover, it can be applied to the conventional protocols without any additional overhead. In fact, the proposed estimators assume simple steps and low complexity, a feature which is strongly demanding for WSNs consisting of cheap and small nodes
7534C71C	We describe hardware and software schemes for achieving precise clock synchronization on SP2 parallel system nodes. The SP2 multistage interconnection network has an unusual hardware feature, a set of distributed counters that the processor nodes may utilize for synchronizing their time–of–day clocks. We describe an algorithm for synchronizing the counters to within less than 200 nanoseconds of each other in a network of up to 512 processor nodes. This is 4–5 orders of magnitude better than what can be achieved by existing software schemes. We also describe experimental system software, called, for synchronizing the node clocks to the Internet time of day, utilizing the synchronous counters in the SP2 network.synchronizes the node clocks typically within 5 μs of each other, which is up to 2–3 orders of magnitude better than could be achieved by previous methods on the SP2 system. Synchronized clocks are useful in parallel and distributed environments, for example for performance measurement, tuning, tracing, debugging, gang scheduling of parallel processes, and timestamping of transactions. We also measure the performance of a widely used time synchronization utility, the Network Time Protocol, using the synchronous counters of the SP2 interconnection network.
7DD1F597	A general protocol for atomic broadcast in networks is presented. The protocol tolerates loss, duplication, reordering, delay of messages, and network partitioning in an arbitrary network of fail-stop sites (i.e. no Byzantine site behavior is tolerated). The protocol is based on majority-concensus decisions to commit on unique ordering of received broadcast messages. Under normal operating conditions, the protocol requires three phases to complete and approximately 4N/V messages where N is the number of sites. This overhead is distributed among the messages of which the delivery decision is made and the heavier the broadcast message traffic, the lower the overhead per broadcast message becomes. Under abnormal operating conditions, a decentralized termination protocol (also presented) is invoked. A performance analysis of this protocol is presented, showing that this protocol commits with high probability under realistic operating conditions without invoking termination protocol if N is sufficiently large. The protocol retains its efficiency in wide-area networks where broadcast communication media are unavailable.
80C888C7	Time-based localization approaches attract a lot of interest due to their high accuracy and potentially low cost for wireless sensor networks (WSNs). However, time-based localization is tightly coupled with clock synchronization. Thus, the reliability of timestamps in time-based localization becomes an important yet challenging task to deal with. In this paper, we propose robust time-based localization strategies to locate a target node with the help of anchors (nodes with known positions) in asynchronous networks. Two kinds of asynchronous networks are considered: one only with clock offsets, labeled quasi-synchronous networks, whereas the other with not only clock offsets but also clock skews, labeled fully asynchronous networks. A novel ranging protocol is developed for both networks, namely asymmetric trip ranging (ATR), to reduce the communication load and explore the broadcast property of WSNs. Regardless of the reliability of the timestamp report from the target node, closed-form least-squares (LS) estimators are derived to accurately estimate the target node position. As a result, we counter the uncertainties caused by the target node by ignoring the timestamps from this node. Furthermore, in order to simplify the estimator in fully asynchronous networks, localization and synchronization are decoupled. A simple yet efficient method is proposed to first Calibrate the Clock Skews of the anchors, and then Estimate the Node Position (CCS-ENP). Finally, Cramér-Rao bounds (CRBs) and simulation results corroborate the efficiency of our localization schemes.
7CF95A36	This letter proposes an energy-efficient clock synchronization scheme for Wireless Sensor Networks (WSNs) based on a novel time synchronization approach. Within the proposed synchronization approach, a subset of sensor nodes are synchronized by overhearing the timing message exchanges of a pair of sensor nodes. Therefore, a group of sensor nodes can be synchronized without sending any extra messages. This paper brings two main contributions: 1. Development of a novel synchronization approach which can be partially or fully applied for implementation of new synchronization protocols and for improving the performance of existing time synchronization protocols. 2. Design of a time synchronization scheme which significantly reduces the overall network-wide energy consumption without incurring any loss of synchronization accuracy compared to other well-known schemes.
7FD9D0A2	The cost of synchronizing a multicomputer increases with system size. For large multicomputers, the time and resources spent to enable each node to estimate the clock value of every other node in the system can be prohibitive. We show how to reduce the cost of synchronization by assigning each node to one or more groups, then having each node estimate the clock values of only those nodes with which it shares a group. Since each node estimates the clock value of only a subset of the nodes, the cost of synchronization can be significantly reduced. We also provide a method for computing the maximum skew between any two nodes in the multicomputer, and a method for computing the maximum time between synchronizations. We also show how the fault tolerance of the synchronization algorithm may be determined.
7849ABA5	Recent advances in micro-electromechanical (MEMS) technology have led to the development of small, low-cost, and low-power sensors. Wireless sensor networks (WSNs) are large-scale networks of such sensors, dedicated to observing and monitoring various aspects of the physical world. In such networks, data from each sensor is agglomerated using data fusion to form a single meaningful result, which makes time synchronization between sensors highly desirable. This paper surveys and evaluates existing clock synchronization protocols based on a palette of factors like precision, accuracy, cost, and complexity. The design considerations presented here can help developers either in choosing an existing synchronization protocol or in defining a new protocol that is best suited to the specific needs of a sensor-network application. Finally, the survey provides a valuable framework by which designers can compare new and existing synchronization protocols.
7D466127	Recently, a few efficient timing synchronization protocols for wireless sensor networks (WSNs) have been proposed with the goal of maximizing the accuracy and minimizing the power utilization. This paper proposes novel clock skew estimators assuming different delay environments to achieve energy-efficient network-wide synchronization for WSNs. The proposed clock skew correction mechanism significantly increases the re-synchronization period, which is a critical factor in reducing the overall power consumption. The proposed synchronization scheme can be applied to the conventional protocols without additional overheads. Moreover, this paper derives the Cramer-Rao lower bounds and the maximum likelihood estimators under different delay models and assumptions. These analytical metrics serves as good benchmarks for the thus far reported experimental results
7D833076	Since wireless ad-hoc networks use shared communication medium, accesses to the medium must be coordinated to avoid packet collisions. Transmission scheduling algorithms allocate time slots to the nodes of a network such that if the nodes transmit only during the allocated time slots, no collision occurs. For real-time applications, by ensuring deterministic channel access, transmission scheduling algorithms have the added significance of making guarantees on transmission latency possible. In this paper we present a distributed transmission scheduling algorithm for hexagonal wireless ad-hoc networks with a particular focus on Wireless Sensor Networks. Afforded by the techniques of ad-hoc networks topology control, hexagonal meshes enable trivial addressing and routing protocols. Our transmission scheduling algorithm constructs network-wide conflict-free packet transmission schedule for hexagonal networks, where the overhead of schedule construction in terms of message exchanges is zero above and beyond that for topology control and other network control related functions. Furthermore, the schedule is optimal in the sense that the bottleneck node does not idle. We also present an implicit clock synchronization algorithm to facilitate scheduling. We derive the real time capacity of our scheduling algorithm. We present evaluations of our scheduling algorithm in the presence of topological irregularities using simulation.
80C59B4C	Over the last years, large-scale decentralized computer networks such as peer-to-peer and mobile ad hoc networks have become increasingly prevalent. The topologies of many of these networks are often highly dynamic. This is especially true for ad hoc networks formed by mobile wireless devices.In this paper, we study the fundamental problem of clock synchronization in dynamic networks. We show that there is an inherent trade-off between the skew S guaranteed along sufficiently old links and the time needed to guarantee a small skew along new links: for any sufficiently large initial skew on a new link, there are executions in which the time required to reduce the skew on the link to O(S) is at least Ω(n/S).We show that this bound is tight for moderately small values of S. Assuming a fixed set of n nodes, an arbitrary pattern of edge insertions and removals, and a weak dynamic connectivity requirement, we present an algorithm that always maintains a skew of O(n) between any two nodes in the network. For a parameter S=Ω(ρn−−√), where ρ is the maximum hardware clock drift, it is further guaranteed that if a communication link between two nodes u,v persists in the network for Θ(n/S) time, the clock skew between u and v is reduced to no more than O(S).
7FC275BB	We introduce the distributed gradientclock synchronization problem. As in traditional distributed clock synchronization, we consider a network of nodes equipped with hardware clocks with bounded drift. Nodes compute logical clock values based on their hardware clocks and message exchanges, and the goal is to synchronize the nodes' logical clocks as closely as possible, while satisfying certain validity conditions. The new feature of gradient clock synchronization GCS for short) is to require that the skew between any two nodesy' logical clocks be bounded by a nondecreasing function of the uncertainty in message delay (call this the distance) between the two nodes, and other network parameters. That is, we require nearby nodes to be closely synchronized, and allow faraway nodes to be more loosely synchronized. We contrast GCS with traditional clock synchronization, and discuss several practical motivations for GCS, mostly arising in sensor and ad-hoc networks. Our main result is that the worst case clock skew between two nodes at distance d or less from each other is Ω(d + logDloglogD), where D is the diameter of the network. This means that clock synchronization is not a localproperty, in the sense that the clock skew between two nodes depends not only on the distance between the nodes, but also on the size of the network. Our lower bound implies, for example, that the TDMA protocol with a fixed slot granularity will fail as the network grows, even if the maximum degree of each node stays constant.
8022410E	This paper presents a synchronization protocol for real-time multimedia application in wireless networks. A multimedia stream is multicast from one to some mobile terminals (MTs). Source and receivers can both be mobile. The protocol allows MTs to receive a multicast stream without breaks in playback as MTs move from cell to cell. The scheme is general because we make no assumption about clocks synchronization and messages are not time-stamped. However, it is used in a network having bounded delays. Verifications show that synchronization properties are respected. Moreover, simulations show that the strategy of known locations reduces the handoff duration compared to the one of unknown locations and then preserves the synchronization.
5F021DDB	Optical networks are widely regarded as the ultimate solution to the bandwidth needs of future communication systems. With fiber-optic links deployed between nodes, the electronic devices at the switch nodes, rather than the transmission medium, limit the bandwidth of a network. In an optically-switched network, a photonic switch can control connection paths without optical-to-electrical (O-E) conversion of the signal, thereby avoiding speed and data-format restrictions im- posed by such conversions. Despite the recognized potential of optically-switched multiwavelength networks, however, their overall effectiveness remains hampered by problems related to signal impairment such as noise, crosstalk and dispersion ac- cumulating over a transparent path, and a finite number of wavelengths available limiting the network size. Moreover, the absence of O-E conversions prevents their application to certain areas, most notably, packet switches.
7DFBE219	Using active Techniques to measure networks, that is by injecting probe packets, has proved to be quite challenging for properties beyond simple end-to-end delay and loss. Some of the greatest difficulties have resulted from our inability to design techniques robust to multi-hop queueing effects. This difficulty is only compounded by the need to keep measurements non-intrusive, that is to minimally affect ongoing data flows. In this paper, we show that novel network primitives based on hop-dependent priority queueing are very effective in addressing these challenges. By enabling these primitives, network operators can perform a variety of active measurements accurately. Such measurement-friendliness results from many factors including ease of applying fundamentally single-hop methods, better measurement capabilities, and easier clock synchronization. Other advantages of our architecture include ease of deployment, simplicity, low overhead and generality, i.e., no constraints on scheduling policies for data packets. We also discuss the challenges faced, for example, in coping with small but unavoidable inaccuracies and with exposing the primitives to end-users.
7A92DF05	Synchronization and localization are critical challenges for the coherent functioning of a wireless network, which are conventionally solved independently. Recently, various estimators have been proposed for pairwise synchronization between immobile nodes, based on time stamp exchanges via two-way communication. In this paper, we consider a network of mobile nodes for which a novel joint time-range model is presented, treating both unsynchronized clocks and the pairwise distances as a polynomial functions of true time. For a pair of nodes, a least squares solution is proposed for estimating the pairwise range parameters between the nodes, in addition to estimating the clock offsets and clock skews. Extending these pairwise solutions to network-wide ranging and clock synchronization, we present a central data fusion based global least squares algorithm. A unique solution is nonexistent without a constraint on the cost function e.g., a clock reference node. Ergo, a constrained framework is proposed and a new Constrained Cramér-Rao Bound (CCRB) is derived for the joint time-range model. In addition, to alleviate the need for a single clock reference, various clock constraints are presented and their benefits are investigated using the proposed solutions. Simulations are conducted and the algorithms are shown to approach the theoretical limits.
7BDEF745	We describe a new fault-tolerant algorithm for solving a variant of Lamport's clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtaines its local time. The algorithm solves the problem of maintaining closely synchronized local times, assuming that processes' local times are closely synchronized initially. The algorithm is able to tolerate the failure of just under one-third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time and its uncertainty, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the basic alborithm. A similar style algorithm can also be used to achieve synchronization initially.
178CABEE	In this paper, a novel clock synch algorithm for WSN based on Pairwise Broadcast (PB) is investigated. Within the proposed approach, a cluster based time synch topology is constructed by broadcast and two-way message exchange mechanism. In each cluster, several assistant Cluster Heads (CHs) are voted upon the neighborhood of member nodes, resulting in fine-gradient cluster segmentation. On the basis of the topology, an improved clock estimator with unknown fixed delay is addressed. Herein, we induce clock parameters by General Linear Model as well as the fixed delay by Maximum Likelihood Estimation. Moreover, the performance analysis of the estimator is carried out. The simulation results reveal that the proposed algorithm is prominent on synch overhead and that it achieves better performance in terms of synch accuracy.
78F80572	Intermittent connection of wireless links, caused by low duty-cycle radio operation, harsh working environment, movement of sensor nodes, etc., makes clock synchronization a challenging task. Prior synchronization approaches in wireless sensor networks (WSNs) typically require that nodes exchange time messages frequently with the reference clock, which is difficult in networks with low or intermittent connectivity. This poster presents RobSync, a robust design for clock synchronization in intermittent-connected wireless networks. Having recognized that clock skew is highly correlated to the voltage supply, we use the local voltage information as a reference for clock self-calibration, which helps reduce the frequency of time-stamp exchanges. To prevent a misuse of the voltage information, leading to error accumulation, a re-synchronization interval adjustment design is developed to make a trade-off between accuracy and energy consumption. We present the theory behind RobSync, and provide preliminary results by experiments to compare our approach and the recent approach.
77881AE4	We study the problem of clock synchronization in highly dynamic networks, where communication links can appear or disappear at any time. The nodes in the network are equipped with hardware clocks, but the rate of the hardware clocks can vary arbitrarily within specific bounds, and the estimates that nodes can obtain about the clock values of other nodes are inherently inaccurate. Our goal in this setting is to output a logical clock at each node, such that the logical clocks of any two nodes are not too far apart, and nodes that remain close to each other in the network for a long time are better synchronized than distant nodes. This property is called gradient clock synchronization.Gradient clock synchronization has been widely studied in the static setting. We show that the bounds for the static case also apply to our highly dynamic setting: if two nodes remain at distance d from each other for sufficiently long, it is possible to synchronize their clocks to within O(d log(D/d)), where D is the diameter of the network. This is known to be optimal for static networks, and since a static network is a special case of a dynamic network, it is optimal for dynamic networks as well. Furthermore, we show that our algorithm has optimal stabilization time: when a path of length d appears between two nodes, the time required until the skew between the two nodes is reduced to O(d log(D/d)) is O(D), which we prove is optimal.
7F0F7624	A clock synchronization scheme that strikes a balance between hardware and software solutions is proposed. The proposed scheme is a software algorithm that uses minimal additional hardware to achieve reasonably tight synchronization. Unlike other software solutions, the guaranteed worst-cast skews can be made insensitive to the maximum variation of message transit delay in the system. The scheme is particularly suitable for large partially connected distributed systems with topologies that support simple point-to-point broadcast algorithms. Examples of such topologies include the hypercube and the mesh interconnection structures.
79291B39	Circuit emulation service (CES) allows time-division multiplexing (TDM) services (T1/E1 and T3/E3 circuits) to be transparently extended across a packet network. With circuit emulation over IP, for instance, TDM data received from an external device at the edge of an IP network is converted to IP packets, sent through the IP network, passed out of the IP network to its destination, and reassembled into TDM bit stream. Clock synchronization is very important for CES. This paper presents a clock synchronization scheme based on a double exponential filtering technique and a linear process model. The linear process model is used to describe the behaviour of clock synchronization errors between a transmitter and a receiver. In the clock synchronization scheme, the transmitter periodically sends explicit time indications or timestamps to a receiver to enable the receiver to synchronize its local clock to the transmitter's clock. A phase-locked loop (PLL) at the receiver processes the transmitted timestamps to generate timing signal for the receiver. The PLL has a simple implementation and provides both fast responsiveness (i.e. fast acquisition of transmitter frequency at a receiver) and significant jitter reduction in the locked state. Copyright © 2006 John Wiley & Sons, Ltd.
7941F5FC	In large scale of WSNs (wireless sensor networks), a centralized algorithm is not suitable for WSNs. Distributed algorithm has the obvious advantage over traditional time synchronization algorithm. The paper proposes a distributed time synchronization algorithm for WSNs, called SNOWBALL effect time synchronization. In the algorithm, a node needs only to communicate with its neighbor node, synchronizing itself clock based on the neighbor nodes information. The algorithm is a continuous synchronization process. The network will reach balance and clock consistency after repeated algorithm iterations. In the algorithm, we achieve network energy optimization by reducing unnecessary message exchange.
7CE46771	We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the first known solution that achieves optimal accuracy—the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.
7FD2FE79	We show that CSMA is able to spontaneously synchronize transmissions in a wireless network with constant-size packets, and that this property can be used to devise efficient synchronized CSMA scheduling mechanisms without message passing. Using tools from queuing theory, we prove that for any connected wireless networks with arbitrary interference constraints, it is possible to implement self-synchronizing TDMA schedules without any explicit message passing or clock synchronization besides transmitting the original data packets, and the interaction can be fully local in that each node decides when to transmit next only by overhearing its neighbors' transmissions. We also provide a necessary and sufficient condition on the emergence of self-synchronization for a given TDMA schedule, and prove that such conditions for self-synchronization can be checked in a finite number of steps for a finite network topology.
7F7D7BBD	The development of tiny, low-cost, low-power and multifunctional sensor nodes equipped with sensing, data processing, and communicating components, have been made possible by the recent advances in micro-electro-mechanical systems (MEMS) technology. Wireless sensor networks (WSNs) assume a collection of such tiny sensing devices connected wirelessly and which are used to observe and monitor a variety of phenomena in the real physical world. Many applications based on these WSNs assume local clocks at each sensor node that need to be synchronized to a common notion of time. This paper reviews the existing clock synchronization protocols for WSNs and the methods of estimating clock offset and clock skew in the most representative clock synchronization protocols for WSNs.
756A3BB8	The problem of achieving optimal clock synchronization in a communication network with arbitrary topology and perfect clocks (that do not drift) is studied. Clock synchronization algorithms are presented for a large family of delay assumptions. Our algorithms are modular and consist of three major components. The first component holds for any type of delay assumptions; the second component holds for a large, natural family of local delay assumptions; the third component must be tailored for each specific delay assumption.Optimal clock synchronization algorithms are derived for several types of delay assumptions by appropriately tuning the third component. The delay assumptions include lower and upper delay bounds, no bounds at all, and bounds on the difference of the delay in opposite directions. In addition, our model handles systems where some processors are connected by broadcast networks in which every message arrives at all the processors at approximately the same time. A composition theorem allows combinations of different assumptions for different links or even for the same link; such mixtures are common in practice.Our results achieve the best possible precision in each execution. This notion of optimality is stronger than the more common notion of worst-case optimality. The new notion of optimality applies to systems where the worst-case behavior of any clock synchronization algorithm is inherently unbounded. 
7868EEE7	This paper describes an experiment designed to evaluate the accuracy of one-way clock synchronization using geostationary satellites with the propagation delays calculated from the satellite's orbital elements. Propagation delays from a ground transmitter via satellite to each of five locations in the North and South American continents were measured and compared with the calculated values. Three months of data are presented along with descriptions of the equipment, timing signal format, and methods for delay calculation and time recovery. The results show that within two weeks of epoch for the orbital elements, clocks can be synchronized to 150 ??s using the Tactical Communications Satellite (TACSAT). If one of the observers of the timing signals was already synchronized to the master clock, his delay measurement could improve the results for TACSAT to 75 ??s. By the same method and within 12 hours of epoch, the results for the Lincoln Experimental Satellite-6 (LES-6) indicated that synchronization to 25 ??s was possible.
76E618BA	We present a novel clock synchronization algorithm and prove tight upper and lower bounds on the worst-case clock skew that may occur between any two participants in any given distributed system. More importantly, the worst-case clock skew between neighboring nodes is (asymptotically) at most a factor of two larger than the best possible bound. While previous results solely focused on the dependency of the skew bounds on the network diameter, we prove that our techniques are optimal also with respect to the maximum clock drift, the uncertainty in message delays, and the imposed bounds on the clock rates. The presented results all hold in a general model where both the clock drifts and the message delays may vary arbitrarily within pre-specified bounds.Furthermore, our algorithm exhibits a number of other highly desirable properties. First, the algorithm ensures that the clock values remain in an affine linear envelope of real time. A better worst-case bound on the accuracy with respect to real time cannot be achieved in the absence of an external timer. Second, the algorithm minimizes the number and size of messages that need to be exchanged in a given time period. Moreover, only a small number of bits must be stored locally for each neighbor. Finally, our algorithm can easily be adapted for a variety of other prominent synchronization models.
63A0B5A5	We consider the problem of clock synchronization in a wireless setting where processors must minimize the number of times their radios are used to save energy. Energy efficiency is a central goal in wireless networks, especially if energy resources are severely limited, as occurs in sensor and ad hoc networks, and in many other settings. The problem of clock synchronization is fundamental and intensively studied in the field of distributed algorithms. In the current setting, the problem is to synchronize clocks of m processors that wake up in arbitrary time points, such that the maximum difference between wake-up times is bounded by a positive integer n. (Time intervals are appropriately discretized to allow communication of all processors that are awake in the same discrete time unit.) Currently, the best-known results for synchronization for single-hop networks of m processors is a randomized algorithm due to Bradonjic et al. [2009] of O(√n/m ⋅ poly-log(n)) radio use times per processor, and a lower bound of Ω (√n/m). The main open question left in their work is to close the poly-log gap between the upper and the lower bound, and to derandomize their probabilistic construction and eliminate error probability. This is exactly what we do in this article. That is, we show a deterministic algorithm with radio use of Θ (√n/m), which exactly matches the lower bound proven in Bradonjic et al. [2009] to a small multiplicative constant. Therefore, our algorithm is optimal in terms of energy efficiency and completely resolves a long sequence of works in this area [Bradonjic et al. 2009; Moscribroda et al. 2006; McGlynn and Borbash 2001; Polastre et al. 2004]. Moreover, our algorithm is optimal in terms of running time as well. To achieve these results, we devise a novel adaptive technique that determines the times when devices power their radios on and off. This technique may be of independent interest.In addition, we prove several lower bounds on the energy efficiency of algorithms for multihop networks. Specifically, we show that any algorithm for multihop networks must have radio use of Ω (√n) per processor. Our lower bounds hold even for specific kinds of networks, such as networks modeled by unit disk graphs and highly connected graphs. Our results imply that the simple deterministic algorithm devised for two-processor networks in Bradonjic et al. [2009] with efficiency O(√n) can be used in multihop networks, and it is the most efficient solution in terms of energy use.
78DA4353	It is known that clock synchronization can be achieved in the presence of faulty processors as long as the nonfaulty processors are connected, provided that some authentication technique is used. Without authentication the number of faults that can be tolerated has been an open question. Here we show that if we restrict logical clocks to running within some linear functions of real time, then clock synchronization is impossible without authentication when one-third or more of the processors are faulty. We also provide a lower bound on the closeness to which simultaneity can be achieved in the network as a function of the transmission and processing delay properties of the network.
7641A8AA	We consider the problem of synchronizing the clocks of processors in a failure-free distributed system when the hardware clocks do not drift but there is uncertainty in the message delays. Our goal is to develop closed form expressions for how closely the clocks can be synchronized. For an arbitrary undirected topology with arbitrary (symmetric) uncertainties, we prove a lower bound of diam/2 on the closeness of synchronization achievable, where diam is the diameter of the graph when the edges are weighted with the uncertainties. We then consider the class of topologies described as k-ary m-cubes, both with and without wrap-around. We assume that every edge has the same uncertainty, u. For the k-ary m-cube without wrap-around, we show that our lower bound diam/2 = um(k-1)/2 is tight, by analyzing the synchronization achieved by a simple algorithm. Since chains, meshes, and hypercubes are special cases of this topology, we have tight bounds for them. For the k-ary m-cube with wrap-around, we show using the same algorithm that our lower bound diam/2 = u*m*floor(k/2)/2 is tight when k is even and is almost tight when k is odd. This result implies tight or almost tight bounds for rings and tori. 
5C270BFF	CPM waveforms, which are documented in MIL-STD-188-181B, are typically demodulated in superheterodyne receivers. This paper provides an outline for an alternate method of demodulation. The pulse count demodulation scheme is typically used as a method for demodulation of FM voice signals. The pulse count demodulator does not require fast A/D converters or automatic gain control anywhere in the receive path of the radio. Thus, the receive radio architecture can be designed without these components, decreasing the power requirements and size of the RF portion of the receiver. This paper investigates the performance characteristics of the CPM signal as demodulated through the pulse count demodulator. Some analysis is performed on the extent of the limitations in amplitude and phase accuracy and resolution. The design of the PCD is discussed and this paper provides the simulated results that represent the ultimate performance of the CPM waveform as demodulated through the pulse count demodulator.
77B78231	We present an improved and optimized formulation for the estimation of the multiscale amplitude-modulation frequency-modulation (AM-FM) estimates when (i) non-separable filters are used and (ii) the variable spacing, local linear phase method is used. Also, we introduce the use of multiscale directional filterbanks for the feature extraction of images. Recently, AM-FM methods have shown promising results in a variety of medical image analysis applications. The 2D optimized AM-FM demodulation described here presents advantages for feature extraction at different frequency scales and orientations that can be used to detect different patterns, directions, or structures in an image. We test the new formulation using a Gaussian amplitude-modulated Quadratic frequency-modulated synthetic image and natural images. The results show that the optimized estimation produces better results, up to 4.9 times for the IF estimation and in 3 orders of magnitude for the IA estimation, for noise-free signals compared to the state-of-the-art methods.
7EDAABCF	The problem of optimum demodulation of PM and FM signals when the phase modulator employed in the transmitter of the communication system exhibits saturation effects is studied. Quasi-optimum receivers with zero and finite lag are designed by making use of the state variable techniques. Performance results are presented graphically for a class of message signals, and it turns out that the effect of saturation in the phase modulator on the average performance of the system is essentially to reduce the modulation index.
7BB04FC7	In this paper, a measurement method for DTMB modulator which performs the estimation of timing and frequency offsets, channel impulse response (CIR), as well as modulation error ratio (MER) by using only two consecutive frames in an iterative manner is proposed to precisely measure the modulator performance in short time. Both simulation and lab testing results show that the measurement method could reflect the performance of DTMB modulator with high accuracy.
7E3A5596	We develop a mathematical framework for quantifying and understanding multidimensional frequency modulations in digital images. We begin with the widely accepted definition of the instantaneous frequency vector (IF) as the gradient of the phase and define the instantaneous frequency gradient tensor (IFGT) as the tensor of component derivatives of the IF vector. Frequency modulation bounds are derived and interpreted in terms of the eigendecomposition of the IFGT. Using the IFGT, we derive the ordinary differential equations (ODEs) that describe image flowlines. We study the diagonalization of the ODEs of multidimensional frequency modulation on the IFGT eigenvector coordinate system and suggest that separable transforms can be computed along these coordinates. We illustrate these new methods of image pattern analysis on textured and fingerprint images. We envision that this work will find value in applications involving the analysis of image textures that are nonstationary yet exhibit local regularity. Examples of such textures abound in nature
7FB42927	Error diffusion halftoning is a popular method of producing frequency modulated (FM) halftones for printing and display. FM halftoning fixes the dot size (e.g., to one pixel in conventional error diffusion) and varies the dot frequency according to the intensity of the original grayscale image. We generalize error diffusion to produce FM halftones with user-controlled dot size and shape by using block quantization and block filtering. As a key application, we show how block-error diffusion may be applied to embed information in hardcopy using dot shape modulation. We enable the encoding and subsequent decoding of information embedded in the hardcopy version of continuous-tone base images. The encoding-decoding process is modeled by robust data transmission through a noisy print-scan channel that is explicitly modeled. We refer to the encoded printed version as an image barcode due to its high information capacity that differentiates it from common hardcopy watermarks. The encoding/halftoning strategy is based on a modified version of block-error diffusion. Encoder stability, image quality versus information capacity tradeoffs, and decoding issues with and without explicit knowledge of the base image are discussed
752793FB	Phase Generated Carrier (PGC) with directly Frequency Modulation (FM) is one of the most important demodulation methods for optical fiber interferometric sensor system. Previous research has confirmed that system performance using Orthogonal Demodulation Type PGC (ODT-PGC) method is determined by many parameters, such as signal phase delay, FM depth, laser intensity accompanying modulation. This article proposes a new PGC demodulation method based on Fixed Phase Delay (FPD-PGC) by 3x2 directional coupler, using second-harmonic components of two interferometric signals to demodulate. The demodulation principle of the new method is described in detail and its performances have been studied. Theoretical analysis and experimental results show that the new method combines main advantages of directional coupler method and ODT-PGC method, and eliminates, to a great extent, the impacts of FM depth, signal phase delay, intensity modulation. Signal-to-total-Harmonic Ratio (SHR) of new method increases more than 30dB compare with ODT-PGC method under the condition of intensity modulation coefficient is 0.4. Besides that, Signal to Noise Ratio (SNR) also improves significantly.
8019E2A9	We propose a digital receiver for FM subcarrier systems including the radio broadcast data systems (RBDS) and the radio data systems (RDS). The RBDS receiver is demodulating the 57 kHz modulated digital signal to the baseband RBDS digital signal. It contains carrier Costas loop synchronization techniques, a symbol timing recovery and decoding techniques, and an RBDS decoder. The RBDS decoder is used to recover the digital signal to the information data and sent to the output of the RBDS system. The main functions in the RBDS decoder are block error detection/correction, block synchronization, and block type detection.
811A640A	In this paper, a measurement method for DTMB modulator which performs the estimation of timing and frequency offsets, channel impulse response (CIR), as well as modulation error ratio (MER) by using only two consecutive frames in an iterative manner is proposed to precisely measure the modulator performance in short time. Both simulation and lab testing results show that the measurement method could reflect the performance of DTMB modulator with high accuracy.
7F884997	Discrete time state-space models for multiple source frequency modulated communications environments are developed and coupled digital phase-locked loop (CDPLL) estimator structures are derived based on the extended Kalman filter (EKF). Unlike related efforts found in the literature, the receiver presented includes both passband and baseband coupling between the DPLLs. Separability of two sources is investigated by examination of state observability and by simulations of the derived estimators. The relationship between the EKF and the CDPLL is presented and their linear and nonlinear behavior discussed. As the CDPLLs exhibit chaotic behavior during acquisition and cannot be linearized during tracking, acquisition and tracking characteristics are extracted from simulations. One simulation result is the effect of state observability on the coupling between the DPLLs; when the states are “strongly” observable, the states are not as tightly coupled. It is also shown that the loop bandwidths decrease during periods of “weak” observability.
80F9FEF7	Many motion-compensated image reconstruction (MCIR) methods have been proposed to correct for subject motion in medical imaging. MCIR methods incorporate motion models to improve image quality by reducing motion artifacts and noise. This paper analyzes the spatial resolution properties of MCIR methods and shows that nonrigid local motion can lead to nonuniform and anisotropic spatial resolution for conventional quadratic regularizers. This undesirable property is akin to the known effects of interactions between heteroscedastic log-likelihoods (e.g., Poisson likelihood) and quadratic regularizers. This effect may lead to quantification errors in small or narrow structures (such as small lesions or rings) of reconstructed images. This paper proposes novel spatial regularization design methods for three different MCIR methods that account for known nonrigid motion. We develop MCIR regularization designs that provide approximately uniform and isotropic spatial resolution and that match a user-specified target spatial resolution. Two-dimensional PET simulations demonstrate the performance and benefits of the proposed spatial regularization design methods.
7DAE9282	An algorithm for the separation and energy-based demodulation of two-component mixtures of AM-FM signals is presented. The proposed algorithm is based on the generating differential or difference equation of the mixture signal and nonlinear differential energy operators.
7F374D53	We propose the use of Amplitude-Modulation Frequency-Modulation (AM-FM) features for representing and retrieving X-Ray images with pneumoconiosis. The AM-FM features are estimated using multiscale filterbanks with wave-lengths related with the standard sizes for grading the level of opacities in X-Rays. The extracted AM-FM features represent opacity profusion in terms of instantaneous frequency (IF) and instantaneous amplitude (IA) features. Here, IF estimates in the medium and high scale frequencies can be used to capture early disease symptoms. AM-FM features from the low and medium scale frequencies are associated with advanced disease stages. We demonstrate the performance of the system in X-ray image retrieval and classification applications.
809138D8	It is shown that the nonlinear energy-tracking signal operator Psi (x)=(dx/dt)/sup 2/-xd/sup 2/x/dt/sup 2/ and its discrete-time counterpart can estimate the AM and FM modulating signals. Specifically, Psi can approximately estimate the amplitude envelope of AM signals and the instantaneous frequency of FM signals. Bounds are derived for the approximation errors, which are negligible under general realistic conditions. These results, coupled with the simplicity of Psi , establish the usefulness of the energy operator for AM and FM signal demodulation. These ideas are then extended to a more general class of signals that are sine waves with a time-varying amplitude and frequency and thus contain both an AM and an FM component; for such signals it is shown that Psi can approximately track the product of their amplitude envelope and their instantaneous frequency. The theoretical analysis is done for both continuous- and discrete-time signals.
7EFC98DF	Multidimensional amplitude-modulation frequency-modulation (AM-FM) models allow us to describe continuous-scale modulations in digital images. AM-FM models have led to a wide range of applications ranging from image and video compression, video image segmentation, to image retrieval in digital libraries. We present new, two-dimensional algorithms that provide significant improvements in both accuracy and speed over previously reported non-parametric approaches. Results are shown for both real and synthetic images.
786B632E	A two-dimensional (2-D) inverse synthetic aperture radar (ISAR) return signal model that employs stepped frequency (SF) modulation is developed. The geometry of the examined ISAR scenario is described by analytical geometrical equations. The target to be imaged is represented by a rectangular grid of point scatterers, moving along a rectilinear trajectory at constant speed, without any rotational motion. Thus, the inverse synthetic aperture results from the translational motion of the target for a short period of time. The process of ISAR signal modelling through coherent summation of the SF-modulated signals reflected from different point scatterers of the target is thoroughly described. Moreover, an efficient ISAR image reconstruction approach, including cross-correlation-based range compression and fast-Fourier-transform-based azimuth compression, is presented through analytical mathematical expressions. Numerical simulations are carried out for various SF ISAR scenarios and high-resolution ISAR images are obtained by applying the proposed ISAR image reconstruction approach. Simulation results (ISAR images and corresponding entropy values) indicate the validity of the proposed 2-D SF ISAR return signal model and the efficiency of the proposed imaging algorithms. Finally, a numerical simulation result is illustrated, which shows the comparison of the performance of the proposed ISAR image reconstruction algorithms based on SF and linear frequency modulation waveforms. It is shown that the two waveforms attain almost the same ISAR image resolution.
7870F50A	A new fault diagnosis method based on empirical mode decomposition (EMD) and homomorphic filtering demodulation is proposed for rolling bearing. The vibration signal of fault rolling bearing is decomposed into a series of intrinsic mode functions (IMFs) by EMD, then extract the envelopes from the outstanding IMFs with various fault characteristic information by homomorphic filtering demodulation and Hilbert envelope demodulation, and do the comparison analysis. The research results show that homomorphic filtering demodulation is superior to Hilbert envelope demodulation, and the combination of EMD and homomorphic filtering demodulation is an effective approach for rolling bearing fault diagnosis.
79EBCADB	In this paper, a new spectral precoding scheme is developed for constant-envelope orthogonal frequency-division multiplexing (CE-OFDM) signals to provide very small power spectral sidelobes. It is shown that the proposed spectrally precoded CE-OFDM scheme can provide higher spectral compactness than previously designed spectrally precoded CE-OFDM block scheme and continuous-phase CE-OFDM scheme.
7DD853D5	Position-sensitive detectors (PSDs), or lateral-effect photodiodes, are commonly used for high-speed, high-resolution optical position measurement. This paper describes the instrument design for multidimensional position and orientation measurement based on the simultaneous position measurement of multiple modulated sources using frequency-domain-multiplexed (FDM) PSDs. The important advantages of this optical configuration in comparison with laser/mirror combinations are that it has a large angular measurement range and allows the use of a probe that is small in comparison with the measurement volume. We review PSD characteristics and quantitative resolution limits, consider the lock-in amplifier measurement system as a communication link, discuss the application of FDM to PSDs, and make comparisons with time-domain techniques. We consider the phase-sensitive detector as a multirate DSP problem, explore parallels with Fourier spectral estimation and filter banks, discuss how to choose the modulation frequencies and sample rates that maximize channel isolation under design constraints, and describe efficient digital implementation. We also discuss hardware design considerations, sensor calibration, probe construction and calibration, and 3-D measurement by triangulation using two sensors. As an example, we characterize the resolution, speed, and accuracy of an instrument that measures the position and orientation of a 10 mm times 5 mm probe in 5 degrees of freedom (DOF) over a 30-mm cube with 4-mum peak-to-peak resolution at 1-kHz sampling.
75EB8088	To have higher resolution of distance in the laser scanner using the phase demodulation method, to demodulate using an intermediate frequency is advantageous. This method is called the multiple phase demodulation method. In the multiple phase demodulation method several frequencies are used for signal processing. These signals are made from the oscillator and the programmable clock source. Even though the clock source is well synchronized with the oscillator, the initial phase error problem can be occurred since the operating points of both signal sources are not exactly simultaneous. In this work, the experimental results are presented to show how the initial phase error problem is solved.
7F6A882D	In wireless sensor networks (WSN), location information acquisition is critical to guarantee their performance. This paper presents a new positioning method in WSN with high precision at reasonable implementation cost for 3D case. Reference nodes with known locations transmit linear frequency modulation continuous wave (FMCW), while other sensor nodes estimate the range difference to them based on the received signals' frequency difference, called time frequency difference arrival (TFDA). The location information can be obtained by solving a set of hyperbolic equations. Two different positioning methods: Taylor iterative method and Chan's method are inspected and compared in terms of accuracy, constraints and computational complexity. This proposed technique is cost-effective, scalable and easy to implement. The simulation results show that the new method enjoys high precision.
7E99AABF	The Teager-Kaiser energy operator and the related energy separation algorithm (ESA) find numerous applications in various problems related to monocomponent AM-FM demodulation. This energy-operator-based approach, however, applies only to signals with narrowband frequency content, i.e., the information bandwidth and frequency deviation about the carrier are small relative to the carrier frequency. For signals with large frequency deviation, modulation indices, this approximation fails, and the ESA incurs large frequency/amplitude demodulation errors. In this letter, we develop a generalized energy-operator-based approach that uses frequency transformations derived from multirate operations such as decimation/interpolation and heterodyning. This generalized approach is shown to produce significant reduction of the demodulation error over the conventional ESA, particularly where the modulation index or frequency deviation is large.
7FFE99D0	This report discusses heterodyne holographic interferometry and time-average holography with a frequency shifted reference beam. Both methods will be used for the measurement and visualization of internal transonic flows, where the target facility is a flutter cascade. The background and experimental requirements for both methods are reviewed. Measurements using heterodyne holographic interferometry are presented. The performance of the laser required for time-average holography of time-varying transonic flows is discussed. 
7EE520BE	In this paper, we investigate the problem of choosing the best domain for embedding watermarks in digital still images. During this paper, twelve watermarking techniques were implemented and evaluated. Four techniques were selected to represent different approaches of embedding data in spatial domain, three approaches using discrete cosine transform (DCT) domain, two approaches using discrete wavelet transform domain (DWT) domain, and two combined DWT-DCT techniques. The algorithms were chosen to represent a range of computational complexities and implementation structures. The performance of the selected algorithms was evaluated with respect to many perceptibility parameters.
7E84C3E6	Two methods of vestigial sideband (VSB) realization, the staggered modulation (SM) and cosine modulation (CM), are analyzed. It is shown that if the SM method is used to eliminate the frequency spectral redundancy, the modulation error rate (MER) performance is lower than CM's. However, the CM scheme is considered to be higher computational complexity. To resolve the existing drawbacks, the improved staggering modulation (ISM) method based on staggered quadrature amplitude modulated (SQAM) is proposed. Utilizing the interpolation filter instead of the time shifting filter, ISM approach simplifies the complexity of modulation without loss of the MER performance.
78962906	An improved demodulation method by solving a quadratic equation was proposed for the phase generated carrier (PGC) scheme with frequency modulation to suppress the impact of laser intensity modulation (LIM). The influence of LIM on total harmonic distortion (THD) was analyzed for PGC algorithm considering laser intensity modulation coefficient (LIMC), signal amplitude, initial phase and modulation depth. According to our analysis, the maximum THD of PGC algorithm is more than -24dB when LIMC is 0.1. While the maximum THD is less than -65dB by using the improved method when LIMC is less than 0.5, which is limited by the low-pass filter. For an experiment system with 0.285 LIMC, the THD was -9dB for PGC algorithm. By using the improved method, the THD approached -50dB
8000711F	This paper presents a computer-aided diagnostic (CAD) system for analyzing chest radiographs based on the International Labor Organization (ILO) standards. We introduce an amplitude-modulation frequency-modulation (AM-FM) based methodology by which a computer-based system will extract AM-FM features and detect those with suspected interstitial lung diseases. For classification, we use Partial Least Squares (PLS) using a low number of extracted factors (making the system robust). We consider several different AM-FM classifiers based on extracting features from individual scales as well as a final classifier that combines results from the individuals scales. We validate our methodology on 11 standard images graded according to the ILO standard. For several scales, as well as for the combined classifier that uses information from all scales, we get excellent classification results (area under the receiver operator characteristics curve equal to 1.0) using a limited number of latent PLS factors.
7ABD9C76	We propose a modular architecture for long-haul optical networks supporting flexible-bandwidth superchannels. Colorless transceivers can be designed to modulate/detect up to M = 4 subcarriers, each at a symbol rate of 12.5 Gbaud, achieving a maximum bit rate of 200 Gbit/s, assuming polarization-multiplexed quadrature phase-shift keying (PM-QPSK). A set of N synchronized transceivers can cooperate to modulate/detect a superchannel comprising N·M subcarriers using no-guard-interval orthogonal frequency-division multiplexing, enabling transmission at bit rates beyond 1 Tbit/s. We analyze and simulate the performance of the proposed architecture in the presence of linear fiber impairments and synchronization errors and establish design requirements for practical deployment of the architecture. Simulation results are shown for transmission of a superchannel comprising 24 subcarriers, which conveys approximately 1.1 Tbit/s with a spectral efficiency of 3.5 bits/s/Hz using PM-QPSK.
7D0C33EE	In this paper, a novel adaptive digital image watermarking model based on modified Fuzzy C-means clustering is proposed. For watermark embedding process, we used Discrete Wavelet Transform (DWT). A segmentation technique XieBeni integrated Fuzzy C-means clustering (XFCM) is used to identify the segments of original image to expose suitable locations for embedding watermark. We also pre-processed the host image using Particle Swarm Optimization (PSO) to lend a hand to the clustering process. The goal is to focus on proper segmentation of the image so that the embedded watermark can withstand common image processing attacks and provide security to digital images. Several attacks were performed on the watermarked images and original watermark was extracted. Performance measures like PSNR, MSE, CC were computed to test the extracted watermarks with and without attacks. Experimental results show that the proposed scheme has performed well in terms of imperceptibility and robustness when compared to other watermarking models.
7D712204	We develop new amplitude-modulated frequency-modulated (AM-FM) based methods to address some issues associated with the semantic gap between visual and mathematical features presented by retinal diseases such as age-related macular degeneration (AMD). Through the processing of simulated and real, clinical retinal images we gain an understanding of the effects of basic morphological characteristics of lesions associated with AMD. Through synthetic simulations, we discuss how histograms of the instantaneous amplitude and the instantaneous frequency magnitude, extracted from different scales, can be used to differentiate between images of different sizes and edge sharpness, while maintaining invariance with respect to rotations. We show that AM-FM features extracted from low and very-low frequency scales can clearly differentiate between retinal images containing Temporal Concentrated Drusen (TCD) and Geographic Atrophy (GA). Shape, size, distribution and edge sharpness are visual features used by ophthalmologists in identifying lesions such as drusen. We propose the use of new AM-FM derived features to quantitatively define these visual descriptions.
7914A0A6	Previously investigated multicomponent AM-FM demodulation techniques either assume that the individual component signals are spectrally isolated from each other or that the components can be isolated by linear time-invariant filtering techniques and, consequently, break down in the case where the components overlap spectrally or when one of the components is stronger than the other. In this paper, we present a nonlinear algorithm for the separation and demodulation of discrete-time multicomponent AM-FM signals. Our approach divides the demodulation problem into two independent tasks: algebraic separation of the components based on periodicity assumptions and then monocomponent demodulation of each component by instantaneously tracking and separating its source energy into its amplitude and frequency parts. The proposed new algorithm avoids the shortcomings of previous approaches and works well for extremely small spectral separations of the components and for a wide range of relative amplitude/power ratios. We present its theoretical analysis and experimental results and outline its application to demodulation of cochannel FM voice signals.
7D7D5BA3	Video communication is often afflicted by various forms of losses, such as packet loss over the Internet. This paper examines the question of whether the packet loss pattern, and in particular, the burst length, is important for accurately estimating the expected mean-squared error distortion resulting from packet loss of compressed video. We focus on the challenging case of low-bit-rate video where each P-frame typically fits within a single packet. Specifically, we: 1) verify that the loss pattern does have a significant effect on the resulting distortion; 2) explain why a loss pattern, for example a burst loss, generally produces a larger distortion than an equal number of isolated losses; and 3) propose a model that accurately estimates the expected distortion by explicitly accounting for the loss pattern, inter-frame error propagation, and the correlation between error frames. The accuracy of the proposed model is validated with H.264/AVC coded video and previous frame concealment, where for most sequences the total distortion is predicted to within plusmn0.3 dB for burst loss of length two packets, as compared to prior models which underestimate the distortion by about 1.5 dB. Furthermore, as the burst length increases, our prediction is within plusmn0.7 dB, while prior models degrade and underestimate the distortion by over 3 dB. The proposed model works well for video-telephony-type of sequences with low to medium motion. We also present a simple illustrative example, of how knowledge of the effect of burst loss can be used to adapt the schedule of video streaming to provide improved performance for a burst loss channel, without requiring an increase in bit rate.
7FA170FD	We consider the problem of error control for receiver-driven layered multicast of audio and video over the Internet. The sender injects into the network multiple source layers and multiple channel coding (parity) layers, some of which are delayed relative to the source, Each receiver subscribes to the number of source layers and the number of parity layers that optimizes the receiver's quality for its available bandwidth and packet loss probability. We augment this layered FEC system with layered pseudo-ARQ. Although feedback is normally problematic in broadcast situations, ARQ can be simulated by having the receivers subscribe and unsubscribe to the delayed parity layers to receive missing information. This pseudo-ARQ scheme avoids an implosion of repeat requests at the sender and is scalable to an unlimited number of receivers, We show gains of 4-18 dB on channels with 20% loss over systems without error control and additional gains of 1-13 dB when FEC is augmented by pseudo-ARQ in a hybrid system, Optimal error control in the hybrid system is achieved by an optimal policy for a Markov decision process.
7FE46870	The quality of service limitation of today's best-effort networks poses major challenge for low-latency video communication. To combat network losses for real-time and on-demand video communication, which exhibits stronger dependency across packets, a network-adaptive coding scheme is employed to dynamically manage the packet dependency using optimal reference picture selection. The selection of the reference is achieved within a rate-distortion optimization framework and is adapted to the varying network conditions. For network-adaptive streaming of prestored video, based on an accurate loss-distortion model, a prescient scheme that optimizes the dependency of a group of packets is proposed to achieve global optimality as well as improved rate-distortion performance. With the improved trade-off between compression efficiency and error resilience, the proposed system does not require retransmission of lost packets, which makes less than one-second low-latency communication possible.
773F1213	We consider an unconventional procedure for communicating to the server the receipt of media packets for Internet video streaming. Instead of separately acknowledging each media packet as it arrives, we periodically send to the server a single acknowledgment packet, denoted rich acknowledgment, that contains information about all media packets that have arrived at the client by the time the rich acknowledgment is sent. We investigate rate-distortion optimized sender-driven streaming that employs rich acknowledgments. Performance gains of up to 1.3 dB for streaming packetized video content are observed over rate-distortion optimized senderdriven systems that employ conventional acknowledgments. 
8123EE44	This paper addresses the problem of streaming packetized media over a lossy packet network in a rate-distortion optimized way. We show that although the data units in a media presentation generally depend on each other according to a directed acyclic graph, the problem of rate-distortion optimized streaming of an entire presentation can be reduced to the problem of error-cost optimized transmission of an isolated data unit. We show how to solve the latter problem in a variety of scenarios, including the important common scenario of sender-driven streaming with feedback over a best-effort network, which we couch in the framework of Markov decision processes. We derive a fast practical algorithm for nearly optimal streaming in this scenario, and we derive a general purpose iterative descent algorithm for locally optimal streaming in arbitrary scenarios. Experimental results show that systems based on our algorithms have steady-state gains of 2-6 dB or more over systems that are not rate-distortion optimized. Furthermore, our systems essentially achieve the best possible performance: the operational distortion-rate function of the source at the capacity of the packet erasure channel.
764E3831	The directional intra prediction (IP) in H.264/AVC and HEVC tends to cause the residue to be anisotropic. To transform the IP residue, Mode Dependent Directional Transform (MDDT) based on Karhunen Loève transform (KLT) can achieve better energy compaction than DCT, with one transform assigned to each prediction mode. However, due to the data variation, different residue blocks with the same IP mode may not have the same statistical properties. Instead of constraining one transform for each IP mode, in this paper, we propose a novel rate-distortion optimized transform (RDOT) scheme which allows a set of specially trained transforms to be available to all modes, and each block can choose its preferred transform to minimize the rate-distortion (RD) cost. We define a cost function which is an estimate of the true RD cost and use the Lloyd-type algorithm (a sequence of transform optimization and data reclassification alternately) to find the optimal set of transforms. The proposed RDOT scheme is implemented in HM9.0 software of HEVC. Experimental results suggest that RDOT effectively achieves 1.6% BD-Rate reduction under the Intra Main condition and 1.6% BD-Rate reduction under the Intra High Efficiency (HE) 10bit condition.
78CFB32A	We consider rate-distortion optimized strategies for dropping frames from multiple conversational and streaming videos sharing limited network node resources. The dropping strategies are based on side information that is extracted during encoding and is sent along the regular bitstream. The additional transmission overhead and the computational complexity of the proposed frame dropping schemes are analyzed. Our experimental results show that a significant improvement in end-to-end performance is achieved compared to priority-based random early dropping.
0319D9B8	The general problem of optimized video encoding has received a great deal of attention in recent years. This paper focuses on the optimization of video coding with frameskip. We propose models that estimate the distortion for coded frames as well as non-coded frames. Using these models in conjunction with well-know models that estimate the rate allows us to formulate a rate control problem that trades-off spatial and temporal quality. Simulation results indicate moderate improvements for low motion test sequences.
7EA6573E	Motion information scalability is an important requirement for a fully scalable video codec, especially in low bit rate or small resolution decoding scenarios, for which the fully scalable motion model (SMM) has been proposed. SMM can collaborate flawlessly with other scalabilities, such as spatial, temporal and quality, in a scalable video codec. It performs better than the nonscalable motion model. To further improve the SMM, this correspondence extends the algorithm to support the hierarchical B frame structure and bidirectional or multidirectional motion estimation. Furthermore, the corresponding rate distortion optimized estimation for improved efficiency in several scenarios is discussed. Several simulation results based upon the updated framework are presented to verify the advantage of this extension.
591D1690	In this paper, we discuss how to apply the rate distortion technique to select the optimal mode in the scalable coding. Firstly, we analyze this problem from a general scalable model and point out that the complicated dependencies among the different macroblocks and layers make the original independency assumption no longer a right approximation. Secondly, we propose an EOD function to estimate this dependency and derive a simple formula of this function. We apply the proposed algorithm to the H.26L PFGS, and the experimental results show that the algorithm significantly improves the coding efficiency of the H.26L PFGS. Further studies on how to design the EOD function more accurately is quite interesting and significant.
808A4297	We describe an effective method for increasing error resilience of video transmission over bit error prone networks. Rate-distortion optimized mode selection and synchronization marker insertion algorithms are introduced. The resulting video communication system takes into account the channel condition and the error concealment method used by the decoder, to optimize video coding mode selection and placement of synchronization markers in the compressed bit stream. The effects of mismatch between the parameters used by the encoder and the parameters associated with the actual channel condition and the decoder error concealment method are evaluated. Results for the binary symmetric channel and wideband code division multiple access mobile network models are presented in order to illustrate the advantages of the proposed method.
80EF36FE	Video distortion metrics based on models of the human visual system have traditionally used comparisons between the distorted signal and a reference signal to calculate distortions objectively. In video coding applications, this is not prohibitive. In quality monitoring applications, however, access to the reference signal is often limited. This paper presents a computationally efficient video distortion metric that can operate in full- or reduced-reference mode as required. The metric is based on a model of the human visual system implemented using the wavelet transform and separable filters. The visual model is parameterized using a set of video frames and the associated quality scores. The visual model's hierarchical structure, as well as the limited impact of fine scale distortions on quality judgments of severely impaired video, are exploited to build a framework for scaling the bitrate required to represent the reference signal. Two applications of the metric are also presented. In the first, the metric is used as the distortion measure in a rate-distortion optimized rate control algorithm for MPEG-2 video compression. The resulting compressed video sequences demonstrate significant improvements in visual quality over compressed sequences with allocations determined by the TM5 rate control algorithm operating with MPEG-2 at the same rate. In the second, the metric is used to estimate time series of objective quality scores for distorted video sequences using reference bitrates as low as 10 kb/s. The resulting quality scores more accurately model subjective quality recordings than do those estimated using the mean squared error as a distortion metric, while requiring a fraction of the bitrate used to represent the reference signal. The reduced-reference metric's performance is comparable to that of the full-reference metrics tested in the first Video Quality Experts Group evaluation.
8048EDA2	Error resilient, low latency video coding for interactive video applications requires progressive intra coding of macroblocks (MBs) to contain the error propagation. Both refresh MB selection (RMS) and refresh rate selection (RRS) impact the subjective video quality in the presence of packet losses. Joint source-channel rate distortion optimization methods attempt to find the best trade-off between compression efficiency and end-to-end distortion at an MB-level and are typically computationally expensive in addition to not being optimal at a picture level. While probabilistic error propagation tracking is used for refresh MB selection in previous work, these picture-level optimal RRS methods model source-channel distortion by mimicking the effect of periodic intra frame coding which does not match well with content adaptive refresh MB selection. In this paper, we propose a frame-level approach to RRS that aligns the joint source-channel rate-distortion trade-off modeling with an enhanced RMS process to achieve an optimal end-to-end distortion that is content, bit-rate and channel adaptive. For typical videoconferencing content, the proposed approach is quite low in complexity, works on par with off-line multi-pass identification of the optimal fixed refresh rate and is quite competitive when compared to the H.264 Joint Model’s lossy rate-distortion optimization technique.
7FB93FC8	This paper addresses the problem of streaming packetized media over a lossy packet network through an intermediate proxy server to a client, in a rate-distortion optimized way. The proxy, located at the junction of the backbone network and the last hop to the client, coordinates the communication between the media server and the client using hybrid receiver/sender-driven streaming in a rate-distortion optimization framework. The framework enables the proxy to determine at every instant which packets, if any, it should either request from the media server or retransmit directly to the client, in order to meet a constraint on the average transmission rate while minimizing the average end-to-end distortion. Performance gains of up to 1.5 dB and up to 4 dB are observed over rate-distortion optimized sender-driven systems for the case when the last hop is wireline and wireless, respectively.
7FEDFE11	New standardization activities have been recently launched by the JCT-VC experts group in order to challenge the current video compression standard H.264/AVC. Several improvements of this standard, previously integrated in the JM key technical area software, are already known and gathered in the high efficiency video coding test model. In particular, competition-based motion vector prediction has proved its efficiency. However, the targeted 50% bitrate saving for equivalent quality is not yet achieved. In this context, this paper proposes to reduce the signaling information resulting from this motion vector competition, by using data hiding techniques. As data hiding and video compression traditionally have contradictory goals, an advanced study of data hiding schemes is first performed. Then, an original way of using data hiding for video compression is proposed. The main idea of this paper is to hide the competition index into appropriately selected chroma and luma transform coefficients. To minimize the prediction errors, the transform coefficients modification is performed via a rate-distortion optimization. The proposed scheme is evaluated on several low and high resolution sequences. Objective improvements (up to 2.40% bitrate saving) and subjective assessment of the chroma loss are reported.
779C3AFF	In this paper, a novel adaptive Lagrange multiplier selection model in rate-distortion optimization (RDO) is proposed to determine the motion estimation mode during motion compensated temporal filter (MCTF) decomposition in the context of 3D wavelet-based scalable video codec (SVC). First, the motion activity of temporal subbands is investigated. Then, the model parameters for different MCTF levels are estimated. Finally, an optimization model for each temporal subband is obtained from the adaptive Lagrange multiplier selection. We demonstrate the accuracy and performance of our proposed model through extensive numerical simulations. Experimental results illustrate that the proposed model is adaptive with the characteristics of the temporal subbands, suggesting that our model can effectively improve the video quality in terms of both the PSNR and the mean structural similarity index (mean SSIM).
7E0721E4	Block-based discrete cosine transform (DCT) has been successfully adopted into several international image/video coding standards, e.g., MPEG-2, H.264/AVC, as it can achieve a good tradeoff between performance and complexity. Although DCT theoretically approximates the optimum Karhunen-Loève transform under first-order Markov conditions, one fixed set of transform basis functions (TBF) cannot handle all the cases efficiently due to the non-stationary nature of video contents. To further improve the performance of block-based transform coding, in this paper, we present the design of rate-distortion optimized transform (RDOT) which contributes to both intraframe and interframe coding. The most important property which makes a difference between RDOT and the conventional DCT is that, in the proposed method, transform is implemented with multiple TBF candidates which are obtained from off-line training. With this feature, for coding each residual block, the encoder is capable to select the optimal set of TBF in terms of rate-distortion performance, and better energy compaction is achieved in the transform domain. To obtain an optimum group of candidate TBF, we have developed a two-step iterative optimization technique for the off-line training, with which the TBF candidates are refined at each iteration until the training process becomes converged. Moreover, analysis on the optimal group of candidate TBF is also presented in this paper, with a detailed description of a practical implementation for the proposed algorithm on the latest VCEG key technical area software platform. Extensive experimental results show that, compared with the conventional DCT-based transform scheme adopted into the state-of-the-art H.264/AVC video coding standard, significant improvement of coding performance has been achieved for both intraframe and interframe coding with our proposed method.
7870E33D	Recent advances in video capturing and display technologies, along with the exponentially increasing demand of video services, challenge the video coding research community to design new algorithms able to significantly improve the compression performance of the current H.264/AVC standard. This target is currently gaining evidence with the standardization activities in the High Efficiency Video Coding (HEVC) project. The distortion models used in HEVC are mean squared error (MSE) and sum of absolute difference (SAD). However, they are widely criticized for not correlating well with perceptual image quality. The structural similarity (SSIM) index has been found to be a good indicator of perceived image quality. Meanwhile, it is computationally simple compared with other state-of-the-art perceptual quality measures and has a number of desirable mathematical properties for optimization tasks. We propose a perceptual video coding method to improve upon the current HEVC based on an SSIM-inspired divisive normalization scheme as an attempt to transform the DCT domain frame prediction residuals to a perceptually uniform space before encoding. Based on the residual divisive normalization process, we define a distortion model for mode selection and show that such a divisive normalization strategy largely simplifies the subsequent perceptual rate-distortion optimization procedure. We further adjust the divisive normalization factors based on local content of the video frame. Experiments show that the proposed scheme can achieve significant gain in terms of rate-SSIM performance when compared with HEVC.
656ECA98	Raster document coders are typically based on the use of a binary mask layer that efficiently encodes the text and graphic content. While these methods can yield much higher compression ratios than natural image compression methods, the binary representation tends to distort fine document details, such as thin lines, and text edges. In this paper, we describe a method for encoding and decoding the binary mask layer that substantially improves the decoded document quality at a fixed bit rate. This method, which we call resolution enhanced rendering (RER), works by adaptively dithering the encoded binary mask, and then applying a nonlinear predictor to decode a gray level mask at the same or higher resolution. We present experimental results illustrating that the RER method can substantially improve document quality at high compression ratios.
80A44F9B	We present an effective framework for increasing the error-resilience of low bit-rate video communications over an error-prone packet-switched network. Our framework is based on the principle of layered coding with transport prioritization. We introduce a rate-distortion optimized mode-selection algorithm for our prioritized layered framework. This algorithm is based on a joint source/channel-coding approach and trades off source coding efficiency for increased bitstream error-resilience to optimize the video coding mode selection within and across layers. The algorithm considers the channel conditions, as well as the error recovery and concealment capabilities, of the channel codec and source decoder, respectively. Important framework parameters including the packetization scheme, decoder error concealment method, and channel codec error-protection strength are considered. The effects of mismatch between the parameters employed by the encoder and the actual channel conditions are considered. Results are presented for a wide range of packet loss rates in order to illustrate the benefits of the proposed framework.
8015072B	The curved wavelet transform (CWT) was developed to enhance compactness of the wavelet transform (WT) representation. Curve determination is critical for the CWT because a well-defined curve set can increase the performance gain in terms of the rate-distortion (R-D). Conventionally, the image to be encoded is divided into blocks and the curve orientation in each block is independently determined through the minimization of its high-pass CWT energy. In this paper, we propose an R-D optimization algorithm for the curve determination, in which variable block size and the impact of neighboring blocks are taken into account. To reduce the computational cost, an alternative sampling strategy is exploited. Experiment results with natural images show that the proposed algorithm can provide better image quality, measured objectively or subjectively, compared to the conventional CWT coding algorithm. Importantly, the proposed approach overcomes the hurdles of computational cost and optimization at the global level opens the door for further performance enhancements of applications with the CWT.
7EC2A91B	This paper addresses the problem of streaming packetized media over a lossy packet network to a wireless client, in a rate-distortion optimized way. We introduce an incremental redundancy error-correction scheme that combats the effects of both packet loss and bit errors in an end-to-end fashion, without support from the underlying network or from an intermediate base station. The scheme is employed within an optimization framework that enables the sender to compute which packets it should send, out of all the packets it could send at a given transmission opportunity, in order to meet an average transmission-rate constraint while minimizing the average end-to-end distortion. Experimental results show that our system is robust and maintains quality of service over a wide range of channel conditions. Up to 8 dB performance gains are registered over systems that are not rate-distortion optimized, at bit-error rates as large as 10/sup -2/.
7DEE704C	In this paper, we introduce a new strategy for processing real-time video packets in programmable network nodes or active routers. We first discuss challenges in transmission of video streams over bandwidth-limited networks, followed by the active approach as an advance for streaming real-time video. In our model, each programmable node (or router) makes admission decision for video frames based on evaluating their potential value. Frames "bid" their expected distortion price and the node chooses the best ones first until resource is fully utilised. We also analyse complexity and overhead to show its clear benefit against other known strategies. Simulation experiments demonstrate consistent outperformance of the framework in comparison to Lagrangian-based rate-distortion optimized schemes.
7D8084CF	This paper addresses the problem of encoder optimization in a macroblock-based multimode video compression system. An efficient solution is proposed in which, for a given image region, the optimum combination of macroblock modes and the associated mode parameters are jointly selected so as to minimize the overall distortion for a given bit-rate budget. Conditions for optimizing the encoder operation are derived within a rate-constrained product code framework using a Lagrangian formulation. The instantaneous rate of the encoder is controlled by a single Lagrange multiplier that makes the method amenable to mobile wireless networks with time-varying capacity. When rate and distortion dependencies are introduced between adjacent blocks (as is the case when the motion vectors are differentially encoded and/or overlapped block motion compensation is employed), the ensuing encoder complexity is surmounted using dynamic programming. Due to the generic nature of the algorithm, it can be successfully applied to the problem of encoder control in numerous video coding standards, including H.261, MPEG-1, and MPEG-2. Moreover, the strategy is especially relevant for very low bit rate coding over wireless communication channels where the low dimensionality of the images associated with these bit rates makes real-time implementation very feasible. Accordingly, in this paper, the method is successfully applied to the emerging H.263 video coding standard with excellent results at rates as low as 8.0 Kb per second. Direct comparisons with the H.263 test model, TMN5, demonstrate that gains in peak signal-to-noise ratios (PSNR) are achievable over a wide range of rates.
7A7264D5	We propose new models and methods for rate-distortion (RD) optimal video delivery over IP, when packets with bit errors are also delivered. In particular, we propose RD optimal methods for slicing and unequal error protection (UEP) of packets over IP allowing transmission of packets with bit errors. The proposed framework can be employed in a classical independent-layer transport model for optimal slicing, as well as in a cross-layer transport model for optimal slicing and UEP, where the forward error correction (FEC) coding is performed at the link layer, but the application controls the FEC code rate with the constraint that a given IP packet is subject to constant channel protection. The proposed method uses a novel dynamic programming approach to determine the optimal slicing and UEP configuration for each video frame in a practical manner, that is compliant with the AVC/H.264 standard. We also propose new rate and distortion estimation techniques at the encoder side in order to efficiently evaluate the objective function for a slice configuration. The cross-layer formulation option effectively determines which regions of a frame should be protected better; hence, it can be considered as a spatial UEP scheme. We successfully demonstrate, by means of experimental results, that each component of the proposed system provides significant gains, up to 2.0 dB, compared to competitive methods
7DE54C8A	This paper presents a rate-distortion optimization approach to hybrid sound coding. The advantages of sinusoidal and transform coding are combined by a rate-distortion optimization mechanism, using a perceptually relevant distortion measure based on spectral auditory masking. As a result, the coder can adapt to the input signal and to constraints such as bit rate. Listening test results show improved performance of the hybrid coder compared to the individual coding paradigms. There is a good correlation between the improved performance as reported by the listeners and the differences in distortion resulting from the perceptually relevant distortion measure. This confirms that the distortion measure used in the optimization is useful; moreover, it shows the feasibility of the rate-distortion optimization approach for hybrid sound coding.
7FC42BEF	This paper presents an algorithm to optimize the tradeoff between rate and expected end-to-end distortion of a video sequence transmitted over a packet network. The approach optimizes the source coding parameters, slicing, network QoS class selection and/or error control coding parameters, and accounts for the effects of compression, packetization, error propagation, and concealment at the decoder. It builds on, and substantially extends the applicability of, the recursive optimal per-pixel estimate (ROPE) technique for end-to-end distortion estimation. A trellis-based algorithm is introduced in order to overcome macroblock interdependencies in the estimation procedure, and allow adaptive slicing. Moreover, we propose a complementary packetization scheme to efficiently arrange the slices into packets for FEC protection while minimizing rate loss due to padding. Simulations demonstrate consistent gains over currently used techniques.
06F0B7DE	Rate-distortion optimization (RDO) plays a significant role in video coding. However, in most RDO methods, the distortion measurement metrics consider only the spatial distortion of statistical pixel errors. People have concerns about not only the information of independent pixels, but also the spatial and temporal correlations between them. In order to make the distortion assessment more consistent with human perception, temporal information of the successive images and the characteristics of human visual perception should be considered as well. In this paper, we propose a rate-distortion model based on spatio-temporal video structural similarity (stVSSIM) index, which takes both spatial and temporal visual quality into account. Meanwhile, to obtain a reasonable trade-off between bit-rate and visual quality dynamically, a perceptual adaptive Lagrange multiplier selection method is presented. Simulation results show that the proposed method averagely reduces 20% bit-rate under the equal visual quality and the adaptive Lagrange multiplier can further improve the results.
7624C006	In this paper we present a state of the art, practical, realtime, region of interest (ROI) video encoder implemented on the Texas Instruments TMS320DM3x SOC. The proposed algorithm is a novel rate distortion optimized ROI coding algorithm with low complexity making it ideal for implementing on embedded video SOCs with low computational and memory resources while achieving excellent perceptual quality. The proposed solution is a complete solution incorporating ROI processing in the entire video chain from front-end face detection to back-end video compression. It is probably one of the first video capture and compression system implemented on an embedded SOC which relies on specialized rate distortion method for ROI coding using object detection methods from the front end or user inputs. Extensive subjective evaluation has been performed on the proposed algorithm for various resolutions ranging from CIF to 1080p video resolutions at different bitrates for over 300 test cases. Significant subjective quality enhancements have been observed for video sequences over all the different video resolutions at various different bitrates. With the proposed algorithm competitive subjective quality is achieved for video conferencing sequences at 300 kbps for 720p and at 96 kbps for CIF when compared to the case where no ROI based rate distortion methods for coding are used. On the Texas Instruments TMS320DM3x SOC the ROI videoencoder achieved realtime performance for 1080p video resolution at 30 fps.
7DDF5EAE	H.264/AVC encoder employs a complex mode-decision technique based on rate-distortion optimization. It calculates rate-distortion cost (RDcost) for all possible modes to choose the best one having the minimum RDcost. This paper presents a frame-layer rate control for H.264/AVC that computes the Lagrange multiplier (/spl lambda//sub MODE/) for mode decision by using a quantization parameter (QP) which may be different from that used for encoding. At the same time, we also compare actual bits produced by previous macroblocks (MBs) with the total bits allocated to these MBs to further modify /spl lambda//sub MODE/. The objective of these measures aims to produce bits as close to the frame target bits for rate control as possible. This is very important in the case of low-bit-rate tight buffer applications. In order to obtain an accurate QP for a frame, we employ a complexity-based bit-allocation scheme and a QP adjustment method. Simulation results comparing with the H.264 Joint Video Team (JVT) rate control method show that the H.264 encoder, using the proposed algorithm, achieves a visual quality improvement of about 0.56 dB, performs better for buffer overflow and underflow, and achieves a smaller PSNR deviation.
03510265	In this paper, we improve the performance of the embedded coder by reorganising its output bitstream in the rate-distortion (R-D)sense. In the proposed rate-distortion optimized embedding (RDE), the coding bit is first allocated to the coefficient with the steepest R-D slope, i.e. the biggest distortion decrease per coding bit. To avoid transmission of the position of the coded coefficient, RDE uses the expectation R-D slope that can be calculated by the coded bits that have already been transmitted to the decoder. RDE also takes advantage of the probability estimation table of the QM-coder so that the calculation of the R-D slope is just a lookup table operation. Extensive experimental results show that RDE significantly improves the coding efficiency.
7F721715	In this paper, a rate estimation technique using the transform coefficient variance is proposed in the rate-distortion cost function for intra mode decision. The experiment shows that the rate estimation using the absolute transform coefficient variance can predict rates more accurate than the one using ρ-domain model. The new cost function brings out an average speed-up factor of 13.8 when compared with the RDO mode decision, but with considerable degradation in the rate-distortion performance. To improve the coding performance, we propose a SATD-based mode decision algorithm, which incorporates both SATD and the variance information into the RDO mode decision method. The results show that the SATD-based mode decision scheme can achieve a significant improvement in computation with negligible PSNR loss.
78DC4B55	For rate-distortion optimized rate allocation in JVT Scalable Video Coding (SVC), the distortion impact of every FGS NAL unit on the global reconstruction quality is calculated by repeatedly bitstream decoding, which leads to high complexity. In this paper, a fast rate allocation algorithm by modeling distortion estimation is proposed. Based on the hypothesis that DCT residual coefficients follow Laplacian distribution, we establish the distortion estimation model by calculating quantization error of each FGS NAL unit and analyzing the prediction in hierarchical B coding structure. Besides, the parameter in the model is updated according to the distribution of residual coefficients decoded at the base layer within every frame. Experimental results show that compared to the existing method of R-D optimized rate allocation in SVC, the proposed method results in a reduction in decoding time of nearly 50%, and save the runtime of rate allocation by 45.3%, while the PSNR loss of decoded sequence is only 0.04 dB on average.
80407F44	Traditional video compression methods remove spatial and temporal redundancy based on the signal statistical correlation. However, to reach higher compression ratios without perceptually degrading the reconstructed signal, the properties of the human visual system (HVS) need to be better exploited. Research effort has been dedicated to modeling the spatial and temporal just-noticeable-distortion (JND) based on the sensitivity of the HVS to luminance contrast, and accounting for spatial and temporal masking effects. This paper describes a foveation model as well as a foveated JND (FJND) model in which the spatial and temporal JND models are enhanced to account for the relationship between visibility and eccentricity. Since the visual acuity decreases when the distance from the fovea increases, the visibility threshold increases with increased eccentricity. The proposed FJND model is then used for macroblock (MB) quantization adjustment in H.264/advanced video coding (AVC). For each MB, the quantization parameter is optimized based on its FJND information. The Lagrange multiplier in the rate-distortion optimization is adapted so that the MB noticeable distortion is minimized. The performance of the FJND model has been assessed with various comparisons and subjective visual tests. It has been shown that the proposed FJND model can increase the visual quality versus rate performance of the H.264/AVC video coding scheme.
7D687567	We propose a rate-distortion optimization (RDO) scheme based on the structural similarity (SSIM) index, which was found to be a better indicator of perceived image quality than mean-squared error, but has not been fully exploited in the context of image and video coding. At the frame level, an adaptive Lagrange multiplier selection method is proposed based on a novel reduced-reference statistical SSIM estimation algorithm and a rate model that combines the side information with the entropy of the transformed residuals. At the macroblock level, the Lagrange multiplier is further adjusted based on an information theoretical approach that takes into account both the motion information content and perceptual uncertainty of visual speed perception. Finally, the mode for H.264/AVC coding is selected by the SSIM index and the adjusted Lagrange multiplier. Extensive experiments show that the proposed scheme can achieve significantly better rate-SSIM performance and provide better visual quality than conventional RDO coding schemes.
80DEF79C	In today's hybrid video coding, Rate-Distortion Optimization (RDO) plays a critical role. It aims at minimizing the distortion under a constraint on the rate. Currently, the most popular RDO algorithm for one-pass coding is the one recommended in the H.264/AVC reference software. It, or HR-lambda for convenience, is actually a kind of universal method which performs the optimization only according to the quantization process while ignoring the properties of input sequences. Intuitively, it is not efficient all the time and an adaptive scheme should be better. Therefore, a new algorithm Lap- lambda is presented in this paper. Based on the Laplace distribution of transformed residuals, the proposed Lap-lambda is able to adaptively optimize the input sequences so that the overall coding efficiency is improved. Cases which cannot be well captured by the proposed models are considered via escape methods. Comprehensive simulations verify that compared with HR-lambda , Lap-lambda shows a much better or similar performance in all scenarios. Particularly, significant gains of 1.79 dB and 1.60 dB in PSNR are obtained for slow sequences and B-frames, respectively.
7FDADB2D	An important motivation for the development of the emerging H.263+ and MPEG-4 coding standards is to enhance the quality of highly compressed video for two-way, real-time communications. In these applications, the delay produced by bits accumulated in the encoder buffer must be very small, typically below 100 ms, and the rate control strategy is responsible for encoding the video with high quality and maintaining a low buffer delay. In this work, we present a simple rate control technique that achieves these two objectives by smartly selecting the values of the quantization parameters in typical discrete cosine transform video coders. To do this, we derive models for bit rate and distortion in this type of coders, in terms of the quantization parameters. Using Lagrange optimization, we minimize distortion subject to the target bit constraint, and obtain formulas that indicate how to choose the quantization parameters. We implement our technique in H.263 and MPEG-4 coders, and compare its performance to TMN7 and VM7 rate control when the encoder buffer is small, for a variety of video sequences and bit rates. This new method has been adopted as a rate control tool in the test model TMN8 of H.263+ and (with some modifications) in the verification model VM8 of MPEG-4.
75114DD2	The SSIM-based rate-distortion optimization (RDO) has been verified to be an effective tool for H.264/AVC to promote the perceptual video coding performance. However, the current SSIM-based RDO is not efficient for improving the perceptual quality of the video streaming application over the error-prone network, because it does not consider the transmission induced distortion in the encoding process. In this paper, a SSIM-based error-resilient RDO scheme for H.264/AVC is proposed to improve the wireless video streaming performance. Firstly, with the help of the SSE-based RDO, we present a low-complexity Lagrange multiplier decision method for the SSIM-based RDO video coding in the error-free environment. Then, the SSIM-based decoding distortion of the user end is estimated at the encoder and is correspondingly introduced into the RDO to involve the transmission induced distortion into the encoding process. Further, the Lagrange multiplier is theoretically derived to optimize the encoding mode selection in the error-resilient RDO process. Experimental results show that the proposed SSIM-based error-resilient RDO can obtain superior perceptual video quality (more structural information) to the traditional SSE-based error-resilient RDO for wireless video streaming at the same bit rate condition.
7D5A3CAD	We propose a new real-time packet scheduling algorithm for streaming scalable H.264. Our algorithm makes use of a packet importance measure, which we define, that takes into consideration transmission history, channel conditions, and the unique decoding dependencies due to the temporal wavelet encoding. Our algorithm utilizes this importance measure to minimize the expected reconstruction distortion at the decoder under a certain rate constraint. In our experimental results we show gains of more than 3 dB in decoded video quality when transmissions are controlled with our algorithm as compared to existing schedulers
7DB51E7D	We introduce two models for predicting the rate and distortion of the matching-pursuit video codec. The first model is based on a pre-coding analysis pass using the full matching-pursuit dictionary. The second model is based on a reduced-complexity analysis pass. We evaluate these models for use within existing rate-distortion optimization techniques. Our prediction results suggest that the models have sufficient accuracy to be useful in this context, and that significant complexity reductions could be achieved compared to exact rate-distortion computation.
7975585E	Reliable transmission of compressed video in a packet lossy environment cannot be achieved without error recovery mechanisms. We describe an effective method for increasing error resilience of video transmission over packet lossy networks such as the Internet. Intra coding (without reference to a previous picture) is a well-known technique for eliminating temporal error propagation in a predictive video coding system. Randomly intra coding of blocks increases error resilience to packet loss. However, when the error concealment used by the decoder is known, intra encoding following a method that optimizes the tradeoffs between compression efficiency and error resilience is a better alternative. In this paper, we present a rate-distortion optimized mode selection method for packet lossy environments that takes into account the network conditions and the error concealment method used at the decoder. We present results for different packet loss rates and typical packet sizes of the Internet, that illustrate the advantages of the proposed method.
81538A7C	This paper presents a scalable rate control (SRC) scheme based on a more accurate second-order rate-distortion model. A sliding-window method for data selection is used to mitigate the impact of a scene change. The data points for updating a model are adaptively selected such that the statistical behavior is improved. For video object (VO) shape coding, we use an adaptive threshold method to remove shape-coding artifacts for MPEG-4 applications. A dynamic bit allocation among VOs is implemented according to the coding complexities for each VO. SRC achieves more accurate bit allocation with low latency and limited buffer size. In a single framework, SRC offers multiple layers of controls for objects, frames, and macroblocks (MBs). At MB level, SRC provides finer bit rate and buffer control. At multiple VO level, SRC offers superior VO presentation for multimedia applications. The proposed SRC scheme has been adopted as part of the International Standard of the emerging ISO MPEG-4 standard.
597EB3E2	In this paper, we introduce a novel architecture for programmable network nodes that work with a large number of real-time video streams. We first discuss challenges in transmission of video streams over bandwidth-limited networks, followed by the active approach as an advance for streaming real-time video. In our model, each programmable node makes admission decision for video frames based on evaluating their potential value. Frames “bid” their expected distortion price and the node chooses the best ones first until resource is fully utilized. Analysis of complexity and overhead shows clear benefit of our framework. Simulation experiments demonstrate its consistent outperformance in comparison to lagrangian-based Rate-Distortion Optimized schemes.
7681C6D9	Most conventional distortion metrics regard a video frame as a static image, and seldom exploit using the motion information of video frames in succession. Moreover, these methods usually calculate the visual distortion based on the independent spatial pixels. Recently, many researches show that the way people perceive the video signals is similar to the way filters process signals in the frequency domain. Therefore, in order to achieve better visual quality, we introduce a novel distortion measurement into the video coding system, which is consistent with human visual perception, and establish a perception-based rate-distortion optimization model. In this paper, we adopt Gabor filter family to decompose the video signals into frequency domain, and combine the video motion information to measure the perceptual distortion. We call it Motion tuned Distortion metric For Video coding (MDFV). After that we set up an MDFV based rate-distortion optimization model to select the best encoding mode. The experimental results show that the proposed approach is effective.
814AC5B0	The rate-distortion efficiency of video compression schemes is based on a sophisticated interaction between various motion representation possibilities, waveform coding of differences, and waveform coding of various refreshed regions. Hence, a key problem in high-compression video coding is the operational control of the encoder. This problem is compounded by the widely varying content and motion found in typical video sequences, necessitating the selection between different representation possibilities with varying rate-distortion efficiency. This article addresses the problem of video encoder optimization and discusses its consequences on the compression architecture of the overall coding system. Based on the well-known hybrid video coding structure, Lagrangian optimization techniques are presented that try to answer the question: what part of the video signal should be coded using what method and parameter settings?.
7F8DB94E	We propose in this paper a novel error resilient transcoding scheme that can be placed at the boundary between wired and wireless networks via heterogeneous network links. This error resilient transcoder shall seamlessly complement the standard Scalable Video Coding (SVC) bitstream to offer additional error resilient adaptation capability for receiving devices. The novel error resilient transcoding scheme consists of three different modules; each is designed to meet various levels of complexity need. The three modules are all based on the Loss-Aware Rate-Distortion Optimization (LA-RDO) mode decision algorithm we have previously developed for SVC. However, each individual module can be tailored to different complexity requirements depending on whether and how the LA-RDO mode decision is implemented. Another innovation of this approach is the design of a fast rate control algorithm in order to maintain consistent bitrates between input and output of the transcoder. This rate control algorithm only needs picture-level bit information for training target quantization parameters. Simulation results demonstrate that, comparing with standard SVC, the proposed approach is able to achieve up to 4 dB gain for the enhancement layer video and up to 1 dB gain for the base layer video.
80D8FBD0	We present a rate-distortion optimal joint source/channel coding framework for efficient and robust low bit rate video communications in packet networks. The proposed algorithm combines layered coding with transport prioritization in a novel operational rate-distortion optimization framework that trades off source coding performance for channel coding protection. The system is demonstrated to achieve significant improvement in reconstructed video quality for a range of packet loss rates.
7CC011D6	This paper proposes an efficient wavelet-based coding scheme with fine grain scalability, where the base layer is encoded with a novel designed of wavelet-based coder, and the enhancement layer is encoded with progressive fine granularity scalable (PFGS) coding. This algorithm involves multi-frame motion compensation, rate-distortion optimizing strategy with Lagrangian cost function and context-based adaptive arithmetic coding. In order to improve efficiency ofthe enhancement layer coding, an improved motion estimation scheme that uses both information from the base layer and the enhancement layer is also proposed in this paper. The novel designed of wavelet-based coder significantly improves the coding efficiency ofthe base layer compared with MPEG-4 ASP (Advanced Simple Profile) and H.26L TML9. The PFGS coding is a significant improvement over MPEG-4 FGS at the enhancement layer. Experiments show that single layer coding efficiency gain of the proposed scheme is about 2.0-3.0dB and 0.3-1.0dB compared with MPEG-4 ASP and H.26L TML9, respectively. The overall coding efficiency gain of the proposed scheme is about 4.0-5.0dB compared with MPEG-4 FGS.
7EF88D59	This work addresses the transmission of pre-encoded JPEG2000 video within a video-on-demand scenario. The primary requirement for the rate allocation algorithm deployed in the server is to match the real-time processing demands of the application. Scalability in terms of complexity must be provided to supply a valid solution by a given instant of time. The FAst rate allocation through STeepest descent (FAST) method introduced in this work selects an initial (and possibly poor) solution, and iteratively improves it until time is exhausted or the algorithm finishes execution. Experimental results suggest that FAST commonly achieves solutions close to the global optimum while employing very few computational resources.
7687FE2D	A high definition video coding technique using super-macroblocks is investigated in this work. Our research is motivated by the observation that the macroblock-based partition in H.264/AVC may not be efficient for high definition video since the maximum macroblock size of 16 × 16 is relatively small against the whole image size. In the proposed super-macboblock based video coding scheme, the original block size M × N in H.264 is scaled to 2M × 2N. Along with the super-macroblock prediction framework, a low-complexity 16 × 16 discrete cosine transform (DCT) is proposed. As compared with the 1D 8 × 8 DCT, only 16 additions are added for a 1D 16 points 16 × 16 DCT. Furthermore, an adaptive scheme is proposed for the selection the best coding mode and best transform size. It is shown by experimental results that the super-macroblock coding scheme can achieve a higher coding gain.
811D8DB5	This paper proposes a scheme for low-delay robust transmission of video signals over packet erasure channels. In applications such as video conferencing, the permissible delay between encoding and playback may be too low to allow retransmission or channel coding approaches which require buffering several video packets. For such a scenario, we present a scheme that provides error robustness using redundant video descriptions applied to pertinent portions of the video signal. In the H.264/AVC specification, this can be efficiently implemented using redundant slices and flexible macroblock ordering (FMO). We describe a model that determines the bit rate of the redundant descriptions such that the expected distortion at the decoder is minimized. Across all the video test sequences used, the average video quality delivered by the proposed scheme is 3.7 dB higher than decoder-based error concealment, and 1.2 dB higher than encoder-based loss-aware rate-distortion optimization.
7A29564A	Motion information scalability is an important requirement for a fully scalable video codec, especially in low bit rate or small resolution decoding scenarios, for which the fully scalable motion model (SMM) has been proposed. SMM can collaborate flawlessly with other scalabilities, such as spatial, temporal and quality, in a scalable video codec. It performs better than the nonscalable motion model. To further improve the SMM, this correspondence extends the algorithm to support the hierarchical B frame structure and bidirectional or multidirectional motion estimation. Furthermore, the corresponding rate distortion optimized estimation for improved efficiency in several scenarios is discussed. Several simulation results based upon the updated framework are presented to verify the advantage of this extension.
757BA39E	The real-time speech hiding is to hide the secret speech into a cover speech in real-time communication systems. By hiding one secret speech into the cover speech, we can get a stego speech, which sounds meaningful and indistinguishable from the original cover speech. Therefore, even if the attackers catch the audio packets on Internet, they would not notice that there is another speech hidden inside it. In this paper, we propose a scheme for speech hiding in a real-time communication system such as voice over Internet Protocol (VoIP). We propose a novel design of real-time speech hiding for G.711 codec, which is widely supported by almost every VoIP device. Experimental results show that the processing time for the proposed algorithm takes only 0.257 ms, which is suitable for real-time VoIP applications.
7619646B	Multimedia applications have been the key driving force in converging fixed, mobile and IP networks. A major hurdle in the realisation of this convergence is obtaining Quality of Service from a heterogeneous, best-effort service network. Interactive voice requires strict bounds on delay, jitter and packet losses, for Different Network Traffic Intensity, whereas video adds significant bandwidth requirements to the network, while Internet only makes its best effort to deliver a packet. Hence, the end-to-end QoS management of heterogeneous networks supporting multimedia services is of paramount importance. We present an empirical performance study of multimedia applications over 802.11 networks within metropolitan area networking environments. Specifically, we study the QoS performance of Voice over IP (VoIP) applications over 802.11-based networks, while sharing the network resources with HTTP and video applications. Using the OPNET simulator, we simulate several realistic application traffic scenarios, and we investigate the performance of VoIP applications by analyzing QoS parameters, such as delay, jitter, MOS, and packet loss ratio. Subsequently, the performance characteristics data of the network, which we obtain through simulations, are used to build a Markov model of the network performance to extend our analysis and gain further insight into the network performance dynamics.
5A8F9D97	We propose a novel online monitoring approach to distinguish between attacks and normal activity in SIP-based Voice over IP environments. We demonstrate the efficiency of the approach even when only limited data sets are used in learning phase. The solution builds on the monitoring of a set of 38 features in VoIP flows and uses Support Vector Machines for classification. We validate our proposal through large offline experiments performed over a mix of real world traces from a large VoIP provider and attacks locally generated on our own testbed. Results show high accuracy of detecting SPIT and flooding attacks and promising performance for an online deployment are measured.
7D39AC23	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
8133D5AC	Real-time services such as VoIP are becoming popular and are major revenue earners for network service providers. These services are no longer confined to the wired domain and are being extended over wireless networks. Although some of the existing wireless technologies can support some low-bandwidth applications, the bandwidth demands of many multimedia applications exceed the capacity of these technologies. The IEEE 802.16-based WiMax promises to be one of the wireless access technologies capable of supporting very high bandwidth applications. In this paper, we exploit the rich set of flexible features offered at the medium access control (MAC) layer of WiMax for the construction and transmission of MAC protocol data units (MPDUs) for supporting multiple VoIP streams. We study the quality of VoIP calls, usually given by R-score, with respect to the delay and loss of packets. We observe that loss is more sensitive than delay; hence, we compromise the delay performance within acceptable limits in order to achieve a lower packet loss rate. Through a combination of techniques like forward error correction, automatic repeat request, MPDU aggregation, and minislot allocation, we strike a balance between the desired delay and loss. Simulation experiments are conducted to test the performance of the proposed mechanisms. We assume a three-state Markovian channel model and study the performance with and without retransmissions. We show that the feedback-based technique coupled with retransmissions, aggregation, and variable length MPDUs are effective and increase the R-score and mean opinion score by about 40 percent.
812A382D	The paper investigates the effects of packet loss and delay jitter on speech quality in voice over Internet protocol (VoIP) scenarios. A new formula is proposed to quantify these effects and incorporated into ITU-T G.107, the E-model. In the simulation, codecs ITU-T G.723.1 and G.729 are used; random packet loss and Pareto distributed network delay are introduced. The prediction errors range between -0.20 and +0.12 MOS (mean opinion score). The formula extends the coverage of the current E-model, and is very useful in MOS prediction as well as network planning.
6E870075	Perceived conversational speech quality is a key quality of service (QoS) metric for voice over IP (VoIP) applications. Speech quality is mainly affected by network impairments, such as delay, jitter and packet loss. Playout buffer algorithms are used to compensate for jitter based on a tradeoff between delay and loss, but can have a significant effect on perceived quality. The main aim in this paper is to assess how buffer algorithms affect perceived speech quality and how to choose the best algorithm and its parameters to obtain optimum perceived speech quality (in terms of an objective mean opinion score). The contributions of the paper are three-fold. First, we introduce a new methodology for predicting conversational speech quality (conversational mean opinion score or MOSc) which combines the latest ITU-T speech quality measurement algorithm (PESQ) and the concepts of the E-model. Second, we assess different playout buffer algorithms using the new MOSc metric on Internet trace data. Our findings indicate that, in general, end-to-end delay has a major effect on the selection of a buffer algorithm and its parameters. For small end-to-end delays, an algorithm that seeks to minimise loss is preferred, whereas for large end-to-end delays, an algorithm that aims at a minimum buffer delay is best. Third, we propose a modified buffer algorithm together with an adaptive parameter adjustment scheme. Preliminary results show that this can achieve an "optimum" perceived speech quality for all the traces considered. The results are based on Internet trace data measurements between UK and USA, UK and China, UK and Germany.
80F515A4	This paper presents an adaptive steganography scheme for Voice over IP (VoIP). Differing from existing steganography techniques for VoIP, this scheme enhances the embedding transparency by taking into account the similarity between Least Significant Bits (LSBs) and embedded messages. Moreover, we introduce the notion of Partial Similarity Value (PSV). By properly setting the threshold PSV, we can adaptively balance the embedding transparency and capacity. We evaluate the effectiveness of this scheme with G.729a as the codec of the cover speech in StegTalk, a covert communication system based on VoIP. The experimental results demonstrate that our technique provides better performance than the traditional method.
7DE3AE79	Transcoding Steganography (TranSteg) is a fairly new IP telephony steganographic method that is characterized by a high steganographic bandwidth, low introduced distortions, and high undetectability. TranSteg utilizes compression of the overt data to free space for the secret data bits. In this paper, we focus on evaluating different possibilities for TranSteg detection. Building on the previous works, we perform a wide analysis of different steganalysis methods to assess the possibility of TranSteg detection and identify the most ‘undetectable’ pairs of voice codecs. 
5F8051C3	According to former results from (Dittmann et al., 2005) in this paper we summarize the design principles from the general approach and introduce extended experimental test results of a voice-over-IP (VoIP) framework including a steganographic channel based on (Dittmann et al., 2005), (Dittmann and Hesse, 2004), (Kraetzer et al., 2006) and (Vogel et al., 2006). We show that using this framework it is largely secure to transmit hidden messages during a VoIP session and demonstrate results with respect to perceptibility for music and speech data
79250463	With the spread of new and innovative Internet services such as SIP-based communications, the challenge of protecting and defending these critical applications has been raised. In particular, SIP firewalls attempt to filter the signaling un- wanted activities and attacks based on the knowledge of the SIP protocol. Optimizing the SIP firewall configuration at real-time by selecting the best filtering rules is problematic because it depends on both natures of the legal traffic and the unwanted activities. More precisely, we do not know exactly how the unwanted activities are reflected in the SIP messages and in what they differ from the legal ones. In this paper, we address the case of Spam over Internet Telephony (SPIT) mitigation. We propose an adaptive solution based on extracting signatures from learnt decision trees. Our sim- ulations show that quickly learning the optimal configura- tion for a SIP firewall leads to reduce at lowest the unso- licited calls as reported by the users under protection. Our results promote the application of machine learning algo- rithms for supporting network and service resilience against such new challenges. 
77FE1304	The design of a VoIP media stream encryption device based on ARM9 CPU is introduced. The device can be deployed between the Soft Switch or IP-PBX and the VoIP terminal, dedicatedly used for the encryption/de-encryption of the VoIP signal and the RTP voice packet. As an illustration, the encryption flow of the packet is described when the VoIP protocol is SIP and the encryption algorithm is RC4. Then a test is implemented to compare the packet before the encryption and after that, the effectiveness of the design is proved.
5D0F37C5	Voice over Internet Protocol (VoIP) refers to the technology used to transport voice in a digitised form over an IP-based network. As a network service, VoIP inherits all the vulnerabilities a best-effort service network suffers since voice frames can often travel over hostile environments such as the Internet. In this paper we perform an experimental analysis on the effects of encryption under burst VoIP traffic conditions in terms of call volumes under the CS-CELP G.729 coder. The effects of encryption have been illustrated by using the NS-2 simulation tool and a fully automated Python script.
7D5CAE14	The IEEE 802.16 system called WiMAX (Worldwide Interoperability for Microwave Access) provides quality of service (QoS) of several types for different service. The WiMAX is expected to support QoS for real time application, such as Voice over IP (VoIP). In this paper, when network congestion occurs, the VoIP bit-rate needs to be adapted to achieve the best speech quality. We propose a new scheme called Adaptive VoIP Level Coding (AVLC). VoIP is sensitive to delay and loss. According to different network conditions such as varied modulation, packet delay, packet loss, and residual time slot, we use G.722.2 codec to adapt each connection’s data rate. Simulation experiments are conducted to test the performance (network delay, packet loss, and R-score) of the proposed mechanisms. In result, we increase the R-score about 40% to 50%.
81670C60	Voice over Internet Protocol (VoIP) is becoming popular to end-users and sparking great interests in broadband wireless networks. As a mobile version of Worldwide Interoperability for Microwave Access (WiMAX), IEEE 802.16e plays an important role in the evolution towards 4G. In this paper, we have investigated the actual quality of VoIP in the 4G field trial, which aims to better understand the mobile WiMAX performance for delay-sensitive services. Distinct from the past research works, we set up two different evaluation systems, professional and user-friendly. The performance measurements utilize Perceptual Evaluation of Speech Quality (PESQ) to evaluate the voice quality, the packet loss, jitter and delay for network tests. Our test results show that the mobile WiMAX network is able to support well the delay-sensitive VoIP service. We find that the VoIP quality on the downlink is perfect. On the uplink, the quality is degraded but still adequate at both cases of the cell edge and handover. Our work makes a contribution in better understanding the mobile WiMAX performance for both industry and academia.
7E951C8D	The quality of service limitation of today's Internet is a major challenge for real-time voice communications. Excessive delay, packet loss, and high delay jitter all impair the communication quality. A new receiver-based playout scheduling scheme is proposed to improve the tradeoff between buffering delay and late loss for real-time voice communication over IP networks. In this scheme the network delay is estimated from past statistics and the playout time of the voice packets is adaptively adjusted. In contrast to previous work, the adjustment is not only performed between talkspurts, but also within talkspurts in a highly dynamic way. Proper reconstruction of continuous playout speech is achieved by scaling individual voice packets using a time-scale modification technique based on the Waveform Similarity Overlap-Add (WSOLA) algorithm. Results of subjective listening tests show that this operation does not impair audio quality, since the adaptation process requires infrequent scaling of the voice packets and low playout jitter is perceptually tolerable. The same time-scale modification technique is also used to conceal packet loss at very low delay, i.e., one packet time. Simulation results based on Internet measurements show that the tradeoff between buffering delay and late loss can be improved significantly. The overall audio quality is investigated based on subjective listening tests, showing typical gains of 1 on a 5-point scale of the Mean Opinion Score.
8046B9CD	This paper examines the capacity and performance characteristics of the digital video broadcasting (DVB)/Digital Audio-Visual Council (DAVIC) cable television protocol for the delivery of low rate isochronous streams for a cable population of up to 700 nodes. Streams (ranging from 8 to 128 kbps) suitable for timing critical services such as compressed/uncompressed voice (e.g., VoIP: G711 and G.7231), audio and low quality video, were considered in order to study the effects on channel capacity when using reservation and fixed access for the delivery of timing critical services. The analysis focuses on the,performance of the upstream channel, which is the limiting factor of community antenna television (CATV) networks and is critical in the delivery of services to individual subscribers on demand. Simulation results indicated that such streams, within the given protocol limitations, can be supported for a particular system population with trade-offs in terms of system throughput and channel utilization. Network capacity, in terms of the number of simultaneous streams supported and link utilization, is significantly affected by packet size. Analysis of the results indicated that for different streams, packet sizes and combined with header suppression, the benefits from the use of fixed access is essential for the support of timing critical services.
7EB78B3B	Traffic engineering and admission control schemes for QoS support over the Internet share the hypothesis that offered traffic must be policed. Recently, measurement-based techniques have been proposed as effective QoS-guarantee mechanisms. For them, a key element is a thorough understanding of the characteristics of regulated traffic. In this paper, we present an analysis of the traffic output process for a leaky-bucket-regulated voice source with silence suppression, which would be a prime candidate for VoIP service. Specifically, we present a procedure for calculating the mean and the variance of the amount of regulated traffic as a function of the measurement period. We evaluate the quality of our results via simulation.
7F900EF9	In this paper, we exploit the flexible features in the medium access control (MAC) layer of WiMax for construction and transmission of MAC protocol data units (MPDU) for supporting multiple VoIP streams over a WiMax link. Quality of VoIP calls, usually given by R-score, is studied with respect to delay and loss of packets. We observe that loss is more sensitive than delay, hence we trade delay for loss. We propose a combination of techniques that exploit the flexibility of the WiMax MAC layer to strike a balance between loss and delay. These techniques are forward error correction, automatic repeat request, MPDU aggregation, and minislot allocation. Simulation experiments are conducted to test the performance of the proposed mechanisms. We assume a three-state Markovian channel model and study the performance with and without retransmissions. We show that the feedback-based technique coupled with retransmissions, aggregation, and variable length MPDUs are effective and increases the R-score by about 40%.
7E86AB80	Voice over IP (VoIP), also known as Internet telephony, is gaining market share rapidly and now competes favorably as one of the visible applications of the Internet. Nevertheless, being an application running over the TCP/IP suite, it is susceptible to flooding attacks. If flooded, as a time-sensitive service, VoIP may show noticeable service degradation and even encounter sudden service disruptions. Because multiple protocols are involved in a VoIP service and most of them are susceptible to flooding, an effective solution must be able to detect and overcome hybrid floods. As a solution, we offer the VoIP flooding detection system (vFDS)-an online statistical anomaly detection framework that generates alerts based on abnormal variations in a selected hybrid collection of traffic flows. It does so by viewing collections of related packet streams as evolving probability distributions and measuring abnormal variations in their relationships based on the Hellinger distance-a measure of variability between two probability distributions. Experimental results show that vFDS is fast and accurate in detecting flooding attacks, without noticeably increasing call setup times or introducing jitter into the voice streams.
80DBB73F	When voice traffic is transported over a packet-based network, a number of conditions different from the ones in the traditional circuit-switched network will have an influence on the quality of the speech signal as perceived by the users. In particular, distortion of the voice signal due to delay jitter and partial loss of signal caused by packet loss will have an important impact. Therefore, in order to determine the influence of delay jitter and packet loss on the perceived quality, an objective speech quality assessment system, called DSLA, is used to predict the mean opinion score for voice connections, which are set up on a test-bed, subject to artificially introduced network conditions. A decomposition of the voice degradation is made into different parts, due to sampling, digitization, encoding/decoding, packet loss and delay jitter, respectively. It is found that delay jitter has a devastating influence on the perceived quality, when no dejittering buffer is used. However, when the received signal is dejittered, the degradation due to jitter is similar to the one caused by packet loss.
7F88127A	This paper presents a VoIP teleconferencing service that does not required expensive equipment and that can be administrated on a user level. The system is designed and developed so as to be independent of the user environment and to offer high QoS by reducing processing delay. This is achieved through the introduction of a high-speed mixing method to synchronize and mix RTP voice streams. The performance of a trial implementation of a teleconferencing server using this service is evaluated, and its effectiveness is confirmed.
6FF3092D	Voice signal processing algorithms such as P.861 PSQM (perceptual speech quality measure) and P.862 PESQ (perceptual evaluation of speech quality) have been recommended by the ITU-T for the objective measurement of subjective speech quality. The algorithms have been implemented in commercially available products to measure the end-to-end speech quality of voice-over-IP (VoIP) calls. Such measurements are carried out by transmitting a packetized reference speech signal and then applying PSQM or PESQ to the received signal and the original reference to obtain an objective quality score. The measurements are intrusive since test calls are injected into the network. We develop a novel passive method for measuring and monitoring speech quality in live (i.e. in progress) VoIP calls. In contrast to the intrusive methods, the passive method does not require the transmission of any reference speech signal. Instead, the method operates directly on VoIP packet streams copied from the network and applies the ITU-T objective speech quality processing algorithms to the copied voice signals and specially constructed 'pseudo-packet' stream signals. Thus there is no need for any sending devices. The proposed passive method inherits the accuracy that is already provided by the ITU-T objective methods.
7C2CDBE3	Extensible Messaging and Presence Protocol (XMPP) is an open, XML-based protocol aimed at near-real-time, extensible instant messaging (IM) and presence information. It has been expanded into the broader realm of message-oriented middleware. Built to be extensible, the protocol has been extended with features such as Voice over IP and file transfer signaling. XMPP protocol has been used by many social networking platforms including gtalk, and facebook; collaborative services like google wave, and gradient; geo-presence systems like Nokia Ovi Contacts; multiplayer games like chesspark, and by many online live customer support and technical support services. In this manuscript, I will introduce XMPP and its extensions. In the tutorials I will demonstrate how you can leverage some of the available open source software and libraries for rapid development of XMPP enabled services and communication platforms. The tutorial slides and supplementary materials will be available on my website: http://drozturk.com/talks.
7A87B931	The success of Skype has inspired a generation of peer-to-peer-based solutions for real-time multimedia services over the Internet. However, there lacks still a robust metric quantifying the perceptual quality of a Skype call. The widely-used PESQ (Perceptual Evaluation of Speech Quality) falls short of modeling super-wideband calls, which are characteristics of SILK Skype's codec made public in 2011. Towards a robust QoE (Quality of Experience) metric for VoIP call analysis, we propose a model, referred to as WF-Regression model, to capture the call rate and perceptual quality relationship. The model is shown through a user study that it is robust, R-square = 0.9990 and outperform PESQ modeling the quality of Skype calls, error ratio = 3.68% vs. 14.59%.
7DF83938	In the network devices for real time streaming, e.g. VoIP phone, video phone and online game, the playout algorithm controls the playout delay to eliminate the jitter and to minimize the overflow packet loss. Conventional algorithms did this based on network delay only; they did not consider the user perceived quality, and aware the codec and communication duplex mode. Therefore, we present two novel approaches: codec aware adaptive playout (CAAP) and duplex aware adaptive playout (DAAP), they intend to optimize the user perceived quality based on codec and communication duplex mode respectively. Because of their different characteristics, CAAP and DAAP can work either alone or together; and their improvements are accumulative. The out-performance of CAAP and DAAP are superior the prior algorithm in our substantial but conservative evaluation. Since no objective mechanisms for measuring the speech quality of two-way communication exist; a new LMOS-DMOS measurement mechanism is also proposed. Although VoIP is an exemplary embodiment in this paper, we formally model our algorithms into a generalized user perceived QoS control as well.
80AA5E5F	Receiver playout buffers are required to smooth network delay variations for multimedia streams. Playout buffer algorithms such as those commonly used in the Internet, autoregressively measure the network delay and variation and adjust the buffer delay accordingly, to avoid packets arriving too late. In this work, we attempt to adjust the buffer delay based on a prediction of the network delay and a similar measure of variation. The philosophy here is that the use of an accurate prediction will adjust the buffer delay more effectively by tracking rapid fluctuations more accurately. Proper buffer delay can lead to either (or both) a lower total end-to-end delay for a fixed packet lateness percentage or fewer late packets for a fixed total end-to-end delay which are both important metrics for applications such as IP telephony. We present a playout algorithm based on a simple normalized least-mean-square (NLMS) adaptive predictor and demonstrate using Internet packet traces that it can yield reductions in average total end-to-end delays.
7DDF15E2	In voice over IP (VoIP) networks, multiple voice frames can be sent within one packet to increase the network efficiency. When packet loss happens, the voice decoder often tries to conceal the erased frames from received parameters. This paper discuss the quantitative dependency of concealment quality on the packetization rate in terms of LPC distortion. The performance of an erasure-robust. concealment method is analyzed and simulated in comparison with the standard G.729 concealment method. Results show that around 1.2 dB improvement on spectral distortion can be obtained with erasure robust. concealment compared with G.729 under 1-6 frames/packet, packetization. A method to determine the maximum packet size given the network load and expected concealment quality is introduced. The proposed method serves as the packetization guidance during the call setup procedure in VoIP sessions.
7DCBCB60	Steganography, as one of alternative techniques for secure communications, has drawn more and more attentions. This paper presents a covert communication model based on least significant bits (LSB) steganography in Voice over IP (VoIP). The model aims at providing nice security of secret messages and real-time performance that is vital for VoIP. Therefore, we employ a simple encryption of secret messages before embedding them. This encryption strikes a good balance between adequate short-term protection for secret messages and low latency for VoIP. Furthermore, we design a structure of embedded messages. It can provide flexible length and avoid effectually both extraction attack and deceptive attack. We evaluate the model with ITU-T G.729a as the codec of the cover speech in StegTalk, our platform for study on covert communications theory in VoIP. In this case, the proposed model can provide two optional covert transmission speeds, i.e. 0.8kb/s and 2.6 kb/s, where the maximum payload ratio is 99.98%. The experimental results show that our method has negligible effects on speech quality and well meets the real-time requirement of VoIP.
01014831	In VoIP applications, packet loss can have a major impact on perceived speech quality. The impact is affected by factors such as packet loss size, loss pattern and loss locations. In this paper, we report an investigation into the impact of loss location on perceived speech quality and the relationships between convergence time and loss location for three different codecs (G.729, G.723.1 and AMR) using perceptual-based objective measurement methods (PSQM+, MNB and EMBSD). Our results show that loss location has a severe effect on perceived speech quality. The loss at unvoiced speech segments has little impact on perceived speech quality for all codecs. However, the loss at the beginning of voiced segments has the most severe impact on perceived speech quality. The convergence time depends on the speech content (voiced/unvoiced). For unvoiced segments, the convergence time is stable whereas for voiced segments it varies but has an upper bound at the end of the segment. Our method allows a more accurate measurement of the exact effect of packet loss on perceived speech quality. This could help in the development of a perceptually relevant packet loss metric, which could be valuable in non-intrusive VoIP measurements.
7E1E9CC9	We measured the capacity for VoIP traffic in an IEEE 802.11b wireless testbed and compared it with the theoretical capacity and our simulation results. We identified factors that have been commonly overlooked in past studies but affect experiments and simulations. We found that in many papers, the capacity for VoIP traffic has been measured via simulations or experiments without considering these factors, showing different capacity in each paper. After these corrections, simulations and experiments yielded a capacity estimate of 15 calls for 64 kb/s CBR VoIP traffic with 20 ms packetization interval and 38 calls for VBR VoIP traffic with a 0.39 activity ratio. Furthermore, we measured the capacity for VoIP traffic using each access category introduced in the 802.11e standard and the effect of the TCP traffic on VoIP traffic. We found that while the 802.11e standard can protect the QoS of VoIP against TCP traffic, it does not improve the capacity due to the significant retransmissions during TXOP.
7E923534	We propose a streaming authentication scheme for the use over the IP telephony. To clear the strict real-time transmission requirements of the IP telephony, the latency and interval between signatures are adjusted dynamically according to the transmission quality of the network. This provides efficient signing and continuous authentication during real-time streaming. We show our scheme's advantages over previously proposed schemes by comparing delay on the sender and receiver side, and tolerance to packet loss.
7F80D600	We propose a new stream cipher suitable for high-speed network applications, called HSSC (high-speed stream cipher). HSSC uses simple structure to allow detailed analysis and high software performance. The software implementation using C is about 2.07 cycles per byte on a Pentium II 450MHz platform. HSSC uses twelve 32-bit variables as internal states, four predefined constants, four key-derived constants and a set of rules to combine the internal states into 128-bit output per round.
7E03F6EF	Telephony over IP is exposed to multiple security threats. Conventional protection mechanisms do not fit into the highly dynamic, open and large-scale settings of VoIP infrastructures, and may significantly impact on the performance of such a critical service. We propose in this paper a runtime risk management strategy based on anomaly detection techniques for continuously adapting the VoIP service exposure. This solution relies on support vector machines (SVM) and exploits dynamic security safeguards to reduce risks in a progressive manner. We describe how SVM parameters can be integrated into a runtime risk model, and show how this framework can be deployed into an Asterisk VoIP server. We evaluate the benefits and limits of our solution through a prototype and an extensive set of experimental results.
652368DD	We present a new and powerful approach for transmitting video over the Internet via dial-up modem links. Currently, the source and channel coding are both done in the user PC. In fact, joint source channel coding has become quite popular. In strong contrast to this, we propose to spatially separate the source and channel coding so that the source coding is performed in the user PC and the channel coding is performed in the remote access concentrator. This results in significantly improved video quality since the modem connection has a very small packet loss rate compared to the Internet. In addition, the bit rate of the modem connection is fixed and much lower than the potentially available bit rate of the Internet. Clearly, using a channel coding scheme powerful enough to protect against the high packet loss rate of the Internet is not necessary for the modem connection, and in fact, it is a waste of the already very constrained modem connection bandwidth. By moving the channel coding from the user PC to the remote access concentrator, we free up bits on the highly reliable modem link for the source coding, which in turn results in a compressed video of higher quality. In the remote access concentrator we then employ a channel coding scheme which increases the overall bit rate but also makes the real time stream robust to the highly likely packet drops in the Internet.
7CEC8964	This paper presents a formal framework for identifying and filtering SPIT calls (SPam in Internet Telephony) in an outbound scenario with provable optimal performance. In so doing, our work is largely different from related previous work: our goal is to rigorously formalize the problem in terms of mathematical decision theory, find the optimal solution to the problem, and derive concrete bounds for its expected loss (number of mistakes the SPIT filter will make in the worst case).This goal is achieved by considering an abstracted scenario amenable to theoretical analysis, namely SPIT detection in an outbound scenario with pure sources. Our methodology is to first define the cost of making an error (false positive and false negative), apply Wald's sequential probability ratio test to the individual sources, and then determine analytically error probabilities such that the resulting expected loss is minimized.The benefits of our approach are: (1) the method is optimal (in a sense defined in the paper); (2) the method does not rely on manual tuning and tweaking of parameters but is completely self-contained and mathematically justified; (3) the method is computationally simple and scalable. These are desirable features that would make our method a component of choice in larger, autonomic frameworks. 
7D662126	In this paper we experimentally study the relationship between resource utilization in the wireless LAN and the quality of VoIP calls transmitted over the wireless medium. Specifically we evaluate how its overall capacity is shared between three basic MAC bandwidth components (load, access, and free) as the number of VoIP calls increases and how it influences transmission impairments (delay, loss, and jitter) and thus call quality. Resource utilization (under the MAC bandwidth components framework) is calculated by a WLAN probe application that passively "sniffs" packets at the L2/MAC layer of the wireless medium and analyses their headers and temporal characteristics. The quality of VoIP calls is predicted using an extended version of the ITU-T E-model, which estimates user satisfaction from time varying transmission impairments. Through experimentation with various codecs and packetization schemes we found that as the load (number of calls) reaches the available capacity level, packet delays and jitter increase dramatically resulting in the call quality becoming degraded. We show how these MAC bandwidth components maybe used to assess the VoIP call quality on 802.11 WLANs
6D785A11	The paper presents a new hidden data insertion procedure based on estimated probability of the remaining time of the call for steganographic method called LACK (Lost Audio PaCKets steganography). LACK provides hidden communication for real-time services like Voice over IP. The analytical results presented in this paper concern the influence of LACK's hidden data insertion procedures on the method's impact on quality of voice transmission and its resistance to steganalysis. The proposed hidden data insertion procedure is also compared to previous steganogram insertion approach based on estimated remaining average call duration. 
58CF9412	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
764AF17F	Cloud Consulting combines open source grid computing for distributed cloud computing and Enterprise Resource Modeling (ERP) to provide Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) through a simple, unified API. Cloud communications refers to using internet-based or cloud-based voice and data communications services, where telecommunications applications, switching, and storage are managed generally by third parties. These services can include capabilities that range from Voice over IP (VoIP) communications to hosted PBX and unified communications delivering voice, fax, video, and data requirements. Provisioning for these services is known as Communication as a Service (CaaS).
8054009E	Steganography is an ancient art that encompasses various techniques of information hiding, the aim of which is to embed secret information into a carrier message. Steganographic methods are usually aimed at hiding the very existence of the communication. Due to the rise in popularity of IP telephony, together with the large volume of data and variety of protocols involved, it is currently attracting the attention of the research community as a perfect carrier for steganographic purposes. This article is a first survey of the existing Voice over IP (VoIP) steganography methods and their countermeasures.
75EAC64A	Delay and packet loss dramatically affect the quality of voice-over-IP (VoIP) calls and depend on the playout buffer scheme implemented at the receiver. The choice of playout algorithm can't be based on statistical metrics without considering the perceived end-to-end conversational speech quality. The authors present a method for evaluating various playout algorithms that extends the E-model concept by estimating user satisfaction from time-varying transmission impairments. This article evaluates several playout algorithms and shows a correspondence between the authors' results and those obtained via statistical loss and delay metrics.
7EFA283A	The adaptive codebook used in CELP (code excited linear prediction) codecs to model the pitch excitation allows the attainment of a high quality of synthesized speech but introduces a strong inter-frame dependency and consequently causes error propagation in case of frame erasure. In a previous work we showed that the error propagation can be greatly reduced by constraining, at the encoder side, the innovative codebook to partially model the pitch excitation. In this paper we extend this work by exploiting, at the decoder side, the pitch-related information present in the innovative excitation to speed up the recovery of the decoder. The method consists in adequately shifting the last pitch pulse present within the corrupted adaptive codebook memory so that it is resynchronized with the excitation parameters of the frame that follows the erased one
7C8D0145	VoIP applications are becoming popular these days. A lot of Internet traffic are being generated by them. Detection of VoIP traffic is becoming important because of QoS issues and security concerns. A VoIP client typically opens a number of network connection between VoIP client and VoIP client, VoIP client and VoIP server. In the case of peer to peer VoIP applications like Skype network, connections may be between client to client, client to Super Node, client to login server, Super Node to Super Node. Typically, VoIP media traffic are carried by UDP unless firewalls blocks UDP, in which case media and signalling traffic are carried by TCP. Many VoIP applications uses RTP to carry media traffic. Notable examples includes GTalk, Google+ Hangouts, Asterisk based VoIP and Apple's FaceTime. On the other hand, Skype uses a proprietary protocol based on P2P architecture. It uses encryption for end to end communications and adopts obfuscation and anti reverse engineering techniques to prevent reverse engineering of the Skype protocol. This makes the detection of Skype flows a challenging task. Although Skype encrypts all communications, still a portion of Skype payload header known as Start of Message (SoM) is left unecrypted. In this paper, we develop a method for detection of VoIP flows in UDP media streams. Our detection method relies on signalling traffic generated by VoIP applications and heuristics based on the information contained in Skype SoM and RTP/RTCP headers.
5C691BA5	The paper presents a new hidden data insertion procedure based on estimated probability of the remaining time of the call for steganographic method called LACK (Lost Audio PaCKets steganography). LACK provides hidden communication for real-time services like Voice over IP. The analytical results presented in this paper concern the influence of LACK's hidden data insertion procedures on the method's impact on quality of voice transmission and its resistance to steganalysis. The proposed hidden data insertion procedure is also compared to previous steganogram insertion approach based on estimated remaining average call duration. 
7F6DAD9F	This paper presents validation and evaluation of mobile VoIP support over WiMAX networks using the FMIPv6-based cross layer handover scheme. A software module has been implemented for the FMIPv6-based handoff scheme. The handoff delay components are formulated. To evaluate its support of mobile VoIP, we carefully assess the handoff delay, the total delay, and the R factor which is a representation of voice user satisfactory degree. Simulation results show that the cross-layering handoff scheme, as compared with the non-cross-layer scheme, successfully decreases layer-3 handoff delay by almost 50%, and is therefore thriving to support mobile VoIP services. We believe this is the first performance evaluation work for the FMIPv6-based cross-layer scheme, and hence an important work for the WiMAX research community.
58C68905	We propose an original method to geoposition an audio/video stream with multiple emitters that are at the same time receivers of the mixed signal. The achieved method is suitable for those comes where a list of positions within a designated area is encoded with a degree of precision adjusted to the visualization capabilities; and is also easily extensible to support new requirements. This method extends a previously proposed protocol, without incurring in any performance penalty.
7FCD235E	Voice over internet protocol (VoIP) provided voice service using Internet. It receives footlights when it escapes an initial curiosity. Interest to VoIP increased, because it can transfer existing phone service and deliver voice data through Internet technology. Communication service providers introduce Next Generation Network and Broadband convergence Network with VoIP as an added service to the consumer. In addition, session initiation protocol (SIP) service can be applied even outside the Internet phone service. However, it is needed to ensure secrecy of VoIP call in a special situation. It is relatively difficult to eavesdrop in the commonly used Public Switched Telephone Network (PSTN) as it is connected with 1:1 circuit. In this paper, we propose a new model of Internet telephone for eavesdrop prevention enabling VoIP (using SIP protocol) to use the virtual private network (VPN) protocol and establish the probability of practical use comparing it with Internet telephone. Copyright © 2010 John Wiley & Sons, Ltd.
78F4BCB6	Existing solutions for securing multimedia streams such as RTP and Ogg, particularly those based on encrypting large portions of the data stream, are highly unscalable, and infringe on the Quality of Service (QoS) requirements as well. In this paper, we propose an efficient selective encryption technique for securing Ogg formatted VoIP/video streams. The proposed solution encrypts only 1.5% of the stream while guaranteeing security/privacy of the entire bit stream. Experimental results based on example hardware platforms while simulating different digital attacks are presented to verify the robustness of the proposed method.
80843370	Recently voice over IP (VoIP) is experiencing a phenomenal growth. Being a real-time service, VoIP is more susceptible to denial-of-service (DoS) attacks than regular Internet services. Moreover, VoIP uses multiple protocols for call control and data delivery, making it vulnerable to various DoS attacks at different protocol layers. An attacker can easily disrupt VoIP services by flooding TCP SYN packets, UDP-based RTP packets, or SIP-based INVITE messages, which pose a critical threat to IP telephony. In this paper, we present an online statistical detection mechanism, called vFDS, to detect DoS attacks in the context of VoIP. The core of vFDS is based on Hellinger distance method, which computes the variability between two probability measures. Using Hellinger distance, we characterize normal protocol behaviors and then detect the traffic anomalies caused by flooding attacks. Our experimental results show that vFDS achieves fast and accurate detection of DoS attacks
80D69E67	With constantly increasing costs of energy, we ask ourselves what we can say about the energy efficiency of existing VoIP systems. To answer that question, we gather information about the existing client-server and peer-to-peer VoIP systems, build energy models for these systems, and evaluate their power consumption and relative energy efficiency through analysis and a series of experiments.Contrary to the recent work on energy efficiency of peer-to-peer systems, we find that even with efficient peers a peer-to-peer architecture can be less energy efficient than a client-server architecture. We also find that the presence of NATs in the network is a major obstacle in building energy efficient VoIP systems. We then provide a number of recommendations for making VoIP systems more energy efficient.
80508BA5	TCP and UDP are the dominant transport protocols today, with TCP being preferred because of the lack of fairness mechanisms in UDP. Some time-dependent applications with small bandwidth requirements, however, occasionally suffer from unnecessarily high latency due to TCP retransmission mechanisms that are optimized for high-throughput streams. Examples of such thin-stream applications are Internet telephony and multiplayer games. For such interactive applications, the high delays can be devastating to the experience of the service. To address the latency issues, we explored application-transparent, sender-side modifications. We investigated whether it is possible to bundle unacknowledged data to preempt the experience of packet loss and improve the perceived latency in time-dependent systems. We implemented and tested this idea in Linux. Our results show that we can reduce the application latency by trading it against bandwidth.
7D6DEB86	Multi media stream service provided by broadband wireless networks has emerged as a important technology and has attracted much attention today. Due to its large coverage area, low cost of deployment and high speed data rates, WiMAX is a promising technology for providing wireless last-mile connectivity. Physical and MAC layer of this technology refer to the IEEE 802.16e standard, which defines 5 different data delivery service classes that can be used in order to satisfy Quality of Service (QoS) requirements of different applications, such as VoIP, videoconference, FTP, Web, etc. In this paper we examine a case of QoS deployment over a cellular WiMAX network and in particular the performance obtained using two different QoS configurations namely ertPS and UGS has been compared. Results indicate that for delay-sensitive traffic ertPS has an edge over UGS and rtPS.
7550F29B	Implement VoIP Based IP Telephony with Open Source Asterisk ArchitectureAsterisk is a leading open source telephony software/system, easily implemented over intranet and internet. Asterisk empowers developers and integrators to create advanced communication solutions. An Asterisk system is a low cost type of a traditional PBX system. Any phone controlled by an Asterisk system can call a VoIP or analog phone controlled or managed by a traditional telephone system or by Asterisk telephone system. In this paper, the authors focus on the deployment and testing of various Open Source Asterisk Services in an enterprise level communication system. Selected services are listed in this paper that can be used to implement a telephone system with good Quality of Services QoS and good Quality of Experience QoE from the personal user to enterprise level users.
036916A5	This paper presents the design and analysis of a multilayer protection scheme against denial-of-service (DoS) attacks in IP telephony enabled enterprise networks. While there are many types of DoS attacks, we focus on flood-based attacks using application layer and transport layer signaling messages in IP telephony. We design sensors to detect and control these types attacks and consider different location of these sensors in the enterprise network. The algorithm for detecting these attacks is based on the well established non-parametric cumulative sum method. The response to the attack uses standard protocol features of IP telephony to control the number of incoming application and transport layer setup requests. We consider different recovery algorithms and compare their performance using our emulation toolkit. Our results show that the detection algorithm can quickly detect both transport and application layer attacks and is robust against various types of attacks. We also show that with proper choice of sensor parameters, the detection algorithm is effective over a wide range of call volumes.
7839BC52	The IP Multimedia Subsystem (IMS), standardized by the 3GPP, is the most promising contender for replacing legacy, voice-dedicated mobile networks with an All-IP technology. As 3GPP IMS standards are in their embryonic state it becomes very crucial to test IMS clients during their development on a platform that simulates the IMS network. IMS client testbed also allows the developers to experiment all error circumstances which are not possible on a live network. Furthermore, it is also very costly to use an operator's live network for testing purpose. This paper highlights challenges involved and our experience of building an IMS client test bed using open source tools like OpenIMScore.
79208D58	The name MPEG-4 high-efficiency AAC (HE-AAC) refers to a family of recent audio coders that was developed by the International Organization for Standardization/International Electrotechnical Commission (ISO/IEC) Moving Picture Experts Group (MPEG) by subsequent extension of the established Advanced Audio Coding (AAC) architecture. These algorithmic extensions facilitate a significant increase in coding efficiency relative to previous standards and other known systems. Thus, they provide a representation for generic audio/music signals that offers high audio quality also to applications limited in transmission bandwidth or storage capacity, such as digital audio broadcasting and wireless music access for cellular phones. This article presents a compact overview of the evolution, technology, and performance of the MPEG-4 HE-AAC coding family.
7B1DD2CD	Based on the knowledge and experiences from existing image steganalysis techniques, the overall objective of the paper is to evaluate existing audio steganography with a special focus on attacks in ad-hoc end-to-end media communications on the example of Voice over IP (VoIP) scenarios. One aspect is to understand operational requirements of recent steganographic techniques for VoIP applications. The other aspect is to elaborate possible steganalysis approaches applied to speech data. In particular we have examined existing VoIP applications with respect to their extensibility to steganographic algorithms. We have also paid attention to the part of steganalysis in PCM audio data which allows us to detect hidden communication while a running VoIP communication with the usage of the PCM codec. In our impelementation we use Jori's Voice over IP library by Jori Liesenborgs (JVOIPLIB) that provides primitives for a voice over IP communication. Finally we show first results of our prototypic implementation which extents the common VoIP scenario by the new feature of steganography. We also show the results for our PCM steganalyzer framework that is able to detect this kind of hidden communication by using a set of 13 first and second order statistics.
7D6F6428	Differing from applying steganography on storage cover media, steganography on voice over IP (VoIP) must often delicately balance between providing adequate security and maintaining low latency for real-time services. This paper presents a novel real-time steganography model for VoIP that aims at providing good security for secret messages without sacrificing real-time performance. We achieve this goal by employing the well-known least-significant-bits (LSB) substitution approach to provide a reasonable tradeoff between the adequate information hiding requirement (good security and sufficient capacity) and the low latency requirement for VoIP. Further, we incorporate the M-sequence technique to eliminate the correlation among secret messages to resist the statistical detection based on the fact that the distribution of the LSBs in the stego-speech is not uniform and to provide a short-term security protection of secret messages. To accurately recover secret messages at the receiver side, we design a synchronization mechanism based on the RSA key agreement and the synchronized sequence transmission using techniques of the protocol steganography, which can effectively enhance the flexibility of the covert communication system and be extended to other steganography schemes based on real-time systems. We evaluate the effectiveness of our model with ITU-T G.729a as the codec of the cover speech in StegTalk, a covert communication system based on VoIP. The experimental results demonstrate that our technique provides good security and transparency for transmitting secret messages while adequately meeting the real-time requirement of VoIP.
7D35A58A	For the limitation of the network resources, to support real-time service in mobile ad hoc networks is challenging, especially for the delay sensitive voice sessions. Firstly, the delay spike phenomenon is described and analyzed in detail in this paper. Then, to alleviate its impairment on the voice quality, based on the prediction of delay spikes, a new source rate adjusting mechanism is proposed, which changes the number of voice frames in the RTP packet according to the network conditions adaptively. The results have shown that our scheme can improve the voice quality obviously; moreover, network congestion can be relieved or even avoided effectively.
75D2EB80	The paper presents a new steganographic method for IP telephony called TranSteg (Transcoding Steganography). Typically, in steganographic communication it is advised for covert data to be compressed in order to limit its size. In TranSteg it is the overt data that is compressed to make space for the steganogram. The main innovation of TranSteg is to, for a chosen voice stream, find a codec that will result in a similar voice quality but smaller voice payload size than the originally selected. Then, the voice stream is transcoded. At this step the original voice payload size is intentionally unaltered and the change of the codec is not indicated. Instead, after placing the transcoded voice payload, the remaining free space is filled with hidden data. TranSteg proof of concept implementation was designed and developed. The obtained experimental results are enclosed in this paper. They prove that the proposed method is feasible and offers a high steganographic bandwidth while introducing small voice degradation. Moreover, TranSteg detection is difficult to perform when compared with existing VoIP steganography methods.
58FF219F	Skype, one of the popular VoIP applications, has its own redundancy mechanism to mitigate the impact of packet loss at the expense of additional bandwidth usage. However, the benefit of voice quality improvement through redundancy is reduced greatly in the case of consecutive loss of Skype packets. When Skype is running over wireless networks, the high likelihood of having consecutive packet loss in a noisy wireless environment could lead to an interesting scenario: using more bandwidth for a lower voice quality.In this thesis, we study the impact of the HARQ retransmission mechanism of WiMAX to the voice quality of Skype. Two key parameters for the HARQ mechanism of WiMAX (i.e. Maximum numbers of retransmission and the delay of sending ACK) and their interplay with voice quality are investigated in this research. While HARQ retransmission may reduce the redundancy usage in Skype, it introduces additional bandwidth usage during the retransmission. The overhead of WiMAX retransmission with different HARQ parameters and overall bandwidth usage of Skype are also discussed in this thesis. Numerical analysis and simulation of voice quality and bandwidth usage of Skype over WIMAX are presented. Results show that HARQ retransmission can reduce the impact of bursty noise and improve the voice quality of skype up to 22%. The overall bandwidth consumption can be reduced by 76% when HARQ is engaged. Combining our studies on voice quality and bandwidth usage, we attempt to obtain the optimal HARQ parameters that can lead to the highest voice quality over bandwidth usage ratio for Skype over WiMAX.
0F885325	Divide-and-rule strategy is popularly adopted in steganography based on Voice-over-IP (VoIP), which often divides a cover into many small parts and performs embedding operation on each one to maintain the real-time requirement of VoIP services. No existing study, however, notices that the cover length of the parts may seriously affect the embedding performance. The goal of this paper is to fill in the gap and present a novel steganographic scheme to achieve the optimal embedding. To attain the goal, we first present an adjustable matrix encoding (AME) approach, which can adaptively generate a guide matrix to accommodate to the given cover length and provide the best performance while guaranteeing the desired embedding capacity. Further, the optimal embedding is modeled as a typical combinatorial optimization problem, of which the optimal solution is an AME or a combination of multiple AMEs. We evaluate the proposed scheme with ITU-T G. 711 (A-law) as the codec of the cover speech and compare it with previous methods. The experimental results demonstrate that the proposed scheme is really feasible in both theory and practice, and can provide consistently optimal embedding performance in any case. We present an optimal embedding scheme for Voice over IP.A novel adjustable matrix encoding (AME) is proposed to suit the limited cover.The optimal embedding using AME is modeled as a combinatorial optimization problem.An algorithm to search the optimal solution is provided.Our scheme can provide consistently optimal embedding performances in any case.
7D3C3318	Steganography is a data hiding process in which information is secured, while transferring data from sender to receiver. In audio steganography, encoding process is carried nut through inactive frames of low bit rate audio streams by performing iLBC (internet Low Bit rate Codec). This methodology can be used in applications such as VoIP (Voice Over Internet Protocol), streaming audio, archival and messaging. Traditionally, data embedding is carried out in the inactive frames rather than the active frame of streams; that is inactive frame has large embedding capacity. In addition VAD (Voice Activity Detection) algorithm is used for detecting inactive frames. The concealment of analysis is encoded by iLBC, that supports two basic frame lengths, giving a bit-rate of 13.3 kbps with an encoding frame length of 30 ms and 15.2 kbps with an encoding frame length of 20 nis . To enhance security in steganography PCC (Parabolic Curve Cryptography) algorithm is implemented.
84F40BB8	Governments and their agencies are often challenged by high cost and flexible telephonic and data services. Emerging technologies, such as those of Voice over Internet Protocol (VoIP) that allow convergent systems where voice and data networks can utilise the same network to provide both services, can be used to improve such services. However, these convergent networks are based on the classical (best-effort) characteristics that come with some weaknesses in respect of quality of service and fair access to network resources. This is true for multimedia applications that need bounds on delay and minimum bandwidth. VoIP is an implementation of these convergence networks that are capable of transporting voice over Internet Protocol (IP) based networks. In order to deploy a VoIP network capable of providing the traditional Public Switched Telephone Network–Private Branch Exchange (PSTN-PBX) scale solution, a number of issues such as services to be offered, end-user terminal, quality of service, security, bandwidth, signalling, protocol and operating legislations must be addressed. The implementation also requires an application software, which can be an open source or a proprietary software. This study examined how Asterisk, an open source VoIP software can be deployed to serve the needs of an educational institution. The educational institution in this case is the University of Namibia which is currently using a conventional PSTN system for voice and fax communication services, as well as the local area network connected to Internet for data services. Like any other open source software, Asterisk comes free of any proprietary costs. The study investigated how this software could be deployed for a longer period at the University of Namibia. Asterisk was deployed on a pilot basis to provide for a larger scale model to cater for the entire university. It was found out that the University of Namibia has a potential to implement the project although implementation can be scaled down so as to support sustainability. Since the software recommended for installation is open source, the project could be used as a source of information by students who specialize in real-time multi-media systems.
2EC0FE68	A fundamental issue in real-time interactive voice transmissions over unreliable IF networks is the loss or late arrival of packets for playback. This problem is especially serious when transmitting low bit rate-coded speech with pervasive dependencies introduced. In this case, the loss or late arrival of a single packet will lead to the loss of subsequent dependent frames. We study end-to-end loss-concealment schemes for ensuring high quality in playback. We propose a novel multiple description-coding method for concealing packet losses in transmitting low bit rate-coded speech. Based on high correlations observed in linear predictor parameters-in the form of Line Spectral Pairs (LSPs)-of adjacent frames, we generate multiple descriptions in senders by interleaving LSPs, and reconstruct lost LSPs in receivers by linear interpolations. As excitation codewords have low correlations, we further enlarge the segment size for excitation generation and replicate excitation codewords in all the descriptions in order to maintain the same transmission bandwidth. Our proposed scheme can be extended easily to more than two descriptions and can adapt its number of descriptions dynamically to network-loss conditions. Experimental results on FS-1016 CELP, ITU G.723.1, and FS MELP coders show good performance of our scheme.
7EEEAF03	Voice over IP (VoIP) applications can choose a plethora of different speech codecs, which differ in bandwidth, listening speech quality, and resilience to quality degradation under packet loss. However, VoIP Codecs also exhibit differences in facets such as computational complexity or traffic generated that impact on the energy consumption of smartphones due to the use of processor.In this work deals with the study of energy consumption differences among VoIP codecs. We compare the execution time required to encode/decode reference conversations. Our results show that computational complexity has a significant impact on battery consumption (a factor of up to 10 was found between different codecs). Based on our results, we provide a ranking of energy efficiency. We also propose a simple algorithm for codec dynamic selection considering the dimensions of quality, energy and bandwidth. Our algorithm reacts to network conditions choosing the codec that provides less battery consumption constrained to user-defined targets for minimum quality and maximum codec bitrate.
800A2FE1	The paper presents a method to improve the recovery of a speech decoder after the reception of one or several late frames. Rather than considering a late frame as "lost", we propose to use it in order to update the internal state of the decoder. This limits, and, in some cases, stops, the error propagation caused by the concealment. Evaluation results show that there is much to be gained in a voice over IP environment, where late frames can be used to improve the robustness against jitter without increasing the overall end-to-end delay.
025B47E4	This paper presents the first formal framework for identifying and filtering SPIT calls (SPam in Internet Telephony) in an outbound scenario with provable optimal performance. In so doing, our work deviates from related earlier work where this problem is only addressed by ad-hoc solutions. Our goal is to rigorously formalize the problem in terms of mathematical decision theory, find the optimal solution to the problem, and derive concrete bounds for its expected loss (number of mistakes the SPIT filter will make in the worst case). This goal is achieved by considering a scenario amenable to theoretical analysis, namely SPIT detection in an outbound scenario with pure sources. Our methodology is to first define the cost of making an error, apply Wald’s sequential probability ratio test, and then determine analytically error probabilities such that the resulting expected loss is minimized. The benefits of our approach are: (1) the method is optimal (in a sense defined in the paper); (2) the method does not rely on manual tuning and tweaking of parameters but is completely self-contained and mathematically justified; (3) the method is computationally simple and scalable. These are desirable features that would make our method a component of choice in larger, autonomic frameworks.
7A5925AC	This paper presents the design and the implementation of Elliptic Curve Cryptography in an Asterisk VoIP server which serves as an exchange for placing voice calls over the internet. Voice over internet protocol refers to the transmission of speech encoded into data packets transmitted across networks. VoIP networks are prone to confidentiality threats due to the weak keys used by the AES algorithm for encryption of the VoIP packets. So, in order to strengthen the key for encryption/decryption, Elliptic Curve Diffie-Hellman (ECDH) Algorithm key agreement scheme is employed with smaller key sizes resulting in faster computations. The elliptic curve used in this paper is a modified NIST P-256 curve and key generation algorithm using split exponents for fast exponentiation has been implemented to speed up and increase the randomness of key generation. The implementation of split exponents also help in increasing the security of the keys generated. The key generated by ECDH is highly secure because the discrete logarithmic problem is very difficult in this scheme. This Method is successfully carrying out voice calls on VoIP clients connected to the internet. This ECDH key exchanging mechanism for voice calls in real time is implemented on an Asterisk PBX (Private Branch eXchange), using AGI(Asterisk Gateway Interface) server.
685518E3	In a mobile ad hoc network, all the nodes will act as the client/server and it has the capability to forward the packets. Security is a major concern in a wireless network due to decentralised topology. Intruders may attack the transmission path between the source and destination. Existing technique used an ad hoc on demand vector AODV routing protocol to detect the attackers by an on demand basis. But it is not sufficient to detect all the denial of a service attack. Therefore, the proposed new protocol as an on-demand attack detection ODAD protocol is designed to detect various attacks in different ways. It contains three modes to detect the various attacks. They are: 1 finding drop node FDN; 2 acknowledgement ACK; 3 self-node correction SNC. These modes are used to detect the attackers inside the mobile ad hoc network and then inform about the intruders to the remaining node in the network. Experimental results analyse the performance of throughput, packet delivery rate, drop rate and delay. This technique decreases the delay and drop rate, and increases the delivery rate and throughput compared with the existing technique.
01DB3F19	A Distributed Denial of Service (DDoCS) attack consumes the resources of a remote host or network by sending a massive amount ofIP packets from many distributed hosts. It is a pressing problem on the Internet as demonstrated by recent attacks on major e-commerce servers andISPs. Since the attack is distributed and the attack tools evolve at a rapid and alarming rate, an effective solution must be formulated using a distributed and adaptive approach. In this paper, we propose a countermeasure againstDDoCS attacks using a method we call Active Shaping. Our method employs the Active Networks technologies, which incorporates programmability into network nodes. The Active Networks technology enables us to deter congestion and bandwidth consumption of the backbone network caused byDDoCS attacks, and to prevent our system from dropping packets of legitimate users mistakenly. This paper introduces the concept of our method, system design and evaluates the effectiveness of our method using a prototype.
80FFEEEE	This paper presents a simple and robust mechanism, called change-point monitoring (CPM), to detect denial of service (DoS) attacks. The core of CPM is based on the inherent network protocol behavior and is an instance of the sequential change point detection. To make the detection mechanism insensitive to sites and traffic patterns, a nonparametric cumulative sum (CUSUM) method is applied, thus making the detection mechanism robust, more generally applicable, and its deployment much easier. CPM does not require per-flow state information and only introduces a few variables to record the protocol behaviors. The statelessness and low computation overhead of CPM make itself immune to any flooding attacks. As a case study, the efficacy of CPM is evaluated by detecting a SYN flooding attack - the most common DoS attack. The evaluation results show that CPM has short detection latency and high detection accuracy
80EC4C12	Distributed Denial of Service (DDoS) flooding attacks are one of the biggest concerns for security professionals. DDoS flooding attacks are typically explicit attempts to disrupt legitimate users' access to services. Attackers usually gain access to a large number of computers by exploiting their vulnerabilities to set up attack armies (i.e., Botnets). Once an attack army has been set up, an attacker can invoke a coordinated, large-scale attack against one or more targets. Developing a comprehensive defense mechanism against identified and anticipated DDoS flooding attacks is a desired goal of the intrusion detection and prevention research community. However, the development of such a mechanism requires a comprehensive understanding of the problem and the techniques that have been used thus far in preventing, detecting, and responding to various DDoS flooding attacks. In this paper, we explore the scope of the DDoS flooding attack problem and attempts to combat it. We categorize the DDoS flooding attacks and classify existing countermeasures based on where and when they prevent, detect, and respond to the DDoS flooding attacks. Moreover, we highlight the need for a comprehensive distributed and collaborative defense approach. Our primary intention for this work is to stimulate the research community into developing creative, effective, efficient, and comprehensive prevention, detection, and response mechanisms that address the DDoS flooding problem before, during and after an actual attack.
8139352D	Bluetooth, a protocol designed to replace peripheral cables, has grown steadily over the last five years and includes a variety of applications. The Bluetooth protocol operates on a wide variety of mobile and wireless devices and is nearly ubiquitous. Several attacks exist that successfully target and exploit Bluetooth enabled devices. This paper describes the implementation of a network intrusion detection system for discovering malicious Bluetooth traffic. The work improves upon existing techniques, which only detect a limited set of attacks (based on measuring anomalies in the power levels of the Bluetooth device). The new method identifies reconnaissance, denial of service, and information theft attacks on Bluetooth enabled devices, using signatures of the attacks. Furthermore, this system includes an intrusion response component to detect attacks in progress, based on the attack classification. This paper presents the implementation of the Bluetooth intrusion detection system and demonstrates its detection, analysis, and response capabilities. The tool includes a visualization interface to facilitate the understanding of Bluetooth enabled attacks. The experimental results show that the system can significantly improve the overall security of an organization by identifying and responding to threats posed to the Bluetooth protocol.
80802D68	Denial of Service (DoS) attacks constitute one of the major threats and among the hardest security problems in today’s Internet. Of particular concern are Distributed Denial of Service (DDoS) attacks, whose impact can be proportionally severe. With little or no advance warning, a DDoS attack can easily exhaust the computing and communication resources of its victim within a short period of time. Because of the seriousness of the problem many defense mechanisms have been proposed to combat these attacks. This paper presents a structural approach to the DDoS problem by developing a classification of DDoS attacks and DDoS defense mechanisms. Furthermore, important features of each attack and defense system category are described and advantages and disadvantages of each proposed scheme are outlined. The goal of the paper is to place some order into the existing attack and defense mechanisms, so that a better understanding of DDoS attacks can be achieved and subsequently more efficient and effective algorithms, techniques and procedures to combat these attacks may be developed.
754763FE	Recently many prominent web sites face so called Distributed Denial of Service Attacks (DDoS). While former security threats could be faced by a tight security policy and active measures like using rewalls, vendor patches etc. these DDoS are new in suchway that there is no completely satisfying protection yet. In this paper we categorize different forms of attacks and give an overview over the most common DDoS tools. Furthermore we present a solution based on Class Based Routing mechanisms in the Linux kernel that will prevent the most severe impacts of DDoS on clusters of web servers with a prepended load balancing server. The goal is to keep the web servers under attack responding to the normal client requests. Some performance tests and a comparison to other approaches conclude our paper.
7D1512FE	Nowadays, web servers are suffering from application layer distributed denial of service (DDoS) attacks, to which network layer solutions is not applicable as attackers are indistinguishable based on packets or protocols. In this study, the authors propose trust management helmet (TMH) as a partial solution to this problem, which is a lightweight mitigation mechanism that uses trust to differentiate legitimate users from attackers. Its key insight is that a server should give priority to protecting the connectivity of good users during application layer DDoS attacks, instead of identifying all the attack requests. The trust to clients is evaluated based on their visiting history and used to schedule the service to their requests. The authors introduce license, for user identification (even beyond NATs) and storing the trust information at clients. The license is cryptographically secured against forgery or replay attacks. The authors realise this mitigation mechanism and implement it as a Java package and use it for evaluation. The simulation results show that TMH is effective in mitigating session flooding attack: even with 20 times number of attackers, more than 99% of the sessions from legitimate users are accepted with TMH; whereas less than 18% are accepted without it. Moreover, we found that the additional computation cost on the deployed server is neglectable and the bandwidth overhead is acceptable.
753525DC	Self-organized maps (SOM) use an unsupervised learning technique to independently organize a set of input patterns into various classes. In this paper, we use an ensemble of SOMs to identify computer attacks and characterize them appropriately using the major classes of computer attacks (denial of service, probe, user-to-root and remote-to-local). The procedure produces a set of confidence levels for each connection as a way to describe the connection's behavior.
8163EF49	This paper presents a new distributed approach to detecting DDoS (distributed denial of services) flooding attacks at the traffic-flow level The new defense system is suitable for efficient implementation over the core networks operated by Internet service providers (ISPs). At the early stage of a DDoS attack, some traffic fluctuations are detectable at Internet routers or at the gateways of edge networks. We develop a distributed change-point detection (DCD) architecture using change aggregation trees (CAT). The idea is to detect abrupt traffic changes across multiple network domains at the earliest time. Early detection of DDoS attacks minimizes the floe cling damages to the victim systems serviced by the provider. The system is built over attack-transit routers, which work together cooperatively. Each ISP domain has a CAT server to aggregate the flooding alerts reported by the routers. CAT domain servers collaborate among themselves to make the final decision. To resolve policy conflicts at different ISP domains, a new secure infrastructure protocol (SIP) is developed to establish mutual trust or consensus. We simulated the DCD system up to 16 network domains on the Cyber Defense Technology Experimental Research (DETER) testbed, a 220-node PC cluster for Internet emulation experiments at the University of Southern California (USC) Information Science Institute. Experimental results show that four network domains are sufficient to yield a 98 percent detection accuracy with only 1 percent false-positive alarms. Based on a 2006 Internet report on autonomous system (AS) domain distribution, we prove that this DDoS defense system can scale well to cover 84 AS domains. This security coverage is wide enough to safeguard most ISP core networks from real-life DDoS flooding attacks.
7E50D7D3	A low-rate distributed denial of service (DDoS) attack has significant ability of concealing its traffic because it is very much like normal traffic. It has the capacity to elude the current anomaly-based detection schemes. An information metric can quantify the differences of network traffic with various probability distributions. In this paper, we innovatively propose using two new information metrics such as the generalized entropy metric and the information distance metric to detect low-rate DDoS attacks by measuring the difference between legitimate traffic and attack traffic. The proposed generalized entropy metric can detect attacks several hops earlier (three hops earlier while the order α = 10 ) than the traditional Shannon metric. The proposed information distance metric outperforms (six hops earlier while the order α = 10) the popular Kullback-Leibler divergence approach as it can clearly enlarge the adjudication distance and then obtain the optimal detection sensitivity. The experimental results show that the proposed information metrics can effectively detect low-rate DDoS attacks and clearly reduce the false positive rate. Furthermore, the proposed IP traceback algorithm can find all attacks as well as attackers from their own local area networks (LANs) and discard attack traffic.
252C6489	Distributed Denial-of-Service attack (DDoS) is one of the most outstanding menaces on the Internet. A DDoS attack generally attempts to overwhelm the victim in order to deny their services to legitimate users. A number of approaches have been proposed for defending against DDoS attacks accurately in real time. However, existing schemes have limits in terms of detection accuracy an d delay if the IDRS (Intrusion Detection and Response System) deployed only at a specific location detects and responds against attacks. As in this case, it is not able to catch the characteristic of the attack which is distributed in large-scale. Moreover, the existing detection schemes have vulnerabilities to intellectual DDoS attacks which are able to avoid its detection threshold or delay its detection time. This paper suggests the effective DDoS defense system which uses the collaborative scheme among distributed IDRSs located in the vicinity of the attack source or victim network. In proposed scheme, both victim and source-end IDRS work synergistically to identify the attack and avoid false alarm rate up to great extent. Additionally, we propose the duplicate detection window scheme to detect various attacks dynamics which increase the detection threshold gradually in early stage. The proposed scheme can effectively detect and respond against these diverse DDoS attack dynamics.
5C00C81D	Traditional networking is being progressively replaced by Software Defined Networking (SDN). It is a new promising approach to designing, building and managing networks. In comparison with traditional routed networks, SDN enables programmable and dynamic networks. Although it promises more flexible network management, one should be aware of current and upcoming security threats accompanied with its deployment. Our goal is to analyze SDN accompanied with OpenFlow protocol from the perspective of Distributed Denial of Service attacks (DDoS). In this paper, we outline our research questions related to an analysis of current and new possibilities of realization, detection and mitigation of DDoS attacks in this environment.
8066C0BF	The interaction between TCP and various active queue management (AQM) algorithms has been extensively analyzed for the last few years. However, the analysis usually assumed that routers and TCP flows are not under any network attacks. In this paper, we investigate how the performance of TCP flows is affected by denial-of-service (DoS) attacks under the drop tail and various AQM schemes. In particular, we consider two types of DoS attacks-the traditional flooding-based DoS (FDDoS) attacks and the recently proposed pulsing DoS (PDoS) attacks. Both analytical and simulation results support that the PDoS attacks are more effective than the FDDoS attacks under the same average attack rate. Moreover, the drop tail surprisingly outperforms the RED-like AQMs when the router is under a PDoS attack, whereas the RED-like AQMs perform better under a severe FDDoS attack. On the other hand, the Adaptive Virtual Queue algorithm can retain a higher TCP throughput during PDoS attacks as compared with the RED-like AQMs.
5D2253F3	The Border Gateway Protocol (BGP) is a fundamental component of the current Internet infrastructure. Due to the inherent trust relationship between peers, control of a BGP router could enable an attacker to redirect traffic allowing man-in-the-middle attacks or to launch a large-scale denial of service. It is known that BGP has weaknesses that are fundamental to the protocol design. Many solutions to these weaknesses have been proposed, but most require resource intensive cryptographic operations and modifications to the existing protocol and router software. For this reason, none of them have been widely adopted. However, the threat necessitates an effective, immediate solution.We propose a system that is capable of detecting malicious inter-domain routing update messages through passive monitoring of BGP traffic. This approach requires no protocol modifications and utilizes existing monitoring infrastructure. The technique relies on a model of the autonomous system connectivity to verify that route advertisements are consistent with the network topology. By identifying anomalous update messages, we prevent routers from accepting invalid routes. Utilizing data provided by the Route Views project, we demonstrate the ability of our system to distinguish between legitimate and potentially malicious traffic.
7B55777A	Distributed Denial of Service (DDoS) attacks have caused continuous critical threats to the Internet services. DDoS attacks are generally conducted at the network layer. Many DDoS attack detection methods are focused on the IP and TCP layers. However, they are not suitable for detecting the application layer DDoS attacks. In this paper, we propose a scheme based on web user browsing behaviors to detect the application layer DDoS attacks (app-DDoS). A clustering method is applied to extract the access features of the web objects. Based on the access features, an extended hidden semi-Markov model is proposed to describe the browsing behaviors of web user. The deviation from the entropy of the training data set fitting to the hidden semi-Markov model can be considered as the abnormality of the observed data set. Finally experiments are conducted to demonstrate the effectiveness of our model and algorithm. 
7ED0AEF8	In this poster, based on our previous work in building a lightweight DDoS (Distributed Denial-of-Services) attacks detection mechanism for web server using TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) and genetic algorithm based instance selection methods, we further propose a more efficient and effective instance selection method, named E-FCM (Extend Fuzzy C-Means). By using this method, we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance. Therefore, the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment.
8010F0E4	In a Delay-Tolerant Network (DTN), data originating from a source node may be delivered to the destination node, despite the non-existence of end-to-end connectivity between them at all times. In an adversarial environment such as a battlefield, DTN nodes could be compromised to launch Denial-of-Service (DoS) attacks by generating excess data, to cause an overflow of the limited resources of the legitimate nodes, hence decreasing the network throughput. A node may also display selfish behavior by generating more data than allowed, to increase its throughput and to decrease the latency of its data packets. In this paper, we term such a DoS attack and selfish data generation behavior, a resource-misuse attack. We study two types of resource-misuse attacks, breadth attacks and depth attacks. Accordingly, we propose different schemes to detect these attacks. Trace-driven simulations using both a synthetic and a real-world trace show that our detection schemes have low average detection latency and additionally, probabilistic detection of the depth attack has low false positive and false negative rates.
7E768027	Mobile Ad Hoc Networks are vulnerable to various types of Denial of Service (DoS) attacks for the absence of fixed network infrastructure. The Gray Hole attack is a type of DoS attacks. In this attack, an adversary silently drops some or all of the data packets sent to it for further forwarding even when no congestion occurs. Firstly, DSR protocol, aggregate signature algorithm and network model were introduced. Secondly, we proposed to use aggregate signature algorithm to trace packet dropping nodes. The proposal was consisted of three related algorithms: the creating proof algorithm, the checkup algorithm and the diagnosis algorithm. The first was for creating proof, and the second was for checking up source route nodes, and the last was for locating the malicious nodes. Finally, the efficiency of the proposal was analyzed. The simulation results using ns-2 show that in a moderately changing network, most of the malicious nodes could be detected, the routing packet overhead was low, and the packet delivery rate has been improved.
80B22E01	Launching a denial of service (DoS) attack is trivial, but detection and response is a painfully slow and often a manual process. Automatic classification of attacks as single- or multi-source can help focus a response, but current packet-header-based approaches are susceptible to spoofing. This paper introduces a framework for classifying DoS attacks based on header content, and novel techniques such as transient ramp-up behavior and spectral analysis. Although headers are easily forged, we show that characteristics of attack ramp-up and attack spectrum are more difficult to spoof. To evaluate our framework we monitored access links of a regional ISP detecting 80 live attacks. Header analysis identified the number of attackers in 67 attacks, while the remaining 13 attacks were classified based on ramp-up and spectral analysis. We validate our results through monitoring at a second site, controlled experiments, and simulation. We use experiments and simulation to understand the underlying reasons for the characteristics observed. In addition to helping understand attack dynamics, classification mechanisms such as ours are important for the development of realistic models of DoS traffic, can be packaged as an automated tool to aid in rapid response to attacks, and can also be used to estimate the level of DoS activity on the Internet.
81EDE1E2	In recent years, DoS (Denial of Service) attack and more powerful DDoS (Distributed DoS) attack pose security problems on the Internet. As the measure to these attacks, it is important to trace attackers and stop the attacks. However, since information of the attacker is ``spoofed'', it is difficult to trace. Therefore, the method of specifying attackers is required. Savage et al. proposed a method to trace flooding attacks by ``marked'' packets. This method, however, has some problems gathering the attack packets through a lot of hops. In this paper, we propose a method to solve this problem by observing the feature of attack traffic and change the ``marking probability'' of the routers. We implement algorithms both of our proposed method and extending marking method to estimate the efficiency of them. From the results of some experiments, we will conclude the effectiveness of our proposed scheme 
80B88823	In this paper we consider how to optimize a new generation of pulsing denial-of-service (PDoS) attacks from the attackers' points of views. The PDoS attacks are 'smarter' than the traditional attacks in several aspects. The most obvious one is that they require fewer attack packets to cause a similar damage. Another is that the PDoS attacks can be tuned to achieve different effects. This paper concentrates on the attack tuning part. In particular, we consider two conflicting goals involved in launching a PDoS attack: (1) maximizing the throughput degradation and (2) minimizing the risk of being detected. To address this problem, we first analyze the TCP throughput and quasi-global synchronization phenomenon caused by the PDoS attack. We then propose a family of objective functions to incorporate the two conflicting goals, and obtain the optimal attack settings. To validate the analytical results, we have carried out extensive experiments using both ns-2 simulation and a test-bed. The overall experimental results match well with the analytical results.
79E6CA97	We describe a two-dimensional architecture for defending against denial of service attacks. In one dimension,the architecture accounts for all resources consumed by each I/O path in the system; this accounting mechanism is implemented as an extension to the path object in the Scout operating system. In the second dimension, the various modules that define each path can be configured in separate protection domains; we implement hardware enforced protection domains, although other implementations are possible. The resulting system—which we call Escort—is the first example of a system that simultaneously does end-to-end resource accounting (thereby protecting against resource based denial of service attacks where principals can be identified) and supports multiple protection domains (thereby allowing untrusted modules to be isolated from each other). The paper describes the Escort architecture and its implementation in Scout, and reports a collection of experiments that measure the costs and benefits of using Escort to protect a web server from denial of service attacks.
7A652F84	Security experts generally acknowledge that the long-term solution to distributed denial of service attacks is to increase the security level of Internet computers. Attackers would then be unable to find zombie computers to control. Internet users would also have to set up globally coordinated filters to stop attacks early. However, the critical challenge in these solutions lies in identifying the incentives for the Internet's tens of millions of independent companies and individuals to cooperate on security and traffic control issues that do not appear to directly affect them. We give a brief introduction to: network weaknesses that DDoS attacks exploit; the technological futility of addressing the problem solely at the local level; potential global solutions; and why global solutions require an economic incentive framework.
7FDCED3F	The paper analyzes a network based denial of service attack for IP (Internet Protocol) based networks. It is popularly called SYN flooding. It works by an attacker sending many TCP (Transmission Control Protocol) connection requests with spoofed source addresses to a victim's machine. Each request causes the targeted host to instantiate data structures out of a limited pool of resources. Once the target host's resources are exhausted, no more incoming TCP connections can be established, thus denying further legitimate access. The paper contributes a detailed analysis of the SYN flooding attack and a discussion of existing and proposed countermeasures. Furthermore, we introduce a new solution approach, explain its design, and evaluate its performance. Our approach offers protection against SYN flooding for all hosts connected to the same local area network, independent of their operating system or networking stack implementation. It is highly portable, configurable, extensible, and requires neither special hardware, nor modifications in routers or protected end systems.
7E0481D7	DDoS attack flows distributed in many links exhibit directional nature, they are usually generated by certain tools and originate from different nodes, but have inherent dependencies in spatial when transit in network. This will cause correlation between the traffic where they traverse deviate from norm. By taking advantage of this feature, we propose a spatial correlation detection method deploying in backbone network to combat DDoS attack. In doing so, we first approximately estimate abnormality of every origin destination (OD) flow through comparing observations with predictions, then for OD flows with same destination, extracting spatial correlation between their abnormality estimations by principle component analysis(PCA). Abrupt change of spatial correlation indicates DDoS attack occurs. We evaluate the detection performance of our method in detecting synthetic DDoS attack that injected on real backbone traffic through receiver operating characteristic (ROC) curve. The contribution of this paper is utilizing spatial correlation between attack flows, rather than the volume of attack purely, facilitates us to detect relatively small attack being masked in tremendous traffic of backbone network. Moreover, contrary to the centralized computation of previous network-wide anomaly detection method, our method can be deployed separately in each node, in such a way that our method can adapt to different size of network, and thus scalable.
7DC336D3	Different signature or misuse based intrusion detection techniques; anomaly detection is accomplished of detecting novel attacks. However, the use of anomaly detection in practice is vulnerable by a high rate of false alarms. Pattern based techniques have been shown to make a low rate of false alarms, but are not as efficient as anomaly detection in detecting novel attacks,particularly when it comes to network probing and Denial-Of-Service (DOS) attacks. In this paper we find a new approach that merge pattern-based and anomaly-based intrusion detection, mitigating the weak point of the two approaches while increasing their strengths. Our approach begins with network protocols, and expands these state machines with information about statistics that need to be maintained to detect anomalies. 
80DF889E	Denial of service (DoS) attack on the Internet has become a pressing problem. In this paper, we describe and evaluate route-based distributed packet filtering (DPF), a novel approach to distributed DoS (DDoS) attack prevention. We show that DPF achieves proactiveness and scalability, and we show that there is an intimate relationship between the effectiveness of DPF at mitigating DDoS attack and power-law network topology.The salient features of this work are two-fold. First, we show that DPF is able to proactively filter out a significant fraction of spoofed packet flows and prevent attack packets from reaching their targets in the first place. The IP flows that cannot be proactively curtailed are extremely sparse so that their origin can be localized---i.e., IP traceback---to within a small, constant number of candidate sites. We show that the two proactive and reactive performance effects can be achieved by implementing route-based filtering on less than 20% of Internet autonomous system (AS) sites. Second, we show that the two complementary performance measures are dependent on the properties of the underlying AS graph. In particular, we show that the power-law structure of Internet AS topology leads to connectivity properties which are crucial in facilitating the observed performance effects.
80B9C13E	While there are many ongoing research efforts for Denial-of-Service (DoS) attacks in the general Internet environment, there is insufficient research on voice networks. In this paper, we present the design and evaluation of a SIP-TRW algorithm for detection of DDoS attack traffic in VoIP networks. We analyzed existing TRW algorithms for detection of DDoS attacks in the Internet. In order to apply existing algorithms to voice networks, we designed connection establishment and release processes, and defined the probability function.In order to verify the proposed algorithm, we determined the threshold value and defined the variables for the virtual traffic and the environment. Numerical results from the equation showed that there is 70% probability that the connection will break. It also showed that attacks will be detected within 1.2 seconds when the rate of attack is 10 packets per second. The detection time is within 0.5 seconds when the rate is 20 packets per second. We used NS-2 simulators to measure detection ratio by attack traffic type, and the detection time by attack speed. The results showed that detection took 4.3 seconds when one INVITE packet was sent every 0.1 seconds. The proposed algorithm detected 13280 out of 15000 different attack types, resulting in an 89% detection ratio.
5A8DE225	Distributed Denial-of-Service (DDoS) attacks are a major problem in the Internet today. In one form of a DDoS attack, a large number of compromised hosts send unwanted traffic to the victim, thus exhausting the resources of the victim and preventing it from serving its legitimate clients. One of the main mechanisms that have been proposed to deal with DDoS is filtering, which allows routers to selectively block unwanted traffic. Given the magnitude of DDoS attacks and the high cost of filters in the routers today, the successful mitigation of a DDoS attack using filtering crucially depends on the efficient allocation of filtering resources. In this paper, we consider a single router, typically the gateway of the victim, with a limited number of available filters. We study how to optimally allocate filters to attack sources, or entire domains of attack sources, so as to maximize the amount of good traffic preserved, under a constraint on the number of filters. We formulate the problem as an optimization problem and solve it optimally using dynamic programming, study the properties of the optimal allocation, experiment with a simple heuristic and evaluate our solutions for a range of realistic attack-scenarios. First, we look at a single-tier where the collateral damage is high due to the filtering at the granularity of domains. Second, we look at the two-tier problem where we have an additional constraint on the number of filters and the filtering is performed on the granularity of attackers and domains. 
778A3BAF	The basis of denial of service (DoS)/distributed DoS (DDoS) attacks lies in overwhelming a victim's computer resources by flooding them with enormous traffic. This is done by compromising multiple systems that send a high volume of traffic. The traffic is often formulated in such a way that it consumes finite resources at abnormal rates either at victim or network level. In addition, spoofing of source addresses makes it difficult to combat such attacks. This paper adopts a twofold collaborative mechanism, wherein the intermediate routers are engaged in markings and the victim uses these markings for detecting and filtering the flooding attacks. The markings are used to distinguish the legitimate network traffic from the attack so as to enable the routers near the victim to filter the attack packets. The marked packets are also helpful to backtrack the true origin of the spoofed traffic, thus dropping them at the source rather than allowing them to traverse the network. To further aid in the detection of spoofed traffic, Time to Live (TTL) in the IP header is used. The mappings between the IP addresses and the markings along with the TTLs are used to find the spurious traffic. We provide numerical and simulated experimental results to show the effectiveness of the proposed system in distinguishing the legitimate traffic from the spoofed. We also give a statistical report showing the performance of our system.
800818D7	This paper presents a new spectral template-matching approach to countering shrew distributed denial-of-service (DDoS) attacks. These attacks are stealthy, periodic, pulsing, and low-rate in attack volume, very different from the flooding type of attacks. They are launched with high narrow spikes in very low frequency, periodically. Thus, shrew attacks may endanger the victim systems for a long time without being detected. In other words, such attacks may reduce the quality of services unnoticeably. Our defense method calls for collaborative detection and filtering (CDF) of shrew DDoS attacks. We detect shrew attack flows hidden in legitimate TCP/UDP streams by spectral analysis against pre-stored template of average attack spectral characteristics. This novel scheme is suitable for either software or hardware implementation.The CDF scheme is implemented with the NS-2 network simulator using real-life Internet background traffic mixed with attack datasets used by established research groups. Our simulated results show high detection accuracy by merging alerts from cooperative routers. Both theoretical modeling and simulation experimental results are reported here. The experiments achieved up to 95% successful detection of network anomalies along with a low 10% false positive alarms. The scheme cuts off malicious flows containing shrew attacks using a newly developed packet-filtering scheme. Our filtering scheme retained 99% of legitimate TCP flows, compared with only 20% TCP flows retained by using the Drop Tail algorithm. The paper also considers DSP, FPGA, and network processor implementation issues and discusses limitations and further research challenges.
795F12BD	The most popular method of authenticating users is through passwords. Though passwords are the most convenient means of authentication, they bring along themselves the threat of dictionary attacks. While offline dictionary attacks are possible only if the adversary is able to collect data for a successful protocol execution by eavesdropping on the communication channel and can be successfully countered by using public key cryptography, online dictionary attacks can be performed by anyone and there is no satisfactory solution to counter them. In this paper, we propose an authentication protocol which is easy to implement without any infrastructural changes and yet prevents online dictionary attacks. Our protocol uses only one way hash functions and eliminates online dictionary attacks by implementing a challenge-response system. This challenge-response system is designed in a fashion that it hardly poses any difficulty to a genuine user but is extremely burdensome, time consuming and computationally intensive for an adversary trying to launch as many as hundreds of thousands of authentication requests as in case of an online dictionary attack. The protocol is perfectly stateless and thus less vulnerable to denial of service (DoS) attacks.
7B0DB9B9	In order to detect the DoS attack (Denial-of-Service attack) when wireless mesh networks adopt AODV routing protocol of Ad Hoc networks. Such technologies as an end-to-end authentication, utilization rate of cache memory, two pre-assumed threshold value and distributed voting are used in this paper to detect DoS attacker, which is on the basic of hierarchical topology structure in wireless mesh networks. Through performance analysis in theory and simulations experiment, the scheme would improve the flexibility and accuracy of DoS attack detection, and would obviously improve its security in wireless mesh networks.
76C0F2C7	This paper proposes a novel feedback-based control technique that tackles distributed denial of service (DDoS) attacks in four consecutive phases. While protection routers close to the server control inbound traffc rate andkeeps the server alive (phase 1), the server negotiate with upstream routers close to traffc sources to install leaky-buckets for its IP address. The negotiation continues until a defense router on each traffc link accepts the request (phase 2). Next, the server through a feedback-control process adjusts size of leaky-buckets until inbound traffc locates in a desired range (phase 3). Then through a ﬁngerprint test, the server detects which port interfaces of defense routers purely carry good traffc and subsequently asks corresponding defense routers to remove the leaky-bucket limitations for those port interfaces. Additionally, the server amends size of leaky-buckets for the defense routers proportional to amount of good traffc that each one carries (phase 4). Simulation-based results shows that our technique effectively, defenses a victim server against various DDoS attacks such that in most cases more than 90% of good inbound traffc reaches the server while the DDoS attack has been controlled as well.
7FA83BCC	A solo attack may cause a big loss in computer and network systems, its prevention is, therefore, very inevitable. Precise detection is very important to prevent such losses. Such detection is a pivotal part of any security tools like intrusion detection system, intrusion prevention system, and firewalls etc. Therefore, an approach is provided in this paper to analyze denial of service attack by using a supervised neural network. The methodology used sampled data from Kddcup99 dataset, an attack database that is a standard for judgment of attack detection tools. The system uses multiple layered perceptron architecture and resilient backpropagation for its training and testing. The developed system is then applied to denial of service attacks. Moreover, its performance is also compared to other neural network approaches which results more accuracy and precision in detection rate.
7AC99BB6	Denial of Service (DoS) attacks are currently one of the biggest risks any organization connected to the Internet can face. Hence, the congestion handling techniques at the edge router(s), such as Active Queue Management (AQM) schemes must take into account such attacks. Ideally, an AQM scheme should (a) ensure that each network flow gets its fair share of bandwidth, and (b) identify attack flows so that corrective actions (e.g. drop flooding traffic) can be explicitly taken against them to further mitigate the DoS attacks. This paper presents a proof-of-concept work on devising such an AQM scheme, which we name Deterministic Fair Sharing (DFS). Most of the existing AQM schemes do not achieve the above goals or have significant room for improvement. DFS uses the concept of weighted fair share (wfs) that allows it to dynamically self-adjust the router buffer usage based on the current level of congestion, while aiding in identifying malicious flows. By using multiple data structures (a comprehensive repository and a cache) for keeping state of legitimate and malicious flows, DFS is able to optimize its runtime performance (e.g. higher bandwidth flows being handled by the cache). We demonstrate the performance advantage of DFS via extensive simulation while comparing against other existing AQM techniques.
7A6EFF3B	In recent years, DoS (Denial of Service) attacks and more powerful DDoS (Distributed DoS) attacks have posed security problems on the Internet. For countermeasure to these attacks, it is important to trace attackers and stop the attacks. However, since information about the attackers is spoofed, such attacks are difficult to trace. Therefore, a method of identifying attackers is required. Savage and colleagues proposed a method of tracing flooding attacks by using marked packets. This method, however, involves problems in gathering the attack packets through numerous hops. In this paper, we propose a method of resolving this problem by observing the features of the attack traffic and changing the marking probability in the routers. We implement algorithms of both our proposed method and the extended marking method to estimate their efficiency. The experimental results confirm the effectiveness of the proposed method
12A33609	Malicious bot programs, the source of distributed denial of service attack, are widespread and the number of PCs which were infected by malicious bot program are increasing geometrically thesedays. The continuous distributed denial of service attacks are happened constantly through these bot PCs and some financial incident cases have found lately. Therefore researches to response distributed denial of service attack are necessary so we propose an effective feature generation method for distributed denial of service attack detection using entropy. In this paper, we apply our method to both the DARPA 2000 datasets and also the distributed denial of service attack datasets that we composed and generated ourself in general university. And then we evaluate how the proposed method is useful through classification using bayesian network classifier.
7953D3EE	The recently proposed TCP-targeted Low-rate Distributed Denial-of-Service (LDDoS) attacks send fewer packets to attack legitimate flows by exploiting the vulnerability in TCP’s congestion control mechanism. They are difficult to detect while causing severe damage to TCP-based applications. Existing approaches can only detect the presence of an LDDoS attack, but fail to identify LDDoS flows. In this paper, we propose a novel metric – Congestion Participation Rate (CPR) – and a CPR-based approach to detect and filter LDDoS attacks by their intention to congest the network. The major innovation of the CPR-base approach is its ability to identify LDDoS flows. A flow with a CPR higher than a predefined threshold is classified as an LDDoS flow, and consequently all of its packets will be dropped. We analyze the effectiveness of CPR theoretically by quantifying the average CPR difference between normal TCP flows and LDDoS flows and showing that CPR can differentiate them. We conduct ns-2 simulations, test-bed experiments, and Internet traffic trace analysis to validate our analytical results and evaluate the performance of the proposed approach. Experimental results demonstrate that the proposed CPR-based approach is substantially more effective compared to an existing Discrete Fourier Transform (DFT)-based approach – one of the most efficient approaches in detecting LDDoS attacks. We also provide experimental guidance to choose the CPR threshold in practice.
797F0A61	Modern organizations rely on passwords for preventing illicit access to valuable data and resources. A well designed password policy helps users create and manage more effective passwords. This paper offers a novel model and tool for understanding, creating, and testing password policies. We present a password policy simulation model which incorporates such factors as simulated users, accounts, and services. This model and its implementation enable administrators responsible for creating and managing password policies to test them before giving them to actual users. It also allows researchers to test how different password policy factors impact security, without the time and expense of actual human studies. We begin by presenting our password policy simulation model. We next discuss prior work and validate the model by showing how it is consistent with previous research conducted on human users. We then present and discuss experimental results derived using the model
7B9E6441	Password systems are a first line of defense that can prevent, deter, and detect abusive acts. They are one of the most cost effective computer resource control mechanisms presently available. This piece explores some of the more salient aspects of password system design, including objectives of password controls, design philosophies, man-machine interface design, system administration, and technical system implementation.
78185880	A very severe issue in today's computing world is the ‘ACCESS’ to an authenticated user. The most common and popularauthentication mechanism is to use alphanumerical usernames and passwords including special characters and digits. But, textual passwords are most of the time prone to dictionary attacks, shoulder surfing, etc. Hence, graphical passwords have been introducedin this paper as an alternative to authentication schemes. Though the graphical password schemes help in generating more userfriendly passwords, they are still vulnerable to shoulder surfing. To address this issue, text along with images can be combined togenerate more secured passwords. Session passwords, as the name suggests can only be entered once and then, a new password isgenerated for the next session; thereby making the authentication process much stronger.
7DC02752	Password restriction policies and advice on creating secure passwords have limited effects on password strength. Influencing users to create more secure passwords remains an open problem. We have developed Persuasive Text Passwords (PTP), a text password creation system which leverages Persuasive Technology principles to influence users in creating more secure passwords without sacrificing usability. After users choose a password during creation, PTP improves its security by placing randomly-chosen characters at random positions into the password. Users may shuffle to be presented with randomly-chosen and positioned characters until they find a combination they feel is memorable. In this paper, we present an 83-participant user study testing four PTP variations. Our results show that the PTP variations significantly improved the security of users' passwords. We also found that those participants who had a high number of random characters placed into their passwords would deliberately choose weaker pre-improvement passwords to compensate for the memory load. As a consequence of this compensatory behaviour, there was a limit to the gain in password security achieved by PTP.
8025530F	This paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. The present design was the result of countering observed attempts to penetrate the system. The result is a compromise between extreme security and ease of use.
7ABA0670	In this paper, two password authentication schemes, based on triangle and straight line encoding, respectively, are proposed. In our schemes, a one-way function and a strong cryptographic operation such as DES (data encryption standard) are adopted. Besides, in each scheme, the system only needs to store a secret key pair, and each user can select his own password freely. Instead of storing a password verification table inside the computer system, both methods proposed only have to store a corresponding table of identities, which is used by the computer system for validating the submitted passwords. Owing to these two schemes quickly and efficiently responding to any log-in attempt, they are suitable for real-time applications. Furthermore, in each scheme, the system does not need to reconstruct any term of the existing key table when a new user is inserted into the system. Thus, these two schemes are suitable for practical implementation.
5E313D6B	 Untraceable dynamic identity-based remote user authentication scheme with verifiable password update.proposed a dynamic identity-based remote user authentication scheme with verifiable password update. They also proved that their scheme could withstand various attacks. Unfortunately, by proposing concrete attacks, we show that their scheme is vulnerable to three kinds of attacks. We also point out that their scheme cannot provide untraceability. The analysis shows that the scheme of Chang et al. is not suitable for practical applications.
7D376B3C	Users rarely choose passwords that are both hard to guess and easy to remember. To determine how to help users choose good passwords, the authors performed a controlled trial of the effects of giving users different kinds of advice. Some of their results challenge the established wisdom.
813378A5	People enjoy the convenience of on-line services, Automated Teller Machines (ATMs), and pervasive computing, but online environments, ATMs, and pervasive computing may bring many risks. In this paper, we discuss how to prevent users’ passwords from being stolen by adversaries. We propose a virtual password concept involving a small amount of human computing to secure users’ passwords in on-line environments, ATMs, and pervasive computing. We adopt user-determined randomized linear generation functions to secure users’ passwords based on the fact that a server has more information than any adversary does. We analyze how the proposed schemes defend against phishing, key logger, and shoulder-surfing attacks. To the best of our knowledge, our virtual password mechanism is the first one which is able to defend against all three attacks together.
7979C29A	Many security primitives are based on hard mathematical problems. Using hard AI problems for security is emerging as an exciting new paradigm, but has been under-explored. In this paper, we present a new security primitive based on hard AI problems, namely, a novel family of graphical password systems built on top of Captcha technology, which we call Captcha as graphical passwords (CaRP). CaRP is both a Captcha and a graphical password scheme. CaRP addresses a number of security problems altogether, such as online guessing attacks, relay attacks, and, if combined with dual-view technologies, shoulder-surfing attacks. Notably, a CaRP password can be found only probabilistically by automatic online guessing attacks even if the password is in the search set. CaRP also offers a novel approach to address the well-known image hotspot problem in popular graphical password systems, such as PassPoints, that often leads to weak password choices. CaRP is not a panacea, but it offers reasonable security and usability and appears to fit well with some practical applications for improving online security.
7A771AF1	Text-based passwords remain the dominant authentication method in computer systems, despite significant advancement in attackers' capabilities to perform password cracking. In response to this threat, password composition policies have grown increasingly complex. However, there is insufficient research defining metrics to characterize password strength and using them to evaluate password-composition policies. In this paper, we analyze 12,000 passwords collected under seven composition policies via an online study. We develop an efficient distributed method for calculating how effectively several heuristic password-guessing algorithms guess passwords. Leveraging this method, we investigate (a) the resistance of passwords created under different conditions to guessing, (b) the performance of guessing algorithms under different training sets, (c) the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements, and (d) the relationship between guess ability, as measured with password-cracking algorithms, and entropy estimates. Our findings advance understanding of both password-composition policies and metrics for quantifying password security.
5F5D0465	There have been many protocols proposed over the years for password authenticated key exchange in the three-party scenario, in which two clients attempt to establish a secret key interacting with one same authentication server. However, little has been done for password authenticated key exchange in the more general and realistic four-party setting, where two clients trying to establish a secret key are registered with different authentication servers. In fact, the recent protocol by Yeh and Sun seems to be the only password authenticated key exchange protocol in the four-party setting. But, the Yeh-Sun protocol adopts the so called “hybrid model”, in which each client needs not only to remember a password shared with the server but also to store and manage the server’s public key. In some sense, this hybrid approach obviates the reason for considering password authenticated protocols in the first place; it is difficult for humans to securely manage long cryptographic keys. In this paper, we propose a new protocol designed carefully for four-party password authenticated key exchange that requires each client only to remember a password shared with its authentication server. To the best of our knowledge, our new protocol is the first password-only authenticated key exchange protocol in the four-party setting.
7DF5E1C8	Text-based passwords are the most common mechanism for authenticating humans to computer systems. To prevent users from picking passwords that are too easy for an adversary to guess, system administrators adopt password-composition policies (e.g., requiring passwords to contain symbols and numbers). Unfortunately, little is known about the relationship between password-composition policies and the strength of the resulting passwords, or about the behavior of users (e.g., writing down passwords) in response to different policies. We present a large-scale study that investigates password strength, user behavior, and user sentiment across four password-composition policies. We characterize the predictability of passwords by calculating their entropy, and find that a number of commonly held beliefs about password composition and strength are inaccurate. We correlate our results with user behavior and sentiment to produce several recommendations for password-composition policies that result in strong passwords without unduly burdening users.
7A407108	 User authentication and key management are two important security issues in WSNs (Wireless Sensor Networks). In WSNs, for some applications, the user needs to obtain real-time data directly from sensors and several user authentication schemes have been recently proposed for this case. We found that a two-factor mutual authentication scheme with key agreement in WSNs is vulnerable to gateway node bypassing attacks and user impersonation attacks using secret data stored in sensor nodes or an attacker's own smart card. In this paper, we propose an improved scheme to overcome these security weaknesses by storing secret data in unique ciphertext form in each node. In addition, our proposed scheme should provide not only security, but also efficiency since sensors in a WSN operate with resource constraints such as limited power, computation, and storage space. Therefore, we also analyze the performance of the proposed scheme by comparing its computation and communication costs with those of other schemes. 
789936C7	System-generated random passwords have maximum password security and are highly resistant to guessing attacks. However, few systems use such passwords because they are difficult to remember. In this paper, we propose a system-initiated password scheme called "Surpass" that lets users replace few characters in a random password to make it more memorable. We conducted a large-scale online study to evaluate the usability and security of four Surpass policies, varying the number of character replacements allowed from 1 to 4 in randomly-generated 8-character passwords. The study results suggest that some Surpass policies (with 3 and 4 character replacements) outperform by 11% to 13% the original randomly-generated password policy in memorability, while showing a small increase in the percentage of cracked passwords. When compared to a user-generated password complexity policy (that mandates the use of numbers, symbols, and uppercase letters) the Surpass policy with 4-character replacements did not show statistically significant inferiority in memorability. Our qualitative lab study showed similar trends. This Surpass policy demonstrated significant superiority in security though, with 21% fewer cracked passwords than the user-generated password policy.
77C25DD1	Passwords are an ubiquitous and critical component of many security systems. As the information and access guarded by passwords become more necessary, we become ever more dependent upon the security passwords provide. The creation and management of passwords is crucial, and for this we must develop and deploy password policies. This paper focuses on defining and modeling password policies for the entire password policy lifecycle. The paper first discusses a language for specifying password policies. Then, a simulation model is presented with a comprehensive set of variables and the algorithm for simulating a password policy and its impact. Finally, the paper presents several simulation results using the password policy simulation tool.
75B0F4D4	Securing computer systems has traditionally been the domain of system administrators using technology to protect computer systems from attack. However, in many systems, human users are critical to the security process. There's a growing realization that technology alone can't protect security if users don't properly deploy and utilize it. Ultimately, it's users who create passwords and choose whether to adhere to security procedures.
7BE61015	While there are many highly secure authentication systems, the user ID and password credential pair remains the most commonly used means of authentication even though it is one of the easiest to break. It is vulnerable to technical and social attacks but proper application of people, process and technical controls in the selection, use and monitoring of passwords can decrease the likelihood of compromise. This paper investigates the technical and social uses, weaknesses, vulnerabilities, attacks and defenses of the user ID and password combination. This reviews existing technology, use and attacks of user IDs and passwords; it will consider what can be done as well as suggest what should be done. Academic research and industry practice are addressed.
7A9CEECB	The aim of the research is to realize a better form of personal identification number (PIN) authentication for a mobile phone without lowering usability and acceptability. Design/methodology/approach – The authors’ approach is to extend the input operation of PIN authentication by allowing more than one number at a time using a multi-touch-enabled screen. The authors also introduced substitution keys to be able to type any combination of a PIN value and an input pattern by multi-touch typing. Findings – The authors conducted a user evaluation study using a Web-based prototype system. The results of the study indicate that PIN input time, input errors and secret memorability of the proposed scheme were no worse than those of conventional PIN authentication. The theoretical security level of the proposed scheme is almost three and a half times than that of the conventional scheme. Originality/value – The paper introduced a multi-touch-allowed secret input operation into a PIN authentication. Though the introduction affected not only an input operation but also a PIN input interface and secret information, it makes possible to realize a better security level without a drastic change of a user interface and taking a longer input time. 
7A28050B	The movement of koi, a highly colored variant of common carp Cyprinus carpio, was monitored for 18 months in the Waikato River, New Zealand, with both radiotelemetry and acoustic telemetry. Koi had large total linear ranges (mean, 39 km), and most fish (74%) monitored for more than 250 d traveled between the Waikato River and lateral habitat. Differences in total linear range and mean daily movement between years suggest a behavioral response associated with reduced habitat availability during periods of low flow. The movements of koi were highly variable and infrequent, suggesting that important long-range movements are missed in short-term studies (<250 d), resulting in underestimations of total linear range. Our data suggest that large numbers of koi could have been intercepted traveling between lateral habitat and the Waikato River during 1 of 2 years, indicating that point source removal would be highly effective.
7A5EC172	As the Internet has grown, its user community has changed from a small tight-knit group of researchers to a loose gathering of people on a global network. The amazing and constantly growing numbers of machines and users ensures that untrustworthy individuals have full access to that network. High speed inter-machine communication and even higher speed computational processors have made the threats of system “crackers”, data theft, and data corruption very real. This paper outlines some of the problems of current password security by demonstrating the ease by which individual at counts may be broken. Various techniques used by crackers are outlined, and finally one solution to this point of system vulnerability, a proactive password checker, is documented.
7C5F4DDD	The authors have developed a software methodology that improves security by using typing biometrics to reinforce password authentication mechanisms. Typing biometrics is the analysis of a user's keystroke patterns. Each user has a unique way of using the keyboard to enter a password; for example, each user types the characters that constitute the password at different speeds. The methodology employs fuzzy logic to measure the user's typing biometrics. This reinforcement is transparent-indiscernible to the users while they are entering the normal authentication.
7CAED90C	This paper presents the first large-scale study of the success of password expiration in meeting its intended purpose, namely revoking access to an account by an attacker who has captured the account's password. Using a dataset of over 7700 accounts, we assess the extent to which passwords that users choose to replace expired ones pose an obstacle to the attacker's continued access. We develop a framework by which an attacker can search for a user's new password from an old one, and design an efficient algorithm to build an approximately optimal search strategy. We then use this strategy to measure the difficulty of breaking newly chosen passwords from old ones. We believe our study calls into question the merit of continuing the practice of password expiration.
7DD95D8C	Despite considerable research on passwords, empirical studies of password strength have been limited by lack of access to plaintext passwords, small data sets, and password sets specifically collected for a research study or from low-value accounts. Properties of passwords used for high-value accounts thus remain poorly understood.We fill this gap by studying the single-sign-on passwords used by over 25,000 faculty, staff, and students at a research university with a complex password policy. Key aspects of our contributions rest on our (indirect) access to plaintext passwords. We describe our data collection methodology, particularly the many precautions we took to minimize risks to users. We then analyze how guessable the collected passwords would be during an offline attack by subjecting them to a state-of-the-art password cracking algorithm. We discover significant correlations between a number of demographic and behavioral factors and password strength. For example, we find that users associated with the computer science school make passwords more than 1.5 times as strong as those of users associated with the business school. while users associated with computer science make strong ones. In addition, we find that stronger passwords are correlated with a higher rate of errors entering them.We also compare the guessability and other characteristics of the passwords we analyzed to sets previously collected in controlled experiments or leaked from low-value accounts. We find more consistent similarities between the university passwords and passwords collected for research studies under similar composition policies than we do between the university passwords and subsets of passwords leaked from low-value accounts that happen to comply with the same policies.
7B04A913	In 2001, Lin et al. proposed an optimal strong-password authentication protocol called the OSAP protocol. However, Chen and Ku pointed out that it is vulnerable to the stolen-verifier attack. In this paper, we shall propose an improved version of the OSAP protocol to enhance the security.
7DCE7511	TMN is applicable to management of a diversity of equipment, networks and services. This diversity and in particular the huge differences in complexity and cost of network elements (e.g. digital switches compared with regenerators) complicates the specification and standardization of security in TMN. However, equipment cost is not itself very useful and appropriate security measures should be based on other factors such as potential loss of revenue. The need for security of TMN is evident even from a generic evaluation of different information assets together with the specification of top-level security objectives. Existing TMN recommendation only allow for a modest level of security. In a number of known management systems and network elements only seldom changed passwords are used. Such password are often transmitted in the clear thus providing only a very limited degree of security. Additional security measures requires that existing TMN protocols are modified to allow for exchange of security information and communication of protected transfer syntax. In addition, management of security in TMN is essential. GULS (generic upper layer security) is a promising candidate facilitating such security exchanges and security transformations. However, the details of which security techniques (e.g. type of authentication) is not specified by GULS and must be provided by additional standards. Even so, GULS (or similar mechanisms) allow for the introduction of a security infrastructure within the TMN which enables different security techniques to be applied when standardised or as needed.
7BFE2F5C	Passwords are the only ubiquitous form of authentication currently available on the web. Unfortunately, passwords are insecure. In this paper we therefore propose the use of strong cryptography, using the fact that users increasingly own a smartphone that can perform the required cryptographic operations on their behalf. This is not as trivial as it sounds. Services will not migrate to new forms of authentication if few users have the means to use it. Similarly, users will not acquire the means if there are few services that accept them. Moreover, enabling one's smartphone to seamlessly sign in at a website when browsing on an arbitrary PC is non-trivial.We propose a system, based on a smartphone app, that can be used to sign in with username and password to arbitrary websites using an arbitrary PC or laptop. We describe the protocol and implementation to achieve this without the need for typing usernames and passwords. Furthermore, we propose an authentication protocol based on public key cryptography, integrated in the same smartphone app. This allows websites to seamlessly migrate towards a much more secure authentication method on the web, independently of each other. A prototype of our system has been developed.
79D8254D	Many portable devices need a simple authentication system to protect them from being used by an unauthenticated person such as a thief. The security of traditional methods such as pin codes or passwords is limited by shoulder surfing where a casual or intentional observer observes an authentication session and derives all information necessary for authentication. Graphical authentication systems have been developed to forestall this attack. We present here an especially simple variant of a graphical authentication system based on the capacity of humans to recognize faces well. In our challenge-response scheme, a user is presented with a row of typically three faces and needs to decide whether the number of “friends” is even or odd. We present here an analysis of security and usability of this scheme.
76771A1D	Text-based passwords are still the most commonly used authentication mechanism in information systems. We took advantage of a unique opportunity presented by a significant change in the Carnegie Mellon University (CMU) computing services password policy that required users to change their passwords. Through our survey of 470 CMU computer users, we collected data about behaviors and practices related to the use and creation of passwords. We also captured users' opinions about the new, stronger policy requirements. Our analysis shows that, although most of the users were annoyed by the need to create a complex password, they believe that they are now more secure. Furthermore, we perform an entropy analysis and discuss how our findings relate to NIST recommendations for creating a password policy. We also examine how users answer specific questions related to their passwords. Our results can be helpful in designing better password policies that consider not only technical aspects of specific policy rules, but also users' behavior in response to those rules.
76239220	We propose a four-party password authenticated inter-domain key exchange protocol which makes use of properties of identity-based cryptography and secret public keys. Being password-based and certificate-free, our protocol is lightweight and is suited to lightweight computing environments, such as pervasive computing. Apart from resistance against offline and active attacks, our protocol additionally provides perfect forward secrecy. We provide heuristic analysis of various security properties. Performance comparisons against other related protocols show that our protocol is efficient.
7FED7E74	To encourage strong passwords, system administrators employ password-composition policies, such as a traditional policy requiring that passwords have at least 8 characters from 4 character classes and pass a dictionary check. Recent research has suggested, however, that policies requiring longer passwords with fewer additional requirements can be more usable and in some cases more secure than this traditional policy. To explore long passwords in more detail, we conducted an online experiment with 8,143 participants. Using a cracking algorithm modified for longer passwords, we evaluate eight policies across a variety of metrics for strength and usability. Among the longer policies, we discover new evidence for a security/usability tradeoff, with none being strictly better than another on both dimensions. However, several policies are both more usable and more secure that the traditional policy we tested. Our analyses additionally reveal common patterns and strings found in cracked passwords. We discuss how system administrators can use these results to improve password-composition policies.
7D2DA1ED	Personal information and organizational information need to be protected, which requires that only authorized users gain access to the information. The most commonly used method for authenticating users who attempt to access such information is through the use of username–password combinations. However, this is a weak method of authentication because users tend to generate passwords that are easy to remember but also easy to crack. Proactive password checking, for which passwords must satisfy certain criteria, is one method for improving the security of user-generated passwords. The present study evaluated the time and number of attempts needed to generate unique passwords satisfying different restrictions for multiple accounts, as well as the login time and accuracy for recalling those passwords. Imposing password restrictions alone did not necessarily lead to more secure passwords. However, the use of a technique for which the first letter of each word of a sentence was used coupled with a requirement to insert a special character and digit yielded more secure passwords that were more memorable.
77905A47	We describe a sequence of five experiments on network security that cast students successively in the roles of computer user, programmer, and system administrator. Unlike experiments described in several previous papers, these experiments avoid placing students in the role of attacker. Each experiment starts with an in-class demonstration of an attack by the instructor. Students then learn how to use open-source defense tools appropriate for the role they are playing and the attack at hand. Threats covered include eavesdropping, dictionary, man-in-the-middle, port scanning, and fingerprinting attacks. Defense skills gained by students include how to forward ports with OpenSSH, how to prevent weak passwords with CrackLib, how to salt passwords, how to set up a simple certifying authority, issue and verify certificates, and guarantee communication confidentiality and integrity using OpenSSL, and how to set up firewalls and IPsec-based virtual private networks. At two separate offerings, tests taken before and after each experiment showed that each has a statistically significant and large effect on students' learning. Moreover, surveys show that students finish the sequence of experiments with high interest in further studies and work in the area of security. These results suggest that the experiments are well-suited for introductory security or networking courses.
7C325CE0	Watermarking of multimedia content is important to authenticate, copy-control and ownership detection. In this paper, multimedia authentication and tamper detection scheme is proposed with the security of AES(Advanced Encryption Standard) ciphered watermarking and hash function. The algorithm embeds two watermarks in the host image for authentication and tamper detection. We first used Unique Identification Code (UIC) as first robust watermark which is then embedded using the 2-level discrete wavelet transform. Hash code of host image is calculated and used as secondary watermark for tamper detection. This method is blind in nature. The PSNR, SSIM values are used as a metric to test the efficiency of the enhanced watermarking technique. The results show that the imperceptibility of method is high as compared to the existing.
5BC1AE79	We describe a digital watermarking method for use in audio, image, video and multimedia data. We argue that a watermark must be placed in perceptually significant components of a signal if it is to be robust to common signal distortions and malicious attack. However, it is well known that modification of these components can lead to perceptual degradation of the signal. To avoid this, we propose to insert a watermark into the spectral components of the data using techniques analogous to spread sprectrum communications, hiding a narrow band signal in a wideband channel that is the data. The watermark is difficult for an attacker to remove, even when several individuals conspire together with independently watermarked copies of the data. It is also robust to common signal and geometric distortions such as digital-to-analog and analog-to-digital conversion, resampling, and requantization, including dithering and recompression and rotation, translation, cropping and scaling. The same digital watermarking algorithm can be applied to all three media under consideration with only minor modifications, making it especially appropriate for multimedia products. Retrieval of the watermark unambiguously identifies the owner, and the watermark can be constructed to make counterfeiting almost impossible. Experimental results are presented to support these claims.
79DCAAEC	A novel wavelet domain based semi-fragile watermarking scheme is presented for securing digital content and to concisely determine the regions where the integrity fails. In addition, another watermark is embedded to perform self-recovery in case of malicious attack. The security weaknesses of the traditional block-based approaches are circumvented by correlating the watermark bits with wavelet coefficients of the approximation subband of the host image. Semi-fragility exhibits robustness to JPEG compression, while recovery attribute makes the scheme suitable for video surveillance and remote sensing applications. Experimental investigations are performed to evaluate the performance of the proposed multiple semi-fragile watermarks and shows the suitability of the proposed approach for accurate authentication and recovery based applications.
7562FC29	Aiming to balance the robustness and imperceptibility of database watermark, propose a wavelet transform (DWT) based blind watermarking algorithm. The algorithm screens candidate attributes that can be embedded watermark and conducts subset segmentation and rearrangement, and then performs DWT transformation to the data subsets and the scrambled watermark image respectively. Embed the compressed low-frequency part of the watermark into the High-frequency part of the data set to achieve data fusion. Theoretical analysis and experiments show that the algorithm enjoys strong robustness and good invisibility.
809D24E9	Reversible semi-fragile authentication watermark (RSAW) is required in an integrated and powerful authentication system. An effective RSAW scheme should have the desirable features: tamper detection and localization, good perceptual invisibility, detection without requiring explicit knowledge of the original image, robustness against lossy compression, noise attack and low-pass filtering to some extent, reversibility on condition that marked image has not been disturbed, and high security against forge attack. To our best knowledge, RSAW schemes that are presented in the literature are not effective enough. This paper proposes a new RSAW scheme which is effective and has additional features as tamper discerning, computational efficiency and multiple encryption keys supporting. Experimental results demonstrate the validity of the proposed RSAW scheme
77E2777E	Digital watermarking is the process of introducing small modifications into a copy of a digital document that can be detected later. The embedded information can be used to determine the document's owner or simply to distinguish several copies. However, coincidental or malicious “attacks” can degrade the robustness of watermark detection. Here, uniform scalar quantization of watermarked documents is investigated theoretically, extending results from theory of dithered quantization, and experimentally. The watermark is embedded by an independent additive pseudo-noise sequence. The statistical distribution of the quantization errors depending on the statistics of the host signal and the watermark is used to determine the robustness of watermark detection via correlation. Experiments with JPEG compression of an image with a DCT-domain additive watermark demonstrate the usefulness of the presented theory.
7578EEF3	In this letter, we present a new audio watermarking technique using -box transformation. Furthermore, we analyze the strength of the proposed watermarking technique with MSE analysis, PSNR analysis, SSIM analysis, robustness analysis, compression attack analysis, noise attack analysis, cropping analysis and capacity analysis and conclude that the proposed method of watermarking is better than many existing techniques.
7D3743F2	This paper presents a new semi-fragile watermarking algorithm for audio authentication, which takes full advantage of scrambing algorithm. Watermark scrambling algorithm can dispel the pixel space relationship of the binary watermark image. After scrambing, the white pixels and black ones of original watermark image are comparatively orderliness, which is robustness against signal processing and fragility to malicious attack. Therefore the proposed scheme can tolerate general signal processing, e.g. MPEG Audio Layer 3 (MP3) compression, and can detect any spiteful tamper on watermarked audio; locate spatial regions from the ruleless of pixels without the help from the original watermark, and even can evaluate the temper degree. Experimental results show that the algorithm can identify intentional content modification and incidental tampering, and also indicate the location where a modification takes place.
79BF98B1	H.264/AVC is a widespread standard for high definition video (HD) for example DVD and HD videos on the internet. To prevent unauthorized modifications, video authentication can be used. In this paper, we present a cryptanalysis of a H.264/AVC video authentication scheme proposed by Saadi et al. [1] at EUSIPCO 2009. Our result will prevent situations where newer schemes are developed from the scheme thus amplifying the flaw. The designers claimed that the scheme can detect modifications on watermarked video. However, we show that an attacker can modify the watermarked video and compute a valid watermark such that the recipient will retrieve a watermark from the modified watermarked video that will match what the recipient computes during video authentication check. Thus, the recipient will think the tampered video is authentic. The first main problem of the scheme is its use of hash functions for watermark generation. Since hash functions are public functions not depending on any secret, the attacker can modify the watermarked video and feed this through the hash function to compute a new watermark. The second problem is that it is possible for the attacker to perform watermark embedding thus producing a modified watermarked video. On receiving the modified video, the recipient recomputes the watermark and compares this with the watermark extracted from the video. They will match because the embedded watermark and recomputed watermark use the same hash function based watermark generation and the same input i.e. the modified video. Our cryptanalysis strategy applies to any watermarking based video authentication scheme where the watermark and embedding are not functions of secrets. As countermeasure, the functions should be designed so that only legitimate parties can perform them. We present two improved schemes that solve this problem based on private key signing functions and message authentication functions respectively. 
7F411E8F	This paper addresses issues that arise in copyright protection systems of digital images, which employ blind watermark verification structures in the discrete cosine transform (DCT) domain. First, we observe that statistical distributions with heavy algebraic tails, such as the alpha-stable family, are in many cases more accurate modeling tools for the DCT coefficients of JPEG-analyzed images than families with exponential tails such as the generalized Gaussian. Motivated by our modeling results, we then design a new processor for blind watermark detection using the Cauchy member of the alpha-stable family. The Cauchy distribution is chosen because it is the only non-Gaussian symmetric alpha-stable distribution that exists in closed form and also because it leads to the design of a nearly optimum detector with robust detection performance. We analyze the performance of the new detector in terms of the associated probabilities of detection and false alarm and we compare it to the performance of the generalized Gaussian detector by performing experiments with various test images.
80B4B08A	One of the threats to watermarking security is unauthorized removal. In this paper, we present a framework based on extended TPM to solve this problem. As to the spread spectrum (SS) watermarking schemes we believe that it has the capability of read- but-not-remove if the platform software environment on which watermark detection process run is trusted. We think using Trusted Computing Platform could enhance the security of watermarking detection. Firstly, to support our framework, we point out that standard TCG must be extended. Secondly, we discuss the security and feasibility about our method. Finally, we use a SS watermarking scheme as an example and come to a conclusion that a traditional watermarking scheme could get the capability of secure detection after being transformed based on our framework.
7811A20D	A ROI (region of interest) of a medical image is an area including important information and must be stored without any distortion. In order to achieve optimal compression as well as satisfactory visualization of medical images, we compress the ROI by lossless compression, and the rest by lossy compression. Furthermore, security is an important issue in web-based medical information system. Watermarking skill is often used for protecting medical images. In this paper, we present a robust technique embedding the watermark of signature information or textual data around the ROI of a medical image based on genetic algorithms. A fragile watermark is adopted to detect any unauthorized modification. The embedding of watermark in the frequency domain is more difficult to be pirated than in spatial domain.
777FAEBB	The paper is focused on the evolution of the watermarking schemes from pixel-based approaches to content based methods. Firstly, the field of watermarking is presented, its implication in the real word is outlined. The second section gives a brief overview of different approaches in the spatial and the frequential domain. We then present different content-based approaches used to improve the invisibility or the synchronisation of the mark. The last section describes our content-based scheme; the synchronisation of the mark after image processing lies in the use of feature point detectors. The image is decomposed into a set of triangles and each triangle is individually marked.
128FCFCC	In-network processing presents a critical challenge for data authentication in wireless sensor networks (WSNs). Current schemes relying on Message Authentication Code (MAC) cannot provide natural support for this operation since even a slight modification to the data invalidates the MAC. Although some recent works propose using privacy homomorphism to support in-network processing, they can only work for some specific query-based aggregation functions, e.g. SUM, average, etc. In this paper, based on digital watermarking, we propose an end-to-end, statistical approach for data authentication that provides inherent support for in-network processing. In this scheme, authentication information is modulated as watermark and superposed on the sensory data at the sensor nodes. The watermarked data can be aggregated by the intermediate nodes without incurring any en route checking. Upon reception of the sensory data, the data sink is able to authenticate the data by validating the watermark, thereby detecting whether the data has been illegitimately altered. In this way, the aggregation–survivable authentication information is only added at the sources and checked by the data sink, without any involvement of intermediate nodes. Furthermore, the simple operation of watermark embedding and complex operation of watermark detection provide a natural solution of function partitioning between the resource limited sensor nodes and the resource abundant data sink. In addition, the watermark can be embedded in both spatial and temporal domains to provide the flexibility between the detection time and detection granularity. The simulation results show that the proposed scheme can successfully authenticate the sensory data with high confidence.
753DC3A7	A  new watermarking scheme having the ability of sharing secret with multi-users is proposed. It splits the original watermark into two shares and embeds one share into the cover image to increase the security. A polarization procedure is performed to establish a polarity stream from the cover image. The second share and the polarity stream are used to generate a master key and several normal keys. In this system, only the super-user can reveal the genuine watermark directly. Other users possess the normal keys can obtain shadow watermarks merely. By combining the shadow watermarks together, the real watermark can be recovered.
802F0C1D	A method is presented for marking high-quality digital images with a robust and invisible watermark. A broad definition of robustness, stated as fundamental, is used. It requires the invisible mark to survive and remain detectable through all image manipulations that in themselves does not damage the image beyond useability. These manipulations include JPEG "lossy" compression and, in the extreme, the printing and rescanning of the image. The watermark is imparted onto an image as a random, bur reproducible, small modulation of its pixel brightnesses, and becomes a permanent part of the marked image. Detecting the imparted watermark, especially after image manipulation, is a daunting task. It is one of detecting the presence of a known small modulation of a random carrier where the carrier is composed of the pixel brightness values of the unmarked image. The method presented exploits the not well understood but superb ability of the human visual system to recognize a correlated pattern in a scatter diagram called a "visualizer-coincidence image." Results of application of the method are presented.
7B8AF693	This paper presents a watermarking scheme, which is robust and can sustain almost all attack on audio file. In the proposed watermarking scheme use a robust watermark which can resist all attacks. The feature of the proposed scheme is to resist minimum mean square error (MMSE) estimation attack. In audio samples an energy efficient watermark is embedded which satisfies the power spectrum condition (PSC). In PSC compliant technique watermark's power spectrum is directly proportional to that of the original signal. The watermark satisfying PSC are proven to be most robust. Energy efficient watermarking scheme resist MMSE as much as possible. Experiments justify that the proposed scheme is inaudible and robust against various attack such as noise adding, resampling, requantization, random cropping and MP3 compression.
76441826	In this paper, a DC-based approach to robust watermarking is proposed. Classical watermarking schemes widely for frequency domain hide information in various frequency components. Due to codec operations, hidden information usually is corrupted and damaged. The lack of robustness against possible codec attack is an important issue of advanced watermarking technique. In this study, we find the polarity of DC components in each image block is robust to the DCT/IDCT operations. Thus, a new watermarking scheme based DC-component is developed. In this proposed scheme, information bits are encoded and hidden in the DC-components of each image block. The experimental results indicate that (1) hidden information can be recovered from compression/decompression attacks and (2) the information-hiding process is low computation- complexity. Besides, the relationship between block size and the amount of hidden information is also investigated to illustrate the deployment strategy in real applications.
7FAA1196	The growth of new imaging technologies has created a need for techniques that can be used for copyright protection of digital images and video. One approach for copyright protection is to introduce an invisible signal, known as a digital watermark, into an image or video sequence. In this paper, we describe digital watermarking techniques, known as perceptually based watermarks, that are designed to exploit aspects of the the human visual system in order to provide a transparent (invisible), yet robust watermark. In the most general sense, any watermarking technique that attempts to incorporate an invisible mark into an image is perceptually based. However, in order to provide transparency and robustness to attack, two conflicting requirements from a signal processing perspective, more sophisticated use of perceptual information in the watermarking process is required. We describe watermarking techniques ranging from simple schemes which incorporate common-sense rules in using perceptual information in the watermarking process, to more elaborate schemes which adapt to local image characteristics based on more formal perceptual models. This review is not meant to be exhaustive; its aim is to provide the reader with an understanding of how the techniques have been evolving as the requirements and applications become better defined.
808BBE9F	3D face recognition is one of the most popular researches at present. However, 3D face model stored in databases can be tampered or overwritten with attacker's own, which results in that legitimate user failed in authentication or lost their identity. In this paper, we provide a security enhancement scheme for biometric system and implement it in 3D face recognition. In our scheme, a digital watermark algorithm for 3D face model authentication is proposed to guarantee that the 3D face model in databases can not be modified. The difference of recognition performance between method with authentication and method without authentication is measured to evaluate our security enhancement scheme. Experiment results show the scheme and 3D face model authentication algorithm we proposed is viable and valid.
770D38E1	In this paper a fragile watermarking technique based on Karhunen-Loève transform (KLT) and genetic algorithms (GA) is proposed. To achieve high sensibility to content manipulations, the proposed algorithm slightly modifies the middle-frequency KLT coefficients while maintaining the visual imperceptibility of the watermark (i.e., the average peak signal-to-noise ratio reported was 52.49 dB). It uses a GA to compensate the errors in the extraction of the authentication information, which is a pseudo-random generated watermark sequence. The experimental tests carried out show that the scheme is able to detect pixel-level alterations localizing the block in which the pixel(s) was modified.
80A861F9	The paper discusses the feasibility of coding an "undetectable" digital water mark on a standard 512/spl times/512 intensity image with an 8 bit gray scale. The watermark is capable of carrying such information as authentication or authorisation codes, or a legend essential for image interpretation. This capability is envisaged to find application in image tagging, copyright enforcement, counterfeit protection, and controlled access. Two methods of implementation are discussed. The first is based on bit plane manipulation of the LSB, which offers easy and rapid decoding. The second method utilises linear addition of the water mark to the image data, and is more difficult to decode, offering inherent security. This linearity property also allows some image processing, such as averaging, to take place on the image, without corrupting the water mark beyond recovery. Either method is potentially compatible with JPEG and MPEG processing.
7AAAD725	In this paper, we present two watermarking approaches that are robust to geometric distortions. The first approach is based on image normalization, in which both watermark embedding and extraction are carried out with respect to an image normalized to meet a set of predefined moment criteria. We propose a new normalization procedure, which is invariant to affine transform attacks. The resulting watermarking scheme is suitable for public watermarking applications, where the original image is not available for watermark extraction. The second approach is based on a watermark resynchronization scheme aimed to alleviate the effects of random bending attacks. In this scheme, a deformable mesh is used to correct the distortion caused by the attack. The watermark is then extracted from the corrected image. In contrast to the first scheme, the latter is suitable for private watermarking applications, where the original image is necessary for watermark detection. In both schemes, we employ a direct-sequence code division multiple access approach to embed a multibit watermark in the discrete cosine transform domain of the image. Numerical experiments demonstrate that the proposed watermarking schemes are robust to a wide range of geometric attacks.
7EF782AE	The issue of copyright protection of digital multimedia data has attracted a lot of attention during the last decade. An efficient copyright protection method that has been gaining popularity is watermarking, i.e., the embedding of a signature in a digital document that can be detected only by its rightful owner. Watermarks are usually blindly detected using correlating structures, which would be optimal in the case of Gaussian data. However, in the case of DCT-domain image watermarking, the data is more heavy-tailed and the correlator is clearly suboptimal. Nonlinear receivers have been shown to be particularly well suited for the detection of weak signals in heavy-tailed noise, as they are locally optimal. This motivates the use of the Gaussian-tailed zero-memory nonlinearity, as well as the locally optimal Cauchy nonlinearity for the detection of watermarks in DCT transformed images. We analyze the performance of these schemes theoretically and compare it to that of the traditionally used Gaussian correlator, but also to the recently proposed generalized Gaussian detector, which outperforms the correlator. The theoretical analysis and the actual performance of these systems is assessed through experiments, which verify the theoretical analysis and also justify the use of nonlinear structures for watermark detection. The performance of the correlator and the nonlinear detectors in the presence of quantization is also analyzed, using results from dither theory, and also verified experimentally.
78FFE60B	Recently, several semi-fragile watermarking approaches with the additional capability of image recovery have been proposed. However, the security, robustness, and image recovery aspect of these approaches have certain shortcomings. In this paper, a novel semi-fragile watermarking framework using integer transform based information embedding and extraction is proposed, which allows accurate authentication and recovery of the image. It is based on integer wavelet transform with improved security against collage attack, enhanced robustness, and capability of producing better quality recovered image. Security is enhanced by correlating the to-be-embedded watermark with the approximation subband of wavelet transform. Similarly, no unprotected area is left for attacks on the image, either in spatial or transform domain. Robustness is enhanced by using the idea of embedding in largest coefficient inside a group and correlating it with the quantized version of the mean of the group. In particular, the recovery approach is improved by introducing lossless compression and BCH coding of the integer DCT based low-pass version of the cover image itself. Alteration sensitivity is improved compared to traditional block-based approaches and thus the proposed approach can concisely determine the regions where the integrity verification fails. Experimental comparisons with existing approaches validate the usefulness of the proposed multiple semi-fragile watermarking approach.
7DAFE534	With rapidly growing interest in ways to hide information, a large number of schemes have been proposed for watermarks and other information in digital pictures, video, audio and other multimedia objects. In an attempt to overcome this problem, watermarking has been suggested in the literature as the most effective means for copyright protection and authentication. The main focus of this thesis is the problem of joint watermarking and compression of images due to bandwidth or storage constraints.
56521F97	The paper proposes the use of digital watermark based authentication for intrusion detection in IEC 61850-automated substations. The watermark can be embedded into the Least Significant Bits of the measurements without visible deterioration in precision. When Intelligent Electronics Devices gets measurements, the watermark in the measurement can be retrieved to determine whether it has been attacked and detect malicious intrusion. The proposed approach is appropriate for the time critical and resource constrained applications in substation automation system for its simplicity. Numerical simulation shows that the process latency and error incurred by watermarking is acceptable and will not impact performance of protective function in IEC 61850 automated substations.
75C44EE3	In-network processing presents a critical challenge for data authentication in wireless sensor networks (WSNs). Current schemes relying on message authentication code (MAC) cannot provide natural support for this operation since even a slight modification to the data invalidates the MAC. In this paper, based on digital watermarking, we propose an end-to-end approach for data authentication in WSNs that provides inherent support for in-network processing. In this scheme, authentication information is modulated as watermark and superposed to the sensory data at the sensor nodes. The watermarked data can be aggregated by the intermediate nodes without incurring any en-route checking. Upon reception of the sensory data, possibly distorted by the operations along the route, the data sink is able to authenticate the data by validating the watermark, detecting whether the data has been altered and where it has occurred. In this way, the aggregation-survivable authentication information is only added at the sources and checked by the data sink, without any involvement of intermediate nodes. Furthermore, the simple operation of watermark embedding and complex operation of watermark detection provide a natural solution of function partitioning between the resource limited sensor nodes and resource abundant data sink. The simulation results show that the proposed scheme can successfully authenticate the sensory data with high confidence.
7C2F314E	In this paper two watermarking algorithms for image content authentication with localization and recovery capability of the tampered regions are proposed. In both algorithms, a halftone version of the original gray-scale image is used as an approximated version of the host image (image digest) which is then embedded as a watermark sequence into given transform domains of the host image. In the first algorithm, the Integer Wavelet Transform (IWT) is used for watermark embedding which is denominated WIA-IWT (Watermarking-based Image Authentication using IWT), while in the second one, the Discrete Cosine Transform (DCT) domain is used for this purpose, we call this algorithm WIA-DCT (Watermarking-based Image Authentication using DCT). In the authentication stage the tampered regions are detected using the Structural Similarity index (SSIM) criterion, which are then recovered using the extracted halftone image. In the recovery stage, a Multilayer Perceptron (MLP) neural network is used to carry out an inverse halftoning process to improve the recovered image quality. The experimental results demonstrate the robustness of both algorithms against content preserved modifications, such as JPEG compression, as well as an effective authentication and recovery capability. Also the proposed algorithms are compared with some previously proposed content authentication algorithms with recovery capability to show the better performance of the proposed algorithms.
76FB3E9A	We present an innovative scheme of blindly extracting message bits when a watermarked image is distorted. In this scheme, we have exploited the capabilities of machine learning (ML) approaches for nonlinearly classifying the embedded bits. The proposed technique adaptively modifies the decoding strategy in view of the anticipated attack. The extraction of bits is considered as a binary classification problem. Conventionally, a hard decoder is used with the assumption that the underlying distribution of the discrete cosine transform coefficients do not change appreciably. However, in case of attacks related to real world applications of watermarking, such as JPEG compression in case of shared medical image warehouses, these coefficients are heavily altered. The sufficient statistics corresponding to the maximum likelihood based decoding process, which are considered as features in the proposed scheme, overlap at the receiving end, and a simple hard decoder fails to classify them properly. In contrast, our proposed ML decoding model has attained highest accuracy on the test data. Experimental results show that through its training phase, our proposed decoding scheme is able to cope with the alterations in features introduced by a new attack. Consequently, it achieves promising improvement in terms of bit correct ratio in comparison to the existing decoding scheme.
7883FD41	Compared with Generalized Gaussian distribution (GGD), Cauchy distribution is superior to describe the statistical distribution of the Intra-coded DCT coefficients in H.264/AVC For the bipolar additive watermark in H.264/AVC video stream, a Cauchy distribution based detection algorithm is proposed by ternary hypothesis testing. Experimental results show that the proposed approach can achieve more than 80% on average for the accuracy of watermark detection.
7CAC2376	Watermarking techniques are applied to digital media to protect their integrity and copyright. The embedding of a watermark, however, often distorts the quality of the protected image. This may be intolerable since the protected media is for preserving artistic and valuable images. Hence, engineers have proposed removable solutions permitting authorized users to restore watermarked images to unmarked images with satisfactory quality. Unfortunately, these mechanisms cannot resist signal processing attacks to protect the ownership. In this article, we propose a novel watermarking mechanism by utilizing pair-difference correlations upon subsampling and the technique of JND. This new approach can guarantee the robust essentials of watermarking schemes. Experimental results reveal that the new method outperforms others in terms of restored image quality. More specifically, this novel approach can resist various attacks to which related works are vulnerable.
82802DDF	A content authentication and tamper recovery scheme for digital speech signal is proposed. In this paper, a new compression method for speech signal based on discrete cosine transform is discussed, and the compressed signals obtained are used to tamper recovery. One block-based large capacity embedding method is explored, which is used for embedding the compressed signals. For the scheme proposed, watermark is generated by frame number and compressed signal. If watermarked speech is attacked, the attacked frames can be located by frame number, and reconstructed by using the compressed signal. Theoretical analysis and experimental results demonstrate that the scheme not only improves the security of watermark system, but also can locate the attacked frames precisely and reconstruct the attacked frames.
7D49B3E7	The widespread use of digital multimedia data has increased the need for effective means of copyright protection. Watermarking has attracted much attention, as it allows the embedding of a signature in a digital document in an imperceptible manner. In practice, watermarking is subject to various attacks, intentional or unintentional, which degrade the embedded information, rendering it more difficult to detect. A very common, but not malicious, attack is quantization, which is unavoidable for the compression and transmission of digital data. The effect of quantization attacks on the nearly optimal Cauchy watermark detector is examined in this paper. The quantization effects on this detection scheme are examined theoretically, by treating the watermark as a dither signal. The theory of dithered quantizers is used in order to correctly analyze the effect of quantization attacks on the Cauchy watermark detector. The theoretical results are verified by experiments that demonstrate the quantization effects on the detection and error probabilities of the Cauchy detection scheme.
6C8550AF	For verification and authentication of the video material and recovery of the original video several security mechanisms are required. The security techniques to realize this solution are introduced in: The verification of the integrity is verified by hash functions.Authenticity is verified by digital signatures using asymmetric cryptography and hash functions. The introduced scheme from [1] uses RSA signatures. The private key of the digital signature mechanism is used to sign the data and the corresponding public key is used for verification of the encrypted data. If the data can be verified the corresponding private key was used for the digital signature generation and the data seems to be authentic as well integer.Furthermore the original content can be reproduced by inverting of the watermark with the well know techniques of Fridrich et al. [2]. Additional secret key cryptography (symmetric crypt function) protects the reproduction. The invertibility is necessary, because we use a digital watermark to embed the authentication message and signature into the media itself. Digital watermark, in this application fragile watermark, changes the data and the original data cannot be reconstructing. To invert the data, the watermark must be removed and the original data reconstructed. The watermark embeds the information into a non visual or acoustical channel of the data after the original data of the channel were compressed and encrypted. The compression realizes the new space for the watermark consisting of the encrypted selected data and security information.
7E430A10	We have entered an era where inexpensive and readily-available equipment can produce perfect copies of digital multimedia materials, such as CD-quality audio, publication-quality images, or digital video. In this environment, it has become easier for malicious parties to make salable copies of copyrighted content without compensation to the content owner. Many media content owners are concerned about the potential loss of revenue from multimedia piracy, especially when the content will be exposed to the Internet. Digital watermarking is seen by many as a potential solution to this problem. Many different watermarking schemes have been proposed. Often, however there is little discussion of how effective a proposed watermarking technique may be at solving a particular problem. We describe a number of proposed image-watermarking application scenarios and form a small number of watermark-application categories. Then, with these applications in mind, we discuss the desired technical properties of watermarks for each category. Finally we discuss some watermarking techniques developed by the authors, in light of the desired properties.
7DB1BDB2	Watermarks allow embedded signals to be extracted from audio and video content for a variety of purposes. One application is for copyright control, where it is envisaged that digital video recorders will not permit the recording of content that is watermarked as "never copy". In such a scenario, it is important that the watermark survive both normal signal transformations and attempts to remove the watermark so that an illegal copy can be made. We discuss to what extent a watermark can be resistant to tampering and describe a variety of possible attacks.
7D3761ED	The watermarking of digital images, audio, video, and multimedia products in general has been proposed for resolving copyright ownership and verifying originality of content. This paper studies the contribution of watermarking for developing protection schemes. A general watermarking framework (GWF) is studied and the fundamental demands are listed. The watermarking algorithms, namely watermark generation, embedding, and detection, are analyzed and necessary conditions for a reliable and efficient protection are stated. Although the GWF satisfies the majority of requirements for copyright protection and content verification, there are unsolved problems inside a pure watermarking framework. Particular solutions, based on product registration and related network services, are suggested to overcome such problems.
7FA69773	This paper proposes a novel watermarking scheme with flexible self-recovery quality. The embedded watermark data for content recovery are calculated from the original discrete cosine transform (DCT) coefficients of host image and do not contain any additional redundancy. When a part of a watermarked image is tampered, the watermark data in the area without any modification still can be extracted. If the amount of extracted data is large, we can reconstruct the original coefficients in the tampered area according to the constraints given by the extracted data. Otherwise, we may employ a compressive sensing technique to retrieve the coefficients by exploiting the sparseness in the DCT domain. This way, all the extracted watermark data contribute to the content recovery. The smaller the tampered area, the more available watermark data will result in a better quality of recovered content. It is also shown that the proposed scheme outperforms previous techniques in general.
7A5CF78E	Many digital watermarking algorithms are proposed in the literature. Broadly these watermarking algorithms can be classified into two main categories. The first category of algorithms uses a pseudo random Gaussian sequence (PRGS) watermark whereas the second category of algorithms uses a binary logo as a watermark. The main advantage of PRGS based watermarking scheme is its ability to detect the presence of watermark without manual intervention. However the main drawback is calculating reliable threshold value. In the similar manner the main advantage of binary logo watermark is that there is no need to calculate threshold value but requires manual intervention to detect the presence of watermark. The advantage and disadvantage of either approach is quite clear hence it would be a good idea to design a watermarking scheme which inherits the advantages from both these approaches. In this paper we present one such approach which is termed as bar-code watermarking. The proposed scheme offers objective as well as subjective detection. A PRGS sequence watermark is represented as a bar-code on a binary logo and embedded in the host image. Watermark detection can be either done subjectively or objectively
77B4C1C7	The work presented here deals with watermarking algorithms. The goal is to show how the Human Visual System (H.V.S) properties can be taken into account in the conception of such algorithms. The construction of the watermarking algorithm presented in this paper needs three steps. In the first one the selection of auspicious sites for the watermark embedding is described. The selection exploits a multi-channel model of the Human Visual System which decomposes the visual input into seventeen perceptual components. Medium and high frequencies are then selected to generate a sites map. This latter is improved by considering some high level uniform areas. The second step deals with the choice of the strength to apply to the selected sites. The strength is determined by considering the H.V.S. sensitivity to the local band limited contrast. In the third step, examples of spatial watermarking embedding and extraction are given. The same perceptual mask has been successfully used in other studies. The watermark results from a binary pseudo-random sequence, of length 64, which is circularly shifted so as to occupy all the sites mentioned above. The watermark extraction exploits the detection theory and requires both the perceptual mask and the original watermark. The extracted watermark is then compared to the original and a normalized correlation coefficient is computed. This coefficient value allows the detection of the copyright.
7FB6F52A	Robustness is one of the crucial issues in digital watermarking. Especially the robustness against geometric distortion and JPEG compression at the same time remains challenging. In this paper, a locally linear embedding (LLE) based watermarking algorithm that is robust against affine transformation is proposed. This algorithm improves the robustness via the intrinsic robustness of the LLE. A random generated watermark is embedded in the coefficients of reconstruction weights of the locally linear embedding. In watermark extraction, the watermark can be extracted almost the same process as the watermark embedding. Experimental results have demonstrated that the proposed watermarking scheme is more robust than other watermarking algorithms reported in the literature. Specifically, it is robust against almost all affine transform related testing functions in StirMark 3.1. While the approach is presented for gray-level images, it can be applied to color images and video sequences.
7DAEB614	Traditional intrusion detection systems (IDS) detect attacks by comparing current behavior to signatures of known attacks. One main drawback is the inability of detecting new attacks which do not have known signatures. In this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. Behavior that deviates from the learned normal model signals possible novel attacks. Our IDS is unique in two respects. First, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. This prevents alarm floods. Second, the IDS learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. On the 1999 DARPA IDS evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (IP addresses and ports, as modeled by most other systems) and protocol anomalies. Because our methods are unconventional there is a significant non-overlap of our IDS with the original DARPA participants, which implies that they could be combined to increase coverage.
7566A357	As the information technology grows interests in the intrusion detection system (IDS), which detects unauthorized usage, misuse by a local user and modification of important data, has been raised. In the field of anomaly-based IDS several data mining techniques such as hidden Markov model (HMM), artificial neural network, statistical techniques and expert systems are used to model network packets, system call audit data, etc. However, there are undetectable intrusion types for each measure and modeling method because each intrusion type makes anomalies at individual measure. To overcome this drawback of single-measure anomaly detector, this paper proposes a multiple-measure intrusion detection method. We measure normal behavior by systems calls, resource usage and file access events and build up profiles for normal behavior with hidden Markov model, statistical method and rule-base method, which are integrated with a rule-based approach. Experimental results with real data clearly demonstrate the effectiveness of the proposed method that has significantly low false-positive error rate against various types of intrusion.
7FACBDE4	The network intrusion detection (NIDS) is faced with the question to detect many kinds of intrusion. In order to detect the complex attack, Network Intrusion detection system need to analysis massive data captured form different network safety equipments. So a new multi relational mining algorithm MRA2 is proposed. MRA2 depend on the association rules mining technology and the probability function dependency method which is proposed through extending the theory of function dependency. MRA2 is able to synthesize the various datalog resources to detect intrusion effectively and reappear to the complex network attack scenario.
7E7969F7	A new approach to representing computer penetrations is introduced called penetration state transition analysis. This approach models penetrations as a series of state transitions described in terms of signature actions and state descriptions. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule-based expert system for detecting penetrations, referred to as STAT.
7DCBAD4B	The paper focuses on intrusion detection and countermeasures with respect to widely-used operating systems and networks. The design and architecture of an intrusion detection system built from distributed agents is proposed to implement an intelligent system on which data mining can be performed to provide global, temporal views of an entire networked system. A starting point for agent intelligence in the system is the research into the use of machine learning over system call traces from the privileged sendmail program on UNIX. The authors use a rule learning algorithm to classify the system call traces for intrusion detection purposes and show the results.
5C2490E0	A modified RBF (radial basis function)-based neural network is proposed for network anomaly detection. Special attention is given to the determination of the parameters of the hidden layer. We propose a novel grid-based approach to compress and cluster the training data. The number, center and radii of the RBFs are determined according to the clustering result. At the detecting stage, we expand each input node with a sigmoid function to meet the type of input data. Experimental result on KDD 99 intrusion detection datasets shows that our RBF based IDS has high detection rate while maintaining a low false positive rate. It also shows the remarkable ability of our IDS to detect new type of attacks.
7E0DD16C	Classification of intrusion attacks and normal network traffic is a challenging and critical problem in pattern recognition and network security. In this paper, we present a novel intrusion detection approach to extract both accurate and interpretable fuzzy IF–THEN rules from network traffic data for classification. The proposed fuzzy rule-based system is evolved from an agent-based evolutionary framework and multi-objective optimization. In addition, the proposed system can also act as a genetic feature selection wrapper to search for an optimal feature subset for dimensionality reduction. To evaluate the classification and feature selection performance of our approach, it is compared with some well-known classifiers as well as feature selection filters and wrappers. The extensive experimental results on the KDD-Cup99 intrusion detection benchmark data set demonstrate that the proposed approach produces interpretable fuzzy systems, and outperforms other classifiers and wrappers by providing the highest detection accuracy for intrusion attacks and low false alarm rate for normal network traffic with minimized number of features.
75F2DC32	This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers’ (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks.
7D83A406	Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection.
7B75FDC9	Traffic anomalies and attacks are commonplace in today's networks, and identifying them rapidly and accurately is critical for large network operators. Intrusion detection systems are an important component of defensive measures protecting computer systems and networks from abuse.For an intrusion detection system, it is important to detect previously known attacks with high accuracy. However, detecting previously unseen attacks is equally important in order to minimize the losses as a result of a successful intrusion. It is also equally important to detect attacks at an early stage in order to minimize their impact. To address these challenges, this paper proposes to improve the efficiency of the network intrusion detection process by including an Event Calculus based specification to detect the registered and expected behaviour of the whole network.
7ED7AE7B	As complete prevention of computer attacks is not possible, intrusion detection systems (IDSs) play a very important role in minimizing the damage caused by different computer attacks. There are two intrusion detection methods: namely misuse- and anomaly-based. A collaborative, intelligent intrusion detection system (CIIDS) is proposed to include both methods, since it is concluded from recent research that the performance of an individual detection engine is rarely satisfactory. In particular, two main challenges in current collaborative intrusion detection systems (CIDSs) research are highlighted and reviewed: CIDSs system architectures and alert correlation algorithms. Different CIDSs system, architectures are explained and compared. The use of CIDSs together with other multiple security systems raise certain issues and challenges in, alert correlation. Several different techniques for alert correlation are discussed. The focus will be on correlation of CIIDS alerts. Computational, Intelligence approaches, together with their applications on IDSs, are reviewed. Methods in soft computing collectively provide understandable, and autonomous solutions to IDS problems. At the end of the review, the paper suggests fuzzy logic, soft computing and other AI techniques, to be exploited to reduce the rate of false alarms while keeping the detection rate high. In conclusion, the paper highlights opportunities for an integrated solution to large-scale CIIDS.
7BAE61AD	There are some issues for the shuffle network intrusion detection, such as high loss detection rates and time-consuming procedures. This paper proposes a shuffle network intrusion detection method fusing the misuse behavior analysis and analyzes the network misuse behavior procedures. According to the damaged data flow balance features by network misuse behavior, the paper applies the hypothesis test in probability theory to evaluate whether the confidence interval excesses 0. If the confidence interval does not contain zero, it indicates the presence of feed-forward network intrusion; otherwise, there is no feed-forward network intrusion. The experimental results show that this method can effectively solve the multi-packet collaborative intrusion problems. Compared to traditional methods, the test speed and accuracy of the method is significantly improved. 
788D804C	Security has become an important issue for networks. Intrusion detection technology is an effective approach in dealing with the problems of network security. In this paper, we present an intrusion detection model based on hybrid fuzzy logic and neural network. The key idea is to take advantage of different classification abilities of fuzzy clustering and neural network for intrusion detection system. The new model has ability to recognize an attack, to differentiate one attack from another (i.e. classifying attacks), and the most important, to detect new attacks with high detection rate and low false negative. Training and testing data were obtained from the Defense Advanced Research Projects Agency intrusion detection evaluation data set. 
75F80375	In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal compression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average. 
772CD65D	 Eight sites participated in the second DARPA off-line intrusion de-tection evaluation in 1999. A test bed generated live background traffic similar to that on a government site containing hundreds of users on thousands of hosts. More than 200 instances of 58 attack types were launched against victim UNIX and Windows NT hosts in three weeks of training data and two weeks of test data. False alarm rates were low (less than 10 per day). Best de-tection was provided by network-based systems for old probe and old denial-of-service (DoS) attacks and by host-based systems for Solaris user-to-root (U2R) attacks. Best overall performance would have been provided by a com-bined system that used both host-based and network-based intrusion detection. De-tection accuracy was poor for previously unseen new, stealthy, and Windows NT attacks. Ten of the 58 attack types were completely missed by all systems. Systems missed attacks because protocols and TCP services were not analyzed at all or to the depth required, because signatures for old attacks did not gen-eralize to new attacks, and because auditing was not available on all hosts. Promising capabilities were demonstrated by host-based systems, by anomaly detection systems, and by a system that performs forensic analysis on file system data.
7E496032	Presented in this paper is the way of getting large signal parameters of MESFET model by the use of small signal S-parameters. Small-signal S-parameters allow expressing to nonlinearity of model, which are used in account of large-signal MESFET model.
5BCE3A64	As network attacks have increased in number and severity over the past few years, intrusion detection system (IDS) is increasingly becoming a critical component to secure the network. Due to large volumes of security audit data as well as complex and dynamic properties of intrusion behaviors, optimizing performance of IDS becomes an important open problem that is receiving more and more attention from the research community. The uncertainty to explore if certain algorithms perform better for certain attack classes constitutes the motivation for the reported herein. In this paper, we evaluate performance of a comprehensive set of classifier algorithms using KDD99 dataset. Based on evaluation results, best algorithms for each attack category is chosen and two classifier algorithm selection models are proposed. The simulation result comparison indicates that noticeable performance improvement and real-time intrusion detection can be achieved as we apply the proposed models to detect different kinds of network attacks.
7F88C3B5	Intrusion detection based upon computational intelligence is currently attracting considerable interest from the research community. Characteristics of computational intelligence (CI) systems, such as adaptation, fault tolerance, high computational speed and error resilience in the face of noisy information, fit the requirements of building a good intrusion detection model. Here we want to provide an overview of the research progress in applying CI methods to the problem of intrusion detection. The scope of this review will encompass core methods of CI, including artificial neural networks, fuzzy systems, evolutionary computation, artificial immune systems, swarm intelligence, and soft computing. The research contributions in each field are systematically summarized and compared, allowing us to clearly define existing research challenges, and to highlight promising new research directions. The findings of this review should provide useful insights into the current IDS literature and be a good source for anyone who is interested in the application of CI approaches to IDSs or related fields.
7EF318D2	Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. We study one such observable-sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences; comparison of relative frequencies of different sequences; a rule induction technique; and hidden Markov models (HMMs). We discuss the factors affecting the performance of each method and conclude that for this particular problem, weaker methods than HMMs are likely sufficient.
77CCBE06	In this paper, a four-angle-star based visualized feature generation approach, FASVFG, is proposed to evaluate the distance between samples in a 5-class classification problem. Based on the four angle star image, numerical features are generated for network visit data from KDDcup99, and an efficient intrusion detection system with less features is proposed. The FASVFG-based classifier achieves a high generalization accuracy of 94.3555% in validation experiment, and the average Mathews correlation coefficient reaches 0.8858.
7D9C83BB	The paper presents a new approach to representing and detecting computer penetrations in real time. The approach, called state transition analysis, models penetrations as a series of state changes that lead from an initial secure state to a target compromised state. State transition diagrams, the graphical representation of penetrations, identify precisely the requirements for and the compromise of a penetration and present only the critical events that must occur for the successful completion of the penetration. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule based expert system for detecting penetrations, called the state transition analysis tool (STAT). The design and implementation of a Unix specific prototype of this expert system, called USTAT, is also presented. This prototype provides a further illustration of the overall design and functionality of this intrusion detection approach. Lastly, STAT is compared to the functionality of comparable intrusion detection tools.
7FFD6B99	Intrusion detection systems monitor system activities to identify unauthorized use, misuse, or abuse. IDSs offer a defense when your system's vulnerabilities are exploited and do so without requiring you to replace expensive equipment. The steady growth in research on intrusion detection systems has created a demand for tools and methods to test their effectiveness. The authors have developed a software platform that both simulates intrusions and supports their systematic methodology for IDS testing.
7A1333F9	Wireless sensor networks (WSNs) are vulnerable to a variety of malicious attacks, especially the packet dropping attack, making security an important research field. Since prevention based techniques are less helpful for guarding against inside attacks, intrusion detection (ID) techniques are indispensable to provide advanced protection. This paper proposes an innovative cellular ID framework for packet dropping attack, which takes the deployment of passive listening nodes into consideration. Performance evaluation made in VisualSense demonstrates its high detection accuracy.
5B7700D1	Wireless ad hoc network is becoming a new research frontier, in which security is an important issue. Usually some nodes act maliciously and they are able to do different kinds of Denial of Service (Dos). Because of the limited resource, intrusion detection system (IDS) runs all the time to detect intrusion of the attacker which is a costly overhead. We use game theory to model the interactions between the intrusion detection system and the attacker, and a realistic model is given by using Bayesian game. We solve the game by finding the Bayesian Nash equilibrium. The results of our analysis show that the IDS could work intermittently without compromising its effectiveness. At the end of this paper, we provide an experiment to verify the rationality and effectiveness of the proposed model.
79406BD7	Intrusion Detection Systems (IDS) have nowadays become a necessary component of almost every security infrastructure. So far, many different approaches have been followed in order to increase the efficiency of IDS. Swarm Intelligence (SI), a relatively new bio-inspired family of methods, seeks inspiration in the behavior of swarms of insects or other animals. After applied in other fields with success SI started to gather the interest of researchers working in the field of intrusion detection. In this paper we explore the reasons that led to the application of SI in intrusion detection, and present SI methods that have been used for constructing IDS. A major contribution of this work is also a detailed comparison of several SI-based IDS in terms of efficiency. This gives a clear idea of which solution is more appropriate for each particular case.
78FC71D4	With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.
7054D090	There is often the need to update an installed intrusion detection system (IDS) due to new attack methods or upgraded computing environments. Since many current IDSs are constructed by manual encoding of expert knowledge, changes to IDSs are expensive and slow. We describe a data mining framework for adaptively building Intrusion Detection (ID) models. The central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session, and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. These rules can then be used for misuse detection and anomaly detection. New detection models are incorporated into an existing IDS through a meta-learning (or co-operative learning) process, which produces a meta detection model that combines evidence from multiple models. We discuss the strengths of our data mining programs, namely, classification, meta-learning, association rules, and frequent episodes. We report on the results of applying these programs to the extensively gathered network audit data for the 1998 DARPA Intrusion Detection Evaluation Program.
7A1919E6	Many different demands can be made of intrusion detection systems. An important requirement is that it be effective i.e. that it should detect a substantial percentage of intrusions into the supervised system, while still keeping the false alarm rate at an acceptable level.This paper aims to demonstrate that, for a reasonable set of assumptions, the false alarm rate is the limiting factor for the performance of an intrusion detection system. This is due to the base-rate fallacy phenomenon, that in order to achieve substantial values of the Bayesian detection rate, P(Intrusion|Alarm), we have to achieve—a perhaps unattainably low—false alarm rate.A selection of reports of intrusion detection performance are reviewed, and the conclusion is reached that there are indications that at least some types of intrusion detection have far to go before they can attain such low false alarm rates.
028BE359	Intrusion detection corresponds to a suite of techniques that are used to identify attacks against computers and network infrastructures. Anomaly detection is a key element of intrusion detection in which perturbations of normal behavior suggest the presence of intentionally or unintentionally induced attacks, faults, defects, etc. This paper focuses on a detailed comparative study of several anomaly detection schemes for identifying different network intrusions. Several existing supervised and unsupervised anomaly detection schemes and their variations are evaluated on the DARPA 1998 data set of network connections [9] as well as on real network data using existing standard evaluation techniques as well as using several specific metrics that are appropriate when detecting attacks that involve a large number of connections. Our experimental results indicate that some anomaly detection schemes appear very promising when detecting novel intrusions in both DARPA’98 data and real network data.
61E164F2	Misuse detection is the process of attempting to identify instances of network attacks by comparing current activity against the expected actions of an intruder. Most current approaches to misuse detection involve the use of rule-based expert systems to identify indications of known attacks. However, these techniques are less successful in identifying attacks which vary from expected patterns. Artificial neural networks provide the potential to identify and classify network activity based on limited, incomplete, and nonlinear data sources. We present an approach to the process of misuse detection that utilizes the analytical strengths of neural networks, and we provide the results from our preliminary analysis of this approach.
78EEC707	A simple CAD model is proposed for the short-channel enhancement-mode MOSFET. The conventional use of drain bias modulation of channel length to describe saturation characteristics has been discarded and replaced by drain bias enhancement of channel velocity. The model possesses continuity of current, transconductance and output conductance throughout the triode, and saturation ranges of operation. It has been tested against experimental transistors and against two-dimensional numerically simulated transistors, and has given satisfactory results in all cases. The model is based on good physics, is easy to understand, is straightforward to use, and is computationally efficient. 
7518A3AF	Today's computer systems are vulnerable both to abuse by insiders and to penetration by outsiders, as evidenced by the growing number of incidents reported in the press. To close all security loopholes from today's systems is infeasible, and no combination of technologies can prevent legitimate users from abusing their authority in a system; thus auditing is viewed as the last line of defense.Over the past several years, the computer security community has been developing automated tools to analyze computer system audit data for suspicious user behavior. This paper describes the use of such tools for detecting computer system intrusion and describes further technologies that may be of use for intrusion detection in the future.
8042453A	Intrusion detection systems (IDSs) attempt to identify unauthorized use, misuse, and abuse of computer systems. In response to the growth in the use and development of IDSs, the authors have developed a methodology for testing IDSs. The methodology consists of techniques from the field of software testing which they have adapted for the specific purpose of testing IDSs. They identify a set of general IDS performance objectives which is the basis for the methodology. They present the details of the methodology, including strategies for test-case selection and specific testing procedures. They include quantitative results from testing experiments on the Network Security Monitor (NSM), an IDS developed at UC Davis. They present an overview of the software platform that has been used to create user-simulation scripts for testing experiments. The platform consists of the UNIX tool expect and enhancements that they have developed, including mechanisms for concurrent scripts and a record-and-replay feature. They also provide background information on intrusions and IDSs to motivate their work.
7C8F7EE3	In order to solve the problems of high false alarm rate and fail rate in intrusion detection system of Computer Integrated Process System (CIPS) network, this paper takes advantage that Genetic Algorithm (GA) possesses overall optimization seeking ability and neural network has formidable approaching ability to the non-linear mapping to propose an intrusion detection model based on Genetic Algorithm Neural Network (GANN) with self-learning and adaptive capacity, which includes data collection module, data preprocessing module, neural network analysis module and intrusion alarm module. To overcome the shortcomings that GA is easy to fall into the extreme value and searches slowly, it improves the adjusting method of GANN fitness value and optimizes the parameter settings of GA. The improved GA is used to optimize BP neural network. Simulation results show that the model makes the detection rate of the system enhance to 97.11%.
811F9DC4	 The author presents the design and implementation of a real-time intrusion detection tool, called USTAT, a state transition analysis tool for UNIX. This is a UNIX-specific implementation of a generic design developed by A. Porras and R.A. Kemmerer (1992) as STAT, a state transition analysis tool. State transition analysis is a new approach to representing computer penetrations. In STAT, a penetration is identified as a sequence of state changes that take the computer system from some initial state to a target compromised state. The development of the first USTAT prototype, which is for SunOS 4.1.1, is discussed. USTAT makes use of the audit trails that are collected by the C2 basic security module of SunOS, and it keeps track of only those critical actions that must occur for the successful completion of the penetration. This approach differs from other rule-based penetration identification tools that pattern match sequences of audit records.
03F42860	In the Clinical Decision Support System (CDSS), over-fitting phenomenon may appear when decision tree algorithm was used. For this problem, this paper will make use of the Rough Set theory to the training set for attribute reduction, the decision tree built by using the decision tree algorithm was used to predict the test data. In this paper, 46 copies of coronary heart disease clinical data were used to test the improved algorithm. Comparing the accuracy of the algorithm and the improved algorithm, we can know that, the improved algorithm has a better recognition rate for the diagnosis of coronary heart disease, and effectively solves the over-fitting phenomenon in the Decision Tree Algorithm.
77531D91	Identify individual and environmental variables associated with caregiver stability and instability for children in diverse permanent placement types (i.e., reunification, adoption, and long-term foster care/guardianship with relatives or non-relatives), following 5 or more months in out-of-home care prior to age 4 due to substantiated maltreatment.Participants were 285 children from the Southwestern site of Longitudinal Studies of Child Abuse and Neglect (LONGSCAN). Caregiver instability was defined as a change in primary caregiver between ages 6 and 8 years. Classification and regression tree (CART) analysis was used to identify the strongest predictors of instability from multiple variables assessed at age 6 with caregiver and child reports within the domains of neighborhood/community characteristics, caregiving environment, caregiver characteristics, and child characteristics.One out of 7, or 14% of the 285 children experienced caregiver instability in their permanent placement between ages 6 and 8. The strongest predictor of stability was whether the child had been placed in adoptive care. However, for children who were not adopted, a number of contextual factors (e.g., father involvement, expressiveness within the family) and child characteristics (e.g., intellectual functioning, externalizing problem behaviors) predicted stability and instability of permanent placements.Current findings suggest that a number of factors should be considered, in addition to placement type, if we are to understand what predicts caregiver stability and find stable permanent placements for children who have entered foster care. These factors include involvement of a father figure, family functioning, and child functioning.Adoption was supported as a desired permanent placement in terms of stability, but results suggest that other placement types can also lead to stability.In fact, with attention to providing biological parents, relative, and non-relative caregivers with support and resources the likelihood that a child will have a stable caregiver may be increased.
80DFDDF3	A common approach to split selection in classification trees is to search through all possible splits generated by predictor variables. A splitting criterion is then used to evaluate those splits and the one with the largest criterion value is usually chosen to actually channel samples into corresponding subnodes. However, this greedy method is biased in variable selection when the numbers of the available split points for each variable are different. Such result may thus hamper the intuitively appealing nature of classification trees. The problem of the split selection bias for two-class tasks with numerical predictors is examined. The statistical explanation of its existence is given and a solution based on the P-values is provided, when the Pearson chi-square statistic is used as the splitting criterion.
7E8F41FE	A new model for supervised classification based on probabilistic decision graphs is introduced. A probabilistic decision graph (PDG) is a graphical model that efficiently captures certain context specific independencies that are not easily represented by other graphical models traditionally used for classification, such as the Naïve Bayes (NB) or Classification Trees (CT). This means that the PDG model can capture some distributions using fewer parameters than classical models. Two approaches for constructing a PDG for classification are proposed. The first is to directly construct the model from a dataset of labelled data, while the second is to transform a previously obtained Bayesian classifier into a PDG model that can then be refined. These two approaches are compared with a wide range of classical approaches to the supervised classification problem on a number of both real world databases and artificially generated data.
7DD171A0	Classification trees are a popular tool in applied statistics because their heuristic search approach based on impurity reduction is easy to understand and the interpretation of the output is straightforward. However, all standard algorithms suffer from a major problem: variable selection based on standard impurity measures as the Gini Index is biased. The bias is such that, e.g., splitting variables with a high amount of missing values—even if missing completely at random (MCAR)—are artificially preferred. A new split selection criterion that avoids variable selection bias is introduced. The exact distribution of the maximally selected Gini gain is derived by means of a combinatorial approach and the resulting -value is suggested as an unbiased split selection criterion in recursive partitioning algorithms. The efficiency of the method is demonstrated in simulation studies and a real data study from veterinary gynecology in the context of binary classification and continuous predictor variables with different numbers of missing values. The proposed method is extendible to categorical and ordinal predictor variables and to other split selection criteria such as the cross-entropy.
79406A62	In many medical applications, longitudinal data sets are available. Longitudinal data, as well as observations from paired organs, show a dependency structure which should be respected in the evaluation. Adler et al. (Comput Stat Data Anal 53(3):718–729, 2009) proposed various bootstrapping strategies for ensemble methods based on classification trees for two measurements of paired organs. These strategies have shown to improve the classification performance compared to the traditional approach, where only one observation per subject is used. We extend the methodology to the situation, where an arbitrary number of observations per individual are available and investigate the performance of the proposed methods with bagged classification trees (bagging) and random forests in the situation of longitudinal data. Moreover, we adapt the estimation of classification performance criteria to repeated measurements data. The clinical data set consists of morphological examinations of both eyes of glaucoma patients and healthy controls over a time period of up to 7 years. The performance of our modified classifiers is evaluated by a subject-based leave-one-out bootstrap ROC analysis. Simulation results and results for the glaucoma data set demonstrate that our proposal is an improvement of adhoc strategies and of the use all measurements of each subject or block strategy.
589C9711	This paper describes the evaluation of a WSD method withinSENSEVAL. This method is based on Semantic Classification Trees (SCTs)and short context dependencies between nouns and verbs. The trainingprocedure creates a binary tree for each word to be disambiguated. SCTsare easy to implement and yield some promising results. The integrationof linguistic knowledge could lead to substantial improvement.
6A1B7A68	Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.
7AF08408	In recent years, classification learning for data streams has become an important and active research topic. A major challenge posed by data streams is that their underlying concepts can change over time, which requires current classifiers to be revised accordingly and timely. To detect concept change, a common methodology is to observe the online classification accuracy. If accuracy drops below some threshold value, a concept change is deemed to have taken place. An implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as a symptom of concept change. Unfortunately however, this assumption is often violated in the real world where data streams carry noise that can also introduce a significant reduction in classification accuracy. To compound this problem, traditional noise cleansing methods are incompetent for data streams. Those methods normally need to scan data multiple times whereas learning for data streams can only afford one-pass scan because of data’s high speed and huge volume. Another open problem in data stream classification is how to deal with missing values. When new instances containing missing values arrive, how a learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored. To solve these problems, this paper proposes a novel classification algorithm, flexible decision tree (FlexDT), which extends fuzzy logic to data stream classification. The advantages are three-fold. First, FlexDT offers a flexible structure to effectively and efficiently handle concept change. Second, FlexDT is robust to noise. Hence it can prevent noise from interfering with classification accuracy, and accuracy drop can be safely attributed to concept change. Third, it deals with missing values in an elegant way. Extensive evaluations are conducted to compare FlexDT with representative existing data stream classification algorithms using a large suite of data streams and various statistical tests. Experimental results suggest that FlexDT offers a significant benefit to data stream classification in real-world scenarios where concept change, noise and missing values coexist.
560B2421	Knowledge of the composition and areal distribution of aquatic vegetation types, as well as their seasonal and interannual variations, is crucial for managing and maintaining the balance of lake ecosystems. In this study, a series of remotely sensed images with a resolution of 30 m (HJ-CCD and Landsat TM) were collected and used to map the distribution of aquatic vegetation types in Taihu Lake, China. Seasonal and interannual dynamics of aquatic vegetation types were explored and analyzed. The distribution areas of Type I (emergent, floating-leaved and floating vegetation) and Type II (submerged vegetation) were used to model their growing season phenology by double logistic functions. The resulting double logistic models showed, the area of Type I reached its peak in mid-August, and the maximum area for Type II occurred in mid-September. From 1984 to 2013, Type I area increased continuously from 59.75 km2 to 148.00 km2 (R2 = 0.84), whereas the area covered by Type II first increased and then decreased, with a trend conforming to a significant quadratic curve (R2 = 0.83). The eutrophication and stable state of Taihu Lake was assessed using a simple indicator which was expressed as a ratio of Type II area to Type I area. The results showed that the eutrophication in the lake might have been increasing in the area studied since 2000. Additionally, the results showed that air temperature had likely a direct effect on the growth of Type I (R2 = 0.66) and a significant, but delayed, effect on the growth of Type II.
76CDF761	Economic evaluation of a new oil well is important for decision-making in the petroleum industry, and this evaluation is based on a good prediction on a well's production. However, it is difficult to accurately predict a well's production due to the complex subsurface conditions of reservoirs. The industrial standard approach is to use either curve-fitting methods or complex and time-consuming reservoir simulations. In this paper, an enhanced decision tree learning approach called neural-based decision tree (NDT) model is applied in an attempt to investigate its performance in predicting petroleum production. The primary strength of this model is that it can capture dependencies among attributes, and therefore, it is likely to provide an improved or more accurate prediction (Lee and Yen, 2002).This paper presents an application of the NDT model for petroleum prediction. Our models were developed based on the five most significant parameters that affect oil production: permeability, porosity, first shut-in pressure, residual oil and saturation of water. The five parameters were used as input variables, and oil production is the output variable for modeling. Four different models were generated in the modeling process, and each involves a different combination of parameters. First, an overall oil production model is developed using the three geoscience parameters of permeability, porosity and first shut-in pressure. Secondly, two different models, with different input parameters, were developed to predict production in the post-water flooding stage only. The results of the above models indicate that data-driven models may not be effective for classifying the data set. Hence, a trend model was developed in an attempt to improve the effectiveness and accuracy of the predictive model. The result shows that the trend model can provide an improved performance, and its performance is comparable to that of the artificial neural network.
74267BE0	This paper presents a novel decision-tree induction for a multi-objective data set, i.e. a data set with a multi-dimensional class. Inductive decision-tree learning is one of the frequently-used methods for a single-objective data set, i.e. a data set with a single-dimensional class. However, in a real data analysis, we usually have multiple objectives, and a classifier which explains them simultaneously would be useful and would exhibit higher readability. A conventional decision-tree inducer requires transformation of a multi-dimensional class into a single-dimensional class, but such a transformation can considerably worsen both accuracy and readability. In order to circumvent this problem we propose a bloomy decision tree which deals with a multi-dimensional class without such transformations. A bloomy decision tree has a set of split nodes each of which splits examples according to their attribute values, and a set of flower nodes each of which predicts a class dimension of examples. A flower node appears not only at the fringe of a tree but also inside a tree. Our pruning is executed during tree construction, and evaluates each class dimension based on Cramér’s V. The proposed method has been implemented as D3-B (Decision tree in Bloom), and tested with eleven data sets. The experiments showed that D3-B has higher accuracies in nine data sets than C4.5 and tied with it in the other two data sets. In terms of readability, D3-B has a smaller number of split nodes in all data sets, and thus outperforms C4.5.
58772DB1	Numerical data poses a problem to symbolic learning methods, since numerical value ranges inherently need to be partitioned into intervals for representation and handling. An evaluation function is used to approximate the goodness of different partition candidates. Most existing methods for multisplitting on numerical attributes are based on heuristics, because of the apparent efficiency advantages. We characterize a class of well-behaved cumulative evaluation functions for which efficient discovery of the optimal multisplit is possible by dynamic programming. A single pass through the data suffices to evaluate multisplits of all arities. This class contains many important attribute evaluation functions familiar from symbolic machine learning research. Our empirical experiments convey that there is no significant differences in efficiency between the method that produces optimal partitions and those that are based on heuristics. Moreover, we demonstrate that optimal multisplitting can be beneficial in decision tree learning in contrast to using the much applied binarization of numerical attributes or heuristical multisplitting.
7DA028E8	The available e-data throughout the Web are growing at such a high rate that data mining on the web is considered the biggest challenge of information technology. As a result it is crucial to find new and innovative ways for classifying and mining those huge amounts of data. In this paper we present an implementation of a state-of-the-art data mining algorithm on a modern FPGA. This is one of the first approaches utilizing the resources of an FPGA to accelerate certain very CPU intensive data-mining/data classification schemes and our real-world results from actual runs on hardware demonstrate that it is a highly promising one. In particular, our FPGA-based system achieves, depending on the data classified, a speedup from 4x and up to 50x (on average 25x) when compared with a state-of-the art multi-core CPU, including I/O overhead.
757153C9	The basic concepts of a multi-stage classification strategy, the decision tree classifier, are presented. The two main methods to design decision trees are presented and discussed along with some experimental results. An attempt is made to describe an Applicable Logic for the design of decision trees. Advantages and disadvantages of the both design approaches are discussed.
75881784	Given learning samples from a raster data set, spatial decision tree learning aims to find a decision tree classifier that minimizes classification errors as well as salt-and-pepper noise. The problem has important societal applications such as land cover classification for natural resource management. However, the problem is challenging due to the fact that learning samples show spatial autocorrelation in class labels, instead of being independently identically distributed. Related work relies on local tests (i.e., testing feature information of a location) and cannot adequately model the spatial autocorrelation effect, resulting in salt-and-pepper noise. In contrast, we recently proposed a focal-test-based spatial decision tree (FTSDT), in which the tree traversal direction of a sample is based on both local and focal (neighborhood) information. Preliminary results showed that FTSDT reduces classification errors and salt-and-pepper noise. This paper extends our recent work by introducing a new focal test approach with adaptive neighborhoods that avoids over-smoothing in wedge-shaped areas. We also conduct computational refinement on the FTSDT training algorithm by reusing focal values across candidate thresholds. Theoretical analysis shows that the refined training algorithm is correct and more scalable. Experiment results on real world data sets show that new FTSDT with adaptive neighborhoods improves classification accuracy, and that our computational refinement significantly reduces training time.
7E7B8D13	Support vector machines (SVMs) have shown strong generalization ability in a number of application areas, including protein structure prediction. However, the poor comprehensibility hinders the success of the SVM for protein structure prediction. The explanation of how a decision made is important for accepting the machine learning technology, especially for applications such as bioinformatics. The reasonable interpretation is not only useful to guide the "wet experiments," but also the extracted rules are helpful to integrate computational intelligence with symbolic AI systems for advanced deduction. On the other hand, a decision tree has good comprehensibility. In this paper, a novel approach to rule generation for protein secondary structure prediction by integrating merits of both the SVM and decision tree is presented. This approach combines the SVM with decision tree into a new algorithm called SVM_DT, which proceeds in three steps. This algorithm first trains an SVM. Then, a new training set is generated through careful selection from the output of the SVM. Finally, the obtained training set is used to train a decision tree learning system and to extract the corresponding rule sets. The results of the experiments of protein secondary structure prediction on RS126 data set show that the comprehensibility of SVM_DT is much better than that of the SVM. Moreover, the generalization ability of SVM_DT is better than that of C4.5 decision trees and is similar to that of the SVM. Hence, SVM_DT can be used not only for prediction, but also for guiding biological experiments
7FB04BF9	The ideal use of small multilayer nets at the decision nodes of a binary classification tree to extract nonlinear features is proposed. The nets are trained and the tree is grown using a gradient-type learning algorithm in the multiclass case. The method improves on standard classification tree design methods in that it generally produces trees with lower error rates and fewer nodes. It also reduces the problems associated with training large unstructured nets and transfers the problem of selecting the size of the net to the simpler problem of finding a tree of the right size. An efficient tree pruning algorithm is proposed for this purpose. Trees constructed with the method and the CART method are compared on a waveform recognition problem and a handwritten character recognition problem. The approach demonstrates significant decrease in error rate and tree size. It also yields comparable error rates and shorter training times than a large multilayer net trained with backpropagation on the same problems.
7887D583	The classification of large dimensional data sets arising from the merging of remote sensing data with more traditional forms of ancillary data causes a significant computational problem. Decision tree classification is a popular approach to the problem. This type of classifier is characterized by the property that samples are subjected to a sequence of decision rules before they are assigned to a unique class. If a decision tree classifier is well designed, the result in many cases is a classification scheme which is accurate, flexible, and computationally efficient. This correspondence provides an automated technique for effective decision tree design which relies only on a priori statistics. This procedure utilizes canonical transforms and Bayes table look-up decision rules. An optimal design at each node is derived based on the associated decision table. A procedure for computing the global probability of correct classification is also provided. An example is given in which class statistics obtained from an actual Landsat scene are used as input to the program. The resulting decision tree design has an associated probability of correct classification of 0.75 compared to the theoretically optimum 0.79 probability of correct classification associated with a full dimensional Bayes classifier. Recommendations for future research are included.
7C66892E	A critical issue in classification tree design-obtaining right-sized trees, i.e. trees which neither underfit nor overfit the data-is addressed. Instead of stopping rules to halt partitioning, the approach of growing a large tree with pure terminal nodes and selectively pruning it back is used. A new efficient iterative method is proposed to grow and prune classification trees. This method divides the data sample into two subsets and iteratively grows a tree with one subset and prunes it with the other subset, successively interchanging the roles of the two subsets. The convergence and other properties of the algorithm are established. Theoretical and practical considerations suggest that the iterative free growing and pruning algorithm should perform better and require less computation than other widely used tree growing and pruning algorithms. Numerical results on a waveform recognition problem are presented to support this view.
7EAD4110	A survey is presented of current methods for decision tree classifier (DTC) designs and the various existing issues. After considering potential advantages of DTCs over single-state classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. The relation between decision trees and neutral networks (NN) is also discussed.
5FE9C8DA	The design of algorithms that explore multiple representation languages and explore different search spaces has an intuitive appeal. In the context of classification problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based on a combination of attributes. The same applies to model-tree algorithms in regression domains, but using linear models at leaf nodes. In this paper, we study where to use combinations of attributes in decision tree learning. We present an algorithm for multivariate tree learning that combines a univariate decision tree with a discriminant function by means of constructive induction. This algorithm is able to use decision nodes with multivariate tests, and leaf nodes that predict a class using a discriminant function. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree. Functional trees can be seen as a generalization of multivariate trees. Our algorithm was compared against to its components and two simplified versions using 30 benchmark data sets. The experimental evaluation shows that our algorithm has clear advantages with respect to the generalization ability and model sizes at statistically significant confidence levels.
7E4C6FAC	Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.
79BCE1C3	Supervised classification is one of the important tasks in remote sensing image interpretation, in which the image pixels are classified to various predefined land use/land cover classes based on the spectral reflectance values in different bands. In reality some classes may have very close spectral reflectance values that overlap in feature space. This produces spectral confusion among the classes and results in inaccurate classified images. To remove such spectral confusion one requires extra spectral and spatial knowledge. This report presents a decision tree classifier approach to extract knowledge from spatial data in form of classification rules using Gini Index and Shannon Entropy (Shannon and Weaver, 1949) to evaluate splits. This report also features calculation of optimal dataset size required for rule generation, in order to avoid redundant Input/output and processing.
8020CD31	This paper describes research to obtain continental and global scale maps of urban land cover from remotely sensed imagery, specifically utilizing newly available one kilometer data from the MODIS sensor. Defining the extent of urban land is crucial, since knowledge of the size and spatial distribution of cities is important on a number of fronts, from resource management to economic development planning to regional and global climate modeling. The algorithm used for this work is a supervised decision tree classifier, and the technique of boosting is exploited to improve classification accuracy and to provide a means to correct major sources of error using available prior information. First results for North America indicate that the incorporation of. ancillary information successfully improves urban classification results, resolving confusion between the urban and barren classes that normally occurs when only MODIS data is used.
80B879CA	This paper introduces a hybrid learning methodology that integrates genetic algorithms (GAs) and decision tree learning (ID3) in order to evolve optimal subsets of discriminatory features for robust pattern classification. A GA is used to search the space of all possible subsets of a large set of candidate discrimination features. For a given feature subset, ID3 is invoked to produce a decision tree. The classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set, which, in turn, is used by the GA to evolve better feature sets. This GA-ID3 process iterates until a feature subset is found with satisfactory classification performance. Experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data. The results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection.
79D84E5F	An algorithm to map burnt areas has been developed for SPOT VEGETATION (VGT) data in Australian woodland savannas. A time series of daily VGT images (15 May to 15 July 1999) was composited into 10-day periods by applying a minimum value criterion to the near-infrared band (0.78–0.89 mm). The algorithm was developed using a classification tree methodology that was confirmed as a powerful means of image classification. This methodology allowed the identification of three classes of burnt surfaces that appear to be differentiated by the proportion of the pixel that is burnt, the intensity of the fire and the density of the tree layer. The performance of the algorithm was assessed by classification of one VGT composite image (31 May–9 June) using, as representative of the ground truth, burnt areas extracted from two Landsat TM scenes (9 June). We randomly extracted 30 windows (each of ~14 km by 14 km) for which we compared the percentage of area burnt as derived from TM and VGT. The estimated mean absolute deviation in the percentage of the area burnt in each window is ±6.3%. In the area common to the two datasets a total amount of 6473 km2 was estimated to be burnt in the VGT classification against 7536 km2 that was burnt according to TM images. The accuracy of the classification was found to vary with the vegetation type being the most accurate estimate in low woodland with an underestimation error of 8.6%. These results show that VGT could be a very useful sensor for burnt area mapping over large woodland areas, although the low spatial resolution and the lack of a thermal band can be a limitation in certain conditions (e.g. understorey burns). The same methodology will be applied to map burnt areas for the entire Australian continent.
7ABDCCD6	The purposes of this study are to identify the strongest clinical parameters in relation to in-hospital mortality, which are available in the earliest phase of the hospitalization of patients, and to create an easy tool for the early identification of patients at risk.The classification and regression tree analysis was applied to data from the Acute Heart Failure Database-Main registry comprising patients admitted to specialized cardiology centers with all syndromes of acute heart failure. The classification model was built on derivation cohort (n = 2543) and evaluated on validation cohort (n = 1387).The classification tree stratifies patients according to the presence of cardiogenic shock (CS), the level of creatinine, and the systolic blood pressure (SBP) at admission into the 5 risk groups with in-hospital mortality ranging from 2.8% to 66.2%. Patients without CS and creatinine level of 155 μmol/L or less were classified into very-low-risk group; patients without CS, creatinine level greater than 155 μmol/L, and SBP greater than 103 mm Hg, into low-risk group, whereas patients without CS, creatinine level greater than 155 μmol/L, and SBP of 103 mm Hg or lower, into intermediate-risk group. The high-risk group patients had CS and creatinine of 140 μmol/L or less; patients with CS and creatinine level greater than 140 μmol/L belong to very-high-risk group. The area under receiver operating characteristic curve was 0.823 and 0.832, and the value of Brier's score was estimated on level 0.091 and 0.084, for the derivation and the validation cohort, respectively.The presented classification model effectively stratified patients with all syndromes of acute heart failure into in-hospital mortality risk groups and might be of advantage for clinical practice.
79931D06	Classification trees (CT) have been used successfully in the past to classify aquatic vegetation from spectral indices (SI) obtained from remotely-sensed images. However, applying CT models developed for certain image dates to other time periods within the same year or among different years can reduce the classification accuracy. In this study, we developed CT models with modified thresholds using extreme SI values (CTm) to improve the stability of the models when applying them to different time periods. A total of 903 ground-truth samples were obtained in September of 2009 and 2010 and classified as emergent, floating-leaf, or submerged vegetation or other cover types. Classification trees were developed for 2009 (Model-09) and 2010 (Model-10) using field samples and a combination of two images from winter and summer. Overall accuracies of these models were 92.8% and 94.9%, respectively, which confirmed the ability of CT analysis to map aquatic vegetation in Taihu Lake. However, Model-10 had only 58.9–71.6% classification accuracy and 31.1–58.3% agreement (i.e., pixels classified the same in the two maps) for aquatic vegetation when it was applied to image pairs from both a different time period in 2010 and a similar time period in 2009. We developed a method to estimate the effects of extrinsic (EF) and intrinsic (IF) factors on model uncertainty using Modis images. Results indicated that 71.1% of the instability in classification between time periods was due to EF, which might include changes in atmospheric conditions, sun-view angle and water quality. The remainder was due to IF, such as phenological and growth status differences between time periods. The modified version of Model-10 (i.e. CTm) performed better than traditional CT with different image dates. When applied to 2009 images, the CTm version of Model-10 had very similar thresholds and performance as Model-09, with overall accuracies of 92.8% and 90.5% for Model-09 and the CTm version of Model-10, respectively. CTm decreased the variability related to EF and IF and thereby improved the applicability of the models to different time periods. In both practice and theory, our results suggested that CTm was more stable than traditional CT models and could be used to map aquatic vegetation in time periods other than the one for which the model was developed.
75542B44	Using 1998 and 1999 singleton birth data of the State of Florida, we study the stability of classification trees. Tree stability depends on both the learning algorithm and the specific data set. In this study, test samples are used in statistical learning to evaluate both stability and predictive performance. We also use the resampling technique bootstrap, which can be regarded as data self-perturbation, to evaluate the sensitivity of the modeling algorithm with respect to the specific data set. We demonstrate that the selection of the cost function plays an important role in stability. In particular, classifiers with equal misclassification costs and equal priors are less stable compared to those with unequal misclassification costs and equal priors.
7767905F	The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.
7598242E	When managers and researchers encounter a data set, they typically ask two key questions: (1) Which model (from a candidate set) should I use? And (2) if I use a particular model, when is it going to likely work well for my business goal? This research addresses those two questions and provides a rule, i.e., a decision tree, for data analysts to portend the “winning model” before having to fit any of them for longitudinal incidence data. We characterize data sets based on managerially relevant (and easy-to-compute) summary statistics, and we use classification techniques from machine learning to provide a decision tree that recommends when to use which model. By doing the “legwork” of obtaining this decision tree for model selection, we provide a time-saving tool to analysts. We illustrate this method for a common marketing problem (i.e., forecasting repeat purchasing incidence for a cohort of new customers) and demonstrate the method's ability to discriminate among an integrated family of a hidden Markov model (HMM) and its constrained variants. We observe a strong ability for data set characteristics to guide the choice of the most appropriate model, and we observe that some model features (e.g., the “back-and-forth” migration between latent states) are more important to accommodate than are others (e.g., the inclusion of an “off” state with no activity). We also demonstrate the method's broad potential by providing a general “recipe” for researchers to replicate this kind of model classification task in other managerial contexts (outside of repeat purchasing incidence data and the HMM framework).
5D25C398	Decision tree learning has become a popular and practical method in data mining because of its high predictive accuracy and ease of use. However, a set of if-then rules generated from large trees may be preferred in many cases because of at least three reasons: (i) large decision trees are difficult to understand as we may not see their hierarchical structure or get lost in navigating them, (ii) the tree structure may cause individual subconcepts to be fragmented (this is sometimes known as the “replicated subtree” problem), (iii) it is easier to combine new discovered rules with existing knowledge in a given domain. To fulfill that need, the popular decision tree learning system C4.5 applies a rule post-pruning algorithm to transform a decision tree into a rule set. However, by using a global optimization strategy, C4.5rules functions extremely slow on large datasets. On the other hand, rule post-pruning algorithms that learn a set of rules by the separate-and-conquer strategy such as CN2, IREP, or RIPPER can be scalable to large datasets, but they suffer from the crucial problem of overpruning, and do not often achieve a high accuracy as C4.5. This paper proposes a scalable algorithm for rule post-pruning of large decision trees that employs incremental pruning with improvements in order to overcome the overpruning problem. Experiments show that the new algorithm can produce rule sets that are as accurate as those generated by C4.5 and is scalable for large datasets.
78504EB4	We present a framework for flexible nonlinear contextual image classification. The framework integrates classical and recent models for image classification, ranging from a multivariate Gaussian classifier, to MLP neural nets, classification trees and recent regression models based on general additive models, and combines them with a Markov random field for spatial context. The effect of using the different nonlinear discriminant functions is compared with the effect of using an MRF model for spatial context. In general, the use of an MRF model results in larger improvements in classification accuracy than using different nonlinear discriminant functions, but the combination of them can give large improvements.
7E53AC45	Decision tree classifiers have received much recent attention, particularly with regards to land cover classifications at continental to global scales. Despite their many benefits and general flexibility, the use of decision trees with high spatial resolution data has not yet been fully explored. In support of the National Park Service (NPS) Vegetation Mapping Program (VMP), we have examined the feasibility of using a commercially available decision tree classifier with multitemporal satellite data from the Enhanced Thematic Mapper-Plus (ETM+) instrument to map 11 land cover types at the Delaware Water Gap National Recreation Area near Milford, PA. Ensemble techniques such as boosting and consensus filtering of the training data were used to improve both the quality of the input training data as well as the final products.Using land cover classes as specified by the National Vegetation Classification Standard at the Formation level, the final land cover map has an overall accuracy of 82% (κ=0.80) when tested against a validation data set acquired on the ground (n=195). This same accuracy is 99.5% when considering only forest vs. nonforest classes. Usage of ETM+ scenes acquired at multiple dates improves the accuracy over the use of a single date, particularly for the different forest types. These results demonstrate the potential applicability and usability of such an approach to the entire National Park system, and to high spatial resolution land cover and forest mapping applications in general.
7683DF85	Multiseason reflectance data from radiometrically and geometrically corrected multispectral SPOT-5 images of 10-m resolution were combined with thorough field campaigns and land cover digitizing using a binary classification tree algorithm to estimate the area of marshes covered with common reeds (Phragmites australis) and submerged macrophytes (Potamogeton pectinatus, P. pusillus, Myriophyllum spicatum, Ruppia maritima, Chara sp.) over an area of 145,000 ha. Accuracy of these models was estimated by cross-validation and by calculating the percentage of correctly classified pixels on the resulting maps. Robustness of this approach was assessed by applying these models to an independent set of images using independent field data for validation. Biophysical parameters of both habitat types were used to interpret the misclassifications. The resulting trees provided a cross-validation accuracy of 98.7% for common reed and 97.4% for submerged macrophytes. Variables discriminating reed marshes from other land covers were the difference in the near-infrared band between March and June, the Optimized Soil Adjusted Vegetation Index of December, and the Normalized Difference Water Index (NDWI) of September. Submerged macrophyte beds were discriminated with the shortwave-infrared band of December, the NDWI of September, the red band of September and the Simple Ratio index of March. Mapping validations provided accuracies of 98.6% (2005) and 98.1% (2006) for common reed, and 86.7% (2005) and 85.9% (2006) for submerged macrophytes. The combination of multispectral and multiseasonal satellite data thus discriminated these wetland vegetation types efficiently. Misclassifications were partly explained by digitizing inaccuracies, and were not related to biophysical parameters for reedbeds. The classification accuracy of submerged macrophytes was influenced by the proportion of plants showing on the water surface, percent cover of submerged species, water turbidity, and salinity. Classification trees applied to time series of SPOT-5 images appear as a powerful and reliable tool for monitoring wetland vegetation experiencing different hydrological regimes even with a small training sample (N = 25) when initially combined with thorough field measurements.
7C8CCCB4	Aquatic vegetation plays an important role in maintaining the balance of lake ecosystems. Thus, classifying and mapping aquatic vegetation is a priority for lake management. Classification tree (CT) approaches have been used successfully to map aquatic vegetation from spectral indices obtained from remotely sensed images. However, due to the effects of extrinsic and intrinsic factors, applying a CT model developed for imagery from one date to imagery from another date or a different dataset likely would reduce the classification accuracy. In this study, three spectral features (SFs) were selected to develop a CT model for identifying aquatic vegetation in Taihu Lake. Three traditional CT models with three SFs were developed using CT analysis based on satellite images acquired on 11 July, 16 August and 26 September 2013, and corresponding ground-truth samples, from the Huangjing-1A/B Charge-Coupled Device (HJ-CCD) images, environment and disaster reduction small satellites that were launched by China Center for Resources Satellite Data and Application (CRESDA). The overall accuracies of traditional CT models were 82%, 80% and 84%. We then tested two methods to modify CT model thresholds to adjust the traditional CT models based on image date to determine if the results would enable us to map and classify aquatic vegetation for periods when no ground-based data were available. We assessed the results with ground-truth samples and area agreement with traditional CT models. Results showed that CT models modified from a linear adjustment based on the relationship between ranked values of SFs between two image dates produced map accuracies comparable with those obtained from the traditional CT models and suggest that the method we proposed is feasible for mapping aquatic vegetation types in lakes when ground data are not available.
76EB6675	Decision tree algorithm is a kind of data mining model to make induction learning algorithm based on examples. It is easy to extract display rule, has smaller computation amount, and could display important decision property and own higher classification precision. For the study of data mining algorithm based on decision tree, this article put forward specific solution for the problems of property value vacancy, multiple-valued property selection, property selection criteria, propose to introduce weighted and simplified entropy into decision tree algorithm so as to achieve the improvement of ID3 algorithm. The experimental results show that the improved algorithm is better than widely used ID3 algorithm at present on overall performance.
75087187	In previous attempts to identify aquatic vegetation from remotely-sensed images using classification trees (CT), the images used to apply CT models to different times or locations necessarily originated from the same satellite sensor as that from which the original images used in model development came, greatly limiting the application of CT. We have developed an effective normalization method to improve the robustness of CT models when applied to images originating from different sensors and dates. A total of 965 ground-truth samples of aquatic vegetation types were obtained in 2009 and 2010 in Taihu Lake, China. Using relevant spectral indices (SI) as classifiers, we manually developed a stable CT model structure and then applied a standard CT algorithm to obtain quantitative (optimal) thresholds from 2009 ground-truth data and images from Landsat7-ETM+, HJ-1B-CCD, Landsat5-TM and ALOS-AVNIR-2 sensors. Optimal CT thresholds produced average classification accuracies of 78.1%, 84.7% and 74.0% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. However, the optimal CT thresholds for different sensor images differed from each other, with an average relative variation (RV) of 6.40%. We developed and evaluated three new approaches to normalizing the images. The best-performing method (Method of 0.1% index scaling) normalized the SI images using tailored percentages of extreme pixel values. Using the images normalized by Method of 0.1% index scaling, CT models for a particular sensor in which thresholds were replaced by those from the models developed for images originating from other sensors provided average classification accuracies of 76.0%, 82.8% and 68.9% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. Applying the CT models developed for normalized 2009 images to 2010 images resulted in high classification (78.0%–93.3%) and overall (92.0%–93.1%) accuracies. Our results suggest that Method of 0.1% index scaling provides a feasible way to apply CT models directly to images from sensors or time periods that differ from those of the images used to develop the original models.
80DCAAC2	This research used classification tree analysis and logistic regression models to identify risk factors related to short- and long-term abstinence. Baseline and cessation outcome data from two smoking cessation trials, conducted from 2001 to 2002 in two Midwestern urban areas, were analyzed. There were 928 participants (53.1% women, 81.8% White) with complete data. Both analyses suggest that relapse risk is produced by interactions of risk factors and that early and late cessation outcomes reflect different vulnerability factors. The results illustrate the dynamic nature of relapse risk and suggest the importance of efficient modeling of interactions in relapse prediction.
78F9553F	Besides serving as prediction models, classification trees are useful for finding important predictor variables and identifying interesting subgroups in the data. These functions can be compromised by weak split selection algorithms that have variable selection biases or that fail to search beyond local main effects at each node of the tree. The resulting models may include many irrelevant variables or select too few of the important ones. Either eventuality can lead to erroneous conclusions. Four techniques to improve the precision of the models are proposed and their effectiveness compared with that of other algorithms, including tree ensembles, on real and simulated data sets. 
7A88004A	The literature on microbial source tracking (MST) suggests that DNA analysis of fecal samples leads to more reliable determinations of bacterial sources of surface water contamination than antibiotic resistance analysis (ARA). Our goal is to determine whether the increased reliability, if any, in library-based MST developed with DNA data is sufficient to justify its higher cost, where the bacteria source predictions are used in TMDL surface water management programs. We describe an application of classification trees for MST applied to ARA and DNA data from samples collected in the Potomac River Watershed in Maryland. Conclusions concerning the comparison of ARA and DNA data, although preliminary at the current time, suggest that the added cost of obtaining DNA data in comparison to the cost of ARA data may not be justified, where MST is applied in TMDL surface water management programs.
7E5348E7	Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.
813A1DD2	Feature subset selection is an important preprocessing step for classification. In biology, where structures or processes are described by a large number of features, the elimination of irrelevant and redundant information in a reasonable amount of time has a number of advantages. It enables the classification system to achieve good or even better solutions with a restricted subset of features, allows for a faster classification, and it helps the human expert focus on a relevant subset of features, hence providing useful biological knowledge.We present a heuristic method based on Estimation of Distribution Algorithms to select relevant subsets of features for splice site prediction in Arabidopsis thaliana. We show that this method performs a fast detection of relevant feature subsets using the technique of constrained feature subsets. Compared to the traditional greedy methods the gain in speed can be up to one order of magnitude, with results being comparable or even better than the greedy methods. This makes it a very practical solution for classification tasks that can be solved using a relatively small amount of discriminative features (or feature dependencies), but where the initial set of potential discriminative features is rather large.
7E2EEE71	Biomarker discovery is an important topic in biomedical applications of computational biology, including applications such as gene and SNP selection from high-dimensional data. Surprisingly, the stability with respect to sampling variation or robustness of such selection processes has received attention only recently. However, robustness of biomarkers is an important issue, as it may greatly influence subsequent biological validations. In addition, a more robust set of markers may strengthen the confidence of an expert in the results of a selection method.Our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm. Secondly, we conducted a large-scale analysis of the recently introduced concept of ensemble feature selection, where multiple feature selections are combined in order to increase the robustness of the final set of selected features. We focus on selection methods that are embedded in the estimation of support vector machines (SVMs). SVMs are powerful classification models that have shown state-of-the-art performance on several diagnosis and prognosis tasks on biological data. Their feature selection extensions also offered good results for gene selection tasks. We show that the robustness of SVMs for biomarker discovery can be substantially increased by using ensemble feature selection techniques, while at the same time improving upon classification performances. The proposed methodology is evaluated on four microarray datasets showing increases of up to almost 30% in robustness of the selected biomarkers, along with an improvement of approximately 15% in classification performance. The stability improvement with ensemble methods is particularly noticeable for small signature sizes (a few tens of genes), which is most relevant for the design of a diagnosis or prognosis model from a gene signature.
21A9A6DF	In clinical medicine, multidimensional time series data can be used to find the rules of disease progress by data mining technology, such as classification and prediction. However, in multidimensional time series data mining problems, the excessive data dimension causes the inaccuracy of probability density distribution to increase the computational complexity. Besides, information redundancy and irrelevant features may lead to high computational complexity and over-fitting problems. The combination of these two factors can reduce the classification performance. To reduce computational complexity and to eliminate information redundancies and irrelevant features, we improved upon a multidimensional time series feature selection method to achieve dimension reduction. The improved method selects features through the combination of the Kozachenko–Leonenko (K–L) information entropy estimation method for feature extraction based on mutual information and the feature selection algorithm based on class separability. We performed experiments on the Electroencephalogram (EEG) dataset for verification and the non-small cell lung cancer (NSCLC) clinical dataset for application. The results show that with the comparison of CLeVer, Corona and AGV, respectively, the improved method can effectively reduce the dimensions of multidimensional time series for clinical data.
7DBCCFE0	The identification of relevant biological features in large and complex datasets is an important step towards gaining insight in the processes underlying the data. Other advantages of feature selection include the ability of the classification system to attain good or even better solutions using a restricted subset of features, and a faster classification. Thus, robust methods for fast feature selection are of key importance in extracting knowledge from complex biological data.In this paper we present a novel method for feature subset selection applied to splice site prediction, based on estimation of distribution algorithms, a more general framework of genetic algorithms. From the estimated distribution of the algorithm, a feature ranking is derived. Afterwards this ranking is used to iteratively discard features. We apply this technique to the problem of splice site prediction, and show how it can be used to gain insight into the underlying biological process of splicing.We show that this technique proves to be more robust than the traditional use of estimation of distribution algorithms for feature selection: instead of returning a single best subset of features (as they normally do) this method provides a dynamical view of the feature selection process, like the traditional sequential wrapper methods. However, the method is faster than the traditional techniques, and scales better to datasets described by a large number of features.
7B1ED9C9	We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. 
7A94F317	We present several methods for full, partial, and practical adaptation. Selector statistics that are measures of skewness, peakedness, and tailweight are used, primarily in estimating loca-tion in some single-sample situations. We note several practical adaptive techniques in current use, including illustrations in-volving stepwise regression, analysis of variance, ridge regres-sion, and splines. We suggest some areas in which future develop-ment of adaptive methods is needed:density estimation; M, R, and L estimation in regression; and dependent data. There is also a need to develop better selector statistics. 
73831E7D	Prostate cancer is complicated by a high level of unexplained variability in the aggressiveness of newly diagnosed disease. Given that this is one of the most prevalent cancers worldwide, finding biomarkers to effectively stratify high risk patient populations is a vital next step in improving survival rates and quality of life after treatment. Materials and Methods: In this study, we selected a dataset consisting of 106 prostate cancer samples, which represent various stages of prostate cancer and developed by RNA-Seq technology. Our objective is to identify differentially expressed transcripts associated with prostate cancer progression using pair-wise stage comparisons. Results: Using machine learning techniques, we identified 44 transcripts that are correlated to different stages of progression. Expression of an identified transcript, USP13, is reduced in stage T3 in comparison with stage T2c, a pattern also observed in breast cancer tumourigenesis. We also identified another differentially expressed transcript, PTGFR, which has also been reported to be involved in prostate cancer progression and has also been linked to breast, ovarian and renal cancers. Conclusions: The results support the use of RNA-Seq along with machine learning techniques as an essential tool in identifying potential biomarkers for prostate cancer progression. Further studies elucidating the biochemical role of identified transcripts in vitro are crucial in validating the use of these biomarkers in the prediction of disease progression and development of effective therapeutic strategies.
7BB1F2A4	In this research, I proposed Emotion Classification of Thai Text based Using Term weighting and Machine Learning Techniques focusing on the comparison of various common term weighting schemes. I found Boolean weighting with Support Vector Machine is most effective in our experiments. I also discovered that the Boolean weighting is suitable for combination with the Information gain feature selection method. The Boolean weighting with Support Vector Machine algorithm yielded the best performance with the accuracy over all algorithms. Based on our experiments, the Support Vector Machine algorithm with the Information gain feature selection yielded the best performance with the accuracy of 77.86%. Our experimental results also reveal that feature weighting methods have a positive effect on the Thai Emotion Classification Framework.
7D6A21F8	In this paper, we address the problems of deformable object matching (alignment) and segmentation with cluttered background. We propose a novel hierarchical log-linear model (HLLM) which represents both shape and appearance features at multiple levels of a hierarchy. This model enables us to combine appearance cues at multiple scales directly into the hierarchy and to model shape deformations at short-range, medium range, and long-range. We introduce the structure-perceptron algorithm to estimate the parameters of the HLLM in a discriminative way. The learning is able to estimate the appearance and shape parameters simultaneously in a global manner. Moreover, the structure-perceptron learning has a feature selection aspect (similar to AdaBoost) which enables us to specify a class of appearance/shape features and allow the algorithm to select which features to use and weight their importance. This method was applied to the tasks of deformable object localization, segmentation, matching (alignment), and parsing. We demonstrate that the algorithm achieves the state of the art performance by evaluation on public dataset (horse and multi-view face).
775B2060	Aquatic weed control through chemical products has attracted much attention in the last years, mainly because of the ecological disorder caused by such plants, and also the consequences to the economical activities. However, this kind of control has been carried out in a non-automatic way by technicians, and may be a not healthy policy, since each species may react differently to the same herbicide. Thus, this work proposes the automatic identification of some species by means of supervised pattern recognition techniques and shape descriptors in order to compose a nearby future expert system for automatic application of the correct herbicide. Experiments using some state-of-the-art techniques have shown the robustness of the employed pattern recognition techniques.
80F1EA90	The current search engines usually return a large number of irrelevant documents for a certain query. As a result, accessing such information and filtering out these documents can cause frustration and often result in waste of time and effort for the users while surfing the web. This is mainly because of the underlying techniques used in these engines. These techniques are mostly based in the frequency of the keywords of the query in the HTML code. In addition, issues such as dealing with classifying the pages found for a query according to previous visits along with features needed to make intelligent decisions regarding the access patterns of the users are not considered. This work presents an intelligent search engine, called ORCA that returns the most relevant documents for user's queries. This search engine analyses the queries and builds themes (models) to be used when the engine is confronted with similar queries. The intelligent component is used for constructing a model of the user behavior and using that model to fetch and even prefetch information and documents considered of interest to the user. It uses both latent semantic analysis and web page feature selection for clustering web pages. Latent semantic analysis is used to find the semantic relations between keywords, and between documents.
5D3D3DF8	Naive Bayes is a well known and studied algorithm both in statistics and machine learning. Bayesian learning algorithms represent each concept with a single probabilistic summary. In this paper we present an iterative approach to naive Bayes. The iterative Bayes begins with the distribution tables built by the naive Bayes. Those tables are iteratively updated in order to improve the probability class distribution associated with each training example. Experimental evaluation of Iterative Bayes on 25 benchmark datasets shows consistent gains in accuracy. An interesting side effect of our algorithm is that it shows to be robust to attribute dependencies
77654CEF	We consider a linear regression problem in a high dimensional setting where the number of covariates p can be much larger than the sample size n. In such a situation, one often assumes sparsity of the regression vector, i.e., the regression vector contains many zero components. We propose a Lasso-type estimator β̂Quad (where ‘Quad’ stands for quadratic) which is based on two penalty terms. The first one is the ℓ1 norm of the regression coefficients used to exploit the sparsity of the regression as done by the Lasso estimator, whereas the second is a quadratic penalty term introduced to capture some additional information on the setting of the problem. We detail two special cases: the Elastic-Net β̂EN introduced in [42], which deals with sparse problems where correlations between variables may exist; and the Smooth-Lasso β̂SL, which responds to sparse problems where successive regression coefficients are known to vary slowly (in some situations, this can also be interpreted in terms of correlations between successive variables). From a theoretical point of view, we establish variable selection consistency results and show that β̂Quad achieves a Sparsity Inequality, i.e., a bound in terms of the number of non-zero components of the ‘true’ regression vector. These results are provided under a weaker assumption on the Gram matrix than the one used by the Lasso. In some situations this guarantees a significant improvement over the Lasso. Furthermore, a simulation study is conducted and shows that the S-Lasso β̂SL performs better than known methods as the Lasso, the Elastic-Net β̂EN, and the Fused-Lasso (introduced in [30]) with respect to the estimation accuracy. This is especially the case when the regression vector is ‘smooth’, i.e., when the variations between successive coefficients of the unknown parameter of the regression are small. The study also reveals that the theoretical calibration of the tuning parameters and the one based on 10 fold cross validation imply two S-Lasso solutions with close performance. 
810F181F	Robustness or stability of feature selection techniques is a topic of recent interest, and is an important issue when selected feature subsets are subsequently analysed by domain experts to gain more insight into the problem modelled. In this work, we investigate the use of ensemble feature selection techniques, where multiple feature selection methods are combined to yield more robust results. We show that these techniques show great promise for high-dimensional domains with small sample sizes, and provide more robust feature subsets than a single feature selection technique. In addition, we also investigate the effect of ensemble feature selection techniques on classification performance, giving rise to a new model selection strategy.
5A30BF03	Identification of individualized feature combinations for survival prediction in breast cancer: a comparison of machine learning techniques"
78848AB6	We present a novel method for detecting near-duplicates from a large collection of documents. Three major parts are involved in our method, feature selection, similarity measure, and discriminant derivation. To find near-duplicates to an input document, each sentence of the input document is fetched and preprocessed, the weight of each term is calculated, and the heavily weighted terms are selected to be the feature of the sentence. As a result, the input document is turned into a set of such features. A similarity measure is then applied and the similarity degree between the input document and each document in the given collection is computed. A support vector machine (SVM) is adopted to learn a discriminant function from a training pattern set, which is then employed to determine whether a document is a near-duplicate to the input document based on the similarity degree between them. The sentence-level features we adopt can better reveal the characteristics of a document. Besides, learning the discriminant function by SVM can avoid trial-and-error efforts required in conventional methods. Experimental results show that our method is effective in near-duplicate document detection.
750A9F88	In the past two decades, the dimensionality of datasets involved in machine learning and data mining applications has increased explosively. Therefore, feature selection has become a necessary step to make the analysis more manageable and to extract useful knowledge about a given domain. A large variety of feature selection techniques are available in literature, and their comparative analysis is a very difficult task. So far, few studies have investigated, from a theoretical and/or experimental point of view, the degree of similarity/dissimilarity among the available techniques, namely the extent to which they tend to produce similar results within specific application contexts. This kind of similarity analysis is of crucial importance when two or more methods are combined in an ensemble fashion: indeed the ensemble paradigm is beneficial only if the involved methods are capable of giving different and complementary representations of the considered domain. This paper gives a contribution in this direction by proposing an empirical approach to evaluate the degree of consistency among the outputs of different selection algorithms in the context of high dimensional classification tasks. Leveraging on a proper similarity index, we systematically compared the feature subsets selected by eight popular selection methods, representatives of different selection approaches, and derived a similarity trend for feature subsets of increasing size. Through an extensive experimentation involving sixteen datasets from three challenging domains (Internet advertisements, text categorization and micro-array data classification), we obtained useful insight into the pattern of agreement of the considered methods. In particular, our results revealed how multivariate selection approaches systematically produce feature subsets that overlap to a small extent with those selected by the other methods.
764B4F14	The web services, a novel paradigm in software technology, have innovative mechanism for rendering services over diversified environment. They promise to allow businesses to adapt rapidly to changes in the business environment and the needs of different customers. The rapid introduction of new web services into a dynamic business environment can adversely affect the service quality and user satisfaction. Consequently, assessment of the quality of web services is of paramount importance in selecting a web service for an application. In this paper, we employed well-known classification models viz., back propagation neural network (BPNN), probabilistic neural network (PNN), group method of data handling (GMDH), classification and regression trees (CART), TreeNet, support vector machine (SVM) and ID3 decision tree (J48) to predict the quality of a web service based on a set of quality attributes. The experiments are carried out on the QWS dataset. We applied 10-fold cross-validation to test the efficacy of the models. The J48 and TreeNet techniques outperformed all other techniques by yielding an average accuracy of 99.72%. We also performed feature selection and found that web-services relevance function (WSRF) is the most significant attribute in determining the quality of a web service. Later, we performed feature selection without WSRF and found that Reliability, Throughput, Successability, Documentation and Response Time are the most important attributes in that order. Moreover, the set of ‘if–then’ rules yielded by J48 and CART can be used as an expert system for web-services classification.
80895228	A filter method of feature selection based on mutual information, called normalized mutual information feature selection (NMIFS), is presented. NMIFS is an enhancement over Battiti's MIFS, MIFS-U, and mRMR methods. The average normalized mutual information is proposed as a measure of redundancy among features. NMIFS outperformed MIFS, MIFS-U, and mRMR on several artificial and benchmark data sets without requiring a user-defined parameter. In addition, NMIFS is combined with a genetic algorithm to form a hybrid filter/wrapper method called GAMIFS. This includes an initialization procedure and a mutation operator based on NMIFS to speed up the convergence of the genetic algorithm. GAMIFS overcomes the limitations of incremental search algorithms that are unable to find dependencies between groups of features.
80643FBF	Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.
7F1C2084	The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.
7FCEC94E	Data mining is the study of how to determine underlying patterns in the data to help make optimal decisions on computers when the database involved is voluminous, hard to characterize accurately and constantly changing. It deploys techniques based on machine learning alongside more conventional methods. These techniques can generate decision or prediction models based on actual historical data. Therefore, they represent true evidence-based decision support. Rainfall prediction is a good problem to solve by data mining techniques. This paper proposes an improved naive Bayes classifier (INCB) technique and explores the use of genetic algorithms (GAs) for the selection of a subset of input features in classification problems. It then carries out a comparison with several other techniques. It compares the following algorithms on real meteorological data in Hong Kong: (1) genetic algorithms with average classification or general classification (GA-AC and GA-C), (2) C4.5 with pruning, and (3) INBC with relative frequency or initial probability density (INBC-RF and INBC-IPD). Two simple schemes are proposed to construct a suitable data set for improving their performance. Scheme I uses all the basic input parameters for rainfall prediction. Scheme II uses the optimal subset of input variables which are selected by a GA. The results show that, among the methods we compared, INBC achieved about a 90% accuracy rate on the rain/no-rain classification problems. This method also attained reasonable performance on rainfall prediction with three-level depth and five-level depth, which are around 65%-70%.
7510E7BD	A plenitude of feature selection (FS) methods is available in the literature, most of them rising as a need to analyze data of very high dimension, usually hundreds or thousands of variables. Such data sets are now available in various application areas like combinatorial chemistry, text mining, multivariate imaging, or bioinformatics. As a general accepted rule, these methods are grouped in filters, wrappers, and embedded methods. More recently, a new group of methods has been added in the general framework of FS: ensemble techniques. The focus in this survey is on filter feature selection methods for informative feature discovery in gene expression microarray (GEM) analysis, which is also known as differentially expressed genes (DEGs) discovery, gene prioritization, or biomarker discovery. We present them in a unified framework, using standardized notations in order to reveal their technical details and to highlight their common characteristics as well as their particularities.
7FB56746	Microarray data classification is a difficult challenge for machine learning researchers due to its high number of features and the small sample sizes. Feature selection has been soon considered a de facto standard in this field since its introduction, and a huge number of feature selection methods were utilized trying to reduce the input dimensionality while improving the classification performance. This paper is devoted to reviewing the most up-to-date feature selection methods developed in this field and the microarray databases most frequently used in the literature. We also make the interested reader aware of the problematic of data characteristics in this domain, such as the imbalance of the data, their complexity, or the so-called dataset shift. Finally, an experimental evaluation on the most representative datasets using well-known feature selection methods is presented, bearing in mind that the aim is not to provide the best feature selection method, but to facilitate their comparative study by the research community.
78D5F25A	Multi-instance learning concerns about building learning models from a number of labeled instance bags, where each bag consists of instances with unknown labels. A bag is labeled positive if one or more multiple instances inside the bag is positive, and negative otherwise. For all existing multi-instance learning algorithms, they are only applicable to the setting where instances in each bag are represented by a set of well defined feature values. In this paper, we advance the problem to a multi-instance multi-graph setting, where a bag contains a number of instances and graphs in pairs, and the learning objective is to derive classification models from labeled bags, containing both instances and graphs, to predict previously unseen bags with maximum accuracy. To achieve the goal, the main challenge is to properly represent graphs inside each bag and further take advantage of complementary information between instance and graph pairs for learning. In the paper, we propose a Dual Embedding Multi-Instance Multi-Graph Learning (DE-MIMG) algorithm, which employs a dual embedding learning approach to (1) embed instance distributions into the informative sub graphs discovery process, and (2) embed discovered sub graphs into the instance feature selection process. The dual embedding process results in an optimal representation for each bag to provide combined instance and graph information for learning. Experiments and comparisons on real-world multi-instance multi-graph learning tasks demonstrate the algorithm performance.
7119C80A	Diagnosing depression in the early curable stages is very important and may even save the life of a patient. In this paper, we study nonlinear analysis of EEG signal for discriminating depression patients and normal controls. Forty-five unmedicated depressed patients and 45 normal subjects were participated in this study. Power of four EEG bands and four nonlinear features including detrended fluctuation analysis (DFA), higuchi fractal, correlation dimension and lyapunov exponent were extracted from EEG signal. For discriminating the two groups, k-nearest neighbor, linear discriminant analysis and logistic regression as the classifiers are then used. Highest classification accuracy of 83.3% is obtained by correlation dimension and LR classifier among other nonlinear features. For further improvement, all nonlinear features are combined and applied to classifiers. A classification accuracy of 90% is achieved by all nonlinear features and LR classifier. In all experiments, genetic algorithm is employed to select the most important features. The proposed technique is compared and contrasted with the other reported methods and it is demonstrated that by combining nonlinear features, the performance is enhanced. This study shows that nonlinear analysis of EEG can be a useful method for discriminating depressed patients and normal subjects. It is suggested that this analysis may be a complementary tool to help psychiatrists for diagnosing depressed patients.
812F24DC	We address the problem of efficiently learning Naive Bayes classifiers under classconditional classification noise (CCCN). Naive Bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn Naive Bayes classifiers under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled. 1.
584A9F54	In this paper, we show how using the Dirichlet Process mixture model as a generative model of data sets provides a simple and effective method for transfer learning. In particular, we present a hierarchical extension of the classic Naive Bayes classifier that couples multiple Naive Bayes classifiers by placing a Dirichlet Process prior over their parameters and show how recent advances in approximate inference in the Dirichlet Process mixture model enable efficient inference. We evaluate the resulting model in a meeting domain, in which the system decides, based on a learned model of the user's behavior, whether to accept or reject the request on his or her behalf. The extended model outperforms the standard Naive Bayes model by using data from other users to influence its predictions.
75CFD48F	In supervised learning it is assumed that it is straightforward to obtain labeled data. However, in reality labeled data can be scarce or expensive to obtain. Active learning (AL) is a way to deal with the above problem by asking for the labels of the most “informative” data points. We propose an AL method based on a metric of classification confidence computed on a feature subset of the original feature space which pertains especially to the large number of dimensions (i.e. examined genes) of microarray experiments. DNA microarray expression experiments permit the systematic study of the correlation of the expression of thousands of genes.Feature selection is critical in the algorithm because it enables faster and more robust retraining of the classifier. The approach that is followed for feature selection is a combination of a variance measure and a genetic algorithm.We have applied the proposed method on DNA microarray data sets with encouraging results. In particular we studied data sets concerning: small round blue cell tumours (4 types), Leukemia (2 types), lung cancer (2 types) and prostate cancer (healthy, unhealthy)
7508EFF3	Considerable effort has been made by researchers in the area of network traffic classification, since the Internet is constantly changing. This characteristic makes the task of traffic identification not a straightforward process. Besides that, encrypted data is being widely used by applications and protocols. There are several methods for classifying network traffic such as known ports and Deep Packet Inspection (DPI), but they are not effective since many applications constantly randomize their ports and the payload could be encrypted. This paper proposes a hybrid model that makes use of a classifier based on computational intelligence, the Extreme Learning Machine (ELM), along with Feature Selection (FS) and Multi-objective Genetic Algorithms (MOGA) to classify computer network traffic without making use of the payload or port information. The proposed model presented good results when evaluated against the UNIBS data set, using four performance metrics: Recall, Precision, Flow Accuracy and Byte Accuracy, with most rates exceeding 90%. Besides that, presented the best features and feature selection algorithm for the given problem along with the best ELM parameters.
76E67A3F	As concurrent and distributive applications are becoming more common and debugging such applications is very difficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instrumentation, timing "noise" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small programs.In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program locations that are in the vicinity of the bugs on a number of industrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with different instrumentations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using instrumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evaluation of the feature but the derivative of that value along the program execution path.As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging algorithms for concurrent applications, based on noise injection based concurrent testing using instrumentation. The infrastructure is described in this paper.
7B5B0392	Age-related macular ARM degeneration is an eye disease, that gradually degrades the macula, a part of the retina, which is responsible for central vision. It occurs in one of the two types, dry and wet age-related macular degeneration. The purpose of this paper is to diagnose the retinal disease age-related macular degeneration. An automated approach is proposed to help in the early detection of age-related macular degeneration using three models and their performances are compared. The amount of the disease spread in the retina can be identified by extracting the features of the retina. Detection of age-related macular degeneration disease has been done using probabilistic neural network PNN, Bayesian classification and support vector machine SVM and the two types of age-related macular degeneration are classified and diagnosed successfully. The results show that SVM achieves a higher performance measure than probabilistic neural network and Bayes classification.
77BEDFB0	With the development and popularization of the remote-sensing imaging technology, there are more and more applications of hyperspectral image classification tasks, such as target detection and land cover investigation. It is a very challenging issue of urgent importance to select a minimal and effective subset from those mass of bands. This paper proposed a hybrid feature selection strategy based on genetic algorithm and support vector machine (GA–SVM), which formed a wrapper to search for the best combination of bands with higher classification accuracy. In addition, band grouping based on conditional mutual information between adjacent bands was utilized to counter for the high correlation between the bands and further reduced the computational cost of the genetic algorithm. During the post-processing phase, the branch and bound algorithm was employed to filter out those irrelevant band groups. Experimental results on two benchmark data sets have shown that the proposed approach is very competitive and effective.
0A088B47	We present a framework for characterizing Bayesian classification methods. This framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the Naive Bayes algorithm at the most restrictive end and the learning of full Bayesian networks at the most general extreme. While much work has been carried out along the two ends of this spectrum, there has been surprising little done along the middle. We analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains. We then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties.
5FC1ECAC	this paper is to investigate the applicability of these techniques to high dimensional problems of Feature Selection. The aim is to establish whether the properties inferred for these techniques from medium scale experiments involving up to a few tens of dimensions extend to dimensionalities of one order of magnitude higher. Further, relative merits of these techniques vis-a-vis such high dimensional problems are explored and the possibility of exploiting the best aspects of these methods to create a composite feature selection procedure with superior properties is considered
7BAB122C	In this study, we developed new computational DNA adduct prediction models by using significantly more diverse training data-set of 217 DNA adducts and 1024 non-DNA adducts, and applying five machine learning methods which include support vector machine (SVM), k-nearest neighbour, artificial neural networks, logistic regression and continuous kernel discrimination. The molecular descriptors used for DNA adduct prediction were selected from a pool of 548 descriptors by using a multi-step hybrid feature selection method combining Fischer-score and Monte Carlo simulated annealing method. Some of the selected descriptors are consistent with the structural and physicochemical properties reported to be important for DNA adduct formation. The y-scrambling method was used to test whether there is a chance correlation in the developed SVM model. In the meantime, fivefold cross-validation of these machine learning methods results in the prediction accuracies of 64.1–82.5% for DNA adducts and 95.1–97.6% for non-DNA adducts, and the prediction accuracies for external test set are 78.2–100% for DNA adducts and 92.6–98.4% for non-DNA adducts. Our study suggested that the tested machine learning methods are potentially useful for DNA adducts identification.
7FFEDC6D	Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approacheswe single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from...
77630BE7	This paper presents a new hybrid genetic algorithm (HGA) for feature selection (FS), called as HGAFS. The vital aspect of this algorithm is the selection of salient feature subset within a reduced size. HGAFS incorporates a new local search operation that is devised and embedded in HGA to fine-tune the search in FS process. The local search technique works on basis of the distinct and informative nature of input features that is computed by their correlation information. The aim is to guide the search process so that the newly generated offsprings can be adjusted by the less correlated (distinct) features consisting of general and special characteristics of a given dataset. Thus, the proposed HGAFS receives the reduced redundancy of information among the selected features. On the other hand, HGAFS emphasizes on selecting a subset of salient features with reduced number using a subset size determination scheme. We have tested our HGAFS on 11 real-world classification datasets having dimensions varying from 8 to 7129. The performances of HGAFS have been compared with the results of other existing ten well-known FS algorithms. It is found that, HGAFS produces consistently better performances on selecting the subsets of salient features with resulting better classification accuracies.
76C82C3C	Feature selection is very important in the mining of multivariate time series data, which is represented in matrix. We propose a novel filter method termed as class separability feature selection (CSFS) for feature selection from multivariate time series with the trace-based class separability criterion. The mutual information matrix between variables is used as the features for classification. And the feature selection algorithm CSFS selects features according to the scores of class separability and variable separability. The proposed method is compared with CLeVer, Corona and AGV on the UCI EEG data sets, and the simulation results substantiate the good performance of CSFS.
5D3348BE	Generic ensemble methods can achieve excellent learning performance, but are not good candidates for active learning because of their different design purposes. We investigate how to use diversity of the member classifiers of an ensemble for efficient active learning. We empirically show, using benchmark data sets, that (1) to achieve a good (stable) ensemble, the number of classifiers needed in the ensemble varies for different data sets; (2) feature selection can be applied for classifier selection from ensembles to construct compact ensembles with high performance. Benchmark data sets and a real-world application are used to demonstrate the effectiveness of the proposed approach.
5E5CE080	We propose a novel feature selection filter for supervised learning, which relies on the efficient estimation of the mutual information between a high-dimensional set of features and the classes. We bypass the estimation of the probability density function with the aid of the entropic-graphs approximation of Rényi entropy, and the subsequent approximation of the Shannon entropy. Thus, the complexity does not depend on the number of dimensions but on the number of patterns/samples, and the curse of dimensionality is circumvented. We show that it is then possible to outperform algorithms which individually rank features, as well as a greedy algorithm based on the maximal relevance and minimal redundancy criterion. We successfully test our method both in the contexts of image classification and microarray data classification. For most of the tested data sets, we obtain better classification results than those reported in the literature.
7B157682	A new suboptimal search strategy for feature selection is presented. It represents a more sophisticated version of “classical” floating search algorithms (Pudil et al., 1994), attempts to remove some of their potential deficiencies and facilitates finding a solution even closer to the optimal one.
75695754	We propose expected attainable discrimination (EAD) as a measure to select discrete valued features for reliable discrimination between two classes of data. EAD is an average of the area under the ROC curves obtained when a simple histogram probability density model is trained and tested on many random partitions of a data set. EAD can be incorporated into various stepwise search methods to determine promising subsets of features, particularly when misclassification costs are difficult or impossible to specify. Experimental application to the problem of risk prediction in pregnancy is described.
78192D7B	Sequential search methods characterized by a dynamically changing number of features included or eliminated at each step, henceforth “floating” methods, are presented. They are shown to give very good results and to be computationally more effective than the branch and bound method.
7FF4A547	Biomarker discovery from high-dimensional data is a crucial problem with enormous applications in biology and medicine. It is also extremely challenging from a statistical viewpoint, but surprisingly few studies have investigated the relative strengths and weaknesses of the plethora of existing feature selection methods. In this study we compare An external file that holds a picture, illustration, etc. Object name is pone.0028210.e001.jpg feature selection methods on An external file that holds a picture, illustration, etc. Object name is pone.0028210.e002.jpg public gene expression datasets for breast cancer prognosis, in terms of predictive performance, stability and functional interpretability of the signatures they produce. We observe that the feature selection method has a significant influence on the accuracy, stability and interpretability of signatures. Surprisingly, complex wrapper and embedded methods generally do not outperform simple univariate feature selection methods, and ensemble feature selection has generally no positive effect. Overall a simple Student's t-test seems to provide the best results.
7D0157AF	It is difficult to find the optimal sparse solution of a manifold learning based dimensionality reduction algorithm. The lasso or the elastic net penalized manifold learning based dimensionality reduction is not directly a lasso penalized least square problem and thus the least angle regression (LARS) (Efron et al., Ann Stat 32(2):407–499, 2004), one of the most popular algorithms in sparse learning, cannot be applied. Therefore, most current approaches take indirect ways or have strict settings, which can be inconvenient for applications. In this paper, we proposed the manifold elastic net or MEN for short. MEN incorporates the merits of both the manifold learning based dimensionality reduction and the sparse learning based dimensionality reduction. By using a series of equivalent transformations, we show MEN is equivalent to the lasso penalized least square problem and thus LARS is adopted to obtain the optimal sparse solution of MEN. In particular, MEN has the following advantages for subsequent classification: (1) the local geometry of samples is well preserved for low dimensional data representation, (2) both the margin maximization and the classification error minimization are considered for sparse projection calculation, (3) the projection matrix of MEN improves the parsimony in computation, (4) the elastic net penalty reduces the over-fitting problem, and (5) the projection matrix of MEN can be interpreted psychologically and physiologically. Experimental evidence on face recognition over various popular datasets suggests that MEN is superior to top level dimensionality reduction algorithms.
6BDF01D4	Image spam is a new trend in the family of email spams. The new image spams employ a variety of image processing technologies to create random noises. In this paper, we propose a semi-supervised approach, regularized discriminant EM algorithm (RDEM), to detect image spam emails, which leverages small amount of labeled data and large amount of unlabeled data for identifying spams and training a classification model simultaneously. Compared with fully supervised learning algorithms, the semi-supervised learning algorithm is more suitedin adversary classification problems, because the spammers are actively protecting their work by constantly making changes to circumvent the spam detection. It makes the cost too high for fully supervised learning to frequently collect sufficient labeled data for training. Experimental results demonstrate that our approach achieves 91.66% high detection rate with less than 2.96% false positive rate, meanwhile it significantly reduces the labeling cost.
002A1815	In this paper we propose an automatic salient object extraction method for nature scene. The proposed method first utilizes an algorithm based on visual attention model to obtain a prior knowledge for Graph Cut, and then constructs the weighted graph of Graph Cut based on super-pixels pre-segmented by the improved watershed algorithm in order to accelerate the speed of proposed method. In this framework, Visual saliency map is obtained using chrominance and intensity features in HSV color space, which provides the approximate region that contains salient object to be segmented. Then the salient object region after extension is cropped as input image, and pre-segmented by the improved watershed algorithm into several regions to construct weighted graph. Finally the salient object is obtained by Graph Cut algorithm. Experiment results show that our algorithm can automatically get salient object without human interactions, and speed up the segmentation without decreasing segmentation accuracy.
80F19426	This paper presents a novel way to learn Chinese polarity lexicons by using both external relations and internal formation of Chinese words, i.e. by integrating two kinds of different but complementary models: graph models and morphological feature-based models. The polarity detection is first treated as a semi-supervised learning in a graph, and then machine learning is used based on morphological features of Chinese words. The results show that the the integration of morphological feature-based models and graph models significantly outperforms the baselines.
7E0D8880	This paper proposes a novel method to apply the standard graph cut technique to segmenting multimodal tensor valued images. The Riemannian nature of the tensor space is explicitly taken into account by first mapping the data to a Euclidean space where non-parametric kernel density estimates of the regional distributions may be calculated from user initialized regions. These distributions are then used as regional priors in calculating graph edge weights. Hence this approach utilizes the true variation of the tensor data by respecting its Riemannian structure in calculating distances when forming probability distributions. Further, the non-parametric model generalizes to arbitrary tensor distribution unlike the Gaussian assumption made in previous works. Casting the segmentation problem in a graph cut framework yields a segmentation robust with respect to initialization on the data tested.
7FEDD688	This paper adds a number of novel concepts into global s/t cut methods improving their efficiency and making them relevant for a wider class of applications in vision where algorithms should ideally run in real-time. Our new Active Cuts (AC) method can effectively use a good approximate solution (initial cut) that is often available in dynamic, hierarchical, and multi-label optimization problems in vision. In many problems AC works faster than the state-of-the-art max-flow methods [2] even if initial cut is far from the optimal one. Moreover, empirical speed improves several folds when initial cut is spatially close to the optima. Before converging to a global minima, Active Cuts outputs a multitude of intermediate solutions (intermediate cuts) that, for example, can be used be accelerate iterative learning-based methods or to improve visual perception of graph cuts realtime performance when large volumetric data is segmented. Finally, it can also be combined with many previous methods for accelerating graph cuts.
7DEB0ED6	Voxel occupancy is one approach for reconstructing the 3-dimensional shape of an object from multiple views. In voxel occupancy, the task is to produce a binary labeling of a set of voxels, that determines which voxels are filled and which are empty. In this paper, we give an energy minimization formulation of the voxel occupancy problem. The global minimum of this energy can be rapidly computed with a single graph cut, using a result due to D. Greig et al. (1989). The energy function we minimize contains a data term and a smoothness term. The data term is a sum over the individual voxels, where the penalty for a voxel is based on the observed intensities of the pixels that intersect it. The smoothness term is the number of empty voxels adjacent to filled ones. Our formulation can be viewed as a generalization of silhouette intersection, with two advantages: we do not compute silhouettes, which are a major source of errors; and we can naturally incorporate spatial smoothness. We give experimental results showing reconstructions from both real and synthetic imagery. Reconstruction using this smoothed energy function is not much more time consuming than simple silhouette intersection; it takes about 10 seconds to reconstruct a one million voxel volume.
8062DE9D	In this paper we present a graph cuts based active contours (GCBAC) approach to object segmentation problems. Our method is a combination of active contours and the optimization tool of graph cuts and differs fundamentally from traditional active contours in that it uses graph cuts to iteratively deform the contour. Consequently, it has the following advantages. (1) It has the ability to jump over local minima and provide a more global result. (2) Graph cuts guarantee continuity and lead to smooth contours free of self-crossing and uneven spacing problems. Therefore, the internal force, which is commonly used in traditional energy functions to control the smoothness, is no longer needed, and hence the number of parameters is greatly reduced. (3). Our approach easily extends to the segmentation of three and higher dimensional objects. In addition, the algorithm is suitable for interactive correction and is shown to always converge. Experimental results and analyses are provided.
092E6612	Semi-supervised learning works on utilizing both labeled and unlabeled data to improve learning performance, which has been receiving increasing attention in many applications such as clustering and classification. In this paper, we focus on the semi-supervised learning methods developed on data graph whose edge weights are measured by low-rank representation (LRR) coefficients. Specifically, we impose two constraints on LRR when constructing the graph: local affinity and distant repulsion, to preserve the data manifold information. The proposed model, termed structure preserving LRR (SPLRR), can preserve the local geometrical structure and without distorting the distant repulsion property. Using the augmented Lagrange multiplier (ALM) method framework, we derive an efficient approach to optimizing the SPLRR model. Experiments are conducted on four widely used data sets to validate the effectiveness of our proposed SPLRR model and the results demonstrate that SPLRR is an excellent model for graph based semi-supervised learning in comparison with the state-of-the-art methods.
7F7034C9	In this paper, we describe the structure of separable self-complementary graphs
6A2D0EE9	We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known viewpoints. Multi-camera scene reconstruction is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility. In this paper, we take an approach that has yielded excellent results for stereo, namely energy minimization via graph cuts. We first give an energy minimization formulation of the multi-camera scene reconstruction problem. The energy that we minimize treats the input images symmetrically, handles visibility properly, and imposes spatial smoothness while preserving discontinuities. As the energy function is NP-hard to minimize exactly, we give a graph cut algorithm that computes a local minimum in a strong sense. We handle all camera configurations where voxel coloring can be used, which is a large and natural class. Experimental data demonstrates the effectiveness of our approach.
7E5B1A47	In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available.
7FFB7F08	In this paper, we propose an interactive color natural image segmentation method. The method integrates color feature with multiscale nonlinear structure tensor texture (MSNST) feature and then uses GrabCut method to obtain the segmentations. The MSNST feature is used to describe the texture feature of an image and integrated into GrabCut framework to overcome the problem of the scale difference of textured images. In addition, we extend the Gaussian Mixture Model (GMM) to MSNST feature and GMM based on MSNST is constructed to describe the energy function so that the texture feature can be suitably integrated into GrabCut framework and fused with the color feature to achieve the more superior image segmentation performance than the original GrabCut method. For easier implementation and more efficient computation, the symmetric KL divergence is chosen to produce the estimates of the tensor statistics instead of the Riemannian structure of the space of tensor. The Conjugate norm was employed using Locality Preserving Projections (LPP) technique as the distance measure in the color space for more discriminating power. An adaptive fusing strategy is presented to effectively adjust the mixing factor so that the color and MSNST texture features are efficiently integrated to achieve more robust segmentation performance. Last, an iteration convergence criterion is proposed to reduce the time of the iteration of GrabCut algorithm dramatically with satisfied segmentation accuracy. Experiments using synthesis texture images and real natural scene images demonstrate the superior performance of our proposed method.
7F3B7CA7	he problem of multilabel classification has attracted great interest in the last decade, where each instance can be assigned with a set of multiple class labels simultaneously. It has a wide variety of real-world applications, e.g., automatic image annotations and gene function analysis. Current research on multilabel classification focuses on supervised settings which assume existence of large amounts of labeled training data. However, in many applications, the labeling of multilabeled data is extremely expensive and time consuming, while there are often abundant unlabeled data available. In this paper, we study the problem of transductive multilabel learning and propose a novel solution, called Trasductive Multilabel Classification (TraM), to effectively assign a set of multiple labels to each instance. Different from supervised multilabel learning methods, we estimate the label sets of the unlabeled instances effectively by utilizing the information from both labeled and unlabeled data. We first formulate the transductive multilabel learning as an optimization problem of estimating label concept compositions. Then, we derive a closed-form solution to this optimization problem and propose an effective algorithm to assign label sets to the unlabeled instances. Empirical studies on several real-world multilabel learning tasks demonstrate that our TraM method can effectively boost the performance of multilabel classification by using both labeled and unlabeled data.
7FC37865	Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.
7D3F70C2	In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available.
7CA1F170	Given an edge-weighted graph , a subset , an integer and a real , the minimum subpartition problem asks to find a family of k nonempty disjoint subsets with , , so as to minimize , where denotes the total weight of edges between X and . In this paper, we show that the minimum subpartition problem can be solved in time. The result is then applied to the minimum k-way cut problem and the graph strength problem to improve the previous best time bounds of 2-approximation algorithms for these problems to .
7CEC1AD4	One of the main difficulties in machine learning is how to solve large-scale problems effectively, and the labeled data are limited and fairly expensive to obtain. In this paper a new semi-supervised SVM algorithm is proposed. It applies tri-training to improve SVM. The semi-supervised SVM makes use of the large number of unlabeled data to modify the classifiers iteratively. Although tri-training doesn't put any constraints on the classifier, the proposed method uses three different SVMs as the classification algorithm. Experiments on UCI datasets show that tri-training can improve the classification accuracy of SVM and can increase the difference of classifiers, the accuracy of final classifier will be higher. Theoretical analysis and experiments show that the proposed method has excellent accuracy and classification speed.
7DE594CD	In image classification and other learning-based object recognition tasks, it is often tedious and expensive to label large training data sets. Discriminant-EM (DEM), proposed as a semi-supervised learning framework, takes both labeled and unlabeled data to learn classifiers. The paper extends the linear DEM to a nonlinear kernel algorithm, KDEM, and evaluates KDEM on both benchmark image databases and synthetic data. Various comparisons with other state-of-the-art learning techniques are investigated.
7D78459A	In semi-supervised classification boosting, a similarity measure is demanded in order to measure the distance between samples (both labeled and unlabeled). However, most of the existing methods employed a simple metric, such as Euclidian distance, which may not be able to truly reflect the actual similarity/distance. This paper presents a novel similarity learning method based on the geodesic distance. It incorporates the manifold, margin and the density information of the data which is important in semi-supervised classification. The proposed similarity measure is then applied to a semi-supervised multi-class boosting (SSMB) algorithm. In turn, the three semi-supervised assumptions, namely smoothness, low density separation and manifold assumption, are all satisfied. We evaluate the proposed method on UCI databases. Experimental results show that the SSMB algorithm with proposed similarity measure outperforms the SSMB algorithm with Euclidian distance.
814FEDA0	In the short time since publication of Boykov and Jolly's seminal paper [2001], graph cuts have become well established as a leading method in 2D and 3D semi-automated image segmentation. Although this approach is computationally feasible for many tasks, the memory overhead and supralinear time complexity of leading algorithms results in an excessive computational burden for high-resolution data. In this paper, we introduce a multilevel banded heuristic for computation of graph cuts that is motivated by the well-known narrow band algorithm in level set computation. We perform a number of numerical experiments to show that this heuristic drastically reduces both the running time and the memory consumption of graph cuts while producing nearly the same segmentation result as the conventional graph cuts. Additionally, we are able to characterize the type of segmentation target for which our multilevel banded heuristic yields different results from the conventional graph cuts. The proposed method has been applied to both 2D and 3D images with promising results.
7D7D5508	Geodesic active contours and graph cuts are two standard image segmentation techniques. We introduce a new segmentation method combining some of their benefits. Our main intuition is that any cut on a graph embedded in some continuous space can be interpreted as a contour (in 2D) or a surface (in 3D). We show how to build a grid graph and set its edge weights so that the cost of cuts is arbitrarily close to the length (area) of the corresponding contours (surfaces) for any anisotropic Riemannian metric. There are two interesting consequences of this technical result. First, graph cut algorithms can be used to find globally minimum geodesic contours (minimal surfaces in 3D) under arbitrary Riemannian metric for a given set of boundary conditions. Second, we show how to minimize metrication artifacts in existing graph-cut based methods in vision. Theoretically speaking, our work provides an interesting link between several branches of mathematics -differential geometry, integral geometry, and combinatorial optimization. The main technical problem is solved using Cauchy-Crofton formula from integral geometry.
7E77CFA2	Several new algorithms for visual correspondence based on graph cuts have recently been developed. While these methods give very strong results in practice, they do not handle occlusions properly. Specifically, they treat the two input images asymmetrically, and they do not ensure that a pixel corresponds to at most one pixel in the other image. In this paper, we present a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms. We give experimental results for stereo as well as motion, which demonstrate that our method performs well both at detecting occlusions and computing disparities.
7E68E6DC	In this paper, we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP estimates for dynamically changing MRF models of labeling problems in computer vision, such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, we show how to efficiently compute the maximum flow in a modified version of the graph. Our experiments showed that the time taken by our algorithm is roughly proportional to the number of edges whose weights were different in the two graphs. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video and compare it with the best known st-mincut algorithm. The results show that the dynamic graph cut algorithm is much faster than its static counterpart and enables real time image segmentation. It should be noted that our method is generic and can be used to yield similar improvements in many other cases that involve dynamic change in the graph
8014E25E	Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.
8150BAE6	In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as "object" or "background" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both "object" and "background" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.
807F0055	Most existing semi-supervised methods implemented either the cluster assumption or the manifold assumption. The performance will degrade if the assumption was not proper for the data. A method was proposed by combining both the cluster assumption and the manifold assumption. A semi-supervised kernel which reflected geometric information of the samples was constructed through warping the Reproducing Kernel Hilbert Space. Then the semi-supervised kernel was used in SVM which was based on cluster assumption, and a progressive learning procedure was used in the proposed method. Experiments had been took on synthetic and real data sets, and the results showed that, compared with the progressive SVM with common kernel and the standard SVM with semi supervised kernel, the proposed method using semi-supervised kernel in progressive SVM had competitive performance.
796A74A9	This paper proposes a novel unsupervised cosegmentation method which automatically segments the common objects in multiple images. It designs a simple superpixel matching algorithm to explore the inter-image similarity. It then constructs the object mask for each image using the matched superpixels. This object mask is a convex hull potentially containing the common objects and some backgrounds. Finally, it applies a new FastGrabCut algorithm, an improved GrabCut algorithm, on the object mask to simultaneously improve the segmentation efficiency and maintain the segmentation accuracy. This FastGrabcut algorithm introduces preliminary classification to accelerate convergence. It uses Expectation Maximization (EM) algorithm to estimate optimal Gaussian Mixture Model(GMM) parameters of the object and background and then applies Graph Cuts to minimize the energy function for each image. Experimental results on the iCoseg dataset demonstrate the accuracy and robustness of our cosegmentation method.
8010A489	A modified version for semi-supervised learning algorithm with local and global consistency was proposed in this paper. The new method adds the label information, and adopts the geodesic distance rather than Euclidean distance as the measure of the difference between two data points when conducting calculation. In addition we add class prior knowledge. It was found that the effect of class prior knowledge was different between under high label rate and low label rate. The experimental results show that the changes attain the satisfying classification performance better than the original algorithms.
08451859	An inescapable bottleneck with learning from large data sets is the high cost of labeling training data. Unsupervised learning methods have promised to lower the cost of tagging by leveraging notions of similarity among data points to assign tags. However, unsupervised and semi-supervised learning techniques often provide poor results due to errors in estimation. We look at methods that guide the allocation of human effort for labeling data so as to get the greatest boosts in discriminatory power with increasing amounts of work. We focus on the application of value of information to Gaussian Process classifiers and explore the effectiveness of the method on the task of classifying voice messages.
7E51F583	Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We present motivation and detailed technical description of the basic combinatorial optimization framework for image segmentation via s/t graph cuts. After the general concept of using binary graph cut algorithms for object segmentation was first proposed and tested in Boykov and Jolly (2001), this idea was widely studied in computer vision and graphics communities. We provide links to a large number of known extensions based on iterative parameter re-estimation and learning, multi-scale or hierarchical approaches, narrow bands, and other techniques for demanding photo, video, and medical applications.
8053771F	Graph matching is an essential problem in computer vision that has been successfully applied to 2D and 3D feature matching and object recognition. Despite its importance, little has been published on learning the parameters that control graph matching, even though learning has been shown to be vital for improving the matching rate. In this paper we show how to perform parameter learning in an unsupervised fashion, that is when no correct correspondences between graphs are given during training. Our experiments reveal that unsupervised learning compares favorably to the supervised case, both in terms of efficiency and quality, while avoiding the tedious manual labeling of ground truth correspondences. We verify experimentally that our learning method can improve the performance of several state-of-the art graph matching algorithms. We also show that a similar method can be successfully applied to parameter learning for graphical models and demonstrate its effectiveness empirically.
75DC2DD5	To understand text contents better, many research efforts have been made exploring detection and classification of the semantic relation between a concept pair. As described herein, we present our study of a semantic relation classification task as a graph-based multi-view learning task. Semantic relation can be naturally represented from two views: entity pair view and context view. Then we construct a weighted complete graph for each view and a bipartite graph to combine information of different views. An instance’s label score is propagated on each intra-view graph and inter-view graph. The proposed algorithm is evaluated using the concept description language for natural language (CDL) corpus and SemEval-2007 Task 04 dataset. The experimental results validate its effectiveness.
789986D9	We propose a graph cut based automatic method for prostate segmentation using image feature, context information and semantic knowledge. A volume of interest (VOI) is first identified using supervoxel oversegmentation and their subsequent classification of the supervoxels. All voxels within the VOI are labeled prostate or background using graph cuts. Semantic information obtained from Random forest (RF) classifiers is used to formulate the smoothness cost. Use of context and semantic information contributes to higher segmentation accuracy than competing methods.
7E0AE1E4	Segmentation of tree-like structure within medical imaging modalities, such as x-ray, MRI, ultrasound, etc., is an important step for analyzing branching patterns involved in many anatomic structures. However, images acquired using these different acquisition techniques frequently have features of poor contrast, blurring and noise, and therefore the segmentation result of traditional image segmentation methods may not be satisfactory. In this paper, we propose a framework for accurate segmentation of the ductal network in x-ray galactograms. Our approach is based on the graph cut algorithm and texture analysis to extract features of skewness, coarseness, contrast, energy and fractal dimension. The features are chosen to capture not only architectural variability of the enhanced ductal tree, but also spatial variations among pixels. The proposed approach was applied to a dataset of 20 galactographic images. We performed receiver operating characteristic (ROC) curve analysis to assess the accuracy. The area under the ROC curve observed was 0.76, indicating that our approach may potentially assist clinicians in the interpretation of breast images and facilitate the investigation of relationships among structure and texture of the branching patterns.
5FC2C7FC	Hierarchical prosody structure generation is a key component for a speech synthesis system. One major feature of the prosody of Mandarin Chinese speech flow is prosodic phrase grouping. In this paper we proposed an approach for prediction of Chinese prosodic phrase boundaries from a limited amount of labeled training examples and some amount of unlabeled data using conditional random fields. Some useful unlabeled data are chosen based on the assigned labels and the prediction probabilities of the current learned model. The useful unlabeled data is then exploited to improve the learning. Experiments show that the approach improves overall performance. The precision and recall ratio are improved.
6D5FAEC4	Recommending phrases from web pages for advertisers to bid on against search engine queries is an important research problem with direct commercial impact. Most approaches have found it infeasible to determine the relevance of all possible queries to a given ad landing page and have focussed on making recommendations from a small set of phrases extracted (and expanded) from the page using NLP and ranking based techniques. In this paper, we eschew this paradigm, and demonstrate that it is possible to efficiently predict the relevant subset of queries from a large set of monetizable ones by posing the problem as a multi-label learning task with each query being represented by a separate label.We develop Multi-label Random Forests to tackle problems with millions of labels. Our proposed classifier has prediction costs that are logarithmic in the number of labels and can make predictions in a few milliseconds using 10 Gb of RAM. We demonstrate that it is possible to generate training data for our classifier automatically from click logs without any human annotation or intervention. We train our classifier on tens of millions of labels, features and training points in less than two days on a thousand node cluster. We develop a sparse semi-supervised multi-label learning formulation to deal with training set biases and noisy labels harvested automatically from the click logs. This formulation is used to infer a belief in the state of each label for each training ad and the random forest classifier is extended to train on these beliefs rather than the given labels. Experiments reveal significant gains over ranking and NLP based techniques on a large test set of 5 million ads using multiple metrics.
7D656EF1	In this paper, we propose a method of object recognition and segmentation using Scale-Invariant Feature Transform (SIFT) and Graph Cuts. SIFT feature is invariant for rotations, scale changes, and illumination changes and it is often used for object recognition. However, in previous object recognition work using SIFT, the object region is simply presumed by the affine-transformation and the accurate object region was not segmented. On the other hand, Graph Cuts is proposed as a segmentation method of a detail object region. But it was necessary to give seeds manually. By combing SIFT and Graph Cuts, in our method, the existence of objects is recognized first by vote processing of SIFT keypoints. After that, the object region is cut out by Graph Cuts using SIFT keypoints as seeds. Thanks to this combination, both recognition and segmentation are performed automatically under cluttered backgrounds including occlusion.
622CA6D3	The graph cut model has been widely used in image segmentation, in which both the region and boundary information play important roles for accurate segmentation. However, how to effectively model and combine these two information is still a challenge. In this paper, we improve the conventional graph cut methods by combining the region and boundary information with an effective and straightforward way. When modeling the region information, the component-wise expectation–maximization for Gaussian mixtures algorithm is used to learn the parameters of the prior knowledge. When modeling the boundary information, the weighting patch is used to represent the similarities of the neighboring pixels. Then the region and boundary information are combined by a weighting parameter, where the weight is small for boundary pixels and is large for non-boundary pixels. Finally, experiments on various images from the Berkeley and MSRC data sets were conducted to demonstrate the effectiveness of the proposed method.
811B19DA	Data describing networks—such as communication networks, transaction networks, disease transmission networks, collaboration networks, etc.—are becoming increasingly available. While observational data can be useful, it often only hints at the actual underlying process that governs interactions and attributes. For example, an email communication network provides insight into its users and their relationships, but is not the same as the “real” underlying social network. In this article, we introduce the problem of graph identification, i.e., discovering the latent graph structure underlying an observed network. We cast the problem as a probabilistic inference task, in which we must infer the nodes, edges, and node labels of a hidden graph, based on evidence. This entails solving several canonical problems in network analysis: entity resolution (determining when two observations correspond to the same entity), link prediction (inferring the existence of links), and node labeling (inferring hidden attributes). While each of these subproblems has been well studied in isolation, here we consider them as a single, collective task. We present a simple, yet novel, approach to address all three subproblems simultaneously. Our approach, which we refer to as C3, consists of a collection of Coupled Collective Classifiers that are applied iteratively to propagate inferred information among the subproblems. We consider variants of C3 using different learning and inference techniques and empirically demonstrate that C3 is superior, both in terms of predictive accuracy and running time, to state-of-the-art probabilistic approaches on four real problems.
5E800C58	An N-dimensional image is divided into “object” and “background” segments using a graph cut approach. A graph is formed by connecting all pairs of neighboring image pixels (voxels) by weighted edges. Certain pixels (voxels) have to be a priori identified as object or background seeds providing necessary clues about the image content. Our objective is to find the cheapest way to cut the edges in the graph so that the object seeds are completely separated from the background seeds. If the edge cost is a decreasing function of the local intensity gradient then the minimum cost cut should produce an object/background segmentation with compact boundaries along the high intensity gradient values in the image. An efficient, globally optimal solution is possible via standard min-cut/max-flow algorithms for graphs with two terminals. We applied this technique to interactively segment organs in various 2D and 3D medical images.
76D5AD7F	In this letter, a modified algorithm is proposed to extend 2-class semi-supervised learning on Laplacian eigenmaps to multi-class learning problems. The modified algorithm significantly increases its learning speed, and at the same time attains a satisfactory classification performance that is not lower than the original algorithm.
765578E2	The fuzzy c-partition entropy has been widely adopted as a global optimization technique for finding the optimized thresholds for multilevel image segmentation. However, it involves expensive computation as the number of thresholds increases and often yields noisy segmentation results since spatial coherence is not enforced. In this paper, an iterative calculation scheme is presented for reducing redundant computations in entropy evaluation. The efficiency of threshold selection is further improved through utilizing the artificial bee colony algorithm as the optimization technique. Finally, instead of performing thresholding for each pixel independently, the presented algorithm oversegments the input image into small regions and uses the probabilities of fuzzy events to define the costs of different label assignments for each region. The final segmentation results is computed using graph cut, which produces smooth segmentation results. The experimental results demonstrate the presented iterative calculation scheme can greatly reduce the running time and keep it stable as the number of required thresholds increases. Quantitative evaluations over 20 classic images also show that the presented algorithm outperforms existing multilevel segmentation approaches.
756C1A4A	The problem of segmenting a foreground object out from its complex background is of great interest in image processing and computer vision. Many interactive segmentation algorithms such as graph cut have been successfully developed. In this paper, we present four technical components to improve graph cut based algorithms, which are combining both color and texture information for graph cut, including structure tensors in the graph cut model, incorporating active contours into the segmentation process, and using a “softbrush” tool to impose soft constraints to refine problematic boundaries. The integration of these components provides an interactive segmentation method that overcomes the difficulties of previous segmentation algorithms in handling images containing textures or low contrast boundaries and producing a smooth and accurate segmentation boundary. Experiments on various images from the Brodatz, Berkeley and MSRC data sets are conducted and the experimental results demonstrate the high effectiveness of the proposed method to a wide range of images.
7AFB42E2	This paper proposes a novel texture segmentation approach using independent-scale component-wise Riemannian-covariance Gaussian mixture model (ICRGMM) in Kullback–Leibler (KL) measure based multi-scale nonlinear structure tensor (MSNST) space. We use the independent-scale distribution and full-covariance structure to replace the covariant-scale distribution and 1D-variance structure used in our previous research. To construct the optimal full-covariance structure, we define the full-covariance on KL, Euclidean, log-Euclidean, and Riemannian gradient mappings, and compare their performances. The comparison experiments demonstrate that the Riemannian gradient mapping leads to its optimum properties over other choices when constructing the full-covariance. To estimate and update the statistical parameters more accurately, the component-wise expectation-maximization for mixtures (CEM2) algorithm is proposed instead of the originally used K-means algorithm. The superiority of the proposed ICRGMM has been demonstrated based on texture clustering and Graph Cuts based texture segmentation using a large number of synthesis texture images and real natural scene textured images, and further analyzed in terms of error ratio and modified F-measure, respectively.
7FED3742	This paper proposes a constrained clustering method that is based on a graph-cut problem formalized by SDP (Semi-Definite Programming). Our SDP approach has the advantage of convenient constraint utilization compared with conventional spectral clustering methods. The algorithm starts from a single cluster of a complete dataset and repeatedly selects the largest cluster, which it then divides into two clusters by swapping rows and columns of a relational label matrix obtained by solving the maximum graph-cut problem. This swapping procedure is effective because we can create clusters without any computationally heavy matrix decomposition process to obtain a cluster label for each data. The results of experiments using a Web document dataset demonstrated that our method outperformed other conventional and the state of the art clustering methods in many cases. Hence we consider our clustering provides a promising basic method to interactive Web clustering.
76D55405	The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?
793E2B82	Techniques for automatic query expansion from top retrieved documents have shown promise for improving retrieval effectiveness on large collections; however, they often rely on an empirical ground, and there is a shortage of cross-system comparisons. Using ideas from Information Theory, we present a computationally simple and theoretically justified method for assigning scores to candidate expansion terms. Such scores are used to select and weight expansion terms within Rocchio's framework for query reweigthing. We compare ranking with information-theoretic query expansion versus ranking with other query expansion techniques, showing that the former achieves better retrieval effectiveness on several performance measures. We also discuss the effect on retrieval effectiveness of the main parameters involved in automatic query expansion, such as data sparseness, query difficulty, number of selected documents, and number of selected terms, pointing out interesting relationships.
7BB25BBA	In this paper, we investigate medical students medical search behavior on a medical domain. We use two behavioral signals: detailed query analysis (qualitative and quantitative) and task completion time to understand how medical students perform medical searches based on varying task complexity. We also investigate how task complexity and topic familiarity affect search behavior. We gathered 80 interactive search sessions from an exploratory survey with 20 medical students. We observe information searching behavior using 3 simulated work task scenarios and 1 personal scenario. We present quantitative results from two perspectives: overall and user perceived task complexity. We also analyze query properties from a qualitative aspect. Our results show task complexity and topic familiarity affect search behavior of medical students. In some cases, medical students demonstrate different search traits on a personal task in comparison to the simulated work task scenarios. These findings help us better understand medical search behavior. Medical search engines can use these findings to detect and adapt to medical students' search behavior to enhance a student's search experience.
59F41337	In information retrieval, queries can fail to find documents due to mismatch in terminology. Query expansion is a well-known technique addressing this problem, where additional query terms are automatically chosen from highly ranked documents, and it has been shown to be effective at improving query performance. However, current techniques for query expansion use fixed values for key parameters, determined by tuning on test collections. In this paper we show that these parameters may not be generally applicable, and more significantly that the assumption that the same parameter settings can be used for all queries is invalid. Using detailed experiments with two test collections, we demonstrate that new methods for choosing parameters must be found. However, our experiments also demonstrate that there is considerable further scope for improvement to effectiveness through better query expansion.
7F448138	With the population of information technique and Web application, Web information retrieval has became an essential part of work, study and life. As the problems of retrieval increasing, many researches pay more attention on retrieval technique. The study presented in this paper proposes the query expansion technique based on ontology. It uses the rich semantic knowledge of ontology to upgrade the retrieval based on keywords to concepts, and combines it with the specialized engine to improve retrieval effect and efficiency. The paper also takes patent information for example to explain its application at the end.
5CE38F5A	This paper presents an ongoing research on the comparison of ontological query expansion methods. Query Expansion is a technique that aims to enhance the results of a search by adding terms to the search query; today, it is a very important research topic in the semantic web and information retrieval areas. Although many efforts have been made from the theoretical point of view to implement effective and general methods for expanding queries, based on both statistical and ontological approaches, the practical applicability of them is nowadays restricted to few and very specific domains. The aim of this paper is the definition of a platform for the implementation of a subset of such methods, in order to make comparisons among them and try to define how and when use ontological QE. This work is part of JUMAS, a research project funded by European Community where query expansion is used to support the retrieval of significant information from audio–video transcriptions in the legal domain.
7CD3D243	Enterprise search is important, and the search quality has a direct impact on the productivity of an enterprise. Many information needs of enterprise search center around entities. Intuitively, information related to the entities mentioned in the query, such as related entities, would be useful to reformulate the query and improve the retrieval performance. However, most existing studies on query expansion are term-centric. In this paper, we propose a novel entity-centric query expansion framework for enterprise search. Specifically, given a query containing entities, we first utilize both unstructured and structured information to find entities that are related to the ones in the query. We then discuss how to adapt existing feedback methods to use the related entities to improve search quality. Experiment results show that the proposed entity-centric query expansion strategy is more effective to improve the search performance than the state-of-the-art pseudo feedback methods on longer, natural language-like queries with entities.
79904BF4	Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26%--29%, and 18%--20% over an optimised, conventional expansion approach.
80BCD8D0	We introduce a new theoretical derivation, evaluation methods, and extensive empirical analysis for an automatic query expansion framework in which model estimation is cast as a robust constrained optimization problem. This framework provides a powerful method for modeling and solving complex expansion problems, by allowing multiple sources of domain knowledge or evidence to be encoded as simultaneous optimization constraints. Our robust optimization approach provides a clean theoretical way to model not only expansion benefit, but also expansion risk, by optimizing over uncertainty sets for the data. In addition, we introduce risk-reward curves to visualize expansion algorithm performance and analyze parameter sensitivity. We show that a robust approach significantly reduces the number and magnitude of expansion failures for a strong baseline algorithm, with no loss in average gain. Our approach is implemented as a highly efficient post-processing step that assumes little about the baseline expansion method used as input, making it easy to apply to existing expansion methods. We provide analysis showing that this approach is a natural and effective way to do selective expansion, automatically reducing or avoiding expansion in risky scenarios, and successfully attenuating noise in poor baseline methods.
69FD3F3F	This paper describes our work at CLEF 2007 Robust Task. We have applied local query expansion using windows of terms, but considering different measures of robustness during the training phase in order to optimize the performance: MAP, GMAP, MMR, GS@10, P@10, number of failed topics, number of topics below 0.1 MAP, and number of topics with P@10=0. The results were not disappointing, but no settings were found that simultaneously improved all measures. A key issue for us was to decide which set of measures we had to select for optimization.This year all our runs also gave good rankings, both base runs and expanded ones. However, our expansion technique does not improve significantly the retrieval performance. At TREC and CLEF Robust Tasks other expansion techniques have been used to improve robustness, but results were not uniform. In conclusion, regarding robustness the objective must be to make good information retrieval systems, rather than to tune some query expansion techniques.
5A10CFD2	In this paper we explain experiments in the medical information retrieval task (ImageCLEFmed). We experimented with query expansion and the amount of textual information obtained from the collection. For expansion, we carried out experiments using MeSH ontology and UMLS separately. With respect to textual collection, we produced three different collections, the first one with caption and title, the second one with caption, title and the text of the section where the image appears, and the third one with the full text article. Moreover, we experimented with textual and visual search, along with the combination of these two results. For image retrieval we used the results generated by the FIRE software. The best results were obtained using MeSH query expansion on shortest textual collection (only caption and title) merging with the FIRE results.
5C1D7D00	Query expansion techniques are used to find the desired set of query terms to improve retrieval performance. One of the limitations with the query expansion techniques is that a query is often expanded only by the linguistic features of terms. This paper presents a novel semantic query expansion technique that combines association rules with ontologies and information retrieval techniques. We propose to use the association rule discovery to find good candidate terms to improve the retrieval performance. These candidate terms are automatically derived from collections and added to the original query. Our method is differentiated from others in that 1) it utilizes the semantics as well as linguistic properties of unstructured text corpus and 2) it makes use of contextual properties of important terms discovered by association rules. Experiments conducted on a subset of TREC collections give quite encouraging results. We achieve from 15.49% to 20.98% improvement in term of P@20 with TREC5 ad hoc queries.
7AA55F5D	Novice users often do not have enough domain knowledge to create good queries for searching information on-line. To help alleviate the situation, exploration techniques have been used to increase the diversity of the search results so that not only those explicitly asked will be returned, but also those potentially relevant ones will be returned too. Most existing approaches, such as collaborative filtering, do not allow the level of exploration to be controlled. Consequently, the search results can be very different from what is expected. We propose an exploration strategy that performs intelligent query processing by first searching usable old queries, and then utilising them to adapt the current query, with the hope that the adapted query will be more relevant to the user's areas of interest. We applied the proposed strategy to the implementation of a personal information assistant (PIA) set up for user evaluation for 3 months. The experimental results showed that the proposed exploration method outperformed collaborative filtering, and mutation and crossover methods by around 25% in terms of the elimination of off-topic results. 
5FB22737	There is increasing interest in improving the robustness of IR systems, i.e. their effectiveness on difficult queries. A system is robust when it achieves both a high Mean Average Precision (MAP) value for the entire set of topics and a significant MAP value over its worst X topics (MAP(X)). It is a well known fact that Query Expansion (QE) increases global MAP but hurts the performance on the worst topics. A selective application of QE would thus be a natural answer to obtain a more robust retrieval system.We define two information theoretic functions which are shown to be correlated respectively with the average precision and with the increase of average precision under the application of QE. The second measure is used to selectively apply QE. This method achieves a performance similar to that with unexpanded method on the worst topics, and better performance than full QE on the whole set of topics.
016F5BB5	The use of temporal data extracted from text, to improve the effectiveness of Information Retrieval systems, has recently been the focus of important research work. Our research hypothesis is that the usage of the temporal relationship between words improves the Information Retrieval results. For this purpose, the texts are temporally segmented to establish a relationship between words and dates found in texts. This approach was applied in Query Expansion systems, using a collection with Portuguese newspaper texts. The results showed that the use of the temporality of words can enhance retrieval effectiveness. In particular for time-sensitive queries, we achieved 9.5% improvement in Precision@10. To our knowledge, this is the first work using temporal text segmentation to improve retrieval results.
772E4A41	This paper introduces a future and past search engine, ChronoSeeker, which can help users to develop long-term strategies for their organizations. To provide on-demand searches, we tackled two technical issues: (1) organizing efficient event searches and (2) filtering out noises from search results. Our system employed query expansion with typical expressions related to event information such as year expressions, temporal modifiers, and context terms for efficient event searches. We utilized a machine-learning technique of filtering noise to classify candidates into information or non-event information, using heuristic features and lexical patterns derived from a text-mining approach. Our experiment revealed that filtering achieved an 85% F-measure, and that query expansion could collect dozens more events than those without expansion. 
811EA444	Tourism information is dynamic and travel routes and decisions are dependent on highly varying factors such as perceived attractive sites, weather conditions, prices, transportation, accommodation, holidays, economic changes and so on. Travel guidebooks and present search engines completely lack this potential for dialog. Travel information search activities involved planning, decision making, anticipation of the trip with other people. The community (network of people) may act as a gateway to the information repository, when a tourist is not able to find the right information himself/herself or does not know about his/her information need. This leads to the collaboration for searching tourism information. Considering the following issues we are motivated to study this topic.
7B145537	This paper examines the meaning of context in relation to ontology based query expansion and contains a review of query expansion approaches. The various query expansion approaches include relevance feedback, corpus dependent knowledge models and corpus independent knowledge models. Case studies detailing query expansion using domain-specific and domain-independent ontologies are also included. The penultimate section attempts to synthesise the information obtained from the review and provide success factors in using an ontology for query expansion. Finally the area of further research in applying context from an ontology to query expansion within a newswire domain is described.
05137BD4	This paper evaluates the retrieval effectiveness of query expansion strategies on a MEDLINE test collection using Cornell University's SMART retrieval system. Three expansion strategies are tested on their ability to identify appropriate McSH terms for user queries: expansion using an inter-field statistical thesaurus, expansion via retrieval feedback and expansion using a combined approach. These expansion strategies do not require prior relevance decisions. The study compares retrieval effectiveness using the original unexpanded and the alternative expanded user queries on a collection of 75 queries and 2334 MEDLINE citations. Retrieval effectiveness is assessed using eleven point average precision scores (11-AvgP). The combination of expansion using the thesaurus followed by retrieval feedback gives the best improvement of 17% over a baseline performance of 0.5169 11-AvgP. However this improvement is almost identical to that achieved by expansion via retrieval feedback (16.4%). Query expansion using the inter-field thesaurus gives a significant but lower performance improvement (9.9%) over the same baseline. This study recommends query expansion using retrieval feedback for adding McSH search terms to a user's initial query.
7C2981BC	This paper proposes a novel query expansion method to improve accuracy of text retrieval systems. Our method makes use of a minimal relevance feedback to expand the initial query with a structured representation composed of weighted pairs of words. Such a structure is obtained from the relevance feedback through a method for pairs of words selection based on the Probabilistic Topic Model. We compared our method with other baseline query expansion schemes and methods. Evaluations performed on TREC-8 demonstrated the effectiveness of the proposed method with respect to the baseline.
77808CC5	An evaluation of graph theoretical clusters of index terms which can be extracted from an automatically indexed document collection, and the effects of employing such clusters in automatic document retrieval is described. The graph theoretical clusters which were developed from six data bases under two different cluster definitions were analyzed for average size and related data. The clusters were also used to expand the queries in each of six data bases to determine the effect of the expansions on the document retrieval results.Although a large variety of clusters and associated query expansions were obtained, no significant improvements in the document retrieval performance were achieved. In some cases, however, significant degradations in the retrieval performance occurred. Although seemingly meaningful clusters can be obtained, the results indicate that the effort involved in finding clusters and adding the clustered terms to queries is far too great to warrant their use in an operational system. The data bases employed were relatively small, and the authors caution against generalizing these results to larger data bases or other situations.
5CEC0571	This paper describes a new method to automatically obtain a new thesaurus which exploits previously collected information. Our method relies on different resources, such as a text collection, a set of source thesauri and other linguistic resources. We have applied different techniques in the different phases of the process. By applying indexing techniques, the text collection provides the set of initial terms of interest for the new thesaurus. Then, these terms are searched in the source thesauri, providing the initial structure of the new thesaurus. Finally, the new thesaurus is enriched by searching for new relationships among its terms. These relationships are first detected using similarity measures and then are characterized with a type (equivalence, hierarchy or associativity) by using different linguistic resources. We have based the system evaluation on the results obtained with and without the thesaurus in an information retrieval task proposed by the Cross-Language Evaluation Forum (CLEF). The results of these experiments have revealed a clear improvement of the performance.
7C318F32	The evaluation of 6 ranking algorithms for the ranking of terms for query expansion is discussed within the context of an investigation of interactive query expansion and relevance feedback in a real operational environment. The yardstick for the evaluation was provided by the user relevance judgements on the lists of the candidate terms for query expansion. The evaluation focuses on the similarities in the performance of the different algorithms and how the algorithms with similar performance treat terms.
767E699B	Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We address the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness.
795DA831	Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed-back model by resampling a given query's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.
7732CCC0	Most casual users of IR systems type short queries. Recent research has shown that adding new words to these queries via odhoc feedback improves the re- trieval effectiveness of such queries. We investigate ways to improve this query expansion process by refining the set of documents used in feedback. We start by using manually formulated Boolean filters along with proxim- ity constraints. Our approach is similar to the one pro- posed by Hearst(l2). Next, we investigate a completely automatic method that makes use of term cooccurrence information to estimate word correlation. Experimental results show that refining the set of documents used in query expansion often prevents the query drift caused by blind expansion and yields substantial improvements in retrieval effectiveness, both in terms of average preci- sion and precision in the top twenty documents. More importantly, the fully automatic approach developed in this study performs competitively with the best manual approach and requires little computational overhead.
7D79605B	Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.
75FC25F4	Automatic query expansion has long been suggested as a technique for dealing with the fundamental issue of word mismatch in information retrieval. A number of approaches to ezpanrnion have been studied and, more recently, attention has focused on techniques that analyze the corpus to discover word relationship (global techniques) and those that analyze documents retrieved by the initial quer~ ( local feedback). In this paper, we compare the effectiveness of these approaches and show that, although global analysis haa some advantages, local analysia is generally more effective. We also show that using global analysis techniques, such as word contezt and phrase structure, on the local aet of documents produces re- sults that are both more effective and more predictable than simple local feedback
78D3F0A3	Research into both the algorithmic and human approaches to information retrieval is required to improve information retrieval system design and database searching effectiveness. This study uses the human approach to examine the sources and effectiveness of search terms selected during mediated interactive information retrieval. The study focuses on determining the retrieval effectiveness of search terms identified by users and intermediaries from retrieved items during term relevance feedback. Results show that terms selected from particular database fields of retrieved items during term relevance feedback (TRF) were more effective than search terms from the intermediary, database thesauri or users' domain knowledge during the interaction, but not as effective as terms from the users' written question statements. Implications for the design and testing of automatic relevance feedback techniques that place greater emphasis on these sources and the practice of database searching are also discussed.
7AB99E2A	Many queries are submitted to search engines by right-clicking the marked text (i.e., the query) in Web browsers. Because the document being read by the searcher often provides sufficient contextual information for the query, search engine could provide much more relevant search results if the query is augmented by the contextual information captured from the source document. How to extract the right contextual information from the source document is the main focus of this study. To this end, we evaluate 7 text component extraction schemes, and 5 feature extraction schemes. The former determines from which text component (e.g., title, meta-data, or paragraphs containing the selected query) to extract contextual information; the latter determines which words or phrases to extract. In total 35 combinations are evaluated and our evaluation results show that noun phrases extracted from all paragraphs that contain the query word is the best option.
7B2AB066	In an era of online retrieval, it is appropriate to offer guidance to users wishing to improve their initial queries. One form of such guidance could be short lists of suggested terms gathered from feedback, nearest neighbors, and term variants of original query terms. To verify this approach, a series of experiments were run using the Cranfield test collection to discover techniques to select terms for these lists that would be effective for further retrieval. The results show that significant improvement can be expected from this approach to query expansion.
7626898C	The effectiveness of queries in information retrieval can be improved through query expansion. This technique automatically introduces additional query terms that are statistically likely to match documents on the intended topic. However, query expansion techniques rely on fixed parameters. Our investigation of the effect of varying these parameters shows that the strategy of using fixed values is questionable.
75762437	Users normally facing difficulties to expresses their information needs into a query format that the search system can use to process. Therefore users frequently modify their query with intention to retrieve better results. The goal of this experiment is to compare the precision of the retrieved results by employing domain ontology to reformulate the user query. We proposed to reformulate the query (initially in natural language) by replacing it with the terms retrieved from ontology. Then we improve the approach by combining the terms from ontology with keyword/s extracted from the initial user's natural language query. Precision measure had been conducted for six queries and the results showed that the idea of combining ontology terms with keyword from query gave a promising result. It achieved higher precision in comparison when using ontology terms alone. This experimental result indicates that query reformulation is more efficient when ontology term/s combined with key element from the user's query.
7AAD636F	Query expansion (QE) and document expansion (DE) have been proved effective for improving the retrieval performance in language modeling approach. However, the issue that which expansion technique is more effective in information retrieval (IR), has not been well studied and discussed. To address this issue, this paper performs an empirical study on QE and DE to examine their effects. Moreover, since QE and DE exploit different corpus structures, we also examine the potential effectiveness of incorporating QE and DE. Experimental results on several TREC test collections show that both QE and DE significantly outperform the classical language model, but the effectiveness of QE and DE is varied in different settings of retrieval. In addition, incorporating QE with DE does not always bring about the best performance.
7FF6DDDA	The ineffectiveness of information retrieval systems often caused by the inaccurate use of keywords in a query. In order to solve the ineffectiveness problem in information retrieval systems, many solutions have been proposed over the years. The most common techniques are revolving around query modification techniques such as query expansion, query refinement, etc. Due to the high similarity in these query modification techniques, people are often confused about their differences. However, few existing survey papers compare their differences. Hence, in this paper, we first briefly discuss the basic technique of query expansion, query suggestion and query refinement, and then make a detailed comparison between these three techniques. We finally show the promising future research trend in the field of query modification.
7EDCAF6E	We present empirical analysis of human searches for information from mobile devices, focusing on temporal dynamics, semantics, and topics of queries. Our analysis is based on a large scale data of mobile search logs over a week period from a major US mobile service provider. We find that human searches appear in bursts over time with the distribution of the query inter arrival time following a power law decay up to a day and decays exponentially beyond. Interestingly, this finding conforms to some other measures of human activity reported in previous studies. We also provide preliminary characterisation results of the semantics and topics of queries, some of which conform to that of previous studies. The results would be of general interest for understanding the dynamics of human activity and, in particular, may be leveraged for the design of mobile services.
8026925D	Utilizing external collections to improve retrieval performance is challenging research because various test collections are created for different purposes. Improving medical information retrieval has also gained much attention as various types of medical documents have become available to researchers ever since they started storing them in machine processable formats. In this paper, we propose an effective method of utilizing external collections based on the pseudo relevance feedback approach. Our method incorporates the structure of external collections in estimating individual components in the final feedback model. Extensive experiments on three medical collections (TREC CDS, CLEF eHealth, and OHSUMED) were performed, and the results were compared with a representative expansion approach utilizing the external collections to show the superiority of our method.
5C6FC0E8	This article presents a new, hybrid approach that projects an initial query result onto global information, yielding a local conceptual overview. Concepts found this way are candidates for query refinement.We show that the resulting conceptual structure after a typical short query of 2 terms, contains refinements that perform just as well as a most accurate query formulation.Subsequently we illustrate that query by navigation is an effective mechanism which in most cases finds the optimal concept in a small number of steps. When an optimal concept is not found, the navigation process still finds an acceptable sub-optimum.
8065B044	This paper reports an evaluation of three methods for the expansion of natural language queries in ranked-out put retrieval systems. The methods are based on term co-oc currence data, on Soundex codes, and on a string similarity measure. Searches for 110 queries in a database of 26,280 titles and abstracts suggest that there is no significant differ ence in retrieval effectiveness between any of these methods and unexpanded searches.
7D7D8752	Term cooccurrence data has been extensively used in document retrieval systems for the identification of indexing terms that are similar to those that have been specified in a user query: these similar terms can then be used to augment the original query statement. Despite the plausibility of this approach to query expansion, the retrieval effectiveness of the expanded queries is often no greater than, or even less than, the effectiveness of the unexpanded queries. This article demonstrates that the similar terms identified by cooccurrence data in a query expansion system tend to occur very frequently in the database that is being searched. Unfortunately, frequent terms tend to discriminate poorly between relevant and nonrelevant documents, and the general effect of query expansion is thus to add terms that do little or nothing to improve the discriminatory power of the original query. © 1991 John Wiley & Sons, Inc.
7EE1FB33	In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR measures beyond expansion from current word-embeddings models and well-established traditional QE methods.
78BB5638	Searching for knowledge in collaborative networked organisations (CNOs) is an important issue as partners must share and use the available knowledge and information spread over their members in the network. Besides that, partners of such networks work in several contexts (roles, activities, processes) and they have naturally different interests. Based on these observations, the aim of this work is to characterise the user context in a CNO and use such a context for customising knowledge search in CNOs. The basic assumption is that the relevance of the search results in the CNO domain is not only defined by the terms of the query but also by the context of the user performing the search. This paper presents an ontology-based model for CNO context as well as a set of rules for query customisation and these elements are framed in an existing framework for knowledge search. Finally, a prototype implementation of the model and rules is presented and some experiments are discussed.
7F0B1B0F	With the open source code movement, code search with the intent of reuse has become increasingly popular. So much so that researchers have been calling it the new facet of software reuse. Although code search differs from general-purpose document search in essential ways, most tools still rely mainly on keywords matched against source code text. Recently, researchers have proposed more sophisticated ways to perform code search, such as including interface definitions in the queries (e.g., return and parameter types of the desired function, along with keywords; called here Interface-Driven Code Search — IDCS). However, to the best of our knowledge, there are few empirical studies that compare traditional keyword-based code search (KBCS) with more advanced approaches such as IDCS. In this paper we describe an experiment that compares the effectiveness of KBCS with IDCS in the task of large-scale code search of auxiliary functions implemented in Java. We also measure the impact of query expansion based on types and WordNet on both approaches. Our experiment involved 36 subjects that produced real-world queries for 16 different auxiliary functions and a repository with more than 2,000,000 Java methods. Results show that the use of types can improve recall and the number of relevant functions returned (#RFR) when combined with query expansion (∼30% improvement in recall, and ∼43% improvement in #RFR). However, a more detailed analysis suggests that in some situations it is best to use keywords only, in particular when these are sufficient to semantically define the desired function.
7C342AEC	In this paper we present the design and evaluation of our biomedical literature searching approaches using the TREC 2004 ad hoc retrieval task in the genomics track. The main approach taken in our system is to expand queries by exploiting the three widely used strategies -local analysis, global analysis, and ontology-based term re-weighting across various search engines. The experimental results show that (1) ontology-based term re-weighting provides the best results among the three query expansion strategies, (2) expanding the initial query with more precise ontology-based term enhances LSI based local analysis substantially, and (3) including context to term re-weighting and LSI further improves the precision. Experimental results also show that the ontology-based term re-weighting with LUCENE or LEMUR search engines increases the average precision by up to 20.3% or 12.1%, respectively, compared to that of the baseline runs. In addition, the LSI-based local analysis increases the average precision by 9.2% with LEMUR search engine. We believe the approaches of the term re-weighting and LSI-based local analysis may be exploited in other bio-medical domains.
7F7D848F	This paper presents the results of an experimental investigation into the effects that some forms of query expansion by term addition or term deletion, have on the retrieval effectiveness of a document retrieval system. The overall search strategy used by a user is an iterative process whereby a set of user-judged relevant documents at any point in the search is used to refine and improve on the remainder of the user's search. At some point during the search, the set of relevant documents found so far can be used to modify the original query, either by the addition or deletion of search terms. This process is called ‘query modification’ or ‘query expansion’. A number of different types of query modification strategies are tried and the results obtained are presented and analysed.
14AE49C6	We describe the participation of the University of Amsterdam’s ILPS group in the blog track at TREC 2008. We mainly explored different ways of using external corpora to expand the original query. In the blog post retrieval task we did not succeed in improving over a simple baseline (equal weights for both the expanded and original query). Obtaining optimal weights for the original and the expanded query remains a subject of investigation. In the blog distillation task we tried to improve over our (strong) baseline using external expansion, but due to differences in the run setup, comparing these runs is hard. Compared to a simpler baseline, we see an improvement for the run using external expansion on the combination of news, Wikipedia and blog posts. 1
02D8DAF0	The focus of our study is zero-hit queries in keyword subject searches and the effort of increasing recall in these cases by reformulating and, then, expanding the initial queries using an external source of knowledge, namely a thesaurus. To this end, the objectives of this study are twofold. First, we perform the mapping of query terms to the thesaurus terms. Second, we use the matched terms to expand the user’s initial query by taking advantage of the thesaurus relations and implementing natural language processing (NLP) techniques. We report on the overall procedure and elaborate on key points and considerations of each step of the process.
7D4465AC	Chinese name translation is a special case of the problem of named entity translation. It is a very challenging problem because there exist many kinds of Romanization systems and some people like to add additional words into their english names. Translating a scholar's name to its corresponding English name could help find information about his academic achievements. In this paper, we provide a classification for Chinese names, and propose a novel approach to mining Chinese name translations from Web corpora. Our approach is based on three kinds of features, namely the phonetic similarity, the smallest distance, and the number of appearances in the neighborhood, to extract name translation candidates by using a query expansion technique and support vector machine (SVM). Experimental results show that our approach can correctly translate the majority of Chinese names.
7E1FECD9	Wikipedia is an online encyclopedia, available in more than 100 languages and comprising over 1 million articles in its English version. If we consider each Wikipedia article as a node and each hyperlink between articles as an arc we have a "Wikigraph", a graph that represents the link structure of Wikipedia. The Wikigraph differs from other Web graphs studied in the literature by the fact that there are explicit timestamps associated with each node's events. This allows us to do a detailed analysis of the Wikipedia evolution over time. In the first part of this study we characterize this evolution in terms of users, editions and articles; in the second part, we depict the temporal evolution of several topological properties of the Wikigraph. The insights obtained from the Wikigraphs can be applied to large Web graphs from which the temporal data is usually not available
7FE594DD	Over the past two decades, national reforestation efforts and anthropogenic pressures have altered the landscape of northeastern China. In an effort to develop an operational forest monitoring system for the expansive northeastern China region, the accuracy of satellite-derived forest metric information has been evaluated. An area within Heilongjiang province served as the study site. This region, which spans portions of both the Tahe and Mohe Bureaus of Forestry, lost over 1 million hectares of forest during a large forest fire in May/June of 1987. One year after the conflagration, a ten-year reforestation effort commenced. Using Landsat data spanning 1986 to 2002, the photosynthetic recovery of the forest vegetation has been documented. Subsequently, Landsat 7 ETM+ data have been used to establish forest metric correlations. To quantify forest metrics, correlations between Landsat spectral information and in situ measurements (species composition, age, average diameter at breast height (dbh), crown closure, height, and timber volume), as surveyed by the local Forest Management Bureau in 1999/2000, were developed. Preliminary results indicate that age, dbh, and average stand height are well correlated to spectral data and can be used to develop predictive models, while timber volume shows little to no relationship. Hereafter, these methodologies and correlations will be extended both spatially and temporally to provide a regional assessment of carbon sequestration.
80167C1F	Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers.
7F14266A	We present an algorithm for jointly learning a consistent bidirectional generative-recognition model that combines top-down and bottom-up processing for monocular 3d human motion reconstruction. Learning progresses in alternative stages of self-training that optimize the probability of the image evidence: the recognition model is tunned using samples from the generative model and the generative model is optimized to produce inferences close to the ones predicted by the current recognition model. At equilibrium, the two models are consistent. During on-line inference, we scan the image at multiple locations and predict 3d human poses using the recognition model. But this implicitly includes one-shot generative consistency feedback. The framework provides a uniform treatment of human detection, 3d initialization and 3d recovery from transient failure. Our experimental results show that this procedure is promising for the automatic reconstruction of human motion in more natural scene settings with background clutter and occlusion.
78A62349	The classification techniques of discharging pulses in EDM have been proved critical in improving productivity, precision and lowering the cost of products, etc. In this paper, an easily implemented method is developed to describe the variations of EDM process, represented by gap states. On the basis of a time series of gap states from a machining process, the paper first studied a general descriptive model for EDM process, and then equivalently simplified the model for application; after spectral analysis, preprocessing of data, parameters selection and model validation proposed a well-defined model. Finally, by using this model structure and size, an online time-varied predictive model was developed. Experimental verifications showed that this predictive model can quickly and accurately provide one step ahead predictions with mean error less than 2%. This model makes clear that variations of EDM process represented by gap states can be predicted online with a high precision.
7E791BA1	This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than "traditional" code metrics, which can only be collected at a later phase of the software development processes.
7AF8CAAA	Stream flow prediction is crucial for water resource planning and management at the watershed scale. This study investigates various modelling approaches, namely, autoregressive integrated moving average, deseasonalized autoregressive moving average, artificial neural network (ANN), simulator for water resources in rural basins, and identification of hydrographs and components from rainfall, evaporation, and stream (IHACRES) models, to simulate and predict stream flow in Kasilian watershed in northern Iran. This research represents a case study on forest watershed modelling with the lack of enough hydro-meteorological data. The comparison of the prediction performance of the models was done based on some error estimation criteria. The results indicate that the ANN and IHACRES models perform better than the two other modelling approaches. The advantages and disadvantages of different hydrological models are discussed.
7AA73EFC	The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module’s fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA’s Metrics Data Program data repository.
775312C0	A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.
7BE3E94B	Different software reliability models can produce very different answers when called on to predict future reliability in a reliability growth context. Users need to know which, if any, of the competing predictions are trustworthy. Some techniques are presented which form the basis of a partial solution to this problem. Rather than attempting to decide which model is generally best, the approach adopted allows a user to decide on the most appropriate model for each application.
79F1D35C	It is shown that neural network reliability growth models have a significant advantage over analytic models in that they require only failure history as input and not assumptions about either the development environment or external parameters. Using the failure history, the neural-network model automatically develops its own internal model of the failure process and predicts future failures. Because it adjusts model complexity to match the complexity of the failure history, it can be more accurate than some commonly used analytic models. Results with actual testing and debugging data which suggest that neural-network models are better at endpoint predictions than analytic models are presented.
7D71156A	Model-based control for two rapid thermal processing systems has been performed. The condition number of the process influences the control strategies selected and the quality of control that can be achieved. For the Texas Instruments RTP system, a successively linearized quadratic dynamic matrix control (QDMC) strategy using a reduced set of outputs has been developed. Experimental results of the control strategy are presented and compared with internal model control with gain scheduling. The nonlinear controller discussed shows superior performance for the test case studied. The second RTP system designed by SEMATECH exhibits improved controllability. This system has the potential for tighter control of wafer temperature using gain scheduling.
766E4289	In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.
7DF540E4	The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a medical imaging system software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models
7F455829	Contemporary evidence suggests that most field faults in software applications are found in a small percentage of the software's components. This means that if these faulty software components can be detected early in the development project's life cycle, mitigating actions can be taken, such as a redesign. For object-oriented applications, prediction models using design metrics can be used to identify faulty classes early on. In this paper we report on a study that used object-oriented design metrics to construct such prediction models. The study used data collected from one version of a commercial Java application for constructing a prediction model. The model was then validated on a subsequent release of the same application. Our results indicate that the prediction model has a high accuracy. Furthermore, we found that an export coupling (EC) metric had the strongest association with fault-proneness, indicating a structural feature that may be symptomatic of a class with a high probability of latent faults.
80017093	One of the challenges that technology computer-aided design must meet currently is the analysis of the performance of groups of components, interconnects, and, generally speaking, large parts of the IC. This enables predictions that the simulation of single components cannot achieve. In this paper, we focus on the simulation of backend processes, interconnect capacitances, and time delays. The simulation flows start from the blank wafer surface and result in device information for the circuit designer usable from within SPICE. In order to join topography and backend simulations, deposition, etching, and chemical mechanical planarization processes in the various metal lines are used to build up the backend stack, starting from the flat wafer surface. Depending on metal combination, line-to-line space, and line width, thousands of simulations are required whose results are stored in a database. Finally, we present simulation results for the backend of a 100-nm process, where the influence of void formation between metal lines profoundly impacts the performance of the whole interconnect stack, consisting of aluminum metal lines, and titanium nitride local interconnects. Scanning electron microscope images of test structures are compared to topography simulations, and very good agreement is found. Moreover, charge-based capacitance measurements were carried out to validate the capacitance extraction, and it was found that the error is smaller than four percent. These simulations assist the consistent fabrication of voids, which is economically advantageous compared to low-/spl kappa/ materials, which suffer from integration problems.
7E183060	Recent studies have shown that trafic in telecommunication networks exhibits long-range dependence (LRD). Accurate modelling and analysis of teletrafic incorporating LRD is therefore required in network engineering. Prediction of trafic levels can play an important role in teletrafic analysis for dynamic resource allocation and traffic management. This paper presents the formulation of a model based recursive , linear minimum mean-square error predic- tor for LRD processes. A Kalman predictor is pro- posed for LAD processes modelled by fractional autoregressive integrated moving average CfARIMA) models. The family of fARIMA models can account for long range, as well as short range and qwi-peri- odic dependencies typical of teletraffic data.
75D8B052	Software has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome.In this research, we identify aspects which could influence project success, build prediction models based on the aspects using data collected from multiple companies, and then test their performance on data from a single organization.A survey-based empirical investigation was used to examine variables and factors that contribute to project outcome. Variables that were highly correlated to project success were selected and the set of variables was reduced to three factors by using principal components analysis. A logistic regression model was built for both the set of variables and the set of factors, using heterogeneous data collected from two different countries and a variety of organizations. We tested these models by using a homogeneous hold-out dataset from one organization. We used the receiver operating characteristic (ROC) analysis to compare the performance of the variable and factor-based models when applied to the homogeneous dataset.We found that using raw variables or factors in the logistic regression models did not make any significant difference in predictive capability. The prediction accuracy of these models is more balanced when the cut-off is set to the ratio of success to failures in the datasets used to build the models. We found that the raw variable and factor-based models predict significantly better than random chance.We conclude that an organization wishing to estimate whether a project will succeed or fail may use a model created from heterogeneous data derived from multiple organizations.
75B4B94E	Quantitative structure–property relationship (QSPR) models are widely used for prediction of properties, activities and/or toxicities of new chemicals. Validation strategies check the reliability of predictions of QSPR models. The classical metrics like Q2 and R2pred (Q2ext) are commonly used, besides other techniques, for internal validation (mostly leave-one-out) and external validation (test set validation) respectively. Recently, we have proposed a set of novel rm2 metrics which has been extensively used by us and other research groups for validation of QSPR models. In the present attempt, some additional variants of rm2 metrics have been proposed and their applications in judging the quality of predictions of QSPR models have been shown by analyzing results of the QSPR models obtained from three different data sets (n = 119, 90, and 384). In each case, 50 combinations of training and test sets have been generated, and models have been developed based on the training set compounds and subsequently applied for prediction of responses of the test set compounds. Finally, models for a particular data set have been ranked according to the quality of predictions. The role of different validation metrics (including classical metrics and different variants of rm2 metrics) in differentiating the “good” (predictive) models from the “bad” (low predictive) models has been studied. Finally, a set of guidelines has been proposed for checking the predictive quality of QSPR models.
7F2DDA1D	Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making process are evaluated.
7D9B3629	A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.
76232CEB	There is no universally applicable software reliability growth model which can be trusted to give accurate predictions of reliability in all circumstances. A technique of analyzing predictive accuracy called the u-plot allows a user to estimate the relationship between the predicted reliability and the true reliability. It is shown how this can be used to improve reliability predictions in a very general way by a process of recalibration. Simulation results show that the technique gives improved reliability predictions in a large proportion of cases. However, a user does not need to trust the efficacy of recalibration, since the new reliability estimates produced by the technique are truly predictive and their accuracy in a particular application can be judged using the earlier methods. The generality of this approach suggests its use whenever a software reliability model is used. Indeed, although this work arose from the need to address the poor performance of software reliability models, it is likely to have applicability in other areas such as reliability growth modeling for hardware.
7D45D473	Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the "quality" of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the "Goldilock's Conjecture," that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian Belief Networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of "software decomposition" in order to test hypotheses about defect introduction and help construct a better science of software engineering.
80212C28	The authors present an automated modeling technique which can be used as an alternative to regression techniques to improve the quality of the software development process. The modeling process will allow for the reliable detection of potential problem areas and for the interpretation of the cause of the problem so that the most appropriate remedial action can be taken. It is shown that it can be used to facilitate the identification and aid the interpretation of the significant trends which characterize high risk components in several Ada systems. The effectiveness of the technique is evaluated based on a comparison with logistic regression based models.
7F34FB79	In this paper we characterize and model the cost of rework in a Component Factory (CF) organization. A CF is responsible for developing and packaging reusable software components. Data was collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC. We then constructed a predictive model of the cost of rework using the C4.5 system for generating a logical classification model. The predictor variables for the model are measures of internal software product attributes. The model demonstrates good prediction accuracy, and can be used by managers to allocate resources for corrective maintenance activities. Furthermore, we used the model to generate proscriptive coding guidelines to improve programming practices so that the cost of rework can be reduced in the future. The general approach we have used is applicable to other environments.
803FCA57	This paper presents a viscous and coulomb friction measurement procedure for mechanical systems equipped with high-precision velocity sensors. The proposed method exploits the structure of the velocity response predicted by several standard friction models when a force or torque is applied in a ramp fashion. Experimental results show the high accuracy of the proposed measurement procedure.
7BAB0624	Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.
7E8EABA8	Empirical studies on software prediction models do not converge with respect to the question "which prediction model is best?" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.
761A7BE9	This paper describes an empirical comparison of several modeling techniques for predicting the quality of software components early in the software life cycle. Using software product measures, we built models that classify components as high-risk, i.e., likely to contain faults, or low-risk, i.e., likely to be free of faults. The modeling techniques evaluated in this study include principal component analysis, discriminant analysis, logistic regression, logical classification models, layered neural networks, and holographic networks. These techniques provide a good coverage of the main problem-solving paradigms: statistical analysis, machine learning, and neural networks. Using the results of independent testing, we determined the absolute worth of the predictive models and compare their performance in terms of misclassification errors, achieved quality, and verification cost. Data came from 27 software systems, developed and tested during three years of project-intensive academic courses. A surprising result is that no model was able to effectively discriminate between components with faults and components without faults.
5B09215F	The information about which modules of a future version of a software system are defect-prone is a valuable planning aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. However, constructing effective defect prediction models in an industrial setting involves a number of key questions. In this paper we discuss ten key questions identified in context of establishing defect prediction in a large software development project. Seven consecutive versions of the software system have been used to construct and validate defect prediction models for system test planning. Furthermore, the paper presents initial empirical results from the studied project and, by this means, contributes answers to the identified questions.
75E31632	In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.
7BB98473	The identification of high-risk components early in the life cycle is addressed. A solution that casts this as a classification problem is examined. The proposed approach derives models of problematic components, based on their measurable attributes and those of their development processes. The models provide a basis for forecasting which components are likely to share the same high-risk properties, such as being error-prone or having a high development cost. Developers can use these classification techniques to localize the troublesome 20% of the system. The method for generating the models, called automatic generation of metric-based classification trees, uses metrics from previous releases or projects to identify components that are historically high-risk.
7FC6F995	The information about which modules in a software system's future version are potentially defective is a valuable aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. Constructing effective defect prediction models in an industrial setting involves the decision from what data source the defect predictors should be derived. In this paper we compare defect prediction results based on three different data sources of a large industrial software system to answer the question what repositories to mine. In addition, we investigate whether a combination of different data sources improves the prediction results. The findings indicate that predictors derived from static code and design analysis provide slightly yet still significant better results than predictors derived from version control, while a combination of all data sources showed no further improvement.
774787F7	We propose the use of regression analysis to generate accurate predictive models for physical metrics using design metrics as input. We validate our approach with 40+ implementations of three systems in two development scenarios: system evolution and first design. Results show maximum prediction errors of 1.66% during system evolution. In a first design scenario, the average error is 15% with the maximum error still below 20% for all physical metrics. This approach provides a fast and accurate strategy to boost embedded software productivity and quality, by estimating Non-Functional Requirements (NFRs) during the first design stages.
7C5F7EE4	It is hard for security practitioners and decision-makers to know what level of protection they are getting from their investments in security, especially when they have invested in a number of technologies and processes which interact and combine together. It is even harder to estimate how well these investments can be expected to protect their organizations in the future as security policies, regulations and the threat environment are constantly changing. In this paper we propose that for measuring the effectiveness of security processes in large organizations, a greater emphasis needs to be put on process-based metrics, in contrast to the more commonly used symptomatic lagging indicators. We show, by means of two case studies, how these process-based metrics can be combined with executable, predictive models, based on a sound mathematical foundation, to both assess organizations' security processes under current conditions and predict how well they are likely to perform in potential future scenarios, which may include changes in working practices, policies or threat levels, or new investments in security. We present two case studies, in the areas of vulnerability threat management, and identity and access management, as significant examples to illustrate how this modeling and simulation-based approach can be used to provide a rich picture of how well existing security processes are protecting the organization and to answer "what-if" questions, such as exploring the effects of a change in security policy or an investment in new security technology. Our approach enables the organization to apply the metrics that are most relevant to its business, and provide a comprehensive view that shows the benefits and losses to the different stakeholders.
7C4BF42C	The software development process is usually affected by many risk factors that may cause the loss of control and failure, thus which need to be identified and mitigated by project managers. Software development companies are currently improving their process by adopting internationally accepted practices, with the aim of avoiding risks and demonstrating the quality of their work.This paper aims to develop a method to identify which risk factors are more influential in determining project outcome. This method must also propose a cost effective investment of project resources to improve the probability of project success.To achieve these aims, we use the probability of success relative to cost to calculate the efficiency of the probable project outcome. The definition of efficiency used in this paper was proposed by researchers in the field of education. We then use this efficiency as the fitness function in an optimization technique based on genetic algorithms. This method maximizes the success probability output of a prediction model relative to cost.The optimization method was tested with several software risk prediction models that have been developed based on the literature and using data from a survey which collected information from in-house and outsourced software development projects in the Chilean software industry. These models predict the probability of success of a project based on the activities undertaken by the project manager and development team. The results show that the proposed method is very useful to identify those activities needing greater allocation of resources, and which of these will have a higher impact on the projects success probability.Therefore using the measure of efficiency has allowed a modular approach to identify those activities in software development on which to focus the project's limited resources to improve its probability of success. The genetic algorithm and the measure of efficiency presented in this paper permit model independence, in both prediction of success and cost evaluation.
7FCFF9BD	Models for projecting software defects from analyses of Ada designs are described. The research is motivated by the need for technology to analyze designs for their likely effect on software quality. The models predict defect density based on product and process characteristics. Product characteristics are extracted from a static analysis of Ada subsystems, focusing on context coupling, visibility, and the import-export of declarations. Process characteristics provide for effects of reuse level and extent of changes. Multivariate regression analyses were conducted with empirical data from industry/government-developed projects: 16 Ada subsystems totaling 149000 source lines of code. The resulting models explain 63-74% of the variation in defect density of the subsystems. Context coupling emerged as a consistently significant variable in the models.
7FD35685	The usefulness of connectionist models for software reliability growth prediction is illustrated. The applicability of the connectionist approach is explored using various network models, training regimes, and data representation methods. An empirical comparison is made between this approach and five well-known software reliability growth models using actual data sets from several different software projects. The results presented suggest that connectionist models may adapt well across different data sets and exhibit a better predictive accuracy. The analysis shows that the connectionist approach is capable of developing models of varying complexity.
7A65CAD0	It would be valuable to use metrics to identify the fault-proneness of software modules. It is important to select the most appropriate particular metric subset for fault-proneness prediction. We proposed an approach of metrics selection, which firstly utilized the correlation analysis to eliminate the high the correlation metrics and then ranked the remaining metrics based on the gray relational analysis. Three classifiers, that were logistic regression model, NaiveBayes, and J48, were utilized to empirically investigate the usefulness of selected metrics. Our results, based on a public domain NASA data set, indicate that 1) proposed method for metrics selection is effective, and 2) using 3–4 metrics gets the balanced performance for fault-proneness prediction of software modules.
7941323C	We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group. 
7957374C	We present the results of an empirical study in which we have investigated machine learning (ML) algorithms with regard to their capabilities to accurately assess the correctability of faulty software components. Three different families of algorithms have been analyzed. We have used (1) fault data collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC and (2) product measures extracted directly from the faulty components of this library.
7FEB3BFA	This paper presents the application of our recently developed theory on model validation for control and controller validation in a prediction error framework to a realistic industrial case study. The industrial application concerns the control of the silicon concentration in a ferrosilicon production process. Our case study produces findings about the design of the validation experiment (validation in open or closed loop). It also illustrates the respective merits of the tools developed, respectively, for control oriented model validation and for the validation of a particular controller.
7F82664B	The goal of software process model reuse should be to provide a comprehensive process model that guides and coordinates all roles within a project. This means that roles do not use their own process model, but all roles use a single, integrated one that represents all needed information. Views are derived to present information necessary for performing a role's tasks. Because every project is unique, reuse of software process models requires tailoring. Reusing generic, large software process models seems to be impractical, because of many interdependencies between generic parts of the model. Process engineering will only be successful when techniques are developed to formulate units of software process modules (staying relatively stable from project to project), to tailor them according to new contexts, and to integrate them into a comprehensive software process model. The hypothesis of this paper is that views are the process modules to be reused. Views are less complex and easier to maintain than a comprehensive process model.
7C1E8237	The shelf-life of ready-to-eat vegetable salads established by manufacturer is usually 7–14 days depending on the type of vegetable, and is determined by loss in organoleptic qualities. A more objective method to predict shelf-life and spoilage would be desirable. The present study monitored the evolution of spoilage organisms in a mixed salad of red cabbage, lettuce and carrot stored at 4°C, 10°C and 15°C. Changes in carbon dioxide and oxygen concentrations and pH were also monitored. Predictive modelling was used to establish a theoretical shelf-life time as a function of temperature. Lactic acid bacteria at levels of 106 cfu/g appeared to be related to both spoilage and theoretically-predicted shelf-life values.
80BB20A1	The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more "messy" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the "best" prediction system.
7DEC9E18	 Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.
7E2B5A1F	Web page metrics is one of the key elements in measuring various attributes of web site. Metrics gives the concrete values to the attributes of web sites which may be used to compare different web pages. The web pages can be compared based on the page size, information quality ,screen coverage, content coverage etc. Internet and website are emerging media and service avenue requiring improvements in their quality for better customer services for wider user base and for the betterment of human kind. E-business is emerging and websites are not just medium for communication, but they are also the products for providing services. Measurement is the key issue for survival of any organization Therefore to measure and evaluate the websites for quality and for better understanding, the key issues related to website engineering is very important. In this paper we collect data from webby awards data (2007-2010) and classify the websites into good sites and bad sites on the basics of the assessed metrics. To achieve this aim we investigate 15 metrics proposed by various researchers. We present the findings of quantitative analysis of web page attributes and how these attributes are calculated. The result of this paper can be used in quantitative studies in web site designing. The metrics captured in the predicted model can be used to predict the goodness of website design.
7EC23763	A new empirical algorithm-CWAVE [1] was developed at the German aerospace center (DLR). A global dataset of two years (September 1998 to December 2000) of ERS SAR data was reprocessed to more than one million SAR imagettes. Met ocean parameters like significant ocean wave height (Hs), wind speed (U10) and mean wave period (Tm-10) are derived from the SAR images using the CWAVE algorithm [1]. The results are compared to collocated ERS altimeter data and in situ measurements from NOAA buoys and observations taken onboard the vessel Polarstern. It is shown that the SAR derived Hs is comparable in quality to altimeter measurements and can thus be used for real time assimilation.
7F452676	Releasing a software at the right time is crucial to any software house. A set of critical questions that any software project manager has to deal with: “Is the software under development ready to be released now?”, “How many faults are remaining in the software?”, “How many changes are required to correct the errors?”, “How much time is needed to change the programs?” and “How much is the maintenance cost after the software is placed into production?”
801ED71F	This paper focuses on the development of computationally-efficient predictive control algorithms for nonlinear parabolic PDEs with state and control constraints arising in the context of diffusion-reaction processes. Specifically, we consider a diffusion-reaction process described by a nonlinear parabolic PDE and address the problem of stabilization of an unstable steady-state subject to input and state constraints. Galerkin's method is used to derive finite-dimensional systems that capture the dominant dynamics of the parabolic PDF, which are subsequently used for controller design. Various MPC formulations are constructed on the basis of the finite dimensional approximations that differ in the way the evolution of the fast eigenmodes is accounted for in the performance objective and state constraints. The impact of these differences on the ability of the predictive controller to enforce state constraints satisfaction in the infinite-dimensional system is analyzed. Finally, the MPC formulations arc applied, through simulation, to the problem of stabilizing an unstable steady-state of a nonlinear model of a diffusion-reaction process subject to state and control constraints.
7C395478	Sentinel lymph node (SN) biopsy offers the possibility of selective axillary treatment for breast cancer patients, but there are only limited means for the selective treatment of SN-positive patients. Eight predictive models assessing the risk of non-SN involvement in patients with SN metastasis were tested in a multi-institutional setting. Data of 200 consecutive patients with metastatic SNs and axillary lymph node dissection from each of the 5 participating centres were entered into the selected non-SN metastasis predictive tools. There were significant differences between centres in the distribution of most parameters used in the predictive models, including tumour size, type, grade, oestrogen receptor positivity, rate of lymphovascular invasion, proportion of micrometastatic cases and the presence of extracapsular extension of SN metastasis. There were also significant differences in the proportion of cases classified as having low risk of non-SN metastasis. Despite these differences, there were practically no such differences in the sensitivities, specificities and false reassurance rates of the predictive tools. Each predictive tool used in clinical practice for patient and physician decision on further axillary treatment of SN-positive patients may require individual institutional validation; such validation may reveal different predictive tools to be the best in different institutions.
6F592096	A discussion is presented of a software process modeling case study. The four primary objectives of software process modeling are summarized. Thirteen substantive requirements for a software modeling approach are identified. The author has developed a model of this process utilizing a commercially available automated system called STATEMATE. Although this system was originally developed to aid in specifying and designing real-time reactive systems software, successful experiences indicate that it is well suited for modeling software processes. STATEMATE offers a representation formalism that is highly visual, yet formally defined.
75B8BB3E	This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases – both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.
7D839AFC	Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.
80C733FD	Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.
59300DF4	Software systems embed in them knowledge about the domain in which they operate. However, this knowledge is "latent". Making such knowledge accessible could be of great value to the organization both as a source of explicit knowledge and to systems development and maintenance. We propose a framework aimed at making domain knowledge embedded in software explicit. The framework is based on identifying domain knowledge acquired during the development process (especially in requirements analysis) and formalizing it. The software architecture is then partitioned into two parts: one represents the domain knowledge and the other responsible for the actual processing (using this knowledge). A specific object-oriented design approach is suggested to accomplish this partitioning.
08660FF1	Inheritance is a key feature of object-oriented programming. Overriding is one of the most important parts of inheritance, allowing a subclass to replace methods implemented in its superclass. Unfortunately, the way programmers use overriding in practise is not well understood.We present the first large-scale empirical study of overriding. We describe a suite of metrics that measure overriding and present a corpus analysis that uses those metrics to analyse 100 open-source applications, containing over 100,000 separate classes and interfaces. We found substantial overriding: most subclasses override at least one method and many classes that only declare overriding methods. We also found questionable uses of overriding, such as removing superclass method implementations by overriding them with empty method bodies.
7B4DA32F	Automated analysis of object-oriented design models can provide insight into the quality of a given software design. Data obtained from automated analysis, however, is often too complex to be easily understood by a designer. This paper examines the use of an automated analysis tool on industrial software UML class models, where one set of models was created as part of the design process and the other was obtained from reverse engineering code. The analysis was performed by DesignAdvisor, a tool developed by Siemens Corporate Research, that supports metrics-based analysis and detection of design guideline violations. The paper describes the lessons learned from using the automated analysis techniques to assess the quality of these models. We also assess the impact of design pattern use in the overall quality of the models. Based on our lessons learned, identify design guidelines that would minimize the occurrence of these errors.
7B07BAE5	This paper aims at empirically exploring the relationships between most of the existing coupling and cohesion measures for object-oriented (OO) systems, and the fault-proneness of OO system classes. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The study described here is a replication of an analogous study conducted in an university environment with systems developed by students. In order to draw more general conclusions and to (dis)confirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals. Results show that many of our findings are consistent across systems, despite the very disparate nature of the systems under study. Some of the strong dimensions captured by the measures in each data set are visible in both the university and industrial case study. For example, the frequency of method invocations appears to be the main driving factor of fault-proneness in all systems. However, there are also differences across studies which illustrate the fact that quality does not follow universal laws and that quality models must be developed locally, wherever needed.
726FC96C	This paper presents the application of neural networks in software quality estimation using object-oriented metrics. In this paper, two kinds of investigation are performed. The first on predicting the number of defects in a class and the second on predicting the number of lines changed per class. Two neural network models are used, they are Ward neural network and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model
76A0F499	Presents a novel way of using object-oriented design metrics to support the incremental development of object-oriented programs. Based on a quality model (the factor-criteria-metrics model), so-called multi-metrics relate a number of simple structural measurements to design principles and rules. Single components of an object-oriented program like classes or subsystems are analyzed to determine whether they conform to specific design goals. Concise measurement reports, together with detailed explanations of the obtained values, identify problem spots in system design and give hints for improvement. This allows the designer to measure and evaluate programs at an appropriate level of abstraction. This paper details the use of the multi-metrics approach for the design and improvement of a framework for industry and its use for graphical applications. Multi-metrics tools were used with several versions of the framework. The measurement results were used in design reviews to quantify the effects of efforts to reorganize the framework. The results showed that this approach was very effective at giving good feedback, even to very experienced software developers. It helped them to improve their software and to create stable system designs.
7C02205D	The object-oriented software package OOQP for solving convex quadratic programming problems (QP) is described. The primal-dual interior point algorithms supplied by OOQP are implemented in a way that is largely independent of the problem structure. Users may exploit problem structure by supplying linear algebra, problem data, and variable classes that are customized to their particular applications. The OOQP distribution contains default implementations that solve several important QP problem types, including general sparse and dense QPs, bound-constrained QPs, and QPs arising from support vector machines and Huber regression. The implementations supplied with the OOQP distribution are based on such well known linear algebra packages as MA27/57, LAPACK, and PETSc. OOQP demonstrates the usefulness of object-oriented design in optimization software development, and establishes standards that can be followed in the design of software packages for other classes of optimization problems. A number of the classes in OOQP may also be reusable directly in other codes.
7BA53FFB	Repeated code modification lowers code quality and impacts object-oriented system design. Object-oriented metrics have proven as indicators of problems in system design. They have been grouped in minimal sets, known as quality models, to assess object-oriented system quality. Improvement can be gained by establishing relationships between quality characteristics and metrics computed from object-oriented diagrams. Quality models include metrics that can produce better code in object-oriented systems. Code quality can also be gained with refactoring. Refactoring is used to reduce complexity and eliminate redundant code. It is important to identify when and where to use refactoring. There are many different approaches. This work presents early stage analysis and focuses on exploring whether object-oriented metrics can be used as indicators where in the code refactoring can be used. Through multiple itera tions of successive measurement and refactoring, relation between metric values and need of refactoring can be concluded.
2DBF3E95	The goals of the Ada Performance Study are described. The methods used are explained. Guidelines for future Ada development efforts are given. The goals and scope of the study are detailed, and the background of Ada development in the Flight Dynamics Division (FDD) is presented. The organization and overall purpose of each test are discussed. The purpose, methods, and results of each test and analyses of these results are given. Guidelines for future development efforts based on the analysis of results from this study are provided. The approach used on the performance tests is discussed. 
7B027B55	Software engineers of multi-agent systems (MASs) are faced with different concerns such as autonomy, adaptation, interaction, collaboration, learning, and mobility, which are essentially different from classical concerns addressed in object-oriented software engineering. MAS developers, however, have relied mostly on object-oriented design techniques and programming languages, such as Java. This often leads to a poor separation of MAS concerns and in turn to the production of MASs that are difficult to maintain and reuse. This paper discusses software engineering approaches for MASs, and presents a new method for integrating agents into object-oriented software engineering from an early stage of design. The proposed approach encourages the separate handling of MAS concerns, and provides a disciplined scheme for their composition. Our proposal explores the benefits of aspect-oriented software development for the incorporation of agents into object-oriented systems. We also illustrate our aspect-oriented approach through the Portalware multi-agent system, a Web-based environment for the development of e-commerce portals. Copyright © 2004 John Wiley & Sons, Ltd.
76F99AE3	The Ada programming language has been criticized for lacking essential runtime system capabilities which support the generation of verifiable, real-time multitasking software. Such a capability is critical to the development of application software for high performance embedded avionic systems. The Ada Real-Time Machine Interface (ARTMI) software component of the F-16 Modular Mission Computer (MMC) provides Ada runtime environment (RTE) extensions that support development of real-time multitasking application software. The extensions eliminate the problem of mutual deadlock and unbounded priority inversion at critical sections through use of the priority ceiling protocol. This allows application software to be designed with task prioritization schemes that permit application of Rate Monotonic Analysis techniques. In addition, the extensions support execution time modification of task priorities. Such a capability can be used to reallocate processing bandwidth to critical system functions as the demands of a mission change.
6D9B5EDE	SMARTNet-2 was developed to provide a high-speed shared memory network for support of real-time parallel processing. Rapid prototyping put early versions of the SMARTNet-2 Diagnostic Software on the various computers being used for hardware board development and testing. This provided quick feedback to the hardware design engineers and ensured that all addressable hardware could be accessed by the software. As the hardware matured and new parts were added it was fairly simple to update the software to accommodate the new hardware. The authors detail the unique aspects of the design and implementation of the SMARTNet-2 diagnostic software. In addition, they discuss the command language and the methods used to achieve fault isolation testing.
6B8C5D5C	Lockheed Martin Tactical Aircraft Systems and Texas Instruments recompiled over 2 million non-comment non-blank source lines of code (SLOC) in the avionics and vehicle management services subdomains, originally written in Ada 83, using four Ada 95 compilers. Two avionics applications were linked and executed using a workstation-based simulator to verify gross functionality. A Distributed Systems Annex (DSA) version of this code was created and executed successfully on multiple workstations across a TCP/IP network. Translating Ada 83 code to Ada 95 compatibility proceeded much faster than expected, about 2000 lines per hour, and was easily accomplished with only a text editor. Once translated, the code proved highly portable between compilers. The greatest challenge was not in Ada version incompatibilities, but in assumptions inherent in the legacy code. Ada 95 compilers will require at least 6 go 12 months of maturation to be capable of supporting a large avionics development effort. Particular weaknesses in is the lack of task-aware pre-processors for handling multiple aircraft configurations from a common source code base, and support for optional language annexes. However, the Ada 95 tools resolve major problem areas of Ada 83 tools, including compilation order dependencies and lack of object-oriented features, and in general compile much faster. When mature, these Ada 95 tools are expected to provide significant productivity gains over their Ada 83 counterparts.
7E3716D0	With the increasing use of object-oriented methods in new software development, there is a growing need to both document and improve current practice in object-oriented design and development. In response to this need, a number of researchers have developed various metrics for object-oriented systems as proposed aids to the management of these systems. In this research, an analysis of a set of metrics proposed by Chidamber and Kemerer (1994) is performed in order to assess their usefulness for practising managers. First, an informal introduction to the metrics is provided by way of an extended example of their managerial use. Second, exploratory analyses of empirical data relating the metrics to productivity, rework effort and design effort on three commercial object-oriented systems are provided. The empirical results suggest that the metrics provide significant explanatory power for variations in these economic variables, over and above that provided by traditional measures, such as size in lines of code, and after controlling for the effects of individual developers.
7F9BE01F	The adoption of and adherence to object-oriented design and programming principles have allowed the software industry to create applications of ever-increasing complexity. A concomitant need arises for strategies to identify, manage, and, wherever possible, reduce this software complexity. One such strategy is the systematic collection, interpretation, and analysis of software metrics, mappings from software objects or constructs to sets of numerical features that quantify relevant software attributes. We describe a novel approach that employs various graph theoretic algorithms to analyze the higher level, application-wide class relationship graphs that emerge from object-oriented software. In addition to the software's overall inheritance tree characteristics, these algorithms will use metrics that reflect information on the import and export coupling of class-attribute and class-method relationships. Further, we incorporate information relating to the response sets for each object in the software, that is, the number of methods that can be executed in response to messages being received by objects.
7884E4DF	Technical, management, and political decisions made in the design phase of this shipboard command and weapons control system (more than 1 million LOC), have led to a significant percentage of reuse on subsequent projects. The practical issues and lessons learned in trying to design for massive software reuse will be discussed. 
7E66A794	A technique for automatically inserting software mechanisms to detect single event upset (SEU) in distributed Ada systems is presented. SEUs may cause information corruption, leading to a change in program flow or causing a program to execute an infinite loop. Two cooperative software mechanisms for detecting the presence of these upsets are described. Automatic insertion of these mechanisms is discussed in relation to the structure of Ada software systems. A program, software modifier for upset detection (SMUD), has been written to automatically modify Ada application software and insert software upset detection mechanisms. As an example, the mechanisms have been incorporated into a system model that employs the MIL-STD-1553B communications protocol. This system model is used as a testbed for verifying that SMUD properly inserts the detection mechanisms. Ada is used for creating the simulation environment to exercise and verify the protocol. Simulation has been used to test and verify the proper functioning of the detection mechanisms. The testing methodology, a short description of the 1553B testbed, and a set of performance measures are presented.
7DDCF7CF	Software metrics is one of the well-known topics of research in software engineering. Metrics are used to improve the quality and validity of software systems. Research in this area focus mainly on static metrics obtained by static analysis of the software. However modern software systems without object oriented design are incomplete. Every system has its own complexity which should be measured to improve the quality of the system. This paper describes the different types of metrics along with the static code metrics and Object oriented metrics. Then the metrics are summarized on the basis of relevance in finding the complexity and hence help in better maintainability of the software code, retaining the quality and making it cost effective.
7A7F3308	An object-oriented approach has become a commonly-used method in software-related activities. Many design metrics for object-oriented systems have been proposed and also employed for predicting and managing the quality of processes and products. To enhance the practical utility of object-oriented metrics in software industry, various researchers have tried to find relations between these metrics and fault proneness, but very few focus on relating them with the number-offaults in different levels as per their severity rating. In this study, empirical validation is carried out on object-oriented design metrics (i.e. Chidamber and Kemerer CK-metrics suite and source lines of codes) for predicting number-of-faults in different severity levels. Different statistical methods are used to analyze the data, including correlation.
79C57B7D	An empirical study of the relationship between object-oriented (OO) metrics and error-severity categories is presented. The focus of the study is to identify threshold values of software metrics using receiver operating characteristic curves. The study used the three releases of the Eclipse project and found threshold values for some OO metrics that separated no-error classes from classes that had high-impact errors. Although these thresholds cannot predict whether a class will definitely have errors in the future, they can provide a more scientific method to assess class error proneness and can be used by engineers easily. Copyright © 2009 John Wiley & Sons, Ltd.
7A9076C5	
The need to develop and maintain large complex software systems in a competitive and dynamic environment has driven interest in new approaches to software design and development. The problems with the classical waterfall model have been cataloged in almost every software engineering text [19,23]. In response, alternative models such as the spiral [2], and fountain [9] have been proposed. Problems with traditional development using the classical life cycle include no iteration, no emphasis on reuse, and no unifying model to integrate the phases. The difference in point of view between following data flows in structured analysis and building hierarchies of tasks in structured design has always been a major problem [4]. Each system is built from scratch and maintenance costs account for a notoriously large share of total system costs. The object-oriented paradigm addresses each of these issues. A look at the object-oriented software life cycle, as described by Meyer [5], Coad and Yourdon [4], and Henderson-Sellers and Edwards [9], identifies the three traditional activities of analysis, design, and implementation. However, each of the referenced descriptions eliminates the distinct boundaries between the phases. The primary reason for this blurring of boundaries is that the items of interest in each phase are the same: objects. Objects and the relationships between objects are identified in both the analysis and design phases. Objects and relationships identified and documented in the analysis phase serve not only as input to the design phase, but as an initial layer in the design. This continuity provides for a much more seamless interface between the phases. Analysts, designers and programmers are working with a common set of items upon which to build. A second reason for the blurring of these boundaries is that the object-oriented development process is iterative. Henderson-Sellers and Edwards further refine this idea by replacing the waterfall model of software development with a fountain model. Development reaches a high level only to fall back to a previous level to begin the climb once again. As an example of the blurring of the traditional boundaries of the life cycle phases, Coad and Yourdon recommend that classification relationships between objects be captured and documented during the object-oriented analysis (OOA) phase. This classification will be directly reflected in the class inheritance structure developed in the design and in the code. This classification is in no way required in order to document the system requirements. In other words, Coad and Yourdon are recommending a traditional design activity in the analysis phase. The blurring of the traditional design and implementation phases has been fueled by the development of encapsulation and abstraction mechanisms in object-oriented and object-based languages. For example, Meyer claims [14] that Eiffel is both a design and an implementation language. He goes on to say that software design is sometimes mistakenly viewed as an activity totally secluded from actual implementation. From his point of view, much is to be gained from an approach that integrates both activities within the same conceptual framework. The object-oriented design paradigm is the next logical step in a progression that has led from a purely procedural approach to an object-based approach and now to the object-oriented approach. The progression has resulted from a gradual shift in point of view in the development process. The procedural design paradigm utilizes functional decomposition to specify the tasks to be completed in order to solve a problem. The object-based approach, typified by the techniques of Yourdon, Jackson and Booth, gives more attention to data specifications than the procedural approach but still utilizes functional decomposition to develop the architecture of a system. The object-oriented approach goes beyond the object-based technique in the emphasis given to data by utilizing the relationships between objects as a fundamental part of the system architecture. The goal in designing individual software components is to represent a concept in what will eventually be an executable form. The Abstract Data Type (ADT) is the object-based paradigm's technique for capturing this conceptual information. The class is the object-oriented paradigm's conceptual modeling tool. The design pieces resulting from the object-oriented design technique represent a tighter coupling of data and functionality than traditional ADTs. These artifacts of the design process used in conjunction with a modeling-based decomposition approach yield a paradigm, a technique, which is very natural and flexible. It is natural in the sense that the design pieces are closely identified with the real-world concepts which they model. It is flexible in the sense of quickly adapting to changes in the problem specifications. Object-oriented remains a term which is interpreted differently by different people. Before presenting an overview of a set of techniques for the design process, we will give our perspective so the reader may judge the techniques in terms of those definitions. Briefly, we adapt Wegner's [27] definition for object-oriented languages to object-oriented design. The pieces of the design are objects which are grouped into classes for specification purposes. In addition to traditional dependencies between data elements, an inheritance relation between classes is used to express specializations and generalizations of the concepts represented by the classes. As natural and flexible as the object-oriented technique is, it is still possible to produce a bad design when using it. We will consider a number of general design criteria and will discuss how the object-oriented approach assists the designer in meeting these criteria. We will refer to a number of design guidelines developed specifically for the object-oriented design paradigm and will discuss how these properties reinforce the concepts of good design. The paradigm sprang from language, has matured into design, and has recently moved into analysis. The blurring of boundaries between these phases has led us to include topics in this article that are outside the realm of design, but which we consider important to understanding the design process. Since the paradigm sprang from language, we define the concepts basic to object-oriented programming in the following section.
77FC0E58	The portability and scalability benefits of Java [1, 2] combined with improved economies of scale resulting from its popularity are motivating many organizations to switch to Java. As organizations switch to Java for new development, many face difficult challenges with respect to maintenance and evolution of their existing legacy systems. In certain critical embedded and real-time domains, important legacy systems are implemented in the Ada language. This paper describes a middleware approach to enable efficient and robust integration of Ada and Java software into mixed-language software systems. Though the technology is designed to generalize to all Ada-Java mixed-language applications, this paper focuses attention on the special challenges unique to stack management of temporary objects, as characterized by the Ravenscar profile of Ada 95 and the JSR-302 subset of traditional Java. The middleware design, known as Ada-Java Method Invocation (AJMI), simplifies inter-language calls, extends object-oriented abstractions across language boundaries, and enables reliable sharing of stack-allocated objects which are integral to the use of Ada and Java in safety-critical systems.
7DA5D03D	This paper describes a design study that explores how multi-touch devices can provide support for developers when carrying out modeling tasks in software development. We investigate how well a multi-touch augmented approach performs compared to a traditional approach and if this new approach can be integrated into existing software engineering processes. For that, we have implemented a fully-functional prototype, which is concerned with agreeing on a good object-oriented design through the course of a Class Responsibility Collaboration (CRC) modeling session. We describe how multi-touch technology helps with integrating CRC cards with larger design methodologies, without loosing their unique physical interaction aspect. We observed high-potential in augmenting such informal sessions in software engineering with novel user interfaces, such as those provided by multi-touch devices.
7F8F8A2A	Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.
80545B71	This paper examines some of the underpinnings of software reliability and software development concerns. A survey was conducted of one company's employees regarding an unsuccessful large-scale program to identify some lessons learned. These lessons point out that one of the largest origins of software problems lies in communication deficiencies. The quality of the development process ultimately determines the quality of the delivered software product. The primary causes of customer problems often stem from difficulties in capturing true customer requirements. An additional challenge is assuring strong integrated product team understanding and communications. This paper identifies structured development process initiatives that assure the building of more reliable code. Then two of the more significant initiatives, object-oriented design and the defect prevention process, are discussed in detail.
2EBA97B6	A case study of the impact of Ada on a Command and Control project completed at the Jet Propulsion Laboratory (JPL) is given. The data for this study was collected as part of a general survey of software costs and productivity at JPL and other NASA sites. The task analyzed is a successful example of the use of rapid prototyping as applied to command and control for the U.S. Air Force and provides the U.S. Air Force Military Airlift Command with the ability to track aircraft, air crews and payloads worldwide. The task consists of a replicated database at several globally distributed sites. The local databases at each site can be updated within seconds after changes are entered at any one site. The system must be able to handle up to 400,000 activities per day. There are currently seven sites, each with a local area network of computers and a variety of user displays; the local area networks are tied together into a single wide area network. Using data obtained for eight modules, totaling approximately 500,000 source lines of code, researchers analyze the differences in productivities between subtasks. Factors considered are percentage of Ada used in coding, years of programmer experience, and the use of Ada tools and modern programming practices. The principle findings are the following. Productivity is very sensitive to programmer experience. The use of Ada software tools and the use of modern programming practices are important; without such use Ada is just a large complex language which can cause productivity to decrease. The impact of Ada on development effort phases is consistent with earlier reports at the project level but not at the module level. 
799CA085	This paper presents a state-of-the-art review of empirical research on object-oriented (OO) design. Many claims about the cognitive benefits of the OO paradigm have been made by its advocates. These claims concern the ease of designing and reusing software at the individual level as well as the benefits of this paradigm at the team level. Since these claims are cognitive in nature, it seems important to assess them empirically. After a brief presentation of the main concepts of the OO paradigm, the claims about the superiority of OO design are outlined.The core of this paper consists of a review of empirical studies of OO design (OOD). We first discuss results concerning OOD by individuals. On the basis of empirical work, we (1) analyse the design activity of novice OO designers, (2) compare OOD with procedural design and (3) discuss a typology of problems relevant for the OO approach. Then we assess the claims about naturalness and ease of OOD. The next part discusses results on OO software reuse. On the basis of empirical work, we (1) compare reuse in the OO versus the procedural paradigm, (2) discuss the potential for OO software reuse and (3) analyse reuse activity in the OO paradigm. Then we assess claims on reusability. The final part reviews empirical work on OOD by teams. We present results on communication, coordination, knowledge dissemination and interactions with clients. Then we assess claims about OOD at the software design team level.
79916FCC	The state of object-oriented is evolving rapidly. This survey describes what are currently thought to be the key ideas. Although it is necessarily incomplete, it contains both academic and industrial efforts and describes work in both the United States and Europe. It ignores well-known ideas, like that of Coad and Meyer [34], in favor of less widely known projects.Research in object-oriented design can be divided many ways. Some research is focused on describing a design process. Some is focused on finding rules for good designs. A third approach is to build tools to support design. Most of the research described in this article does all three.We first present work from Alan Snyder at Hewlett-Packard on developing a common framework for object-oriented terminology. The goal of this effort is to develop and communicate a corporate-wide common language for specifying and communicating about objects.We next look into the research activity at Hewlett-Packard, led by Dennis de Champeaux. De Champeaux is developing a model for object-based analysis. His current research focuses on the use of a trigger-based model for inter-object communications and development of a top-down approach to analysis using ensembles.We then survey two research activities that prescribe the design process. Rebecca Wirfs-Brock from Tektronix has been developing an object-oriented design method that focuses on object responsibilities and collaborations. The method includes graphical tools for improving encapsulation and understanding patterns of object communication. Trygve Reenskaug at the Center for Industriforskning in Oslo, Norway has been developing an object-oriented design method that focuses on roles, synthesis, and structuring. The method, called Object-Oriented Role Analysis, Syntheses and Structuring, is based on first modeling small sub-problems, and then combining small models into larger ones in a controlled manner using both inheritance (synthesis) and run-time binding (structuring).We then present investigations by Ralph Johnson at the University of Illinois at Urbana-Champaign into object-oriented frameworks and the reuse of large-scale designs. A framework is a high-level design or application architecture and consists of a suite of classes that are specifically designed to be refined and used as a group. Past work has focused on describing frameworks and how they are developed. Current work includes the design of tools to make it easier to design frameworks.Finally, we present some results from the research group in object-oriented software engineering at Northeastern University, led by Karl Lieberherr. They have been working on object-oriented Computer Assisted Software Engineering (CASE) technology, called the Demeterm system, which generates language-specific class definitions from language-independent class dictionaries. The Demeter system include tools for checking design rules and for implementing a design.
5C9BCB40	About ten years ago, the first serious applications of concept lattices in software analysis were published. Today, a wide range of applications of concept lattices in static and dynamic analysis of software artefacts is known. This overview summarizes important papers from the last ten years, and presents three methods in some detail: 1. methods to extract classes and modules from legacy software; 2. the Snelting/Tip algorithm for application-specific, semantics-preserving refactoring of class hierarchies; 3. Ball’s method for infering dynamic dominators and control flow regions from program traces. We conclude with some perpectives on further uses of concept lattices in software technology.
74707DC0	The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts.
8025DE28	Agent-oriented software engineering (AOSE) has become an active area of research in recent years. We look at the use of agent-oriented concepts for software analysis. Using agent-oriented analysis may offer benefits even if the system is implemented without an agent-based language or framework (e.g. using an object-oriented detailed design and language). We examine the software analysis components of a number of existing agent-oriented methodologies. We discuss the benefits that can be gained by using agent-oriented concepts, and where the concepts require further development. Based on this analysis, we present the agent-oriented methodology that we are developing, and describe an example of how it may be applied for software analysis.
795ECF43	This study aims at developing and empirically testing hypotheses about professional designers' cognitive activities when using object-oriented methodology (OOD) versus using traditional functional decomposition methodologies (TFD). Our preliminary results indicate that OOD may achieve substantial time savings over TFD in logical design. The verbal protocols from a pilot study show that OOD may achieve these time savings: 1) by simplifying rule induction processes used in functional decomposition; 2) by guiding designers on how to build more effective problem spaces; and 3) by allowing designers to run mental simulation more efficiently and more effectively.
7E67E534	In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes
7F62DF4A	Many software development organizations are adopting object-oriented methodologies as their primary paradigm for software development. The object-oriented method appears to increase programmer productivity, reduce the overall cost of the software and, perhaps most importantly, creates software that promotes reuse and subsequently is easier to modify. Consistent with the change in industry, many universities and industry training organizations are currently in the process of integrating object orientation into their curriculum. There are several approaches including horizontal integration (integrating a small dose of the object orientation into many courses) and vertical integration (having a large dose of the concepts in a single course). In 1996, the Systems Analysis department of Miami University, USA, opted for the latter approach and added a new course to its curriculum. It is a course that is intended to provide some in-depth exposure to object-oriented design and implementation. It should be of particular value to faculty in computer science and information systems departments (both at the 4-year and 2-year institutions) as well as those in industry training organizations who are looking for ways to incorporate the object orientation into their curriculum. In this paper, the authors describe the choices their department made, what worked well, and what needs to be improved.
60E0AAD7	This paper presents an empirical study of the software reuse activity by expert designers in the context of object-oriented design. Our study focuses on the three following aspects of reuse : (1) the interaction between some design processes, e.g. constructing a problem representation, searching for and evaluating solutions, and reuse processes, i.e. retrieving and using previous solutions, (2) the mental processes involved in reuse, e.g. example-based retrieval or bottom-up versus top-down expanding of the solution, and (3) the mental representations constructed throughout the reuse activity, e.g. dynamic versus static representations. Some implications of these results for the specification of software reuse support environments are discussed. 
7D1C7594	This study provides an initial assessment of the U.S.'s industrial capacity to produce MCCR software. A Survey of senior government and industry people showed that 90 percent of them expected a serious problem with the nation's capacity to produce military software over the next 5 years. They ranked acquisition and labor factors as contributing most to the failure of military system development contracts to meet schedule or costs. The study team also analyzed available data about the supply of labor (new graduates and experienced scientists and engineers) and three aspects of demand (Ada systems, PDSS, and related commercial applications) before concluding there is a serious capacity problem. The report describes, organizational, and technological issues affecting software production capacity and concludes with some preliminary recommendations for DoD and industry initiatives. (Author) (kr)
2FBC050F	The SEL is an organization sponsored by NASA/GSFC to investigate the effectiveness of software engineering technologies applied to the development of applications software. The SEL was created in 1977 and has three organizational members: NASA/GSFC, Systems Development Branch; The University of Maryland, Computer Sciences Department; and Computer Sciences Corporation, Systems Development Operation. The goals of the SEL are as follows: (1) to understand the software development process in the GSFC environments; (2) to measure the effect of various methodologies, tools, and models on this process; and (3) to identify and then to apply successful development practices. The activities, findings, and recommendations of the SEL are recorded in the Software Engineering Laboratory Series, a continuing series of reports that include the Ada Performance Study Report. This paper describes the background of Ada in the Flight Dynamics Division (FDD), the objectives and scope of the Ada Performance Study, the measurement approach used, the performance tests performed, the major test results, and the implications for future FDD Ada development efforts. 
8120C03C	Loud discussions concerning various ways of teaching object-orientation have taken place without much empirical evidence for any position. This paper reports qualitative observations of learning of object-ori-ented programming in an introductory course. The students were found to cope reasonably well with the object-oriented concepts, and they had learnt procedural programming first. However, they modeled the real world domain to be represented in the program by imagination and through coding. Their problems may be attributed to the high complexity generated by having to relate to four - six areas of attention. Three ways of improving teaching are suggested, making the areas of attention and the ways to relate them more explicit for the students, forcing modeling by means of a tool, and reducing complexity by means of programming environments that visualize objects and their behavior. 
77A26FC8	Object oriented design brought up improvement of productivity and software quality by adopting some concepts such as inheritance and encapsulation. However, both the number of software`s classes and object couplings are increasing as the software volume is becoming larger. The object coupling between classes is closely related with software complexity, and high complexity causes decreasing software quality. In order to solve the object coupling issue, IT-field researchers adopt a component based development and software quality metrics. The component based development requires explicit representation of dependencies between classes and the software quality metrics evaluates quality of software. As part of the research, we intend to gain a basic data that will be used on decomposing software. We focused on properties of the linkage between classes rather than previous studies evaluated and accumulated the qualities of individual classes. Our method exploits machine learning technique to analyze the properties of linkage and predict the strength of dependency between classes, as a new perspective on analyzing software property.
7E017F8E	The role of reuse within software engineering is examined. The benefits of reuse are stated, inhibitors to successful reuse are pointed out, and technical issues for achieving reuse are examined. Specific attention is paid to how object-oriented software engineering enhances or reduces the potential for reuse as compared with traditional (i.e. not based on objects or object-oriented concepts) software engineering. Three major activities for supporting reuse are described. The first is the intentional development of reusable artifacts, the second is the representation and classification of artifacts into repositories, and the last is the utilization of the artifacts from repositories. The object-oriented model is found to be significantly similar to the traditional model in many aspects. However, differences are found insofar as object-oriented design supports the design techniques of abstraction and factorization, which utilize object inheritance to yield reusable objects.
0B76970E	This paper aims at empirically exploring the relationships between most of the existing design coupling, cohesion, and inheritance measures for object-oriented (OO) systems, and the fault-proneness of OO system classes. The underlying goal of this study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. In addition, we aim at assessing whether such relationships, once modeled, can be used to effectively drive and focus inspections or testing. The study described here is a replication of an analogous study conducted in a university environment with systems developed by students. In order to draw more general conclusions and to (dis)confirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals. Results show that many of our findings are consistent across systems, despite the very disparate nature of the systems under study. Some of the strong dimensions captured by the measures in each data set are visible in both the university and industrial case study. For example, the frequency of method invocations appears to be the main driving factor of fault-proneness in all systems. However, there are also differences across studies, which illustrate the fact that, although many principles and techniques can be reused, quality does not follow universal laws and quality models must be developed locally, wherever needed.
7DA864F8	The Submarine Message Buffer (SMB) is a ml time embedded message processing system developed at the Naval Command, Control and Ocean Surveillance Center, Research, Development, Test and Evaluation Division (NRaD). The SMB is sponsored by the Space and Naval Warfare Systems Command (SPAWAR) to support modernization of SSN (Los Angels and Seawolf class submarines) radio rooms. The development strategy adopted for the SMB concentrated on the reuse of Ada software. This paper will focus on the design, processes and methodologies, which were used in the development of this system. Metrics will also be provided showing why this system has been identified as an Ada success story by the Ada Joint Program Office among others
5FBCD285	Based on our experience using active learning methods to teach object-oriented software design we propose a game-based approach to take the classroom experience into a virtual environment.The different pedagogical approaches that our active method supports, have motivated us to tailor an architecture that supports the creation of different variations of role-play environments, ranging from open-ended trial and error approaches to highly constrained settings where students can not get very far from the solution. We also describe a prototype that instantiates this architecture called ViRPlay3D2.
13ACDC2A	This study investigates the needs of software developers in the area of software reuse. The study formulates recommendations on how a software repository can meet the needs of the software development community. The Naval Software Reuse System (NSRS) is the focus of this research and the findings are intended for their use. The study focuses on eight core questions that are the central issues for the NSRS. On the basis of these eight core questions, a survey was developed that elicited the opinions of Department of the Navy software developers. The analysis of the survey provides conclusions for seven of the eight core questions. These conclusions lead to recommendations that for the most part are completely within the control of Naval Computers and Telecommunications Station (NCTS) Washington. NCTS Washington is the agency presently building the NSRS.
75E04CFE	Software designers in the object-oriented paradigm can make use of modeling tools and standard notations such as UML. Nevertheless, casual observations from collocated design collaborations suggest that teams tend to use physical mediums to sketch a plethora of informal diagrams in varied representations that often diverge from UML. To better understand such collaborations and support them with tools, we need to understand the origins, roles, uses, and implications of these alternate representations. To this end we conducted observational studies of collaborative design exercises, in which we focused on representation use.Our primary finding is that teams intentionally improviserepresentations and organize design information in responseto ad-hoc needs, which arise from the evolution of the design, and which are difficult to meet with fixed standard notations. This behavior incurs orientation and grounding difficulties for which teams compensate by relying on memory, other communication mediums, and contextual cues. Without this additional information the artifacts are difficult to interpret and have limited documentation potential. Collaborative design tools and processes should therefore focus on preserving contextual information while permitting unconstrained mixing and improvising of notations.
7D53B04F	A modeling and prototyping approach for defining system and software requirements and validating them through rapid prototyping is described, and the results of a case study for generating prototype software (representing the total system hardware and software) directly and automatically from the requirements are presented. System/software definitions, which include the requirements and design (architecture), are addressed. The case study applied this approach to selected systems of Boeing 747-400 aircraft. A formal model of system specification was generated. The rapid prototyping task automatically generated thousands of lines of Ada source code from the specification model. The software was executed successfully the first time. The functions and behavior of the system were demonstrated and validated by its users. This study indicated that early execution and validation of system requirements, through the use of formal modeling and rapid prototyping with direct user involvement, can be accomplished.
78154E04	The AFHRL has a long term research project to develop an Integrated Maintenance Information System (IMIS). The project was initiated in 1979 and is expected to be completely by 1991. The objective of IMIS is to give maintenance technicians a very small size portable computer/display that will interface with on-aircraft systems and other computer systems to provide a single, integrated source of the information needed to perform maintenance on the line and in the shop. The system will display graphic instructions, provide intelligent diagnostic advice, provide aircraft battle damage assessment aids, analyze in-flight access and interrogate on-board built-in-test capabilities. It will also provide the technician with easy and efficient methods to receive work orders, report maintenance actions, order parts from supply, and offer complete computer-aided training lessons and simulations.The objectives of the study were to review and evaluate the requirements specifications and to make recommendations to accomplish total compatibility of IMIS with current and future requirements of the users with an emphasis on methodologies/tools for software requirements specification. The study concentrated on structured tools which could be used for IMIS. The various reports submitted by contractors and those developed in-house related to concepts of IMIS, feasibility aspects and information requirements of maintenance technicians were reviewed by the researcher, and it was observed that structured tools have not been used in defining user requirements or in development of software. The information requirements of maintenance technicians were presented using process flowcharts and narration. A great deal of mental effort is required to understand these charts. Checking for consistency and completeness of requirements is a formidable task when these charts are used.It was recommended that structured techniques should be used at the various phases of IMIS software development life cycle. They would provide efficient documentation of various phases and also lead to solutions in successive phases. They could also be used as management tools in assigning tasks to various contractors and in monitoring the progress of IMIS project. Some of the tools which could be used in the IMIS project are: Data Flow Diagrams, Structure Charts, Warnier-Orr Methodology, Nassi-Shneiderman Charts, Decision Tables, and Entity-Relationship Diagrams.It was also recommended that automated structured tools should be adapted as it is not possible to enforce uniformity and consistency if the documentation is done manually. An automated tool for DFD was obtained and some of the information requirements were presented through the DFD. An effort is in progress to select appropriate automated tools for various phases of IMIS life cycle.
5C5C50B8	Thispaper aims at empirically exploring the relationships betweenmost of the existing design coupling, cohesion, and inheritancemeasures for object-oriented (OO) systems, and the fault-pronenessof OO system classes. The underlying goal of this study is tobetter understand the relationship between existing design measurementin OO systems and the quality of the software developed. in addition,we aim at assessing whether such relationships, once modeled,can be used to effectively drive and focus inspections or testing.The study described here is a replication of an analogous studyconducted in a university environment with systems developedby students. In order to draw more general conclusions and to(dis)confirm the results obtained there, we now replicated thestudy using data collected on an industrial system developedby professionals. Results show that many of our findings areconsistent across systems, despite the very disparate natureof the systems under study. Some of the strong dimensions capturedby the measures in each data set are visible in both the universityand industrial case study. For example, the frequency of methodinvocations appears to be the main driving factor of fault-pronenessin all systems. However, there are also differences across studies,which illustrate the fact that, although many principles andtechniques can be reused, quality does not follow universal lawsand quality models must be developed locally, wherever needed.
7B86871D	This paper describes an Essential Software Framework for Meshfree Methods (ESFM). Through thorough analyses of many existing meshfree methods, their common elements and procedures are identified, and a general procedure is formulated into ESFM that can facilitate their implementations and accelerate new developments in meshfree methods. ESFM also modulates performance-critical components such as neighbor-point searching, sparse-matrix storage, and sparse-matrix solver enabling developed meshfree analysis programs to achieve high-performance. ESFM currently consists of 21 groups of classes and 94 subclasses, and more algorithms can be easily incorporated into ESFM. Finally, ESFM provides a common ground to compare various meshfree methods, enabling detailed analyses of performance characteristics.
5E824C4A	Software architecture descriptions are high-level models of software systems. Most existing special-purpose architectural notations have a great deal of expressive power but are not well integrated with common development methods. Conversely, mainstream development methods are accessible to developers, but lack the semantics needed for extensive analysis. In our previous work, we described an approach to combining the advantages of these two ways of modeling architectures. While this approach suggested a practical strategy for bringing architectural modeling into wider use, it introduced specialized extensions to a standard modeling notation, which could also hamper wide adoption of the approach. This paper attempts to assess the suitability of a standard design method “as is” for modeling software architectures.
7F412202	Software managers need to have information in order to control a software development process. One way to get this information is using data from old software projects. When this information is not available, software managers can not make precise plans to develop software. In this paper we are going to present an intelligent system that given the estimated software size in Function Points (FP) or in Source Lines of Code (SLOC), it will determine the staff, schedule, effort, and paperwork needed to complete the software life cycle (Analysis, Feasibility, Design, Implementation, Integration, and Testing). The system will also estimate how many defects will be expected during the software life cycle. With this information, software managers can make appropriate plans to develop software.
77C8DE95	Software engineers rely on program documentation as an aid in understanding the functional nature, high-level design, and implementation details of complex applications. However, no one really knows what types of documentation are truly useful to software engineers to aid system understanding. This workshop focuses on issues related to this fundamental problem, such as what formats the documentation should take, who should produce it, and when. The juxtaposition of a technical communication audience with software engineering researchers and practitioners will provide new insights into the problem.
80FB6B66	An information retrieval technique, latent semantic indexing, is used to automatically identify traceability links from system documentation to program source code. The results of two experiments to identify links in existing software systems (i.e., the LEDA library, and Albergate) are presented. These results are compared with other similar type experimental results of traceability link identification using different types of information retrieval techniques. The method presented proves to give good results by comparison and additionally it is a low cost, highly flexible method to apply with regards to preprocessing and/or parsing of the source code and documentation.
7DDD1E4E	This paper presents the findings of a survey on the use of UML in software maintenance, carried out with 178 professionals working on software maintenance projects in 12 different countries. As part of long-term research we are carrying out to investigate the benefits of using UML in software maintenance, the main objectives of this survey are: 1) to explore whether UML diagrams are being used in software industry maintenance projects; 2) to see what UML diagrams are the most effective for software maintenance; 3) to find out what the perceived benefits of using UML diagrams are; and 4) to contextualize the kind of companies that use UML documentation in software maintenance. Some complementary results based on the way the documentation is used (whether it is UML-based or not) during software maintenance are also presented.
81B5134E	Focuses on investigating the combined use of semantic and structural information of programs to support the comprehension tasks involved in the maintenance and reengineering of software systems. "Semantic information" refers to the domain-specific issues (both the problem and the development domains) of a software system. The other dimension, structural information, refers to issues such as the actual syntactic structure of the program, along with the control and data flow that it represents. An advanced information retrieval method, latent semantic indexing, is used to define a semantic similarity measure between software components. Components within a software system are then clustered together using this similarity measure. Simple structural information (i.e. the file organization) of the software system is then used to assess the semantic cohesion of the clusters and files with respect to each other. The measures are formally defined for general application. A set of experiments is presented which demonstrates how these measures can assist in the understanding of a nontrivial software system, namely a version of NCSA Mosaic.
7F5143D2	The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54% increase in the functional correctness of changes (p=0.03), and an insignificant 7% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56%) - at the expense of an insignificant 14% increase in development time caused by the overhead of updating the UML documentation (p=0.35).
7F4B8C8F	The field of software documentation is reviewed by examining manual writing before and after 1985. Changes in the field include an increased emphasis on satisfaction of users, improved management strategies and improved design techniques. Three books on software documentation published since 1988 are surveyed, and it is argued that the trends after 1985 reinforce a social constructionist view of documentation.
7E2B7A9B	The project METAMORPHOS is a two-year Italian research project, funded by the Ministry of University and Research, aimed at facilitating the selection and the adoption of reverse engineering and migration techniques and tools in industry. To pursue such an objective, the project aims at empirically evaluating techniques and tools that can potentially fulfill industry needs. The project focuses in particular on migration activities towards distributed architectures-such as Web-based and service-oriented-and mobile devices that are nowadays gaining an increasing diffusion and central role.
806DBF3A	The Sixth International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2011) will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering artifacts, ranging from user needs to models to source code. The objective of the 6th edition of TEFSE is to build on the work the traceability research community has completed in identifying the open traceability challenges. In particular, it is intended to be a working event focused on discussing the main problems related to software artifact traceability and propose possible solutions for such problems. Moreover, the workshop also aims at identifying key issues concerning the importance of maintaining the traceability information during software development, to further improve the cooperation between academia and industry and to facilitate technology transfer.
7C1BAA8B	How can organizational factors such as structure and culture have an impact on the working conditions of developers? This study is based on ten months of observation of an in-house software development project within a large telecommunications company. The observation was conducted during mandatory weekly status meetings, where technical and managerial issues were raised and discussed. Preliminary results show that many decisions made under the pressure of certain organizational factors negatively affected software quality. This paper describes cases depicting the complexity of organizational factors and reports on ten issues that have had a negative impact on quality, followed by suggested avenues for corrective action.
597F1F45	The contribution of formal modeling approaches in software development has always been a subject of debates. The proponents of model-driven development argue that big upfront designs although require substantial investment will payoff later in the implementation phase in terms of increased productivity and quality. On the other hand, software engineers who are not very keen on modeling perceive the activity as simply a waste of time and money without any real contribution to the final software product. Considering present advancement of model-based software development in software industry, we are challenged to investigate the real contribution of modeling in software development. Therefore, in this paper we report on an empirical investigation on the impact of UML modeling on the quality of software system. In particular, we focus on defect density as a measure of software quality. Based on a significant industrial case study, we have found that the use of UML modeling potentially reduces defect density in software system.
7C8C5DFC	The language PL360, together with its phrase structure grammar, is used as a concrete basis for illustrating an idea called syntax-directed documentation. This idea is: (1) to use the phrase structure of a program to define the structure of a formal documentation for that program; (2) to use the syntactic types and identifiers in the resulting structure to trigger the automatic formation of questions to the programmer, whose answers will become part of that documentation; and (3) to provide automatic storage and retrieval facilities so that other programmers who want to understand or modify the program can access the resulting documentation, which is cross-indexed in various ways by syntactic types and objects. A small PL360 program, already found in the literature, is worked out as an example.
7A94C4AC	The Unified Modeling Language (UML) is an object-oriented analysis and design language widely used to created artifacts during the software system lifecycle. UML being a standard notation, without specific guidelines as to how to use it, it must be applied in the context of a specific software development process. The Unified Process (UP) is one such process, extensively used by the object-oriented community, which delivers software best practices via guidelines for all software lifecycle activities. The UP suggests many artifacts to be produced during the software lifecycle. But many practitioners are reluctant to use those artifacts as they question their benefits. System Sequence Diagrams and System Operation Contracts are artifacts, suggested by Larman in his well-known methodology, to complement standard UP artifacts with the intent of better understanding the input and output events related to the system being designed. This paper presents the results of controlled experiments that investigate the impact of using these artifacts during software development. One way to do that is to study the extent to which those artifacts improve the quality of the Domain Model or reduce the effort necessary to complete this Domain Model. Results show that the use of those artifacts mildly improves the quality of the Domain Model, as long as sufficient training is provided. On the other hand, there is no noticeable evidence that those two artifacts reduce the time to produce the Domain Model.
79D4DFB3	Software development activities face several challenges when they take place in geographically distributed settings. These challenges are often related to communication, collaboration and, especially, information diffusion among team members. This is especially important in impact analysis activities where all stakeholders need to align their work to understand the impact of a change. To properly support this, it is necessary to understand the different ways in which impact analysis activities are performed. In our previous work we identified two views of impact analysis: an organizational and an individual view. The first view is important for managers to understand the impact of the changes in the activities of the team, whereas the second refers to the developers' strategies to deal with the impact of changes in their daily work. This paper discusses these views and based on them describes a tool, called Wolf, to support impact analysis activities. Our tool, based on semi-automatically generated traceability links, provides different visualizations to support communication, collaboration and information diffusion among stakeholders thereby facilitating impact analysis activities in geographically distributed settings.
75FFFC30	Although various success stories of model-based approaches are reported in literature, there is still a significant resistance to model-based development in many software organizations because the UML is perceived to be expensive and not necessarily cost-effective. It is also important to gather empirical evidence in which context and under which conditions the UML makes or does not make a practical difference.Our objective is to provide empirical evidence as to which UML diagrams are more helpful during software maintenance: Forward Designed (FD) UML diagrams or Reverse Engineered (RE) UML diagrams.We carried out a family of experiments which consisted of one experiment and two replications with a total of 169 Computer Science undergraduate students.The individual data analysis and the meta-analysis conducted on the whole family, show a tendency in favor of FD diagrams and are significantly different as regards the effectiveness and efficiency of the subjects who participated and played the role of maintainers. The analysis of the qualitative data, collected using a post-experiment survey, reveals that the subjects did not consider RE diagrams helpful.Our findings show that there are some objective results (descriptive statistics or statistical tests) related to the maintenance effectiveness and efficiency in favor of the use of FD UML diagrams during software maintenance. Subjective opinions also lead us to recommend the use of UML diagrams (especially class diagrams) created during the design phase for software maintenance because they improve the understanding of the system in comparison with RE diagrams. Nevertheless, we can only assume that these results are valid in the context of Computer Science undergraduate students when working with small systems related to well-known domains, and other contexts should be explored in order to reaffirm the results in an industrial context by carrying out replications with professionals.
7BDA9AF8	We carried out a family of experiments to investigate whether the use of UML models produced in the requirements analysis process helps in the comprehensibility and modifiability of source code. The family consists of a controlled experiment and 3 external replications carried out with students and professionals from Italy and Spain. 86 participants with different abilities and levels of experience with UML took part. The results of the experiments were integrated through the use of meta-analysis. The results of both the individual experiments and meta-analysis indicate that UML models produced in the requirements analysis process influence neither the comprehensibility of source code nor its modifiability.
7D3D1228	The paper describes the initial results of applying Latent Semantic Analysis (LSA) to program source code and associated documentation. Latent Semantic Analysis is a corpus-based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). The intent of applying Latent Semantic Analysis to software components is to automatically induce a specific semantic meaning of a given component. Here LSA is used as the basis to cluster software components. Results of applying this method to the LEDA library and MINIX operating system are given. Applying Latent Semantic Analysis to the domain of source code and internal documentation for the support of software reuse is a new application of this method and a departure from the normal application domain of natural language.
7D5A8C13	During its lifecycle, software experiences numerous changes that are either due to bug fixes or to the incorporation of new features. Implementing such changes is often more difficult than expected, which frequently leads to underestimation of the associated implementation effort. This paper describes work that aims to understand change impact based on the visualization of the characteristics of change requests using standard diagrams (e.g. UML diagrams). Each diagram depicts different aspects of a software system and can illustrate factors that might drive the implementation effort. Using examples, in which we analyze three change requests, we describe how we determined the characteristics of the change requests. We furthermore discuss how the impact of change requests on diagrams relates to the implementation effort. Our overall goal is to develop a new impact analysis approach that helps determine software change impact based on the use of specific diagrams
7734A618	This paper presents a research proposal on how and why we are investigating the benefits of the Unified Modelling Language (UML) in software maintenance tasks. The principal objective is to present the main research questions, along with an explanation of which research methods we propose to use to obtain the answers to these questions in an empirical manner.
7F23FE81	A typical approach to software maintenance is analyzing just the source code, applying some patches, releasing the new version, and then updating the documentation. This quick-fix approach usually leads to documentation not aligned with the current system and degrades the original system structure, thus rendering the evolution of the system costly and error-prone. Although there are alternative maintenance models which avoid these problems, by analyzing and updating the system documentation first, the quick-fix approach continues to be popular because of the time pressure for new releases and the resistance to change of maintenance programmers. In this paper, we propose an iterative reengineering model which can be run each time the maintainability and reliability of a software system degrade under a tolerance level. The reengineering process, applied after a number of modifications, can result in renovation of the current system or simply in realignment of the documentation. In this context, reengineering is no longer a one-shot process but becomes an ordinary process which runs concurrently with the quick-fix maintenance process. The results obtained with an industrial case study are presented and the lessons learned are discussed.
7DC42E05	The paper describes the results of applying Latent Semantic Analysis (LSA), an advanced information retrieval method, to program source code and associated documentation. Latent semantic analysis is a corpus based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). Here LSA is used as the basis to cluster software components. This clustering is used to assist in the understanding of a nontrivial software system, namely a version of Mosaic. Applying latent semantic analysis to the domain of source code and internal documentation for the support of program understanding is a new application of this method and a departure from the normal application domain of natural language.
807BAAFB	 In order to find out the state of software maintenance in Hong Kong, we have conducted a questionnaire survey, supplemented by telephone interviews and industrial visits. Our results indicate that, in Hong Kong, about 66% of the total software life cycle cost is spent on software maintenance. The average application system is about 5.1 years old, consisting of 577 programs and 308,000 lines of code. The most often cited problems in software maintenance are staff turnover, poor documentation and changing user requirements. We also observed that software maintenance work has not been given proper recognition. This paper attempts to contrast our survey findings with that of a similar survey conducted in USA. We also present some initial results of a series of experiments to compare OO with 4GL and conventional development approaches.
7FF0E7E6	The Unified Modeling Language (UML) is becoming the de facto standard for software analysis and design modeling. However, there is still significant resistance to model-driven development in many software organizations because it is perceived to be expensive and not necessarily cost-effective. Hence, it is important to investigate the benefits obtained from modeling. As a first step in this direction, this paper reports on controlled experiments, spanning two locations, that investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements in the functional correctness of changes as well as the quality of their design. However, there does not seem to be any saving of time. For simpler tasks, the time needed to update the UML documentation may be substantial compared with the potential benefits, thus motivating the need for UML tools with better support for software maintenance
7CB7F8D7	The Unified Modeling Language (UML) is a family of design notations that is rapidly becoming a de facto standard software design language. UML provides a variety of useful capabilities to the software designer, including multiple, interrelated design views, a semiformal semantics expressed as a UML meta model, and an associated language for expressing formal logic constraints on design elements. The primary goal of this work is an assessment of UML's expressive power for modeling software architectures in the manner in which a number of existing software architecture description languages (ADLs) model architectures. This paper presents two strategies for supporting architectural concerns within UML. One strategy involves using UML "as is," while the other incorporates useful features of existing ADLs as UML extensions. We discuss the applicability, strengths, and weaknesses of the two strategies. The strategies are applied on three ADLs that, as a whole, represent a broad cross-section of present-day ADL capabilities. One conclusion of our work is that UML currently lacks support for capturing and exploiting certain architectural concerns whose importance has been demonstrated through the research and practice of software architectures. In particular, UML lacks direct support for modeling and exploiting architectural styles, explicit software connectors, and local and global architectural constraints.
7E9CC159	traceMaintainer is a tool that supports an approach for maintaining post-requirements traceability relations after changes have been made to traced model elements. The update of traceability relations is based upon predefined rules, where each rule is intended to recognize a development activity applied to a model element. Little manual effort or interaction with the developer is required. traceMaintainer can currently be used with a number of commercial software development tools and enables the update of traceability relations stored within these tools. This paper provides an overview of traceMaintainer's architecture and major components.
7A36EB5E	Software architecture design documentation should communicate design decisions effectively. However, little is known about the way recipients respond to the different types of media used in documentation. We therefore conducted a controlled experiment to study whether visual or textual artifacts are more effective in communicating architecture software design decisions to software developers. Our participant group consisted of 47 participants from both industry and academia. Our results show that neither diagrams nor textual descriptions proved to be significantly more efficient in terms of communicating software architecture design decisions. Remarkably, participants who predominantly used text, scored significantly better, overall and with respect to topology related questions. Furthermore, surprisingly, diagrams were not able to alleviate the difficulties participants with a native language other than English had in extracting information from the documentation. In combination, these findings at the very least question the role of diagrams in software architecture documentation.
81223C97	An accurate set of traceability relations between software development artifacts is desirable to support evolutionary development. However, even where an initial set of traceability relations has been established, their maintenance during subsequent development activities is time consuming and error prone, which results in traceability decay. This paper focuses solely on the problem of maintaining a set of traceability relations in the face of evolutionary change, irrespective of whether generated manually or via automated techniques, and it limits its scope to UML-driven development activities post-requirements specification. The paper proposes an approach for the automated update of existing traceability relations after changes have been made to UML analysis and design models. The update is based upon predefined rules that recognize elementary change events as constituent steps of broader development activities. A prototype traceMaintainer has been developed to demonstrate the approach. Currently, traceMaintainer can be used with two commercial software development tools to maintain their traceability relations. The prototype has been used in two experiments. The results are discussed and our ongoing work is summarized.
7F8651AD	Modern large-scale software development is a complex undertaking and coordinating various processes is crucial to achieve efficiency. The alignment between requirements and test activities is one very important aspect. Production and maintenance of software result in an ever-increasing amount of information. To be able to work efficiently under such circumstances, navigation in all available data needs support. Maintaining traceability links between software artifacts is one approach to structure the information space and support this challenge. Many researchers have proposed traceability recovery by applying information retrieval (IR) methods, utilizing the fact that artifacts often have textual content in natural language. Case studies have showed promising results, but no large-scale in vivo evaluations have been made. Currently, there is a trend among our industrial partners to move to a specific new software engineering tool. Their aim is to collect different pieces of information in one system. Our ambition is to develop an IR-based traceability recovery plug-in to this tool. From this position, right in the middle of a real industrial setting, many interesting observations could be made. This would allow a unique evaluation of the usefulness of the IR-based approach.
760A508C	For most managers setting out to acquire a new MRP system for their company, the review and evaluation of product documentation is a critical first step in the acquisition and implementation process. Objective data is needed in order to narrow down the available alternatives. This study investigates how vendors describe and differentiate their products within their promotional literature, and what claims they make for them. Evaluation of documentation covering 69 manufacturing software products shows that vendors are (1) targeting products across very broad industry groups, (2) emphasizing service-related attributes in client relationships, but (3) are ignoring implementation issues, and (4) are failing to provide explicit advice to prospective users on potential costs and benefits.
79AEDE90	This  workshop  will demonstrate  and  explain  the creation  of  documentation  for  Aspect-Oriented programs   written   in   Java.   Fundamentals   of Aspect-Oriented   Programming,   separation   of concerns,  and  as  well  as  motivation  for  Aspect-Oriented Programming , will be presented.
7D52E96E	An approach to storing source code using a relational database is described. The goal of this approach is to assist in the maintenance of source code, especially in producing cross references documentation. Why this approach was chosen and how the system's architecture was set up are explained. The emphasis, however, is placed upon the results obtained by using this system, in terms of volume and response time.
7D935EA2	An intelligent, knowledge-based maintenance tool is described. The tool is intended to help reduce the amount of time spent on analyzing code. Code analysis is performed when a programmer is familiarizing himself with a piece of code and when the effects of a proposed modification of the code is being assessed. The research project is described, and its theoretical foundation is examined.
7BAA01F5	The importance of software documentation and the effects of poor documentation in data processing are often underrated. Little research has been published that evaluates the quality of software documentation. The evaluation of textual attributes such as comprehensibility, readability, etc., has seen more studies and relatively sound results. Thus this paper exclusively handles textual documentation, whereby the methods are employed for the evaluation of software documentation. We first introduce the methods used in the measurement of text comprehensibility. Then RMS (Readability Measuring System), a tool developed by the author to support the measuring process, is presented. The third part of the paper presents empirical results and discusses the experience gained in the application of the RMS tool.
806FB17F	Graphical documentation is often characterized as an effective aid in program understanding. However, it is an open question exactly which types of graphical documentation are most suitable for which types of program understanding tasks (and in which specific usage contexts). The Unified Modeling Language (UML) is the de facto standard for modeling modern software applications. This paper describes an experiment to assess the qualitative efficacy of UML diagrams in aiding program understanding. The experiment had participants analyze a series of UML diagrams and answer a detailed questionnaire concerning a hypothetical software system. Results from the experiment suggest that the UML's efficacy in support of program understanding is limited by factors such as ill-defined syntax and semantics, spatial layout, and domain knowledge.
7DD2EFFC	For anything but the simplest of software systems, the ease and costs associated with change management can become critical to the success of a project. Establishing traceability initially can demand questionable effort, but sustaining this traceability as changes occur can be a neglected matter altogether. Without conscious effort, traceability relations become increasingly inaccurate and irrelevant as the artifacts they associate evolve. Based upon the observation that there are finite types of development activity that appear to impact traceability when software development proceeds through the construction and refinement of UML models, we have developed an approach to automate traceability maintenance in such contexts. Within this paper, we describe the technical details behind the recognition of these development activities, a task upon which our automated approach depends, and we discuss how we have validated this aspect of the work to date.
5DB778AF	Model Driven Architecture (MDA) was born as a standard of Model Driven Development (MDD) to increment the productivity in the software industry. Actually, there exist different models based tools to get an acceptable degree of automation in the applications generation. Many of those tools allow users to build software with high productivity in their respective domains. According their authors, the very challenge of MDA approach and tools is to put emphasis in a satisfactory degree of productivity and, at the same time, increasing others important software qualities such as portability, interoperability, reusability and maintainability. This paper intends to verify the effectiveness of such benefits analyzing the development of a real application using a MDA tool compared with a traditional approach. For analysis purpose, a series of metrics that can help to evaluate the different aspects related to the benefits of MDA was adopted.
802BC80F	Traditionally, diagrams used in software systems modelling have been two dimensional (2D). This is probably because graphical notations, such as those used in object-oriented and structured systems modelling, draw upon the topological graph metaphor, which, at its basic form, receives little benefit from three dimensional (3D) rendering. This paper presents a series of 3D graphical notations demonstrating effective use of the third dimension in modelling. This is done by e.g. connecting several graphs together, or in using the Z co-ordinate to show special kinds of edges. Each notation combines several familiar 2D diagrams, which can be reproduced from 2D projections of the 3D model. 3D models are useful even in the absence of a powerful graphical workstation: even 2D stereoscopic projections can expose more information than a plain planar diagram.
7BBBFC44	When compared to traditional development methods, agile development practices are associated with more direct communication and less documentation. However, few empirical studies exist that investigate the role of documentation in agile development teams. We thus employed a questionnaire to measure the perceptions of a group of agile practitioners with regard to the documentation in their projects. We obtained responses from 79 agile software development professionals and 8 teams in 13 different countries. Our findings include that over half of developers in our data set find documentation important or even very important but that too little documentation is available in their projects. Agile practitioners do not seem to agree with the agile principle that "The most efficient and effective method of conveying information to and within a development team is face-to-face conversation." We were able to validate this result for a set of dissimilar agile teams in various domains.
7C056534	A Component Based System (CBS) is integration centric with a focus on selecting individual components that match stakeholder requirements. In reality, components are usually designed for general purposes and finding the ideal one is often very difficult. Fundamental to CBS success is the ability to minimize the mismatch between stakeholder expectations and functionality provided by available components. Recent research indicates that the Framework for Integrated Test (Fit), a test driven development, helps to better understand customer requirements. This paper presents an initial study to investigate the potential use of acceptance tests (Fit tables) during the component selection process. The analysis shows that Fit tables help improve the component selection phase of a CBS development life cycle.
7EBEFDF1	A KBRPS (knowledge-based rapid prototyping system) system for automated support of software documentation his been developed. KBRPS contains a graphic representation of a conceptual model for modeling system structures; a language, FSRSL (frame-based software requirements specification language) for describing the textual form of the conceptual model and further specifying detailed system behaviors; a database for stored specifications files; and a knowledge base for stored rules of software development. The FSRSL specifications stored in the database can be retrieved by a query system for generating formal documents. These documents, which are helpful for understanding the maintained system, include the conceptual model, the FSRSL specifications, the abstract relations of hierarchical specifications, and even the answers to particular questions.
7D0A7A81	The importance of accurate technical documentation to the maintenance process is presented. A possible solution to the lack of adequate technical documentation is suggested in the form of a partially automated in-line documentation system. This system uses software metrics to determine where comments should be placed in source programs. The system can be used as a tool in software development and/or software maintenance.
7D001A43	Automated code generators are increasingly used in safety-critical applications, but since they are typically not qualified, the generated code must still be fully tested, reviewed, and certified. For mathematical and engineering software this requires reviewers to trace subtle details of textbook formulas and algorithms to the code, and to match requirements (e.g., physical units or coordinate frames) not represented explicitly in models or code. We support these tasks by using the AutoCert verification system to identify and verify mathematical concepts in the code, recovering verified traceability links between concepts, code, and verification conditions. We then exploit these links to construct a natural language report that provides a high-level structured argument explaining where the code uses specified assumptions and why and how it complies with the requirements. We have applied our approach to generate review documents for several sub-systems of NASA's Project Constellation.
7E96AE65	Programmers often use graphical forms of documentation that rely on software visualization techniques to make complicated information easier to understand. However, it is an open question exactly which types of graphical documentation are most suitable for which types of program understanding tasks (and in which specific usage contexts). The Unified Modeling Language (UML) is the de facto standard for modeling modern software applications. This paper focuses on discussing techniques for the efficacy of UML diagrams in aid of program understanding. The majority of the workshop will involve participants analyzing a series of UML diagrams and answering a detailed questionnaire concerning a hypothetical software system.
7FCFB6D5	This paper focuses on documentation practices in scientific software development and takes into account two perspectives: that of scientists who develop software (scientists-developers) and of those scientists who use it. We describe documentation practices of scientists-developers. We also explore the role of users' community in scientific software documentation. We compare our findings to studies about documentation practices of professional software developers and discuss how production of documentation in scientific software development can be supported.
7E5BF652	A new metric for evaluating the cost effectiveness of technical reviews is described. The proposed metric is based on the degree to which testing costs are reduced by technical reviews. The metric can be interpreted as combining two conventional metrics. Using an experimental evaluation of the conventional metrics and the proposed metric for data collected in an industrial environment, the authors show the validity and usefulness of the proposed metric. In particular, they present a method to estimate a value of the proposed metric by using only the values obtained at review phase.
76677ED7	Agile and lean software development methodologies appear as a popular alternative to plan-driven methodologies, but these methodologies have no structure in the process of user requirements specification. This paper shows how Method for Elicitation, Documentation and Validation of Software User Requirements (MEDoV) supports agile and lean software development methodologies. The MEDoV helps stakeholders in their everyday work with minimal impact on agility. The method also ensures an active role of business users, a wide-picture what usually is the problem for agile development, and definition of non-functional requirements, what the even bigger problem is. The MEDoV ensures that no unnecessary features are produced so that no extra code is created, and maintenance is easier, as well as code correction and improvement. Using models enables agility in product maintenance, especially for integrated systems where one change can have multiple impacts on different parts of system.
7D09D29E	In this paper, we present our approach for automatic SystemC code generation from UML models at early stages of Systems On Chip (SOC) design. A particularity of our proposed approach is the fact that SystemC code generation process is performed through two levels of abstraction. In the first level, we use UML hierarchic sequence diagrams to generate a SystemC code that targets algorithmic space exploration and simulation. In the second level of abstraction, messages that occur in sequence diagrams are implemented using UML activity diagrams whose actions are expressed in the C++ Action Language (AL) included in the Rhapsody environment from which a full SystemC code is generated for both simulation and synthesis.
5B6A5210	We give a new description of the so-called hyperbolic codes from which the minimum distance and the generator matrix are easily determined. We also give a method for the determination of the dimension of the codes and finally some results on the weight hierarchy are presented.
7CCEC64C	The instruction-set of a target architecture is represented as a set of attribute-grammar productions. A code generator is obtained automatically for any compiler using attributed parsing techniques. A compiler built on this model can automatically perform most popular machine-dependent optimizations, including peephole optimizations. The code generator is also easily retargetable to different machine architectures.
7B6281C3	This paper discusses algorithms which transform expression trees into code for register machines. A necessary and sufficient condition for optimality of such an algorithm is derived, which applies to a broad class of machines. A dynamic programming algorithm is then presented which produces optimal code for any machine in this class; this algorithm runs in time linearly proportional to the size of the input.
7D46C75E	A concept for the automatic generation of opti- mization code for a class of non-linear optimization problems is described and realized at the example of an electric power system optimal power flow problem. The equations are structured based on a node and edge structure given from a network. The goal of this domain engineering approach is the high-level symbolic formulation of this structured op- timization problem and the subsequent complete automatic code generation of the solution algorithm in Matlab. The main algorithmic step is the iterative solution of a sparse lin- ear system of equations applied to the Karush-Kuhn-Tucker optimality conditions of the optimization problem. The ma- trix elements of this linear system to be solved during the solution process consist of sums of first and second order derivative terms of the original, high-level entered function parts. Applying this concept leads to a high quality domain software which seems to form a good compromise both for the developer and the software end-user organisation: High quality requirements can be satisfied with respect to speed, algorithmic robustness, easy core code (model) enhancement and maintenance capabilities for the developer and easy end- user model software parametrization.
7587A118	The problem of generating "optimal" programs for the evaluation of arithmetic expres- sions on a machine with a finite depth stack is studied. Efficient algorithms are given for constructing optimal programs in the case where the expressions are trees, there are no data dependencies, and the operators have limited algebraic properties.
7A487701	We propose a new family of asymptotically good binary codes, generalizing previous constructions of expander codes to t-uniform hypergraphs. We also describe an efficient decoding algorithm for these codes, that for a certain region of rates improves the known results for decoding distance of expander codes.The construction is based on hypergraphs with a certain “expansion” property called herein ϵ-homogeneity. For t-uniform t-partite Δ-regular hypergraphs, the expansion property required is roughly as follows: given t sets, A1,…,At, one at each side, the number of hyper-edges with one vertex in each set is approximately what would be expected had the edges been chosen at random. We show that in an appropriate random model, almost all hypergraphs have this property, and also present an explicit construction of such hypergraphs.Having a family of such hypergraphs, and a small code C0{0,1}Δ, with relative distance δ0 and rate R0, we construct “hypergraphs codes”. These have rate R0−(t−1), and relative distance ≥δ0t/(t−1)−o(1). When t=2l we also suggest a decoding algorithm, and prove that the fraction of errors that it decodes correctly is at least. In both cases, the o(1) is an additive term that tends to 0 as the length of the hypergraph code tends to infinity.
7564799E	We have constructed a local code generator for the VAX-112 using a parser-like instruction pattern matcher. The code generator replaces the second pass of the UNIX3 Portable “Crdquo; compiler. This paper describes the design of the code generator and the special considerations imposed by the pattern matching process. We summarize the structure of the machine description grammar and its associated semantic actions, as well as the tools we developed to manipulate the large VAX description. In our experience, this approach makes the instruction selection phase of the compiler easier and faster to implement, and more likely to be correct than traditional techniques.
7890E2BC	An automatic FORTRAN code generation and determination of energy eigenvalues for periodic potentials with wells and barriers are presented. These problems serve as test cases in developing tools and techniques for generation of numerically efficient FORTRAN software and for more general; efficient and flexible ways of programming. Several problems in different areas of science are described, and the advantages of the approach as well as limitations of the tools presently available are discussed.
80FE0294	Model driven software development, domain specific languages and other generative programming approaches have gained much attention. There is a large number of tools available, starting from simple code generators to full-blown tool suites. There are several success stories and process frameworks about applying full scale MDSD approaches, but there is no or little help for small-size projects. To fill this gap, we designed a new development process which targets small and middle size projects with limited resources. According to our experience the process is working efficiently in small projects even if they cannot afford bigger initial resource investment. In this experince report we document an industrial project in which the process was used. We present this process in the form which is applied in the first project and also discuss our experience and the limitations of the process.
8168E903	Minimum mean squared error (MMSE) fixed-lag smoothing is used in conjunction with DPCM (differential pulse code modulation) to develop a code generator employing delayed decoding. This smoothed DPCM (SDPCM) code generator is compared to DPCM and interpolative DPCM (IDPCM) code generators at rates 1 and 2 for tree coding several synthetic sources, as well as to a DPCM code/generator at rate 2 for speech sources. The (M,L) algorithm is used for tree searching, and SDPCM outperforms IDPCM and DPCM at rate 2 for synthetic sources with M=1, 4, 8, and 12, and at rate 1 with M>or=4. For speech, SDPCM provides a slight improvement in MSE (mean squared error) over DPCM codes, which is also evident in sound spectrograms and listening tests.
7CE0258A	This paper describes a tool called vpoiso that was developed to isolate errors automatically in the vpo compiler system. The two general types of compiler errors isolated by this tool are optimization and nonoptimization errors. When isolating optimization errors, vpoiso relies on the vpo optimizer to identify sequences of changes, referred to as transformations, that result in semantically equivalent code and to provide the ability to stop performing improving (or unnecessary) transformations after a specified number have been performed. A compilation of a typical program by vpo often results in thousands of improving transformations being performed. The vpoiso tool can automatically isolate the first improving transformation that causes incorrect output of the execution of the compiled programs by using a binary search that varies the number of improving transformation performed. Not only is the illegal transformation automatically isolated, but vpoiso also identifies the location and instant the transformation is performed in vpo. Nonoptimization errors occur from problems in the front end, code generator, and necessary transformations in the optimizer. If another compiler is available that can produce correct (but perhaps more inefficient) code, then vpoiso can isolate nonoptimization errors to a single function. Automatic isolation of compiler errors facilitates retargeting a compiler to a new machine, maintenance of the compiler, and supporting experimentation with new optimizations.
7E962882	Designing an efficient floating-point implementation of a function based on polynomial evaluation requires being able to find an accurate enough evaluation code, exploiting at most the target architecture features. This article introduces CGPE, a tool dealing with the generation of fast and certified codes for the evaluation of bivariate polynomials. First we discuss the issue underlying the evaluation scheme combinatorics before giving an overview of the CGPE tool. The approach we propose consists in two steps: the generation of evaluation schemes by using some heuristics so as to quickly find some of low latency, and the selection that mainly consists in automatically checking their scheduling on the given target and validating their accuracy. Then, we present on-going development and ideas for possible improvements of the whole process. Finally, we illustrate the use of CGPE on some examples, and show how it allows us to generate fast and certified codes in a few seconds and thus to reduce the development time of libms like FLIP.
80F84BF0	An algorithm is given to translate a relatively low-level intermediate representation of a program into assembly code or machine code for a target computer. The algorithm is table driven. A construction algorithm is used to produce the table from a functional description of the target machine. The method produces high quality code for many commercially available computers. By replacing the table, it is possible to retarget a compiler for another kind of computer. In addition techniques are given to prove the correctness of the translator.
77BC50F5	Digital signal processors provide specialized SIMD (single instruction multiple data) operations designed to dramatically increase performance in embedded systems. While these operations are simple to understand, their unusual functions and their parallelism make it difficult for automatic code generation algorithms to use them effectively. In this paper, we present a new optimizing code generation method that can deploy these operations successfully while also verifying that the generated code is a correct translation of the input program.
7E8DE245	Unlike for conventional compilers for imperative programming languages such as C or ADA, no establishedmethods for safeguarding artifacts generated by model-based code generators exist despite progress in the field of formalverification. Several test approaches dominate the engineering practice. This paper describes a general and toolindependenttest architecture for code generators used in model-based development. We evaluate the effectiveness of ourtest approach by means of testing optimizations performed by the TargetLink code generator, a widely accepted andcomplex development tool used in automotive model-based development.
78EE79D2	This paper presents some code generation issues in the context of the PQCC Production-Quality Compiler-Compiler project [8]. The approach taken is unusual in several ways. The machine-dependent information for selection of code sequences, register assignments, etc., has been separated throughout, in tabular form, from the machine-independent algorithms. This not only greatly simplifies the development of code generators for new machines or languages, but paves the way for automatic generation of these tables from formal machine descriptions such as ISP [1]. A parse-tree-like internal program representation is used, facilitating the use of context and data dependency information about expressions. The code generation process has been broken into several phases. This leads to simplification and better understanding of the code generation process, and also allows important improvements in the quality of generated code. The algorithms for preliminary determination of addressing modes, allocation of registers and other locations, and the code selection case analysis are dlscussed. The algorithms described In the paper are being implemented and used in the PQCC compiler.
7659FCEF	Affix grammars are used to describe the instruction set of a target architecture for purposes of compiler code generation. A code generator is obtained automatically for a compiler using attributed parsing techniques. A compiler built on this model can automatically perform most popular machine-dependent optimizations, including peephole optimizations. Code generators based on this model demonstrate retargetability for the VAX1-11, iAPX2-86, Z-80003, PDP4-11, MC-68000, NS32032, FOM, and IBM-370 architectures.
8614DF9E	We introduce the paradigm of schedule-carrying code (SCC). A hard real-time program can be executed on a given platform only if there exists a feasible schedule for the real-time tasks of the program. Traditionally, a scheduler determines the existence of a feasible schedule according to some scheduling strategy. With SCC, a compiler proves the existence of a feasible schedule by generating executable code that is attached to the program and represents its schedule. An SCC executable is a real-time program that carries its schedule as code, which is produced once and can be revalidated and executed with each use. We evaluate SCC both in theory and practice. In theory, we give two scenarios, of nonpreemptive and distributed scheduling for Giotto programs, where the generation of a feasible schedule is hard, while the validation of scheduling instructions that are attached to the programs is easy. In practice, we implement SCC and show that explicit scheduling instructions can reduce the scheduling overhead up to 35% and can provide an efficient, flexible, and verifiable means for compiling Giotto programs on complex architectures, such as the TTA.
7B6BC907	Production code generation with Model-Based Design has successfully replaced manual coding across various industries and application domains. Furthermore, code generated from executable graphical models is increasingly being deployed in high-integrity embedded applications.To validate the model-to-code translation process, generated software components and its precursory stages (i.e. models) should be subjected to an appropriate combination of quality assurance measures. For high-integrity applications, compliance with safety standards such as IEC 61508 needs to be demonstrated as well.On principle, translation validation of generated code could be carried out in the same manner as for manually written code. However, this would not leverage the advantages of Model-Based Design and w.r.t. process efficiency this would leave something to be desired. Therefore, engineering methods and tools for effective and efficient translation validation of generated code are highly desirable. As a step towards this goal, a workflow for verification and validation of models and generated code will be proposed and as far as possible mapped onto the objectives of IEC 61508-3. A cornerstone of this workflow is testing for numerical equivalence between models and generated code.
7C6E07EA	Work with compiler compilers has dealt principally with automatic generation of parsers and lexical analyzers. Until recently, little work has been done on formalizing and generating the back end of a compiler, particularly an optimizing compiler. This paper describes formalizations of machines and code generators and describes a scheme for the automatic derivation of code generators from machine descriptions. It was possible to separate all machine dependence from the code generation algorithms for a wide range of typical architectures (IBM-360, PDP-11, PDP-10, Intel 8080) while retaining good code quality. Heuristic search methods from work in artificial intelligence were found to be both fast and general enough for use in generation of code generators with the machine representation proposed. A scheme is proposed to perform as much analysis as possible at code generator generation time, resulting in a fast pattern-matching code generator. The algorithms and representations were implemented to test their practicality in use.
7FE5061F	The use of fiber-optic lattices for the generation of quasi-prime codes is described, and their application to CDMA communications is discussed. Theoretical and experimental results (including typical autocorrelation and crosscorrelation performance) are presented for the programmable all-optical generation and decoding of such codes using electrooptically switchable lattices.
78E903FC	Distributed applications typically interact with a number of heterogeneous and autonomous components that evolve independently. Methodical development of such applications can benefit from approaches based on domain-specific languages (DSLs). However, the evolution and customization of heterogeneous components introduces significant challenges to accommodating the syntax and semantics of a DSL in addition to the heterogeneous platforms on which they must run. In this paper, we address the challenge of implementing code generators for two such DSLs that are flexible (resilient to changes in generators or input formats), extensible (able to support multiple output targets and multiple input variants), and modular (generated code can be rewritten). Our approach, Clearwater, leverages XML and XSLT standards: XML supports extensibility and mutability for inprogress specification formats, and XSLT provides flexibility and extensibility for multiple target languages. Modularity arises from using XML meta-tags in the code generator itself, which supports controlled addition, subtraction, or replacement to the generated code via XML-weaving. We discuss the use of our approach and show its advantages in two non-trivial code generators: the Infopipe Stub Generator (ISG) to support distributed flow applications, and the Automated Composable Code Translator to support automated distributed application deployment. As an example, the ISG accepts as input an XML description and generates output for C, C++, or Java using a number of communications platforms such as sockets and publish-subscribe. 
7E09E3E2	Emerging embedded systems require heterogeneous multiprocessor SoC architectures that can satisfy both high-performance and programmability. However, as the complexity of embedded systems increases, software programming on an increasing number of multiprocessors faces several critical problems, such as multithreaded code generation, heterogeneous architecture adaptation, short design time, and low cost implementation. In this paper, we present a software code generation flow based on Simulink to address these problems. We propose a functional modeling style to capture data-intensive and control-dependent target applications, and a system architecture modeling style to seamlessly transform the functional model into the target architecture. Both models are described using Simulink. From a system architecture Simulink model, a code generator produces a multithreaded code, inserting thread and communication primitives to abstract the heterogeneity of the target architecture. In addition, the multithread code generator called LESCEA applies the extensions of dataflow based memory optimization techniques, considering both data and control dependency. Experimental results on a Motion-JPEG decoder and an H.264 decoder show that the proposed multithread code generator enables easy software programming on different multiprocessor architectures with substantially reduced data memory size (up to 68.0%) and code memory size (up to 15.9%). 
76DE5053	Activity Diagram is an important component of the set of diagrams used in UML. The OMG document on UML 2.0 proposes a Petri net based semantics for Activity Diagrams. While Petri net based approach is useful and interesting, it does not exploit the underlying inherent reactive behaviour of activity diagrams. In the rst part of the paper, we shall capture activity diagrams in synchronous language framework to arrive at executional models which will be useful in model based design of software. This also enables validated code generation using code generation mechanism of synchronous language environments such as Esterel and its programming environments. Further, the framework leads to scalable verication methods. The traditional semantics proposed in OMG standard need enrichment when the activ-ities are prone to failure and need compensating actions. Such extensions are expected to have applications in modelling complex business processes. In the second part of the paper, we propose an enrichment of the UML Activity Diagrams that include compensable actions. We shall use some of the foundations on Compensable Trans-actions and Communicating Sequential Processes due to Tony Hoare. This enriched formalism allows UML Activity Diagrams to model business processes that can fail and require compensating actions.
7AA248FE	Compiler-component generators, such as lexical analyzer generators and parser generators, have long been used to facilitate the construction of compilers. A tree-manipulation language called twig has been developed to help construct efficient code generators. Twig transforms a tree-translation scheme into a code generator that combines a fast top-down tree-pattern matching algorithm with dynamic programming. Twig has been used to specify and construct code generators for several experimental compilers targeted for different machines.
7756D700	We present an on-the-fly mechanism that detects access conflicts in executions of multi-threaded Java programs. Access conflicts are a conservative approximation of data races. The checker tracks access information at the level of objects (object races) rather than at the level of individual variables. This viewpoint allows the checker to exploit specific properties of object-oriented programs for optimization by restricting dynamic checks to those objects that are identified by escape analysis as potentially shared. The checker has been implemented in collaboration with an "ahead-of-time"Java compiler. The combination fo static program analysis (escape-analysis) and inline instrumentation during code generation allows us to reduce the runtime overhead of detecting access conflicts. This overhead amounts to about 16-129% in time and less than 25% in space for typical benchmark applications and compares favorably to previously published on-the-fly mechanism that incurred an overhead of about a factor of 2-80 in time and up to a factor of 2 in space.
59A65B29	The FALCON development environment was designed around three basic data representations: scalars, vectors, and dense matrices. Utilizing the FALCON interactive restructuring system, the environment has been enhanced to allow the identification of structures within sparse matrices, such as diagonal matrices or symmetric matrices, and the use of this information for improving performance of the generated code. In addition, the environment supports the modification of the representation of the data. Such modifications have been shown to provide significant performance improvements.
7D597C9A	Fixed-point DSPs are a class of embedded processors with highly irregular architectures. This irregularity makes it difficult to generate high-quality machine code from programming languages such as C. In this paper we present a novel constraint driven approach to code selection for irregular processor architectures, which provides a twofold improvement of earlier work. First, it handles complete data flow graphs instead of trees and thereby generates better code in the presence of common subexpressions. Second, the presented technique is not restricted to computation of a single solution, but it generates alternative solutions. This feature enables the tight coupling of different code generation phases, resulting in better exploitation of instruction-level parallelism. Experimental results indicate that our technique is capable of generating machine code that competes well with handwritten assembly code.
756DBD80	Product  (AN)   codes   constructed   in   weighted   number   systems   are  investigatedwith the aim of devising error control features suitable for application in arithmetic units.The previous theoretical framework, which was derived in the hypothesis of codes defined ina virtual range M = k A, is restated for any physical interval  M = bn, where b is the radix ofthe system and n is the number of digits.Some  general   properties   holding  for   radix-b-AN  codes   are  reconsidered   and  necessaryand sufficient conditions for single and double error detection are derived for binary codesand for  a  sample non binary case.  Single  error  correction is discussed  as  well  and  a  fasterror  decoding procedure is suggested and implemented.Finally, modular AN codes are introduced in order to enable the use of product codes instandard ALU's representing relative integers by means of a radix complement notation. It isshown   that    the  above     properties   keep   their   validity   and  that   concurrent   single   errorcorrection can be associated with arithmetic computation without increasing the time spent
for processing.
7A8C9B1F	In this paper we address the problem of code generation for basic blocks in heterogeneous memory-register DSP processors. We propose a new a technique, based on register-transfer paths, that can be used for efficiently dismantling basic block DAGs (Directed Acyclic Graphs) into expression trees. This approach builds on recent results which report optimal code generation algorithm for expression trees for these architectures. This technique has been implemented and experimentally validated for the TMS320C25, a popular fixed point DSP processor. The results show that good code quality can be obtained using the proposed technique. An analysis of the type of DAGs found in the DSPstone benchmark programs reveals that the majority of basic blocks in this benchmark set are expression trees and leaf DAGs. This leads to our claim that tree based algorithms, like the one described in this paper, should be the technique of choice for basic blocks code generation with heterogeneous memory register architectures.
7B770A00	Luby Transform (LT) codes are becoming increasingly important in transmission due to the ability to protect the source data in high reliability economically. LT codes generate rateless packets on-the-fly through randomly chosen encoding degree, so it cannot control or manage the service quality according to distinct service demands of clients. In this article, we extended LT codes with a service-driven control policy. The proposed encoder arranges the encoding degree through the basic building structures via various control policies to fulfill the requests of majority of clients. With the proposed control policies in the multicasting or broadcasting channel, source data of most clients can be recovered as soon as possible. Besides, comparing to LT codes in point-to-point protocol, the proposed codes also introduce lower decoding overhead, better intermediate performance, and built-in unequal error protection property through appropriately arranging the placements of source data in the coding graph.
8088DBFF	The class of codes discussed in this paper has the property that its error-correction capability is described in terms of correcting errors in specific digits of a code word even though other digits in the code may be decoded incorrectly. To each digit of the code words is assigned an error protection levelf_{i}. Then, ifferrors occur in the reception of a code word, all digits which have protectionf_{i}greater than or equal tofwill be decoded correctly even though the entire code word may not be decoded correctly. Methods for synthesizing these codes are described and illustrated by examples. One method of synthesis involves combining the parity check matrices of two or more ordinary random error-correcting codes to form the parity check matrix of the new code. A decoding algorithm based upon the decoding algorithms of the component codes is presented. A second method of code generation is described which follows from the observation that for a linear code, the columns of the parity check matrix corresponding to the check positions must span the column space of the matrix. Upper and lower bounds are derived for the number of check digits required for such codes. The lower bound is based upon counting the number of unique syndromes required for a specified error-correction capability. The upper bound is the result of a constructive procedure for forming the parity check matrices of these codes. Tables of numerical values for the upper and lower bounds are presented.
033B186E	Code generators are widely used in the development of embedded software to automatically generate executable code from graphical specifications. However, at present, code generators are not as mature as classical compilers and they need to be extensively tested. This paper proposes a technique for systematically deriving suitable test cases for code generators, involving the interaction of chosen sets of rules. This is done by formalising the behaviour of a code generator by means of graph transformation rules and exploiting unfolding-based techniques. Since the representation of code generators in terms of graph grammars typically makes use of rules with negative application conditions, the unfolding approach is extended to deal with this feature.
817790BF	This paper describes a compiler with a code generator and machine-directed peephole optimizer that are tightly integrated. Both functions are performed by a single rule-based rewriting system that matches and replaces patterns. This organization helps make the compiler simple, fast, and retargetable. It also corrects certain phase-ordering problems.
760F714F	This paper investigates the problem of aligning array data and processes in a distributed-memory implementation. We present complete algorithms for compile-time analysis, the necessary program restructuring, and subsequent code-generation, and discuss their complexity. We finally evaluate the practical usefulness by quantitative experiments.The technique presented analyzes complete programs, including branches, loops, and nested parallelism. Alignment is determined with respect to offset, stride, and general axis relations. Placement of both data and processes are computed in a unifying framework based on an extended preference graph and its analysis. Dynamic redistributions are derived.The experimental results are very encouraging. The optimization algorithms implemented in our Modula-2* compiler improved the execution times of the programs by an average over 40% on a MasPar MP-1 with 16384 processors.
7E7106BF	A novel family of codes for the detection, localization and correction of unidirectional errors for DRAMs in proposed. For DRAM memories, multi-bit soft and hard errors are a growing concern, as the cell sizes continue to decrease with aggressive scaling down. As a result, the classical error detection/correction codes become less efficient to deal with multiple errors in a single word [16]. Berger codes are effective in detecting 100% of the unidirectional errors, but introduce considerable delay and does not provide any error correction feature. In order to decrease the computing time of the code check bits and, at the same time, to provide the possibility to localize the segment(s) where single or multiple errors have occurred, a modification of the Berger code is proposed, called Carry Save K Berger code (CS-K Berger). Basically consists in using Carry Save representations of the partial additions of the word segments. The number of check bits increases as compared with Berger code and consequently the code redundancy overhead increases but the delay in computing the check bits and the corresponding checking is drastically reduced. In addition, some patterns of unidirectional errors become correctable. The proposed CS-K Berger codes for different values of K are analyzed for different word lengths. The hardware required for code generation and verification is discussed as well, as the possibility of using this code for word segment replacement in online applications of DRAMs.
7DF2A908	Application domain specific DSP cores are becoming increasingly popular due to their advantageous trade-off between flexibility and cost. However, existing code generation methods are hampered by the combination of tight timing and resource constraints, imposed by the throughput requirements of DSP algorithms together with a fixed core architecture. In this paper, we present a method to model resource and instruction set conflicts uniformly and statically before scheduling. With the model we exploit the combination of all possible constraints, instead of being hampered by them. The approach results in an exact and run time efficient method to solve the instruction scheduling problem, which is illustrated by real life examples.
80FF05C4	This paper addresses the problem of automated code generation for a High Level Architecture compliant federate application given its behavior model. The behavior model is a part of the architectural model of a federation that the federate can participate in. The federate behavior model is based on Live Sequence Charts, adopted as the behavioral specification formalism in the Federation Architecture Metamodel (FAMM). FAMM serves as a formal language for describing federation architectures. An objective is to help the testing of federation architecture by means of its prototype implementation early in the development lifecycle. A further objective is to help developers construct complete federate applications that are well modularized. The approach to achieve these objectives is aspect-oriented in that the generated code, which handles the federate's interactions with a federation execution, serves as the base code, where the computation logic is to be weaved as an aspect. 
78ACEABC	The problem of evaluating arithmetic expressions on a machine with N ≥ 1 general purpose registers is considered. It is initially assumed that no algebraic laws apply to the operators and operands in the expression. An algorithm for evaluation of expressions under this assumption is proposed, and it is shown to take the shortest possible number of instructions. It is then assumed that certain operators are commutative or both commutative and associative. In this case a procedure is given for finding an expression equivalent to a given one and having the shortest possible evaluation sequence. It is then shown that the algorithms presented here also minimize the number of storage references in the evaluation.
7ECB9BCD	In this paper we define a subclass of comma-free codes which has a property called path invariance. The main advantage of codes in this subclass lies in the ease of establishing the positions of the divisions between words. Certain path-invariant comma-free dictionaries usingKsymbols to form n-symbol words are developed and their properties are studied. The number of words in these dictionaries is determined to beL(K-L)^{[n/2]}K^{[(n-1)/2]}whereLis a parameter which equals one whenn \geq 4K/3, and[x]denotes the integral part ofx. That this is the maximum obtainable dictionary size is proved for a special case. The ability of these codes to correct registration (synchronization) errors whennconsecutive symbols are available (as opposed to the2nconsecutive symbols required by general fixed-word-length comma-free codes) is demonstrated. A comparison of dictionary sizes is made for path-invariant comma-free codes, general fixed-word-length comma-free codes, and codes using one symbol as a comma. In the rangeK \leq 6andn \leq 9the path-invariant dictionaries are about\frac{1}{2}to\frac{3}{4}the size of the corresponding general comma-free dictionaries. Asymptotic dictionary sizes are obtained forK \rightarrow \inftyand forn \rightarrow \infty.
7FD0233E	Exotic instructions are complex instructions, such as block move, string search, and string edit, which are found on most conventional computers. Recent retargetable code generator and instruction set analysis systems have not dealt with exotic instructions. A method to analyze exotic instructions is presented which provides the information needed by a retargetable code generator. The analysis uses source-to-source transformations to prove the equivalence of high-level language operators to exotic instructions. Examples are presented which illustrate the analysis process.
747D68DA	Design automation for embedded systems comprising both hardware and software components demands for code generators integrated into electronic CAD systems. These code generators provide the necessary link between software synthesis tools in HW/SW codesign systems and embedded processors. General-purpose compilers for standard processors are often insufficient, because they do not provide flexibility with respect to different target processors and also suffer from inferior code quality. While recent research on code generation for embedded processors has primarily focussed on code quality issues, in this contribution we emphasize the importance of retargetability, and we describe an approach to achieve retargetability. We propose usage of uniform, external target processor models in code generation, which describe embedded processors by means of RT-level netlists. Such structural models incorporate more hardware details than purely behavioral models, thereby permitting a close link to hardware design tools and fast adaptation to different target processors. The MSSQ compiler, which is part of the MIMOLA hardware design system, operates on structural models. We describe input formats, central data structures, and code generation techniques in MSSQ. The compiler has been successfully retargeted to a number of real-life processors, which proves feasibility of our approach with respect to retargetability. We discuss capabilities and limitations of MSSQ, and identify possible areas of improvement.
7755A285	Some key ideas originating in the domain of software technology have been used in the design of the architecture description language S*M.
7A36F8DB	From the viewpoint of software engineering implementation, a MDA-based development approach for software mining systems is proposed in this paper. The development process starts from the describing of platform independent models. Then, by tightly integrating different functional modules such as storage management, data preprocessing, mining operations and mining base management, the approach succeeds in transforming models between different levels. This approach may not only be a theoretical guidance for model transformation, but also can be a measurement for validating the mapping rules between models at different abstract levels of the same system, and thus can make an effect support to model driven software engineering.
7AC6606E	Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.
7D5F3DA7	This paper introduces the PARSA (PARallel program Scheduling and Assessment) parallel software development tool to address the efficient partitioning and scheduling of parallel programs on multiprocessor systems. The PARSA environment consists of a user-friendly (visual), interactive, compile-time environment for partitioning, scheduling, and performance evaluation/tuning of parallel programs on different parallel computer architectures.
772D9D7E	This paper deals with distributed multiagent reconfigurable embedded-control systems following the component-based International Industrial Standard IEC61499 in which a function block (FB) is an event-triggered software component owning data and a control application is a distributed network of FBs. We define an architecture of reconfigurable multiagent systems, where a reconfiguration agent modeled by nested state machines is affected to each device of the execution environment to apply local automatic reconfigurations, and a coordination agent is proposed for any coordination between devices in order to guarantee safe and adequate distributed reconfigurations. A communication protocol is proposed in our research to handle coordinations between agents by using well-defined coordination matrices. We define, in addition, Extensible Markup Language (XML) based implementations for both kinds of agents, where XML code blocks are exchanged between devices. The contributions of the paper are applied to two benchmark production systems available in our laboratory.
7F3F3936	Unique characteristics of biological systems are described, and similarities are made to certain computing architectures. The security challenges posed by these characteristics are discussed. A method of securely isolating portions of a design using introspective capabilities of a fine-grain self-configurable device is presented. Experimental results are discussed, and plans for future work are given.
7F26E2C0	Software architecture describes the structure of a system, enabling more effective design, program understanding, and formal analysis. However, existing approaches decouple implementation code from architecture, allowing inconsistencies, causing confusion, violating architectural properties, and inhibiting software evolution. ArchJava is an extension to Java that seamlessly unifies software architecture with implementation, ensuring that the implementation conforms to architectural constraints. A case study applying ArchJava to a circuit-design application suggests that ArchJava can express architectural structure effectively within an implementation, and that it can aid in program understanding and software evolution.
7F1249D2	An architecture-independent software development approach for parallel processing systems is presented. This approach is based on the parallel object oriented and functional computation model PROOF and separates the architecture dependent issues from software development. It also facilitates software development for any parallel processing systems by relieving the programmers from the consideration of processor topology and various parallelization aspects of the software. Our approach allows the exploitation of parallelism at both levels of granularity: object level and method level, thereby making our approach effective for software development for various MIMD computers. Software developed using our approach reflects the parallel structure of the problem space which makes the software more understandable and modifiable. A framework consisting of object-oriented analysis, object-design, coding and transformation phases is presented for software development for parallel processing systems. An example is given to illustrate this approach.
765D6FA7	Content Management Systems are commonly used to develop web portals and virtual communities. Many of these systems are built using a multitier architecture approach for both the framework and the modules to provide the portal's functionalities. However, expanding upon these systems through creating or purchasing new modules can be challenging and inefficient. Large development teams are susceptible to a lost in productivity due to modular inter-dependencies. And smaller teams may struggle to manage the portal's growth or to provide users with reliable functionality. The challenges associated with modular software are examined in this paper through the analysis of the construction of an active virtual community: ProjectSTEM.net. Based on these observations, a method for developing custom software using multitier architecture is introduced. In this study, this method was found to increase the productivity of the development team, the functional reliability of the software and the commercial viability of the modules. The method was also found to decrease the storage needs for the systems and facilitate easier migration of data if the portal was to be moved to another platform.
79ED41AD	Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecture-based evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples.
8164D439	A packaging system that allows diverse software components to be easily interconnected within heterogeneous programming environments is described. Interface software and stubs are generated for programmers automatically once the programmers express their application's geometry in a few simple rules and module interconnection language attributes. By generating custom interface code for each application, based on analysis and extraction of interfacing requirements, the system is able to produce executables whose run-time performance is comparable to manually integrated applications. The system is implemented within the Unix environment.
815C5F85	The configuration complexity of preproduction sites coupled with access-control mechanisms often impede the software development life cycle. Virtualization is a cost-effective way to remove such barriers and provide a test environment similar to the production site, reducing the burden in IT administrators. An eclipse-based virtualization tool framework can offer developers a personal runtime environment for launching and testing their applications. The authors have followed a model-driven architecture (MDA) approach that integrates best-of-breed virtualization technologies, such as Xen and VDE.
7FC9497C	While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits ("widgets"). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for intercomponent communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.
808999EB	Modern concurrent and distributed applications are becoming increasingly complex; so, in order to provide fault tolerance, special structuring mechanisms are required to help reduce this complexity. Unfortunately, such structuring techniques are mostly introduced as design and implementation features, which complicates their employment. The approach we propose relies on introducing the appropriate software structuring together with associated fault tolerance measures at the earlier phases of software development and on supporting it with special software architectures and design patterns.
80219DBE	Software development environments (SDEs) are examined from the environment architect's perspective. The main interest is in investigating two related issues of building SDEs: open architecture and tool integration. A taxonomy of architectural mechanisms that solely determine the underlying architecture of an SDE is presented. The significance of this taxonomy is that it makes it possible to study the intrinsic properties of SDE architecture with respect to technical issues such as portability, tool integration, tool construction, tailorability, and evolution of SDEs. Four types of mechanisms are identified: modeling and schema-definition mechanisms, access mechanisms, interprocess communication mechanisms, and execution mechanisms. These four types of mechanisms reflect the relationship of tools to the SDE architecture. For each type of mechanism, various approaches proposed in the literature are compared and openness and integration are discussed. It is suggested that truly open and integrated SDEs can be achieved through careful selection of architectural mechanisms and good implementation strategies of these mechanisms.
7DEDA25C	To maintain and understand large applications, it is crucial to know their architecture. The first problem is that unlike classes and packages, architecture is not explicitly represented in the code. The second problem is that successful applications evolve over time, so their architecture inevitably drifts. Reconstructing the architecture and checking whether it is still valid is therefore an important aid. While there is a plethora of approaches and techniques supporting architecture reconstruction, there is no comprehensive state of the art and it is often difficult to compare the approaches. This article presents a state of the art on software architecture reconstruction approaches
7A80B7F9	We're happy to get on the road with the column. Be aware that you may be carrying 100 million lines of code with you the next time you take the car. Jiirgen Moessinger is vice president for automotive systems integration at Bosch and describes the challenges and opportunities that software brings to the automotive industry.
0FC2B3F6	The data acquisition system for loadcell calibration has been upgraded from a dedicated instrument controller to a desktop computer with a Microsoft(registered) WIN95 operating system. This paper includes a discussion of the selection of the instrumentation, software, design of the switching network, and software development. The system is controlled and monitored by software developed using National Instruments graphical programming language, LabVIEW. Data reports are generated in Microsoft Excel using Object Linking and Embedding (OLE) routines from LabVIEW. Data are archived in Microsoft Access with OLE routines from LabVIEW. Reports and historical data are electronically available to the test facilities at Arnold Engineering Development Center (AEDC) over the base Intranet.
7D729C18	Modern software frameworks provide a set of commonand prefabricated software artifacts that supportengineers in developing large-scale software systems.Framework-related information can be implemented insource code, comments or configuration files, but in thelatter two cases, current reverse engineering approachesmiss important facts reducing the quality of subsequentanalysis tasks. We introduce a generic fact extractionapproach for framework-based systems by combiningtraditional parsing with lexical pattern matching toobtain framework-specific facts from all three sources.We evaluate our approach with an industrial softwareapplication that was built using the Avalon/Phoenixframework. In particular we give examples to point outthe benefits of considering framework-related informationand reflect experiences made during the case study.
7F726C81	A cooperative system architecture is proposed for systemization of scheduling work, taking account of the desirable relationship between an expert and a computer. The architecture emphasizes the significance of a system structure in which decision-making and trial operation can be easily performed by the expert and the system can simulate the expert's intentions. Since an expert expresses his intentions mainly by mouse operations, using only a narrow bandwidth of communication from a human to a computer, the authors propose that domain knowledge should be used to grasp the expert's intentions. The load-balancing problem is analyzed and an example is given that shows such a problem is cooperatively solved by the InterBALANCE system.
7ED8344D	OMG’s Model Driven Architecture (MDA) has deeply influenced modern-day software development, not only by providing promising means for automating the software process, but also through revitalizing the role of modeling in software development, the importance of which had been neglected during the recent euphoria over lightweight development methods. However, MDA’s need to remain reasonably abstract means that it is more a software development approach rather than a standalone methodology, and therefore needs methodology support to be practically useful. Several MDA-based methodologies exist today, yet the need remains for the definition of an instantiable MDA-based development process. We propose a generic lifecycle for MDA-based software development that can be used as a basis for constructing MDA-based methodologies through a Method Engineering (ME) process. The phases and activities of the proposed lifecycle are described herein, with a number of prominent MDA-based methodologies assessed as to their degree of conformance to the proposed lifecycle.
7F3C2074	A description is given of Aquarius IIU, a complex system integrating a high-performance symbolic microprocessor, an instruction prefetcher, snooping data and instruction caches, a VME bus interface, and a set of controllers. Aquarius IIU is based on the high performance VLSI-PLM chip that runs the Warren abstract machine instruction set. Many of these nodes have been connected using a shared bus to form a multiprocessor which has its own shared memory and snooping caches and is used as a backend Prolog engine to the host (SUN3/160). On every node, there are two controllers per data and instruction cache that cooperate to support Berkeley's snooping cache-lock state protocol, which minimizes bus traffic associated with locking blocks. The nodes share memory using the signals of the VME bus; the page faults and memory management are handled by the host. A top-down method was used in the design of the Aquarius IIU node, while a bottom-up method was used in the simulations. In designing and simulating complex systems such as the Aquarius IIU, the procedure followed was found to be advantageous.
7E481A85	Multi-core programming is no more a luxury; it is now a necessity, because even embedded processors are becoming multi-core. However, the state-of-the-art techniques such as OpenMP and the Intel Threading Building Block (TBB) library are far from user-friendly due to the tedious work needed in explicitly designing multi-core programs and debugging. At the present days, a solution for above problems will be that to enhance the abstract level of multicore embedded software design. By leveraging on the expertise gained from Verifiable Embedded Real-Time Application Framework (VERTAF), we propose a Multi-Core version of VERTAF, called VERTAF/ Multi-core (VMC in short). VMC is an integrated development environment for multi-core embedded software architecture. Developers would be able to 1. describe their system requirements with SysML by using this environment, 2. model their design with SysML standard notation, 3. automatically apply a pattern structure into their design for a high quality multicore embedded system, 4. generate source code through a well-designed model; 5. map to different hardware architecture as assigned by the model, and 6. finally we can test the code.Using the model driven architecture (MDA) design flow in SysML, we saw a significantly improvement on productivity and quality of a multicore embedded programming over traditional approach.
7F3D2FB0	Continuous availability is a critical requirement for an important class of software systems. For these systems, runtime system evolution can mitigate the costs and risks associated with shutting down and restarting the system for an update. We present an architecture-based approach to runtime software evolution and highlight the role of software connectors in supporting runtime change. An initial implementation of a tool suite for supporting the runtime modification of software architectures, called ArchStudio, is presented. 
709EB156	Instructional systems development (ISD) has proved to be effective for developing software maintenance education courses. The process of ISD involves the establishment of specific performance objectives which are addressed through classroom instruction and laboratory experience. Students graduate when they are able to pass stringent tests against the established objectives. ISD incorporates the following five stages: analyze system requirements; define education or training requirements; develop objectives and tests; plan, develop, and validate instruction; and conduct and evaluate instruction. Progressing through these stages ensures that an effective course of instruction is in place. An example is presented of the application of ISD.
7E044973	The demand for flexible and scalable mechatronic system is becoming more and more attractive. Modular systems made from a large number of modules can be formed various physical or logical topologies, consequently to achieve the capabilities to optimize their configurations to suit different applications. Although, modular concept will bring new scalable and extendable products for customers, it puts new requirements for the product development. Shift from tightly-coupled systems to service-oriented architecture will bring deep impact to the research and development of the modular embedded devices, applications and services, i.e. function elements can be easily added or removed without modifying the whole hardware and software of the systems. In this paper, we first discuss the opportunities, challenges and requirements for the modular mechatronic systems development, and then, present directions for loose-coupled modular devices based on service-oriented control architecture, in particular, this paper outlines the implementation of this architecture adopted by the Miniaturized Modular Devices project.
78938265	Software architectures promote development focused on modular building blocks and their interconnections. Since architecture-level components often contain complex functionality, it is reasonable to expect that their interactions will also be complex. Modeling and implementing software connectors thus becomes a key aspect of architecture-based development. Software interconnection and middleware technologies such as RMI, CORBA, ILU, and ActiveX provide a valuable service in building applications from components. The relation of such services to software connectors in the context of software architectures, however, is not well understood. To understand the tradeoffs among these technologies with respect to architectures, we have evaluated several off-the-shelf middleware technologies and identified key techniques for utilizing them in implementing software connectors. Our platform for investigation was C2, a component- and message-based architectural style. By encapsulating middleware functionality within software connectors, we have coupled C2's existing benefits such as component interchangeability, substrate independence and structural guidance with new capabilities of multi-lingual, multi-process and distributed application development in a manner that is transparent to architects.
808E770D	One of the challenging problems for software developers is guaranteeing that a system as built is consistent with its architectural design. In this paper, we describe a technique that uses run time observations about an executing system to construct an architectural view of the system. With this technique, we develop mappings that exploit regularities in system implementation and architectural style. These mappings describe how low-level system events can be interpreted as more abstract architectural operations. We describe the current implementation of a tool that uses these mappings, and show that it can highlight inconsistencies between implementation and architecture.
5F7C941F	 The objective of this work was to explore algorithms and their implementation for future advanced parallel systems. These systems are assumed to have hundreds or even thousands of processors and to be able to concentrate their computing power on one or a small number of tasks. The three principal questions to be explored were: 1) Are there algorithms for the crucial applications which have enough parallelism to allow the power of the advanced parallel systems to be fully exploited?; 2) What languages and implementation tools are needed for efficient programming of these algorithms?; 3) What are the relative performances of different algorithm types? Of different architecture types? Of different implementation languages? The research results obtained appear in this report.
77E1901E	File sharing applications using Peer-to-Peer (P2P) networks such as Bittorrent or eDonkey rapidly attracted a lot of people and proved the efficiency and interest of this P2P technology. Distribution of video and of live contents also experienced the P2P mechanisms with success. PPLive, UUSee and others have many of customers, hundreds of channels and thousands of concurrent users. However, major content providers are reluctant to use this technology because no solution to ensure the distribution of only legal contents is provided. In the same way, network operators do not really push towards P2P content distribution because bad organization of the overlay can lead to overload the network and consume a lot of networks resources. In this paper, a secure and legal network-aware P2P video system is introduced, which aims at overcoming those two drawbacks. The design of the system and the evaluation of a prototype showed good results and let us be optimistic about a possible deployment of P2P systems for video delivery, having the support of content providers as well as network operators.
7EE734F3	There are six independent projects within the DSSA program. Four of these projects are working in specific, military-significant domains. Those domains are Avionics Navigation, Guidance and Flight Director for Helicopters; Command and Control; Distributed Intelligent Control and Management for Vehicle Management; Intelligent Guidance, Navigation and Control for Missiles. In addition, there are two projects working on underlying support technology. Hybrid (discrete and continuous, non-linear) Control and Prototyping Technology. 
7F9BE110	As the size of software systems increases, the algorithms and data structures of the computation no longer constitute the major design problems. When systems are constructed from many components, the organization of the overall system -- the software architecture -- presents a new set of design problems. This level of design has been addressed in a number of ways including informal diagrams and descriptive terms, module interconnection languages, templates and frameworks for systems that serve the needs of specific domains, and formal models of component integration mechanisms. In this paper we provide an introduction to the emerging field of software architecture. We begin by considering a number of common architectural styles upon which many systems are currently based and show how different styles can be combined in a single design. Then we present six case studies to illustrate how architectural representations can improve our understanding of complex software systems. Finally, we survey some of the outstanding problems in the field, and consider a few of the promising research directions.
7EFA2FA0	The available frameworks for adaptive systems propose various solutions for addressing and implementing software which is adaptable at runtime. These solutions have different perspective on adaptivity: architectural, structural, or behavioral. Hence, based on which criteria should be decided which framework to adopt? In this paper, we present a possible approach to evaluate the available frameworks through a set of metrics which aim to capture the main issues of adaptive systems from the development perspective.
7FEA9F6C	Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition of components into systems. These abstractions are higher level than the elements usually supported by programming languages and tools. They capture packaging and interaction issues as well as computational functionality. Well-established (if informal) patterns guide the architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions used in practice by software designers. The implementation provides a testbed for experiments with a variety of system construction mechanisms. It distinguishes among different types of components and different ways these components can interact. It supports abstract interactions such as data flow and scheduling on the same footing as simple procedure call. It can express and check appropriate compatibility restrictions and configuration constraints. It accepts existing code as components, incurring no runtime overhead after initialization. It allows easy incorporation of specifications and associated analysis tools developed elsewhere. The implementation provides a base for extending the notation and validating the model.
733C702D	The continually increasing integration density of integrated circuits portrays important paradigm shifts in next-generation designs, especially in the direction of systems-on-a-chip. Hybrid architectures mixing a variety of computational models are bound to be integrated on a single die. This opens the door for creative high-performance low-energy solutions to the programming problem using techniques such as reconfiguration to construct optimized architectures for a given computational problem. Exploiting the opportunities offered by these architectural innovations obviously requires a clear understanding of the trade-off's offered by the various architectural models and styles, as well as a well-thought out design methodology, combining high-level prediction and analysis tools with partitioning, optimization and mapping techniques. This paper presents an overview of opportunities of these reconfigurable architectures in the architecture domain.
7D0A1DDD	 The integration of new or existing software components into established architectures and the ability to deal with heterogeneity are key requirements for middleware and development frameworks for robotic systems. This paper presents SPICA, a software development framework for communication infrastructures of autonomous mobile robots. Utilizing the model-driven software development paradigm, communication and data flow can be defined on an abstract level. For this purpose, domain-specific languages and tools are provided that allow specification and generation of module communication infrastructures for communication between modules along with primitives for data management. The high-level platform-independent specifications are automatically transformed into low-level platform and programming language-specific source code. We illustrate the applicability of our approach with an elaborate example describing the design of a soccer robot architecture that has proven its strength during RoboCup 2006. Our experiences have revealed that SPICA is advantageous for prototyping as well as for building high performance systems
63DB3066	The theory of reactive systems, introduced by Leifer and Milner and previously extended by the authors, allows the derivation of well-behaved labelled transition systems (LTS) for semantic models with an underlying reduction semantics. The derivation procedure requires the presence of certain colimits (or, more usually and generally, bicolimits) which need to be constructed separately within each model. In this paper, we offer a general construction of such bicolimits in a class of bicategones of cospans. The construction sheds light on as well as extends Ehrig and Konig's rewriting via borrowed contexts and opens the way to a unified treatment of several applications.
05E53073	This report describes a procedure for designing computer systems that are developed specifically to be a component of a more complex system. Two significant characteristics of such design problems are the following: the computer system interface is determined by factors outside the control of the computer system designer, and the specifications of that interface are likely to change throughout the life cycle of the system. The purpose of the procedure described in this report is to reduce maintenance costs by means of a software organization that insulates most of the programs from changes in the interface. The procedure is based on the systematic compilation of an assumption list. The assumption list describes those aspects of the interface that future users and other knowledgeable persons consider essential and therefore stable. Other aspects of the interface are ignored. An abstract interface is designed on the basis of this assumption list. A specification of the abstract interface is used to procure the major components of the system. This report explains the principles behind the procedure and illustrates its use. The success of the procedure is primarily limited by the ability of designers and future users to compile an accurate list of assumptions. A side benefit of the procedure is simpler, better structured software. Successful application of the procedure should result in both increased reliability and reduced lift-cycle costs. (Author)
80E31996	Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation, and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.
8083377D	With the shrinking defense budget, the U. S. Department of Defense (DoD) has relied more on commercial-off-the-shelf (COTS) and contracted software systems. Government contractors and commercial developers currently rely heavily on semi-formal methods such as the Unified Modeling Language (UML) in developing the models and requirements for these software systems. The correctness of specifications in such languages cannot be tested, in general, until they are implemented. Due to the inherent safety requirements for mission critical systems, formal specification methods would be preferable. This thesis contrasts the development of a combat system for the Navy using the formal specification language SPEC with development using the semi-formal method UML. The application being developed is a ship recognition system that utilized image data, detected emitters, and ship positioning to correlate ship identification. The requirements analysis and architectural design for this system are presented.
65AB8165	System integration refers to combining some or all of the functionality of two or more software and/or database systems. This differs from systems interfacing which refers to linking the functionality of two or more systems via another system component specifically designed for that purpose. Systems integration implies making two or more systems work as one system and not necessarily to make multiple systems work in tandem. This paper advocates the advantages of using CASE tool development software for systems integration activities. It also addresses other beneficial software project management data which is created as a result.
80F48122	Most traditional systems produce a fixed quality of output under a fixed execution environment given identical input, even when the user does not need that quality of solution. We propose a new software architecture that can do quality-driven operations. The key concept of the architecture is that it contains multiple configurations so that it can adapt itself to fit various user requirements and constraints. The adaptation involves the tradeoff of a set of quality of service factors in order to provide the best feasible performance. A simple application is presented to demonstrate the power of the architecture. This quality-driving concept can be introduced to many domains; in fact, any resource-constrained or pay-per-use applications can be improved by the concept to provide a range of quality of services.
7F42F2D5	Experience from a dozen years of analyzing software engineering processes and products is summarized as a set of software engineering and measurement principles that argue for software engineering process models that integrate sound planning and analysis into the construction process. In the TAME (Tailoring A Measurement Environment) project at the University of Maryland, such an improvement-oriented software engineering process model was developed that uses the goal/question/metric paradigm to integrate the constructive and analytic aspects of software development. The model provides a mechanism for formalizing the characterization and planning tasks, controlling and improving projects based on quantitative analysis, learning in a deeper and more systematic way about the software process and product, and feeding the appropriate experience back into the current and future projects. The TAME system is an instantiation of the TAME software engineering process model as an ISEE (integrated software engineering environment). The first in a series of TAME system prototypes has been developed. An assessment of experience with this first limited prototype is presented including a reassessment of its initial architecture.
7D2E7D95	Gaining an architectural level understanding of a software system is important for many reasons. When the description of a system's architecture does not exist, attempts must be made to recover it. In recent years, researchers have explored the use of clustering for recovering a software system's architecture, given only its source code. The main contributions of this paper are given as follows. First, we review hierarchical clustering research in the context of software architecture recovery and modularization. Second, to employ clustering meaningfully, it is necessary to understand the peculiarities of the software domain, as well as the behavior of clustering measures and algorithms in this domain. To this end, we provide a detailed analysis of the behavior of various similarity and distance measures that may be employed for software clustering. Third, we analyze the clustering process of various well-known clustering algorithms by using multiple criteria, and we show how arbitrary decisions taken by these algorithms during clustering affect the quality of their results. Finally, we present an analysis of two recently proposed clustering algorithms, revealing close similarities in their apparently different clustering approaches. Experiments on four legacy software systems provide insight into the behavior of well-known clustering algorithms and their characteristics in the software domain.
6D6C6B05	Telepathology systems involving robotic controls are often very expensive and hard to justify by telepathology providers. In this paper, a telepathology system is discussed which costs less than $25,000 while supplying excellent diagnostic-functionality and network connectivity. The architecture proposed allows for dynamic interaction between pathologists that scales to the network bandwidth. Three important factors influence diagnostic accuracy in a telepathology system. First, preliminary data supports that dynamic systems perform better than static ones mainly because of better physician interaction and the ability to visualize the proper subset of diagnostic clues. Second, a frame rate below 8 frames/sec will not convey sufficient information for real-time interaction. Last, synchronization of the video between the 2 sites ensures that proper fields are transmitted. When considering telepathology systems, one must weigh the benefits, and the costs involved in the implementation. This design represents the compromise between robotic control and static systems.
80CABE78	Automotive systems engineering has made significant progress in using formal methods to design safe hardware-software systems. The architectures and design methods could become a model for safe and cost-efficient embedded software development as a whole. This paper gives several examples from the leading edge of industrial automotive applications.
738D7C85	Phase change memory (PCM) is considered as a promising alternative of DRAM-based main memory in embedded systems. A PCM cell can be dynamically programmed to be in either multiple-level cell (MLC) mode or single-level cell (SLC) mode. With this morphable feature, we can utilize the high-density of MLC and low-latency of SLC, to satisfy various memory requirements of specific applications in embedded systems. However, compared to its SLC counterpart, the lifetime of MLC is limited. In particular, a program in embedded systems usually exhibits an extremely unbalanced write pattern, which may accelerate the wear-out of MLC cells in morphable PCM. To address this issue, this paper proposes a simple and effective wear-leveling technique, named Mixer, to enhance the lifetime of morphable PCM considering the program specific features. We first build an Integer Linear Programming (ILP) formulation to produce optimal SLC/MLC partition and data allocation, to achieve a balanced write distribution in morphable PCM with low memory access cost. The basic idea is to allocate fast SLC and MLC cells for write intensive variables and other ordinary variables, respectively. We then propose a polynomial time algorithm to achieve near-optimal results. The evaluation results show that the proposed technique can effectively extend the lifetime of morphable PCM in embedded systems compared with previous work.
80F89D7B	We sketch work-in-progress in the area of systems architecting, which we regard as a fundamental element of system-level requirements engineering. We present our ideas on systems architecting, and relate them to the field of Architecture proper (as in buildings). We present our notion of system-wide scenarios and how these can be derived from contextual analysis of information systems, using a well-established qualitative data analysis methodology from the social sciences.
7E608632	The authors summarize the goals of metric-driven analysis and feedback systems and describe a prototype system, Amadeus, which defines abstract interfaces and embodies architectural principles for these types of systems. Metric-driven analysis and feedback systems enable developers to define empirically guided processes for software development and maintenance. The authors provide an overview of the Amadeus system operation, including an example of the empirically guided process, a description of the system characteristics, an explanation of the system conceptual operation, and a summary of the users' view of the system. The centerpiece of the system is a pro-active server, which interprets scripts and coordinates event monitoring and agent activation. Amadeus provides an extensible framework for adding new empirically based analysis techniques.
7DDAED8F	In this paper, we present field-programmable gate-array (FPGA)-based configurable architectures that are able to perform frequency-diverse target detection for real-time ultrasonic imaging. Three design methodologies are explored including the execution of the detection algorithm on an embedded microprocessor, the creation of a dedicated hardware solution, and the use of hardware/software codesign principles. In addition to the design flow, this paper presents the impact of parameter changes on the detection-algorithm performance and FPGA implementation results. Experimental studies show that the proposed configurable systems are able to meet real-time operation requirements, and the algorithm performs robustly.
75F930F7	Undocumented evolution of a software system and its underlying architecture drives the need for the architectures recovery from the systems implementation-level artifacts. While a number of recovery techniques have been proposed, they suffer from known inaccuracies. Furthermore, these techniques are difficult to evaluate due to a lack of ground-truth architectures that are known to be accurate. To address this problem, we argue for establishing a suite of ground-truth architectures, using a recovery framework proposed in our recent work. This framework considers domain-, application-, and contextspecific information about a system, and addresses an inherent obstacle in establishing a ground-truth architecture the limited availability of engineers who are closely familiar with the system in question. In this paper, we present our experience in recovering the ground-truth architectures of four open-source systems. We discuss the primary insights gained in the process, analyze the characteristics of the obtained ground-truth architectures, and reflect on the involvement of the systems engineers in a limited but critical fashion. Our findings suggest the practical feasibility of obtaining ground-truth architectures for large systems and encourage future efforts directed at establishing a large scale repository of such architectures. 
7D17A232	There are many economic and technical arguments for the reduction of the number of Electronic Control Units (EC Us) aboard a car. One of the key obstacles to achieve this goal is the limited composability, fault isolation and error containment of today's single- processor architectures. However, significant changes in the chip architecture are taking place in order to manage the synchronization, energy dissipation, and fault-handling requirements of emerging billion transistor SoCs (systems-on-a-chip). The single processor architecture is replaced by multi-core SoCs that communicate via networks-on-chip (NoC). These emerging multi-core SoCs provide an ideal execution environment for the integration of multiple automotive ECUs into a single SoC This paper presents a model-based software development method for designing applications using these multi-core SoCs.
5E219F49	The Analytic System and Software for Evaluating Safeguards and Security (ASSESS) has been released for use by DOE field offices and their contractors. In October, 1989, we offered a prototype workshop to selected representatives of the DOE community. Based on the prototype results, we held the first training workshop at the Central Training Academy in January, 1990. Four additional workshops are scheduled for FY 1990. ASSESS is a state-of-the-art analytical tool for management to conduct integrated evaluation of safeguards systems at facilities handling facilities. Currently, ASSESS focuses on the threat of theft/diversion of special nuclear material by insiders, outsiders, and a special form of insider/outsider collusion. ASSESS also includes a neutralization module. Development of the tool is continuing. Plans are underway to expand the capabilities of ASSESS to evaluate against violent insiders, to validate the databases, to expand the neutralization module, and to assist in demonstrating compliance with DOE Material Control and Accountability (MC A) Order 5633.3. These new capabilities include the ability to: compute a weighted average for performance capability against a spectrum of insider adversaries; conduct defense-in-depth analyses; and analyze against protracted theft scenarios. As they become available, these capabilities will be incorporated in our training program. ASSESS is being developed jointly by Lawrence Livermore and Sandia National Laboratories under the sponsorship of the Department of Energy (DOE) Office of Safeguards and Security.
8070DACF	Future space missions call for advanced computing system architectures fulfilling strict size, weight and power consumption (SWaP) requisites, decreasing the mission cost and ensuring the safety and timeliness of the system. The AIR (ARINC 653 in Space Real-Time Operating System) architecture defines a partitioned environment for the development and execution of aerospace applications, following the notion of time and space partitioning (TSP), preserving application timing and safety requisites. Due to the change of the mission plans or in the presence of unexpected events, it may be necessary or useful to be able to reconfigure the scheduling of the system applications at execution time. In this paper we present an algorithm for updating application schedules, showing results from the proof-of-concept prototype in the scope of the AIR architecture.
788ED83F	Hierarchical graphs are widely used as models of the structure of software systems. A central problem in the visualization of hierarchical graphs is the computation of layouts, i.e. of positions of the nodes in two- or three-dimensional space. We derive requirements for graph layouts from various software analysis questions, and classify the required layouts along three dimensions: layouts with meaningful distances between single nodes vs. layouts with meaningful distances between groups of nodes, layouts reflecting adjacency vs. layouts reflecting hierarchy, and layouts that faithfully reflect the size of subgraphs vs. layouts where certain subgraphs are magnified. We present a fairly simple and theoretically validated energy model for computing such layouts.
7880312C	Now that multicore chips are common, providing an approach to parallel programming that is usable by regular programmers has become even more important. This cloud has one silver lining: providing useful speedup on a program is useful in and of itself, even if the resulting performance is lower than the best possible parallel performance on the same program. To help achieve this goal, Yada is an explicitly parallel programming language with sequential semantics. Explicitly parallel, because we believe that programmers need to identify how and where to exploit potential parallelism, but sequential semantics so that programmers can understand and debug their parallel programs in the way that they already know, i.e. as if they were sequential.The key new idea in Yada is the provision of a set of types that support parallel operations while still preserving sequential semantics. Beyond the natural read-sharing found in most previous sequential-like languages, Yada supports three other kinds of sharing. Writeonce locations support a single write and multiple reads, and two kinds of sharing for locations updated with an associative operator generalise the reduction and parallel-prefix operations found in many data-parallel languages. We expect to support other kinds of sharing in the future.We have evaluated our Yada prototype on eight algorithms and four applications, and found that programs require only a few changes to get useful speedups ranging from 2.2 to 6.3 on an 8-core machine. Yada performance is mostly comparable to parallel implementations of the same programs using OpenMP or explicit threads.
70022CC5	We adopt a layered approach to autonomic systems to simplify the specification, implementation and evaluation of self-* behaviours. Autonomy is rendered in two dimensions and interfaces suitable for tool based development are incorporated. To exemplify our approach we present an emergent solution for airspace management. This is representative of many problems in which local neighbour interactions can be utilised to give stable and robust global behaviour
7B0B5ECA	Parallel programming on loosely coupled distributed systems involves many system dependent tasks such as sensing node availability, creating remote processes, programming inter-process communication and synchronization, etc. Very often these system-dependent tasks are handled at the programmer level. This has complicated the process of parallel programming on distributed systems. The portability of these programs is also severely affected. The programmer may also start his remote processes on heavily loaded nodes, thereby degrading the overall performance of the system. To overcome these difficulties, we introduce a language construct called parset at the programming level. Parset captures various kinds of coarse grain parallelism occurring in distributed systems. It also provides scalability to distributed programs. We show that this construct greatly simplifies writing programs on distributed systems providing transparency to various system dependent tasks.
781E3669	This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective "unsupervised" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving Gaussian mixtures. These experiments testify for the good performance of our approach.
5F86BBDC	Minimally Synchronous Parallel ML (MSPML) is a functional parallel programming language. It is based on a small number of primitives on a parallel data structure. MSPML programs are written like usual sequential ML program and use this small set of functions. MSPML is deterministic and deadlock free. The execution time of the programs can be estimated.Divide-and-conquer is a natural way of expressing parallel algorithms. MSPML is a flat language: it is not possible to split the parallel machine in order to implement divide-and-conquer parallel algorithms. This paper presents an extension of MSPML to deal with this kind of algorithms: a parallel composition primitive.
7FB99442	A new architecture for mobile radio networks, called the linked cluster architecture, is described, and methods for implementing this architecture using distributed control techniques are presented. We illustrate how fully distributed control methods can be combined with hierarchical control to create a network that is robust with respect to both node loss and connectivity changes. Two distributed algorithms are presented that deal with the formation and linkage of clusters and the activation of the network links. To study the performance of our network structuring algorithms, a simulation model was developed. The use of Simula to construct, software simulation tools is illustrated. Simulation results are shown for the example of a high frequency (HF) intratask force (ITF) communication network.
7F82D4FE	We explore the idea of evidence accumulation (EAC) for combining the results of multiple clusterings. First, a clustering ensemble - a set of object partitions, is produced. Given a data set (n objects or patterns in d dimensions), different ways of producing data partitions are: 1) applying different clustering algorithms and 2) applying the same clustering algorithm with different values of parameters or initializations. Further, combinations of different data representations (feature spaces) and clustering algorithms can also provide a multitude of significantly different data partitionings. We propose a simple framework for extracting a consistent clustering, given the various partitions in a clustering ensemble. According to the EAC concept, each partition is viewed as an independent evidence of data organization, individual data partitions being combined, based on a voting mechanism, to generate a new n × n similarity matrix between the n patterns. The final data partition of the n patterns is obtained by applying a hierarchical agglomerative clustering algorithm on this matrix. We have developed a theoretical framework for the analysis of the proposed clustering combination strategy and its evaluation, based on the concept of mutual information between data partitions. Stability of the results is evaluated using bootstrapping techniques. A detailed discussion of an evidence accumulation-based clustering algorithm, using a split and merge strategy based on the k-means clustering algorithm, is presented. Experimental results of the proposed method on several synthetic and real data sets are compared with other combination strategies, and with individual clustering results produced by well-known clustering algorithms.
7DDC65F8	Over the past few years, there has been a renewed interest in the consensus clustering problem. Several new methods have been proposed for finding a consensus partition for a set of n data objects that optimally summarizes an ensemble. In this paper, we propose new consensus clustering algorithms with linear computational complexity in n. We consider clusterings generated with a random number of clusters, which we describe by categorical random variables. We introduce the idea of cumulative voting as a solution for the problem of cluster label alignment, where unlike the common one-to-one voting scheme, a probabilistic mapping is computed. We seek a first summary of the ensemble that minimizes the average squared distance between the mapped partitions and the optimal representation of the ensemble, where the selection criterion of the reference clustering is defined based on maximizing the information content as measured by the entropy. We describe cumulative vote weighting schemes and corresponding algorithms to compute an empirical probability distribution summarizing the ensemble. Given the arbitrary number of clusters of the input partitions, we formulate the problem of extracting the optimal consensus as that of finding a compressed summary of the estimated distribution that preserves the maximum relevant information. An efficient solution is obtained using an agglomerative algorithm that minimizes the average generalized Jensen-Shannon divergence within the cluster. The empirical study demonstrates significant gains in accuracy and superior performance compared to several recent consensus clustering algorithms.
5E7FA96D	The monitor construct has been implemented in several concurrent and/or parallel programming languages for shared-memory system environments, Extensions of the monitor to support process synchronization in distributed systems have also been proposed. But, most existing work only provides the architecture design of the distributed monitor. There is no discussion about the algorithmic and implementation issues. Also, none of them consider how to implement conditional variables. In this paper, we present the design and implementation of a distributed monitor construct, named DisMoniC, for programming process synchronization in distributed systems. DisMoniC is generic in the sense that it can be used with any distributed mutual exclusion (DME) algorithm to implement exclusive access to the monitor operations. Time-efficient algorithms are proposed to implement conditional process synchronization in the distributed monitor. We also present performance evaluation of the proposed construct.
5EEA5FD7	There are several parallel programming models available for numerical computations at different levels of expressibility and ease of use. For the development of new domain specific programming models, a splitting into a distributed data container and parallel data iterators is proposed. Data distribution is implemented in application specific libraries. Data iterators are directly analysed and compiled automatically into parallel code. Target architectures of the source-to-source translation include shared (pthreads, Cell SPE), distributed memory (MPI) and hybrid programming styles. A model applications for grid based hierarchical numerical methods and an auto-parallelizing compiler are introduced.
7C31495C	Novel memory programming methods and corresponding memory structures are presented in this paper. Unlike conventional memory programming, this programming technique does not require deterministic switching of memory elements. This technique explicitly exploits the probabilistic switching characteristics of memory elements such as spin-transfer torque magnetic tunnel junction (STT-MTJ) to reduce programming power and delay. This technique also allows multilevel cell (MLC) spin-transfer torque magnetoresistive random access memory (STT-MRAM) to be fabricated with existing STT-MTJ fabrication processes, thus making high capacity STT-MRAM chips readily achievable. The optimal STT-MTJ switching probabilities are given in this paper for reaching minimum programming delay, power, and iteration. Moreover, this paper proves, by applying probabilistic programming to existing STT-MTJs, both programming delay and power can be reduced to levels beyond the reach of conventional deterministic programming. Furthermore arbitrarily small programming bit error rate (BER) can be accomplished in theory using probabilistic programming without much penalty on average programming delay and power. On the contrary, deterministic programming always presents finite programming BER, which is expensive to reduce in terms of programming power and delay. The MLC capability of STT-MTJ clusters has also been confirmed using fabricated STT-MTJ devices. The major circuitries for implementing probabilistically programmed MLC STT-MRAM are also presented in this paper.
7E74D135	We introduce a middleware infrastructure that provides software services for developing and deploying high-performance parallel programming models and distributed applications on clusters and networked heterogeneous systems. This middleware infrastructure utilizes distributed agents residing on the participating machines and communicating with one another to perform the required functions. An intensive study of the parallel programming models in Java has helped identify the common requirements for a runtime support environment, which we used to define the middleware functionality. A Java-based prototype, based on this architecture, has been developed along with a Java object-passing interface (JOPI) class library. Since this system is written completely in Java, it is portable and allows executing programs in parallel across multiple heterogeneous platforms. With the middleware infrastructure, users need not deal with the mechanisms of deploying and loading user classes on the heterogeneous system. Moreover, details of scheduling, controlling, monitoring, and executing user jobs are hidden, while the management of system resources is made transparent to the user. Such uniform services are essential for facilitating the development and deployment of scalable high-performance Java applications on clusters and heterogeneous systems. An initial deployment of a parallel Java programming model over a heterogeneous, distributed system shows good performance results. In addition, a framework for the agents' startup mechanism and organization is introduced to provide scalable deployment and communication among the agents.
7FECDA80	This study explores the appropriateness of the locality of air monitoring stations which are meant to indicate air quality in the area. Daily variations in NO2 and PM10 concentrations at 14 monitoring stations in Hong Kong are examined. The daily variations in NO2 at a number of background monitoring stations exhibit patterns similar to variations in traffic volume while variations in PM10 concentration exhibit less discernible pattern. Principal component analysis (PCA) and cluster analysis (CA) are applied to analyse NO2 and PM10 measurements between January 2001 and December 2005. The results show that NO2 concentrations at background stations within the urban area are highly influenced by vehicle emissions. The effect vehicle emission has on NO2 at stations within new towns is smaller. CA results also show that variations in PM10 concentrations are distinguished by the area the station is located in. PCA results show that there are two principal components (PC's) associated with variations in roadside concentration of PM10. The strong influence of roadside emissions towards concentrations of NO2 and PM10 at a number of urban background stations may be due to their close proximity to busy roadways and the high density of surrounding tall buildings, which creates an enclosure that hinders dispersion of roadside emissions and results in air pollution behaviour that reflects variation in traffic.
7FA60214	Clustering is a discovery process in data mining. It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. Many advanced algorithms have difficulty dealing with highly variable clusters that do not follow a preconceived model. By basing its selections on both interconnectivity and closeness, the Chameleon algorithm yields accurate results for these highly variable clusters. Existing algorithms use a static model of the clusters and do not use information about the nature of individual clusters as they are merged. Furthermore, one set of schemes (the CURE algorithm and related schemes) ignores the information about the aggregate interconnectivity of items in two clusters. Another set of schemes (the Rock algorithm, group averaging method, and related schemes) ignores information about the closeness of two clusters as defined by the similarity of the closest items across two clusters. By considering either interconnectivity or closeness only, these algorithms can select and merge the wrong pair of clusters. Chameleon's key feature is that it accounts for both interconnectivity and closeness in identifying the most similar pair of clusters. Chameleon finds the clusters in the data set by using a two-phase algorithm. During the first phase, Chameleon uses a graph partitioning algorithm to cluster the data items into several relatively small subclusters. During the second phase, it uses an algorithm to find the genuine clusters by repeatedly combining these subclusters.	
7804FE7F	Coverage preservation, unique ID assignment and extension of network lifetime are important features for wireless sensor networks. Grouping sensor nodes into clusters is an effective way to improve the network performance. By analyzing communication energy consumption of the clusters and the impact of node failures on coverage with different densities, we propose a DEECIC (Distributed Energy-Efficient Clustering with Improved Coverage) algorithm. DEECIC aims at clustering with the least number of cluster heads to cover the whole network and assigning a unique ID to each node based on local information. In addition, DEECIC periodically updates cluster heads according to the joint information of nodes’ residual energy and distribution. The algorithm requires neither time synchronization nor knowledge of a node’s geographic location. Simulation results show that the proposed algorithm can prolong the network lifetime and improve network coverage effectively.
7C2C3EA0	22 Physico-chemical variables have been analyzed in water samples collected every three months for two and a half years from three sampling stations located along a section of 25 km of a river affected by man-made and seasonal influences. Exploratory analysis of experimental data have been carried out by box plots, ANOVA, display methods (principal component analysis) and unsupervised pattern recognition (cluster analysis) in an attempt to discriminate sources of variation of water quality. PCA has allowed the identification of a reduced number of “latent” factors with a hydrochemical meaning: mineral contents, man-made pollution and water temperature. Spatial (pollution from anthropogenic origin) and temporal (seasonal and climatic) sources of variation affecting quality and hydrochemistry of river water have been differentiated and assigned to polluting sources. An ANOVA of the rotated principal components has demonstrated that (i) mineral contents are seasonal and climate dependent, thus pointing to a natural origin for this polluting form and (ii) pollution by organic matter and nutrients originates from anthropogenic sources, mainly as municipal wastewater. The application of PCA and cluster analysis has achieved a meaningful classification of river water samples based on seasonal and spatial criteria.
7C271CAE	Clustering partitions a collection of objects into groups called clusters, such that similar objects fall into the same group. Similarity between objects is defined by a distance function satisfying the triangle inequality; this distance function along with the collection of objects describes a distance space. In a distance space, the only operation possible on data objects is the computation of distance between them. All scalable algorithms in the literature assume a special type of distance space, namely a k-dimensional vector space, which allows vector operations on objects. We present two scalable algorithms designed for clustering very large datasets in distance spaces. Our first algorithm BUBBLE is, to our knowledge, the first scalable clustering algorithm for data in a distance space. Our second algorithm BUBBLE-FM improves upon BUBBLE by reducing the number of calls to the distance function, which may be computationally very expensive. Both algorithms make only a single scan over the database while producing high clustering quality. In a detailed experimental evaluation, we study both algorithms in terms of scalability and quality of clustering. We also show results of applying the algorithms to a real life dataset.
6C904FD2	In this study, we evaluated the performance of Shanghai's air quality monitoring network (AQMN) using principal components analysis, an assignment method, and cluster analysis. Our goal was to improve the utilization of monitoring stations and evaluate Shanghai's air quality more comprehensively and accurately. Specifically, we (i) identified similar pollution sources or behaviors in the monitoring areas; (ii) identified redundant monitoring stations and re-evaluated the AQMN's performance without them; and (iii) proposed adjustments to the AQMN. We used data on particulates less than 2.5 μm (PM2.5) and 10 μm (PM10) in diameter, sulfur dioxide (SO2), nitrogen dioxide (NO2), ozone (O3), and carbon monoxide (CO) at stations in and around Shanghai from 1 January to 22 August 2014. For each pollutant, we grouped the monitoring stations into clusters based on their different pollution behaviors, revealing redundancy and inefficiency in the current AQMN that resulted from the concentrated station distribution and similarity of the monitoring environments. The analysis results showed that there exist redundant stations in the current AQMN of Shanghai. Furthermore, we proposed adjustments to Shanghai's AQMN: transfer four redundant stations and build a new station in the directions of the Taicang Experimental Primary School, Kunshan Zhenchuan Middle School, Suzhou Industrial Park, Wujiang Industrial Zone, and Jiaxing Monitoring Station. Our analysis suggests that, in addition to industrial, transportation, construction, and population influences inside Shanghai, external pollutants significantly affect Shanghai's air quality. Therefore, it is necessary to jointly prevent and control regional air pollution both in Shanghai and in neighboring cities.
42D8D46C	Efficient management of energy resources is a challenging research area in Wireless Sensor Networks (WSNs). Recent studies have revealed that clustering is an efficient topology control approach for organizing a network into a connected hierarchy which balances the traffic load of the sensor nodes and improves the overall scalability and the lifetime of WSNs. Inspired by the advantages of clustering techniques, we have three main contributions in this paper. First, we propose an energy efficient cluster formation algorithm called Active Node Cluster Formation (ANCF). The core aim to propose ANCF algorithm is to distribute heavy data traffic and high energy consumption load evenly in the network by offering unequal size of clusters in the network. The developed scheme appoints each cluster head (CH) near to the sink and sensing event while the remaining set of the cluster heads (CHs) are appointed in the middle of each cluster to achieve the highest level of energy efficiency in dense deployment. Second, we propose a lightweight sensing mechanism called Active Node Sensing Algorithm (ANSA). The key aim to propose the ANSA algorithm is to avoid high sensing overlapping data redundancy by appointing a set of active nodes in each cluster with satisfy coverage near to the event. Third, we propose an Active Node Routing Algorithm (ANRA) to address complex inter and intra cluster routing issues in highly dense deployment based on the node dominating values. Extensive experimental studies conducted through network simulator NCTUNs 6.0 reveal that our proposed scheme outperforms existing routing techniques in terms of energy efficiency, end-to-end delay and data redundancy, congestion management and setup robustness.
791CA357	We combine image-processing techniques with a powerful new statistical technique to detect linear pattern production faults in woven textiles. Our approach detects a linear pattern in preprocessed images via model-based clustering. It employs an approximate Bayes factor which provides a criterion for assessing the evidence for the presence of a defect. The model used in experimentation is a (possibly highly elliptical) Gaussian cloud superimposed on Poisson clutter. Results are shown for some representative examples, and contrasted with a Hough transform. Software for the statistical modeling is available.
7F02386E	We explore the idea of evidence accumulation for combining the results of multiple clusterings. Initially, n d-dimensional data is decomposed into a large number of compact clusters; the K-means algorithm performs this decomposition, with several clusterings obtained by N random initializations of the K-means. Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the data partitions are mapped into a co-association matrix of patterns. This n/spl times/n matrix represents a new similarity measure between patterns. The final clusters are obtained by applying a MST-based clustering algorithm on this matrix. Results on both synthetic and real data show the ability of the method to identify arbitrary shaped clusters in multidimensional data.
7A1A88AA	The study presents the application of selected chemometric techniques: cluster analysis, principal component analysis, factor analysis and discriminant analysis, to classify a river water quality and evaluation of the pollution data. Seventeen stations, monitored for 16 physical and chemical parameters in 4 seasons during the period 1999–2003, located at the Bagmati river basin in Kathmandu Valley, Nepal were selected for the purpose of this study. The results allowed, determining natural clusters of monitoring stations with similar pollution characteristics and identifying main discriminant variables that are important for regional water quality variation and possible pollution sources affecting the river water quality. The analysis enabled to group 17 monitoring sites into 3 regions with 5 major discriminating variables: EC, DO, CL, NO2N and BOD. Results revealed that some locations were under the high influence of municipal contamination and some others under the influence of minerals. This study demonstrated that chemometric method is effective for river water classification, and for rapid assessment of water qualities, using the representative sites; it could serve to optimize cost and time without losing any significance of the outcome.
766E9DA6	Survey properties of soil and rock mass have always been associated with uncertainty. Hence, the behavior of the soil or rock cannot be investigated specifically by choosing a value specified for these properties. One of the most common systems for studying properties of rock mass is the rock mass classification system (RMR) which was developed by Bieniawski. In this system the input parameters are divided into several classes, and each class has particular rating. In this system, because of uncertainties of the input parameters, determining the definite boundary between the classes and assigning a specified value to a particular class is difficult, so when the input parameters are close to the boundary between the classes, the class rating with certainity is not decided. The aim of this paper is to propose a hybrid nonlinear Chaotic and Neuro-Fuzzy system modeling for the basic RMR system uncertainty based on continuous functions. This model also proves the theory of Bieniawski that is based on nonlinear systems by using chaos theory and mathematical relations. The main advantage of proposed model is to directly predict output of RMR system classification system without considering the input parameters so that it leads to better results and a higher level of prediction rock quality.
805E1A24	The novel concept of pseudoerrors for a self-organizing neuro-fuzzy system (SO-NFS) is proposed for tracking control problem. To demonstrate the proposed approach, an example of motion control of an auto-warehousing crane system is illustrated, which can move back and forth in x,y, and z directions to access and store cargoes. The proposed SO-NFS shows excellent performance in control of the crane system for different loading conditions and varying distances in all directions.
7871F90D	A method for determining the mutual nearest neighbours (MNN) and mutual neighbourhood value (mnv) of a sample point, using the conventional nearest neighbours, is suggested. A nonparametric, hierarchical, agglomerative clustering algorithm is developed using the above concepts. The algorithm is simple, deterministic, noniterative, requires low storage and is able to discern spherical and nonspherical clusters. The method is applicable to a wide class of data of arbitrary shape, large size and high dimensionality. The algorithm can discern mutually homogenous clusters. Strong or weak patterns can be discerned by properly choosing the neighbourhood width.
7D078885	This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.
76459DD9	Conservation of energy and fault tolerance are two major issues in the deployment of a wireless sensor network (WSN). Design of clustering and routing algorithms for a large scale WSN should incorporate both these issues for the long run operation of the network. In this paper, we propose distributed clustering and routing algorithms jointly referred as DFCR. The algorithm is shown to be energy efficient and fault tolerant. The DFCR uses a distributed run time recovery of the sensor nodes due to sudden failure of the cluster heads (CHs). It takes care of the sensor nodes which have no CH within their communication range. We perform extensive experiments on the proposed algorithm using various network scenarios. The experimental results are compared with the existing algorithms to demonstrate the strength of the algorithm in terms of various performance metrics.
7C443D8B	The recent advances in genomic technologies and the availability of large-scale microarray datasets call for the development of advanced data analysis techniques, such as data mining and statistical analysis to cite a few. Among the mining techniques proposed so far, cluster analysis has become a standard method for the analysis of microarray expression data. It can be used both for initial screening of patients and for extraction of disease molecular signatures. Moreover, clustering can be profitably exploited to characterize genes of unknown function and uncover patterns that can be interpreted as indications of the status of cellular processes. Finally, clustering biological data would be useful not only for exploring the data but also for discovering implicit links between the objects. To this end, several clustering approaches have been proposed in order to obtain a good trade-off between accuracy and efficiency of the clustering process. In particular, great attention has been devoted to hierarchical clustering algorithms for their accuracy in unsupervised identification and stratification of groups of similar genes or patients, while, partition based approaches are exploited when fast computations are required. Indeed, it is well known that no existing clustering algorithm completely satisfies both accuracy and efficiency requirements, thus a good clustering algorithm has to be evaluated with respect to some external criteria that are independent from the metric being used to compute clusters. In this paper, we propose a clustering algorithm called M-CLUBS (for Microarray data CLustering Using Binary Splitting) exhibiting higher accuracy than the hierarchical ones proposed so far while allowing a faster computation with respect to partition based approaches. Indeed, M-CLUBS is faster and more accurate than other algorithms, including k-means and its recently proposed refinements, as we will show in the experimental section. The algorithm consists of a divisive phase and an agglomerative phase; during these two phases, the samples are repartitioned using a least quadratic distance criterion possessing unique analytical properties that we exploit to achieve a very fast computation. M-CLUBS derives good clusters without requiring input from users, and it is robust and impervious to noise, while providing better speed and accuracy than methods, such as BIRCH, that are endowed with the same critical properties. Due to the structural feature of microarray data (they are represented as arrays of numeric values), M-CLUBS is suitable for analyzing them since it is designed to perform well for Euclidean distances. In order to stronger the obtained results we interpreted the obtained clusters by a domain expert and the evaluation by quality measures specifically tailored for biological validity assessment.
7EB83834	Applications running on exascale machines will be complex in many ways. They will involve dynamic and adaptive refinements, and will be composed of multiple, independently developed modules, often involving a multiphysics simulation. The programming models of this era must have several characteristics. First, they need to do away with the notion of processors, and automate resource management via adaptive runtime systems. Data structure-specific frameworks and domain-specific environments will be needed to further simplify programming. More importantly, parallel mini-languages need to be developed, such that each language captures only a restricted subset of possible parallel interactions, but allows for a simple expression of them. Coupled with interoperability and parallel composition, which must be supported in many ways, including message-driven runtime systems, this will create a productive ecosystem of parallel programming models for the exascale era.
0D1E4331	The aim of this study was to evaluate the performance of two statistical methods, principal component analysis (PCA) and cluster analysis (CA), for the management of air quality monitoring network (AQMN) of Oporto Metropolitan Area (Oporto-MA). The specific objectives were: (i) to identify city areas with similar air pollution behaviours; and (ii) to locate emission sources. The statistical methods were applied to the mass concentrations of carbon monoxide (CO), nitrogen dioxide (NO2) and ozone (O3), collected in the AQMN of Oporto-MA from January 2003 to December 2005.It was demonstrated that for each pollutant the monitoring sites are grouped into different classes based on their air pollution behaviour. The sites were divided: (i) into three different groups for CO and for NO2 and (ii) into two groups for O3. It was also found that several monitoring sites covered city areas characterized by the same specific air pollution behaviour, suggesting then an ineffective management of the air quality-monitoring system. The redundant equipment should be transferred to other monitoring sites allowing enlargement of the monitored area. The conclusions obtained with the statistical methods were supported by the location of main emission sources through the analysis of the wind direction. Four main emission sources of CO and NO2 were located. Additionally, it was concluded that the sea wind had an important contribution towards the increase in the O3 concentration. For all pollutants, two sites were always coupled in one group due to the different air pollution behaviour presented in the analysed period.
791E94C8	Clustering is the process of organizing dataset into isolated groups such that data points in the same are more similar and data points of different groups are more dissimilar. The k-modes algorithm well known for its simplicity is a popular partitioning algorithm for clustering categorical data. In this paper, we discuss the limitations of distance function used in this algorithm with an illustrative example and then we propose a similarity coefficient based on Information Entropy. We analyze the time complexity of the k-modes algorithm with proposed similarity coefficient. The main advantage of this coefficient is that it improves the clustering accuracy while retaining scalability of the k-modes algorithm. We perform the scalability tests on synthetic datasets.
76802CE9	This case study reports different multivariate statistical techniques applied for evaluation of temporal/spatial variations and interpretation of a large complex water-quality data set obtained during monitoring of Gomti River in Northern part of India. Water quality of the Gomti River, a major tributary of the Ganga River was monitored at eight different sites selected in relatively low, moderate and high pollution regions, regularly over a period of 5 years (1994-1998) for 24 parameters. The complex data matrix (17,790 observations) was treated with different multivariate techniques such as cluster analysis, factor analysis/principal component analysis (FA/PCA) and discriminant analysis (DA). Cluster analysis (CA) showed good results rendering three different groups of similarity between the sampling sites reflecting the different water-quality parameters of the river system. FA/PCA identified six factors, which are responsible for the data structure explaining 71% of the total variance of the data set and allowed to group the selected parameters according to common features as well as to evaluate the incidence of each group on the overall variation in water quality. However, significant data reduction was not achieved, as it needed 14 parameters to explain 71% of both the temporal and spatial changes in water quality. Discriminant analysis showed the best results for data reduction and pattern recognition during both temporal and spatial analysis. Discriminant analysis showed five parameters (pH, temperature, conductivity, total alkalinity and magnesium) affording more than 88% right assignations in temporal analysis, while nine parameters (pH, temperature, alkalinity, Ca-hardness, DO, BOD, chloride, sulfate and TKN) to afford 91% right assignations in spatial analysis of three different regions in the basin. Thus, DA allowed reduction in dimensionality of the large data set, delineating a few indicator parameters responsible for large variations in water quality. This study presents necessity and usefulness of multivariate statistical techniques for evaluation and interpretation of large complex data sets with a view to get better information about the water quality and design of monitoring network for effective management of water resources.
7A897473	This paper introduces a novel distance measure for clustering high dimensional data based on the hitting time of two Minimal Spanning Trees (MST) grown sequentially from a pair of points by Prim’s algorithm. When the proposed measure is used in conjunction with spectral clustering, we obtain a powerful clustering algorithm that is able to separate neighboring non-convex shaped clusters and to account for local as well as global geometric features of the data set. Remarkably, the new distance measure is a true metric even if the Prim algorithm uses a non-metric dissimilarity measure to compute the edges of the MST. This metric property brings added flexibility to the proposed method. In particular, the method is applied to clustering non Euclidean quantities, such as probability distributions or spectra, using the Kullback–Leibler divergence as a base measure. We reduce computational complexity by applying consensus clustering to a small ensemble of dual rooted MSTs. We show that the resultant consensus spectral clustering with dual rooted MST is competitive with other clustering methods, both in terms of clustering performance and computational complexity. We illustrate the proposed clustering algorithm on public domain benchmark data for which the ground truth is known, on one hand, and on real-world astrophysical data on the other hand.
5AD9E47D	We report a comparative study using three different chemometric techniques to evaluate both spatial and temporal changes in Suquı́a River water quality, with a special emphasis on the improvement obtained using discriminant analysis for such evaluation. We have monitored 22 parameters at different stations from the upper, middle, and beginning of the lower river basin during at least two years including 232 different samples. We obtained a complex data matrix, which was treated using the pattern recognition techniques of cluster analysis (CA), factor analysis/principal components (FA/PCA), and discriminant analysis (DA). CA renders good results as a first exploratory method to evaluate both spatial and temporal differences, however it fails to show details of these differences. FA/PCA needs 13 parameters to point out 71% of both temporal and spatial changes; consequently data reduction from FA/PCA in this case is not as considerable as expected. However, FA/PCA allows to group the selected parameters according to common features as well as to evaluate the incidence of each group on the overall change in water quality, specially during the analysis of temporal changes. DA technique shows the best results for data reduction and pattern recognition during both temporal and spatial analysis. DA renders an important data reduction using 6 parameters to afford 87% right assignations during temporal analysis. Besides, it uses only 5 parameters to yield 75% right assignations during the spatial analysis of four different basin areas. DA allowed us to greatly reduce the dimensionality of the starting data matrix, pointing out to a few parameters that indicate the biggest changes in water quality as well as variation patterns associated with seasonal variations, urban run-off, and pollution sources, presenting a novel approach for water quality assessments.
7E027E2A	In Bilbao (Spain), an air quality network measures sulphur dioxide levels at 4 locations. The objective of this paper is to develop a practical methodology to identify redundant sensors and evaluate a network's capability to correctly follow and represent SO2 fields in Bilbao, in the frame of a continuous network optimization process.The methodology is developed and tested at this particular location, but it is general enough to be useable at other places as well, since it is not tied neither to the particular geographical characteristics of the place nor to the phenomenology of the air quality over the area.To assess the spatial variability of SO2 measured at 4 locations in the area, three different techniques have been used: Self-Organizing Maps (SOMs), cluster analysis (CA) and Principal Component Analysis (PCA). The results show that the three techniques yield the same results, but the information obtained via PCA can be helpful not only for that purpose but also to throw light on the major mechanisms involved. This might be used in future network optimization stages. The main advantage of cluster analysis and SOMs is that they provide readily interpretable results. All the calculations have been carried out using the freely available software R.
80401A34	Distributed clustering is a new research field of data mining now. In this paper, one of distributed clustering named DCBKC (Distributed Clustering Based on K-means and Coarse-grained parallel genetic algorithm) based on K-means and Coarse-grained Parallel Genetic Algorithm is advanced. The algorithm can solve local clustering problem of distributed clustering effectively, reflect all of local data characters, enhance local data’s perspectivity and decrease network overload at a way by adopting proper migration strategy simultaneously. Both theory analysis and experimental results confirm that DCBKC is feasible.
7FA23803	Cross-national research on health system performance can yield important findings for public policy purposes. We seek to further this research by examining the problem of selection bias, an important methodological issue that investigators initially should consider. Because of the logistical difficulties and enormous expense involved in collecting voluminous data from many countries, researchers often must rely on information contained in data sets of international organizations, such as the World Health Organization (WHO) and the Organization for Economic Cooperation and Development. Under the circumstances, the comparisons that researchers can make will depend to a great extent on the availability and richness of data for certain measures. This situation raises the potential for selection or experimenter bias. We use multivariate statistics to group countries with similar characteristics, an approach that we believe will mitigate the problem. We perform a cluster analysis of 186 countries using principal components derived from 7 demographic variables and 27 mortality and burden of disease variables. Our analysis produced six clusters that we believe represent suitable groupings for comparative purposes.
80636922	Currently in the Internet many collaborative tagging sites exist, but there is the need for a service to integrate the data from the multiple sites to form a large and unified set of collaborative data from which users can have more accurate and richer information than from a single site. In our paper, we have proposed a collective collaborative tagging (CCT) service architecture in which both service providers and individual users can merge folksonomy data (in the form of keyword tags) stored in different sources to build a larger, unified repository. We have also examined a range of algorithms that can be applied to different problems in folksonomy analysis and information discovery. These algorithms address several common problems for online systems: searching, getting recommendations, finding communities of similar users, and finding interesting new information by trends. Our contributions are to a) systematically examine the available public algorithms' application to tag-based folksonomies, and b) to propose a service architecture that can provide these algorithms as online capabilities.
8062E538	We provide an overall framework for learning in search based systems that are used to find optimum solutions to problems. This framework assumes that prior knowledge is available in the form of one or more heuristic functions (or features) of the problem domain. An appropriate clustering strategy is used to partition the state space into a number of classes based on the available features. The number of classes formed will depend on the resource constraints of the system. In the training phase, example problems are run using a standard admissible search algorithm. In this phase, heuristic information corresponding to each class is learned. This new information can be used in the problem solving phase by appropriate search algorithms so that subsequent problem instances can be solved more efficiently. In this framework, we also show that heuristic information of forms other than the conventional single valued underestimate value can be used, since we maintain the heuristic of each class explicitly. We show some novel search algorithms that can work with some such forms. Experimental results have been provided for some domains.
5D87E519	Implicitly parallel programming languages place the burden of exploiting and managing parallelism upon the compiler and runtime system, rather than on the programmer. This paper describes the design of NIP, a runtime system for supporting implicit parallelism in languages which combine both functional and object-oriented programming. NIP is designed for scaleable distributed memory systems including networks of workstations and custom parallel machines. The key components of NIP are: a parallel task execution unit which includes an efficient method for lazily creating parallel tasks from loop iterations; a distributed shared memory system optimised for parallel object-oriented programs; and a load balancing system for distributing work over the nodes of the parallel system. The paper describes the requirements placed on the runtime system by an implicitly parallel language and then details the design of the components that comprise NIP, showing how the components meet these requirements. Performance results for NIP running programs on a network of workstations are presented and analysed.
75CEF15A	Minimizing energy dissipation and maximizing network lifetime are among the central concerns when designing applications and protocols for sensor networks. Clustering has been proven to be energy-efficient in sensor networks since data routing and relaying are only operated by cluster heads. Besides, cluster heads can process, filter and aggregate data sent by cluster members, thus reducing network load and alleviating the bandwidth. In this paper, we propose a novel distributed clustering algorithm where cluster heads are elected following a three-way message exchange between each sensor and its neighbors. Sensor’s eligibility to be elected cluster head is based on its residual energy and its degree. Our protocol has a message exchange complexity of and a worst-case convergence time complexity of . Simulations show that our algorithm outperforms EESH, one of the most recently published distributed clustering algorithms, in terms of network lifetime and ratio of elected cluster heads.
807A339B	Topology control in a sensor network balances load on sensor nodes and increases network scalability and lifetime. Clustering sensor nodes is an effective topology control approach. We propose a novel distributed clustering approach for long-lived ad hoc sensor networks. Our proposed approach does not make any assumptions about the presence of infrastructure or about node capabilities, other than the availability of multiple power levels in sensor nodes. We present a protocol, HEED (Hybrid Energy-Efficient Distributed clustering), that periodically selects cluster heads according to a hybrid of the node residual energy and a secondary parameter, such as node proximity to its neighbors or node degree. HEED terminates in O(1) iterations, incurs low message overhead, and achieves fairly uniform cluster head distribution across the network. We prove that, with appropriate bounds on node density and intracluster and intercluster transmission ranges, HEED can asymptotically almost surely guarantee connectivity of clustered networks. Simulation results demonstrate that our proposed approach is effective in prolonging the network lifetime and supporting scalable data aggregation.
7F9D2BC6	Up to now, it still remains a big challenge for us to build a high performance geo-computing system with high processing speed and also be easy of use by domain researchers. The unprecedented scale data and various complex algorithms pose many computational and management challenges. To properly settle these main issues above, a new system framework for high performance geo-computing is presented in this paper. A High Performance Geo-data Object Storage System (HPGOSS) base on parallel file System is used for eliminating I/O performance bottleneck and deal with the data managing problem result from the close relevancy between geo-information and remote sensing image data. Parallel programming models for fast parallelization of geo-computing algorithms are proposed. In addition, the job scheduling strategy and workflow engine are also discussed. Finally, such system could provide a parallel geo-computing environment with high performance, easy to use, optimal resource utilization, and high scalability.
7900DDE3	Multivariate statistical techniques, such as cluster analysis (CA), factor analysis (FA), principal component analysis (PCA) and discriminant analysis (DA) were applied to the data set on water quality of the Gomti river (India), generated during three years (1999–2001) monitoring at eight different sites for 34 parameters (9792 observations). This study presents usefulness of multivariate statistical techniques for evaluation and interpretation of large complex water quality data sets and apportionment of pollution sources/factors with a view to get better information about the water quality and design of monitoring network for effective management of water resources. Three significant groups, upper catchments (UC), middle catchments (MC) and lower catchments (LC) of sampling sites were obtained through CA on the basis of similarity between them. FA/PCA applied to the data sets pertaining to three catchments regions of the river resulted in seven, seven and six latent factors, respectively responsible for the data structure, explaining 74.3, 73.6 and 81.4% of the total variance of the respective data sets. These included the trace metals group (leaching from soil and industrial waste disposal sites), organic pollution group (municipal and industrial effluents), nutrients group (agricultural runoff), alkalinity, hardness, EC and solids (soil leaching and runoff process). DA showed the best results for data reduction and pattern recognition during both temporal and spatial analysis. It rendered five parameters (temperature, total alkalinity, Cl, Na and K) affording more than 94% right assignations in temporal analysis, while 10 parameters (river discharge, pH, BOD, Cl, F, PO4, NH4–N, NO3–N, TKN and Zn) to afford 97% right assignations in spatial analysis of three different regions in the basin. Thus, DA allowed reduction in dimensionality of the large data set, delineating a few indicator parameters responsible for large variations in water quality. Further, receptor modeling through multi-linear regression of the absolute principal component scores (APCS-MLR) provided apportionment of various sources/factors in respective regions contributing to the river pollution. It revealed that soil weathering, leaching and runoff; municipal and industrial wastewater; waste disposal sites leaching were among the major sources/factors responsible for river quality deterioration.
766897A3	Clustering is an efficient topology control method which balances the traffic load of the sensor nodes and improves the overall scalability and the life time of the wireless sensor networks (WSNs). However, in a cluster based WSN, the cluster heads (CHs) consume more energy due to extra work load of receiving the sensed data, data aggregation and transmission of aggregated data to the base station. Moreover, improper formation of clusters can make some CHs overloaded with high number of sensor nodes. This overload may lead to quick death of the CHs and thus partitions the network and thereby degrade the overall performance of the WSN. It is worthwhile to note that the computational complexity of finding optimum cluster for a large scale WSN is very high by a brute force approach. In this paper, we propose a novel differential evolution (DE) based clustering algorithm for WSNs to prolong lifetime of the network by preventing faster death of the highly loaded CHs. We incorporate a local improvement phase to the traditional DE for faster convergence and better performance of our proposed algorithm. We perform extensive simulation of the proposed algorithm. The experimental results demonstrate the efficiency of the proposed algorithm.
7E068751	Multivariate statistical techniques, such as cluster analysis (CA), principal component analysis (PCA), factor analysis (FA) and discriminant analysis (DA), were applied for the evaluation of temporal/spatial variations and the interpretation of a large complex water quality data set of the Fuji river basin, generated during 8 years (1995–2002) monitoring of 12 parameters at 13 different sites (14 976 observations). Hierarchical cluster analysis grouped 13 sampling sites into three clusters, i.e., relatively less polluted (LP), medium polluted (MP) and highly polluted (HP) sites, based on the similarity of water quality characteristics. Factor analysis/principal component analysis, applied to the data sets of the three different groups obtained from cluster analysis, resulted in five, five and three latent factors explaining 73.18, 77.61 and 65.39% of the total variance in water quality data sets of LP, MP and HP areas, respectively. The varifactors obtained from factor analysis indicate that the parameters responsible for water quality variations are mainly related to discharge and temperature (natural), organic pollution (point source: domestic wastewater) in relatively less polluted areas; organic pollution (point source: domestic wastewater) and nutrients (non-point sources: agriculture and orchard plantations) in medium polluted areas; and organic pollution and nutrients (point sources: domestic wastewater, wastewater treatment plants and industries) in highly polluted areas in the basin. Discriminant analysis gave the best results for both spatial and temporal analysis. It provided an important data reduction as it uses only six parameters (discharge, temperature, dissolved oxygen, biochemical oxygen demand, electrical conductivity and nitrate nitrogen), affording more than 85% correct assignations in temporal analysis, and seven parameters (discharge, temperature, biochemical oxygen demand, pH, electrical conductivity, nitrate nitrogen and ammonical nitrogen), affording more than 81% correct assignations in spatial analysis, of three different sampling sites of the basin. Therefore, DA allowed a reduction in the dimensionality of the large data set, delineating a few indicator parameters responsible for large variations in water quality. Thus, this study illustrates the usefulness of multivariate statistical techniques for analysis and interpretation of complex data sets, and in water quality assessment, identification of pollution sources/factors and understanding temporal/spatial variations in water quality for effective river water quality management.
7BF77DA5	In this paper, we consider an entropy criterion to estimate the number of clusters arising from a mixture model. This criterion is derived from a relation linking the likelihood and the classification likelihood of a mixture. Its performance is investigated through Monte Carlo experiments, and it shows favorable results compared to other classical criteria.
7E05645F	This paper introduces a new parallel programming model motivated by: 1) the concept that computation should move to, and execute near, the global data which it accesses, 2) a set of extended memory semantics to provide fine-grained global synchronization, 3) architectural support for fast lightweight thread creation/destruction/migration, and 4) the need for a high performance language to provide the programmer with transparency to the generated code while protecting them from making low-level errors. Using pseudocode examples, we compare this new model to several other high performance languages: Chapel, Fortress, and UPC, in terms of 1) expressibility of parallel structures, 2) facility in synchronizing communication to avoid race conditions, and 3) ability to diagnose/resolve possible performance issues that result from the mapping of these structures to hardware and system software. The new model, combined with appropriate architectural support, provides equal potential for expressibility and safety while giving the programmer more direct insight into the code that ultimately executes.
7D1222C0	Clustering is a very important tool in data mining and is widely used in on-line services for medical, financial and social environments. The main goal in clustering is to create sets of similar objects in a data set. The data set to be used for clustering can be owned by a single entity, or in some cases, information from different databases is pooled to enrich the data so that the merged database can improve the clustering effort. However, in either case, the content of the database may be privacy sensitive and/or commercially valuable such that the owners may not want to share their data with any other entity, including the service provider. Such privacy concerns lead to trust issues between entities, which clearly damages the functioning of the service and even blocks cooperation between entities with similar data sets. To enable joint efforts with private data, we propose a protocol for distributed clustering that limits information leakage to the untrusted service provider that performs the clustering. To achieve this goal, we rely on cryptographic techniques, in particular homomorphic encryption, and further improve the state of the art of processing encrypted data in terms of efficiency by taking the distributed structure of the system into account and improving the efficiency in terms of computation and communication by data packing. While our construction can be easily adjusted to a centralized or a distributed computing model, we rely on a set of particular users that help the service provider with computations. Experimental results clearly indicate that the work we present is an efficient way of deploying a privacy-preserving clustering algorithm in a distributed manner.
7FE8D880	Swarm intelligence forms the core of a new class of algorithms inspired by the social behavior of insects that live in swarms. Its attractive features include adaptation, robustness and a distributed, decentralized nature, rendering swarm-based algorithms well-suited for routing in wireless or satellite networks, where it is difficult it implement centralized network control. We propose one such routing algorithm, dubbed adaptive swarm-based distributed routing (adaptive-SDR), which is scalable, robust and suitable to handle large amounts of network traffic, while minimizing delay and packet loss.
7BBB7054	Here we tested the application of a nodal analysis for the elaboration of biotic indices for particular stressing conditions. The work was carried out in an intermittent Mediterranean stream where superficial flow was absent during summer. The river was perturbed by an effluent with high pH, sulphates, nitrates and conductivity. “Summer” and “winter” samples were treated separately. We first identified groups of sites differing in taxonomical composition by cluster analysis. Then we tested whether groups of sites also differed in their abiotic characteristics. In the following step, groups of cooccurring taxa were also identified by cluster analysis. The indicator value of a taxa group was measured by fidelity measurements for site groups. Indicator taxa were incorporated in a water quality table. The biotic index in the water quality table clearly discriminated impacted from reference sites in the two following years and was correlated with the first axis of a correspondence analysis biplot which also discriminated impacted from clean sites. We suggest that nodal analysis can be a reliable technique for the identification of bioindicators and the elaboration of biotic indices.
774C2C5B	In implementations of neuromorphic computing systems (NCS), memristor and its crossbar topology have been widely used to realize fully connected neural networks. However, many neural networks utilized in real applications often have a sparse connectivity, which is hard to be efficiently mapped to a crossbar structure. Moreover, the scale of the neural networks is normally much larger than that can be offered by the latest integration technology of memristor crossbars. In this work, we propose AutoNCS -- an EDA framework that can automate the NCS designs that combine memristor crossbars and discrete synapse modules. The connections of the neural networks are clustered to improve the utilization of the memristor elements in crossbar structures by taking into account the physical design cost of the NCS. Our results show that AutoNCS can substantially enhance the utilization efficiency of memristor crossbars while reducing the wirelength, area and delay of the physical designs of the NCS.
808B43EF	The large prevalence of multimedia systems in recent years makes the security of multimedia communications an important and critical issue. We study the problem of securing the delivery of scalable video streams so that receivers can ensure the authenticity of the video content. Our focus is on recent scalable video coding (SVC) techniques, such as H.264/SVC, which can provide three scalability types at the same time: temporal, spatial, and visual quality. This three-dimensional scalability offers a great flexibility that enables customizing video streams for a wide range of heterogeneous receivers and network conditions. This flexibility, however, is not supported by current stream authentication schemes in the literature. We propose an efficient and secure authentication scheme that accounts for the full scalability of video streams, and enables verification of all possible substreams that can be extracted from the original stream. In addition, we propose an algorithm for minimizing the amount of authentication information that need to be attached to streams. The proposed authentication scheme supports end-to-end authentication, in which any third-party entity involved in the content delivery process, such as stream adaptation proxies and caches, does not have to understand the authentication mechanism. Our simulation study with real video traces shows that the proposed authentication scheme is robust against packet losses, incurs low computational cost for receivers, has short delay, and adds low communication overhead. Finally, we implement the proposed authentication scheme as an open source library called svcAuth, which can be used as a transparent add-on by any multimedia streaming application.
74E8AB32	Policy-based networking (PBN) is gaining a wide acceptance in IP management, resulting in a more unified control and management approach toward complexity of IP management. QoS interworking in IP management based on PBN is going to provide QoS guaranteed/differentiated IP connection services. QoS interworking issues between enterprise network and public IP network are studied. The exponential growth of intranet/Internet interworking itself may put PBN in jeopardy. To counter the increasing management complexity, two approaches, policy abstraction and hierarchy organization with precedence rules are proposed and studied.
7B9D73D9	Hyperspectral imaging instruments capture and collect hundreds of different wavelength data corresponding to the same surface. As a result, tons of information must be stored, processed and transmitted to ground. However, the downlink bandwidth is limited, and transmitting all data from the satellite to ground is a slow task that jeopardizes the use of this information for applications under real-time or near real-time constraints. This is the reason why most of the research activity is moving towards developing solutions which are able to process this data on-board, sending back to ground only relevant information.In this context, this paper presents three different FPGA-based architectures, implemented on an FPGA, which perform the Modified Vertex Component Analysis (MVCA) algorithm, as part of the hyperspectral linear unmixing processing chain. As a consequence, not only high performance but flexible and reusable designs have been designed. Moreover, final results demonstrate the influence of the implemented parallelization methodology over the final performance which motivates the system adaptivity through the scalability feature.
7D996511	In this paper, we derive the rate distortion lower bounds of spatially scalable video coding techniques. The methods we evaluate are subband and pyramid motion compensation where temporal redundancies in the same spatial layer as well as interlayer spatial redundancies are exploited in the enhancement layer encoding. The rate distortion bounds are derived from rate distortion theory for stationary Gaussian signals where mean square error is used as the distortion criteria. Assuming that the base layer is encoded by a nonscalable video coder, we derive the rate distortion functions for the enhancement layer, which depend upon the power spectral density of the input signal, the motion prediction error probability density function and the base layer encoding performance. We will show that pyramid and subband methods are expected to outperform independently encoding the enhancement layer using motion-compensated prediction, in terms of rate distortion efficiency, when the base layer is encoded at a relatively higher quality or less accurate displacement estimation happens in the enhancement layer.
7E6B030D	In this paper, we propose a scalable service scheme for secure group communication in grid. In the service scheme, a series of methods and strategies are presented, such as the initialization methods for group member, administrative domain and virtual organization, the key distribution strategy and the rekeying strategy. In order to improve the scalability of this service scheme, the services for a group are logically divided into two hierarchical levels, which is in accordance with the characteristics of group communication in grid. In addition, in order to show the efficiency and the scalability of the service scheme, simulation experiments are done. The results show that the service scheme is efficient and scalable. Thus, the service scheme can satisfy the requirement of people in large-scale, dynamic grid environment.
75B7139F	The new vision of cloud computing demands scalable, available and autonomic software platforms in order to deploy applications and services accessible anywhere and anytime. Multi-tier architectures are an important building block for many applications that are deployed in the cloud. This paper presents a novel caching and replication infrastructure that facilitates the scalable and elastic deployment of multi-tier architectures. Our Elastic SI-Cache is a novel multi-version cache that attains high performance and consistency in multi-tier systems. In contrast to most existing caches, Elastic SI-Cache provides snapshot isolation coherently across all tiers. Furthermore, Elastic SI-Cache supports scalable replication of the different tiers where replicas can be added or removed dynamically as needed, making the cache amenable for cloud computing environments. Elastic SI-Cache has been implemented and integrated into an open source JEE application server and its performance evaluated with the industrial benchmark SPECjAppServer.
7A792280	The public-key cryptography is indispensable for securing the smart grid communications. In this paper, we propose a hierarchical and fully-connected public key infrastructure that considers the smart grid characteristics. In the proposed public key infrastructure, each certificate authority is responsible for managing the public-key certificates for a geo-bounded small area. We also propose a novel format for the certificates that does not only bind a node's identity to its public key but also to its privileges and permissions. Finally we propose efficient and scalable certificate- renewing scheme that can much reduce the overhead of renewing certificates. Our verifications and evaluations demonstrate that using public key cryptography is essential for securing the smart grid and our proposals are scalable. Moreover, the simulation results demonstrate that the certificate-renewing scheme can significantly reduce the overhead of certificate renewals.
7B964191	Advanced metering infrastructure (AMI) is a major part of a smart grid system, and it deals with both data collection from smart meters and processing of those data. The traditional AMI architecture uses a centralized operation center with a centralized meter data management system (MDMS), which makes this system non-scalable. The system needs to be scalable so that with increased demand, it can be expanded at minimal cost. In this paper, we used two types of scalable distributed communication architectures, as initially proposed by Zhou et al. [1], namely, communication architecture with distributed MDMS and fully distributed communication architecture to minimize the deployment cost. We modified the analysis approach and used MATLAB-based code incorporating a Heuristic algorithm to determine close-to-optimal solutions for optimization problems. The unique feature of our work is the process of calculating accumulated bandwidth distance, in which distances between different components of an AMI were calculated according to the practical grid system layout of a city's infrastructure system. Theoretically developed scalability analysis was performed following [1], and the results were compared with the simulated results to indicate the validity of the asymptotic theoretical analysis. In our simulation, we found that the average distance between MDMS and the operation center was significantly different from that of Zhou et al. [1]. Our simulation results also indicated that both of the proposed architectures were scalable with significantly lower total deployment cost compared to the existing centralized communication architecture.
7F7C98E5	In this paper we propose a wavelet based coding algorithm for color images using a luminance/chrominance color space. Data rate scalability is achieved by using an embedded coding scheme, which is similar to Shapiro's (1993) embedded zerotree wavelet (EZW) algorithm. In a luminance/chrominance color space, the three color components have little statistical correlation. However, observations are made that at the spatial locations where chrominance signals have large transitions, it is highly likely for the luminance signal to have large transitions. This interdependence between the color components is exploited in the algorithm.
7EAADF32	In s-to-p broadcasting, s processors in a processor machine contain a message to be broadcast to all the processors, 1/spl les/s/spl les/p. We present a number of different broadcasting algorithms that handle all ranges of s. We show how the performance of each algorithm is influenced by the distribution of the s source processors and by the relationships between the distribution and the characteristics of the interconnection network. For the Intel Paragon we show that for each algorithm and machine dimension there exist ideal distributions and distributions on which the performance degrades. For the Cray T3D we also demonstrate dependencies between distributions and machine sizes. To reduce the dependence of the performance on the distribution of sources, we propose a repositioning approach. In this approach, the initial distribution is turned into an ideal distribution of the target broadcasting algorithm. We report experimental results for the Intel Paragon and Cray T3D and discuss scalability and performance.
6E42AD7F	There are many scenarios in which the same data must be delivered over a packet switched network to a large set of receivers. The Internet enables efficient multipoint transmissions through IP multicast by allowing data transmission to all receivers with a single send. Most approaches to scalable reliable multicast utilize receiver-oriented retransmissions. Defining an API for receiver-oriented reliable multicast is difficult because it is not clear how to manage the sender's cache and to schedule repairs. We outline an approach to defining an API based on logical cache persistence that addresses these problems. We also we explore the issues involved in defining an API for reliable multicast protocol on the Internet that can scale to millions of receivers.
763CE745	The content authenticity of the multimedia delivery is important issue with rapid development and widely used of multimedia technology. Till now many authentication solutions had been proposed, such as cryptology and watermarking based methods. However, in latest heterogeneous network the video stream transmission has been coded in scalable way such as H.264/SVC, there is still no good authentication solution. In this paper, we firstly summarized related works and proposed a scalable content authentication scheme using a ratio of different energy (RDE) based perceptual hashing in Q/S dimension, which is used Dempster-Shafer theory and combined with the latest scalable video coding (H.264/SVC) construction. The idea of “sign once and verify in scalable way” can be realized. Comparing with previous methods, the proposed scheme based on perceptual hashing outperforms previous works in uncertainty (robustness) and efficiencies in the H.264/SVC video streams. At last, the experiment results verified the performance of our scheme.
78114AC2	In both the regular and the irregular MPI (Message-Passing Interface) collective communication and reduction interfaces there is a correspondence between the argument lists and certain MPI derived datatypes. As a means to address and alleviate well-known memory and performance scalability problems in the irregular (or vector) collective interface definitions of MPI we propose to push this correspondence to its natural limit, and replace the interfaces of the MPI collectives with a different set of interfaces that specify all data sizes and displacements solely by means of derived datatypes. This reduces the number of collective (communication and reduction) interfaces from 16 to 10, significantly generalizes the operations, unifies regular and irregular collective interfaces, makes it possible to decouple certain algorithmic decisions from the collective operation, and moves the interface scalability issue from the collective interfaces to the MPI derived datatypes. To complete the proposal we discuss the memory scalability of the derived datatypes and suggest a number of alternative datatypes for MPI, some of which should be of independent interest. A running example illustrates the benefits of this alternative set of collective interfaces. Implementation issues are discussed showing that an implementation can be undertaken within any reasonable MPI library implementation.
80E27171	We propose an efficient strategy for resource management for scalable QoS guaranteed real-time communication services. This strategy is based on sink trees, and is particularly well suited for differentiated-services based architectures. We first show that finding a set of sink-trees in a given network is NP-complete. Then we propose a heuristic algorithm that always efficiently produces a set of sink-trees for a given network. Sink-tree based resource management integrates routing and resource reservation along the routes, and therefore has a number of advantages over other resource management scheme, in terms of: admission probability, link resource utilization, flow set up latency, signaling overhead, and routing over-head. In this paper we show by simulation experiments that even for simple cases the sink-tree based approach shows excellent results in terms of admission probability.
7E101CFA	The IBM Cyclops-64 (C64) chip employs a multithreaded architecture that integrates a large number of hardware thread units on a single chip. A cellular supercomputer is being developed based on a 3D-mesh connection of the C64 chips. This paper introduces the Cyclops datagram protocol (CDP) developed for the C64 supercomputer system. CDP is inspired by the TCP/IP protocol, yet simpler and more compact. The implementation of CDP leverages the abundant hardware thread-level parallelism provided by the C64 multithreaded architecture. The main contributions of this paper are: (1) We have completed a design and implementation of CDP that is used as the fundamental communication infrastructure for the C64 supercomputer system. (2) CDP successfully exploits the massive thread-level parallelism provided on the C64 hardware, achieving good performance scalability; (3) CDP is quite efficient. Its peak throughput reaches 884Mbps on the gigabit Ethernet, even it is running at the user-level on a single-processor Linux machine; (4) Extensive application test cases are passed and no reliability problems have been reported.
80485FDF	Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes an overlay-based Internet Indirection Infrastructure ( I3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows I3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.
7D11BF0C	In this paper, we investigate the scalability of three communication architectures for advanced metering infrastructure (AMI) in smart grid. AMI in smart grid is a typical cyber-physical system (CPS) example, in which large amount of data from hundreds of thousands of smart meters are collected and processed through an AMI communication infrastructure. Scalability is one of the most important issues for the AMI deployment in smart grid. In this study, we introduce a new performance metric, accumulated bandwidthdistance product (ABDP), to represent the total communication resource usages. For each distributed communication architecture, we formulate an optimization problem and obtain the solutions for minimizing the total cost of the system that considers both the ABDP and the deployment cost of the meter data management system (MDMS). The simulation results indicate the significant benefits of the distributed communication architectures over the traditional centralized one. More importantly, we analyze the scalability of the total cost of the communication system (including MDMS) with regard to the traffic load on the smart meters for both the centralized and the distributed communication architectures. Through the closed form expressions obtained in our analysis, we demonstrate that the total cost for the centralized architecture scales linearly as O(λN), with N being the number of smart meters, and λ being the average traffic rate on a smart meter. In contrast, the total cost for the fully distributed communication architecture is O(λ2/3 N2/3), which is significantly lower.
81565DAF	Scalable Video Coding is an H.264/AVC scalable extension that is used to provide various network-friendly scalability using a single bit stream. In SVC, the newly adapted predictive coding techniques can achieve high encoding efficiency, but they increase the computational complexity. To reduce the computational complexity, we present a fast mode decision algorithm based on the block complexity function considering the degree of correlation between base layer and enhancement layer. The simulation results show that the encoding time of the proposed fast mode decision algorithm for the combined scalability is about 54.72% compared with normal method although the loss of visual quality is negligible.
7F3DE411	A novel frame-level rate control (RC) algorithm is presented in this paper for temporal scalability of scalable video coding. First, by introducing a linear quality dependency model, the quality dependency between a coding frame and its references is investigated for the hierarchical B-picture prediction structure. Second, linear rate-quantization (R-Q) and distortion-quantization (D-Q) models are introduced based on different characteristics of temporal layers. Third, according to the proposed quality dependency model and R-Q and D-Q models for each temporal layer, adaptive weighting factors are derived to allocate bits efficiently among temporal layers. Experimental results on not only traditional quarter common intermediate format/common intermediate format but also standard definition and high definition sequences demonstrate that the proposed algorithm achieves excellent coding efficiency as compared to other benchmark RC schemes.
7EFFF39D	Scalable video coding is attractive due to the capability of reconstructing lower resolution or lower quality signals from partial bit streams. This allows for simple solutions in adaptation to network and terminal capabilities. Different modalities of scalability are specified by video coding standards like MPEG-2 and MPEG-4. This paper gives a short overview over these techniques and analyzes in more detail the encoder/decoder drift problem, which is the major reason why scalable coding has been significantly less efficient than single-layer coding in most of these implementations. Only recently, new scalable video coding technology has evolved, which seems to close the gap of compression performance compared to state of the art single-layer video coding. New methods of efficient enhancement layer prediction were developed to improve traditional (motion-compensated hybrid) scalable coders, providing more flexible compromises on the drift problem. As a new technology trend, motion-compensated spatiotemporal wavelet coding has matured which entirely discards the drift and allows most flexible combinations of spatial, temporal, and signal-to-noise ratio (SNR) scalability with fine granularity over a broad range of data rates.
80FB17B6	This paper describes the file format defined for scalable video coding. Techniques in the file format enable rapid extraction of scalable data, corresponding to the desired operating point. Significant assistance to file readers can be provided, and there is also great flexibility in the ways that the techniques can be used and combined, corresponding to different usages and application scenarios.
7D194C2D	Scalable video coding (SVC) is currently developed as an extension of H.264/AVC. In the SVC encoder, an exhaustive search technique is employed to select the best coding mode for each macro block (MB). This technique achieves an optimal trade-off between rate and distortion, but it requires an extremely large encoding time. In this paper, we propose a fast mode decision algorithm for inter-layer coding at the enhancement layer. The proposed algorithm predicts the mode of each MB at the enhancement layer using the modes of a co-located MB and its neighboring MBs at the base layer. The proposed method can achieve a time saving of up to 74% in spatial scalability and 63% in coarse grain quality scalability with negligible loss of quality and bit rate increment.
8086CE01	We propose a novel architecture for scalable video coding, namely, progressive fine granularity scalable (PEGS) coding, which can provide a high coding efficiency along with good bandwidth adaptation and error recovery properties. Unlike the fine granularity scalable (FGS) coding in the MPEG-4 proposal, some of the enhancement layers in a current frame are predicted from a high quality enhancement layer in a reference frame, rather than always from the base layer. Using a high quality enhancement layer as the reference makes the motion prediction more accurate to improve the coding efficiency. On the other hand, the use of multiple layers of different quality references may also result in increases and fluctuations of the prediction residues to be coded when switching the references, which may limit the coding efficiency improvement. A multiple-layer conditional replenishment approach is used to eliminate this kind of fluctuation. Experimental results show that our coding scheme can improve the coding efficiency up to 0.5 dB compared with fine granularity scalability coding.
8043E91D	In this paper, we present a new wavelet based rate scalable video compression algorithm. We will refer to this new technique as the scalable adaptive motion compensated wavelet (SAMCoW) algorithm. SAMCoW uses motion compensation to reduce temporal redundancy. The prediction error frames and the intracoded frames are encoded using an approach similar to the embedded zerotree wavelet (EZW) coder. An adaptive motion compensation (AMC) scheme is described to address error propagation problems. We show that, using our AMC scheme, the quality of the decoded video can be maintained at various data rates. We also describe an EZW approach that exploits the interdependency between color components in the luminance/chrominance color space. We show that, in addition to providing a wide range of rate scalability, our encoder achieves comparable performance to the more traditional hybrid video coders, such as MPEG1 and H.263. Furthermore, our coding scheme allows the data rate to be dynamically changed during decoding, which is very appealing for network-oriented applications.
7D640F01	Scalable video coding is an ongoing standard, and the current working draft (WD) is an extension of H.264/AVC. In the WD, an exhaustive search technique is employed to select the best coding mode for each macroblock. This technique achieves the highest possible coding efficiency, but it results in extremely large encoding time which obstructs it from practical use. This paper proposes a fast mode decision algorithm for inter-frame coding for spatial, coarse grain signal-to-noise ratio, and temporal scalability. It makes use of the mode-distribution correlation between the base layer and enhancement layers. Specifically, after the exhaustive search technique is performed at the base layer, the candidate modes for enhancement layers can be reduced to a small number based on the correlation. Experimental results show that the fast mode decision scheme reduces the computational complexity significantly with negligible coding loss and bit-rate increases
7B639BB8	This paper proposes GIA, a scalable architecture for global IP-anycast. Existing designs for providing IP-anycast must either globally distribute routes to individual anycast groups, or confine each anycast group to a pre-configured topological region. The first approach does not scale because of excessive growth in the routing tables, whereas the second one severely limits the utility of the service. Our design scales by dividing inter-domain anycast routing into two components. The first component builds inexpensive default anycast routes that consume no bandwidth or storage space. The second component, controlled by the edge domains, generates enhanced anycast routes that are customized according to the beneficiary domain's interests. We evaluate the performance of our design using simulation, and prove its practicality by implementing it in the Multi-threaded Routing Toolkit.
7D12F2C0	This paper proposes an architecture for inter-domain routing, called DTIA - Dynamic Topological Information Architecture. DTIA separates the issues of reachability and routing, and this paper addresses the first one. One major requirement has been not to change IP packets and the commercial relations in the Internet. DTIA is based on the knowledge of a static network formed by the Autonomous Systems (AS) and an algorithm to manage link failures. We use the concept of a region as a mechanism to sustain scale. DTIA supports the most important functionalities of BGP: some of them are built in and others can be implemented on top of the reachability level or the routing level. The main concerns we aim to solve are taking advantage of multihoming, increase the robustness in terms of convergence, reduce the churn rate and range of routing events, and due to forwarding packets by AS identifiers and topologic links (as opposed to prefix policy defined) reduce the growth of the routing table.
772B6552	The IP-based addressing scheme currently supporting the whole routing architecture embeds some well-known limitations that may significantly hinder the deployment of new applications and services on the Internet. Indeed, it is widely accepted that the unstoppable growth of Internet users is producing two well-known problems: (1) depletion of addresses, motivated by a design limitation of the currently deployed addressing scheme, and (2) the semantic overload of addresses. The main negative consequences of these problems may be summarized as: (i) exacerbating the geometrical growth of the routing tables, and (ii) affecting other network features, such as traffic engineering and mobility, in terms of resilience and disruption tolerant communications.The relevant consequences that addressing brings to the overall network operation is pushing the networking community to study and propose new addressing architectures that may limit or even remove the negative effects (affecting network performance) stemmed from the currently deployed addressing architecture. To this end, researchers working on this area must have a perfect understanding of the weaknesses and limitations coming up from the nowadays architecture as well as a comprehensive knowledge of the alternatives proposed so far along with the most appealing research trends. Aligned to this scenario, this paper comes up with the aim of assisting the reader to both: (i) get insights about the most prominent limitations of the currently deployed addressing architecture, and (ii) survey the existing proposals based on ID/Locator Split Architectures (ILSAs) including an analysis of pros and cons, as well as a taxonomy aiming at formulating a design space for evaluating and designing existing and future ILSAs.
77C866FE	Internet is facing many challenges that cannot be solved easily through ad hoc patches. To address these challenges, many research programs and projects have been initiated and many solutions are being proposed. However, before we have a new architecture that can motivate Internet service providers (ISPs) to deploy and evolve, we need to address two issues: 1) know the current status better by appropriately evaluating the existing Internet; and 2) find how various incentives and strategies will affect the deployment of the new architecture. For the first issue, we define a series of quantitative metrics that can potentially unify results from several measurement projects using different approaches and can be an intrinsic part of future Internet architecture (FIA) for monitoring and evaluation. Using these metrics, we systematically evaluate the current interdomain routing system and reveal many “autonomous-system-level” observations and key lessons for new Internet architectures. Particularly, the evaluation results reveal the imbalance underlying the interdomain routing system and how the deployment of FIAs can benefit from these findings. With these findings, for the second issue, appropriate deployment strategies of the future architecture changes can be formed with balanced incentives for both customers and ISPs. The results can be used to shape the short- and long-term goals for new architectures that are simple evolutions of the current Internet (so-called dirty-slate architectures) and to some extent to clean-slate architectures.
7D873E94	Rate scalable video compression is appealing for low bit rate applications, such as video telephony and wireless communication, where bandwidth available to an application cannot be guaranteed. In this paper, we investigate a set of strategies to increase the performance of SAMCoW, a rate scalable encoder. These techniques are based on based on wavelet decomposition, spatial orientation trees, and motion compensation.
7E663076	Attempts to generalize the Internet’s point-to-point communication abstraction to provide services like multicast, any- cast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes a general, overlay-based Internet Indirection In- frastructure ( 3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows 3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this ap- proach, we have designed and built a prototype based on the Chord lookup protocol.
7CC2095B	Hashing has enjoyed a great success in large-scale similarity search. Recently, researchers have studied the multi-modal hashing to meet the need of similarity search across different types of media. However, most of the existing methods are applied to search across multi-views among which explicit bridge information is provided. Given a heterogeneous media search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH simultaneously learns hash functions embedding heterogeneous media into different Hamming spaces, and translators aligning these spaces. Unlike almost all existing methods that map heterogeneous data in a common Hamming space, mapping to different spaces provides more flexible and discriminative ability. We empirically verify the effectiveness and efficiency of our algorithm on two real world large datasets, one publicly available dataset of Flickr and the other MIRFLICKR-Yahoo Answers dataset.
80A1F6BB	Especially for large Network Operators (NOs), eliminating routing anomalies is an important aspect for the internal BGP design. To avoid such unwanted effects, classical architectures [1], [2], [3] have need of certain restrictions kept in both, the network design and the iBGP peering. However, these restrictions are in conflict with the optimized network designs NOs seek. In this study we develop an inherently anomaly-free iBGP architecture that takes the demands of NOs into account. It is based on a central server that guarantees consistent local views in the entire system. This is done by exchanging additional routing information with the Border Routers of the Autonomous System. The architecture implements the results of iBGP analyses made in [4]. Despite not being that as flexible as classical information reduction techniques, our design scales equal or better in practice. All required protocol extensions are supported by IETF Internet Drafts from persons affiliated with important router vendors.
7EAA2130	This paper presents a novel architecture of MPEG-4 zerotree encoder. Under the architecture, a fast technique of label coefficients is proposed to reduce the recursive scan. It is achieved by exploiting the feature of the MPEG-4 zerotree symbol alphabet. A new combined structure of ZTR address buffer and the significant flag bit is described. It can simplify the skipping of the significant coefficients and locating the descendent coefficients of ZTR/VZTR. Furthermore, a preprocessor is given for independent encoding of each tree in individual bitplane.
812477A0	In s-to-p broadcasting, s processors in a processor machine contain a message to be broadcast to all the processors, 1/spl les/s/spl les/p. We present a number of different broadcasting algorithms that handle all ranges of s. We show how the performance of each algorithm is influenced by the distribution of the s source processors and by the relationships between the distribution and the characteristics of the interconnection network. For the Intel Paragon we show that for each algorithm and machine dimension there exist ideal distributions and distributions on which the performance degrades. For the Cray T3D we also demonstrate dependencies between distributions and machine sizes. To reduce the dependence of the performance on the distribution of sources, we propose a repositioning approach. In this approach, the initial distribution is turned into an ideal distribution of the target broadcasting algorithm. We report experimental results for the Intel Paragon and Cray T3D and discuss scalability and performance.
8152D1DA	Many challenges to the Internet including global routing scalability have drawn significant attention from both industry and academia, and have generated several new ideas for the next generation. MILSA (Mobility and Multihoming supporting Identifier Locator Split Architecture) and related enhancements are designed to address the naming, addressing, and routing scalability challenges, provide mobility and multihoming support, and easy transition from the current Internet. In this paper, we synthesize our research into a multiple-tier realm-based framework and present the fundamental principles behind the architecture. Through detailed presentation of these principles and different aspects of our architecture, the underlying design rationale is justified. We also discuss how our proposal can meet the IRTF RRG design goals. As an evolutionary architecture, MILSA balances the high-level long-run architecture design with ease of transition considerations. Additionally, detailed evaluation of the current inter-domain routing system and the achievable improvements deploying our architecture is presented that reveals the roots of the current difficulties and helps to shape our deployment strategy.
7F4A214A	This paper describes the first scalable implementation of a text processing engine used in visual analytics tools. These tools aid information analysts in interacting with and understanding large textual information content through visual interfaces. By developing a parallel implementation of the text processing engine, we enabled visual analytics tools to exploit cluster architectures and handle massive datasets. The paper describes key elements of our parallelization approach and demonstrates virtually linear scaling when processing multi-gigabyte data sets such as Pubmed. This approach enables interactive analysis of large datasets beyond capabilities of existing state-of-the art visual analytics tools.
7F7FB540	Motion-compensated fine-granularity scalability (MC-FGS) with leaky prediction has been shown to provide an efficient tradeoff between compression gain and error resilience, facilitating the transmission of video over dynamic channel conditions. In this paper, we propose an n-channel symmetric motion-compensated multiple description (MD) coding and transmission scheme for the delivery of scalable video over orthogonal frequency division multiplexed systems, utilizing the concepts of partial and leaky predictions. We investigate the proposed MD coding and transmission scheme using a cross-layer design perspective. In particular, we construct the symmetric motion-compensated MD codes based on the diversity order of the channel, defined as the ratio of the overall bandwidth of the system to the coherence bandwidth of the channel. We show that knowing the diversity order of a physical channel can assist an MC-FGS video coder in selecting the motion-compensation prediction point, as well as on the use of leaky prediction. More importantly, we illustrate how the side information can reduce the drift management problem associated with the construction of symmetric motion-compensated MD codes. We provide results based on both an information-theoretic approach and simulations.
5D3D697B	Designing massively scalable, highly available big data systems is an immense challenge for software architects. Big data applications require distributed systems design principles to create scalable solutions, and the selection and adoption of open source and commercial technologies that can provide the required quality attributes. In big data systems, the data management layer presents unique engineering problems, arising from the proliferation of new data models and distributed technologies for building scalable, available data stores. Architects must consequently compare candidate database technology features and select platforms that can satisfy application quality and cost requirements. In practice, the inevitable absence of up-to-date, reliable technology evaluation sources makes this comparison exercise a highly exploratory, unstructured task. To address these problems, we have created a detailed feature taxonomy that enables rigorous comparison and evaluation of distributed database platforms. The taxonomy captures the major architectural characteristics of distributed databases, including data model and query capabilities. In this paper we present the major elements of the feature taxonomy, and demonstrate its utility by populating the taxonomy for nine different database technologies. We also briefly describe QuABaseBD, a knowledge base that we have built to support the population and querying of database features by software architects. QuABaseBD links the taxonomy to general quality attribute scenarios and design tactics for big data systems. This creates a unique, dynamic knowledge resource for architects building big data systems.
815BA7A5	This paper presents the design of a new Internet routing architecture (NIRA). In today's Internet, users can pick their own ISPs, but once the packets have entered the network, the users have no control over the overall routes their packets take. NIRA aims at providing end users the ability to choose the sequence of Internet service providers a packet traverses. User choice fosters competition, which imposes an economic discipline on the market, and fosters innovation and the introduction of new services.This paper explores various technical problems that would have to be solved to give users the ability to choose: how a user discovers routes and whether the dynamic conditions of the routes satisfy his requirements, how to efficiently represent routes, and how to properly compensate providers if a user chooses to use them. In particular, NIRA utilizes a hierarchical provider-rooted addressing scheme so that a common type of domain-level route can be efficiently represented by a pair of addresses. In NIRA, each user keeps track of the topology information on domains that provide transit service for him. A source retrieves the topology information of the destination on demand and combines this information with his own to discover end-to-end routes. This route discovery process ensures that each user does not need to know the complete topology of the Internet.
7DF1DA2B	MPAs (massively parallel architectures) should address two fundamental issues for scalability: synchronization and communication latency. Dataflow architectures cause problems of excessive synchronization costs and inefficient execution of sequential programs while they offer the ability to exploit massive parallelism inherent in programs. In contrast, MPAs based on the von Neumann computational model may suffer from inefficient synchronization mechanism and communication latencies. DAVRID (Dataflow von Neumann, RISC Hybrid) is a massively parallel multithreaded architecture. By combining the advantages of the von Neumann model and the dataflow model, DAVRID preserves good single thread performance and tolerates latency and synchronization costs. We describe the DAVRID architecture and evaluate it through simulation results over several benchmarks.
809FFAF6	The scalable video coding (SVC) standard as an extension of H.264/AVC allows efficient, standard-based temporal, spatial, and quality scalability of video bit streams. Scalability of a video bit stream allows for media bit rate as well as for device capability adaptation. Moreover, adaptation of the bit rate of a video signal is a desirable key feature, if limitation in network resources, mostly characterized by throughput variations, varying delay or transmission errors, need to be considered. Typically, in mobile networks the throughput, delay and errors of a connection (link) depend on the current reception conditions, which are largely influenced by a number of physical factors. In order to cope with the typically varying characteristics of mobile communication channels in unicast, multicast, or broadcast services, different methods for increasing robustness and achieving quality of service are desirable. We will give an overview of SVC and its relation to mobile delivery methods. Furthermore, innovative use cases are introduced which apply SVC in mobile networks.
810F8A3E	A challenge faced by smart grid systems is providing highly reliable transmissions to better serve different types of electrical applications and improve the energy efficiency of the system. Although wireless networking technologies can provide highspeed and cost-effective solutions, their performance may be impaired by various factors that affect the reliability of smart grid networks. Here, we first suggest the use of IEEE 802.11s-based wireless LAN mesh networks as high-speed wireless backbone networks for smart grid infrastructure to provide high scalability and flexibility while ensuring low installation and management costs. Thereafter, we analyze some vital problems of the IEEE 802.11s default routing protocol (named hybrid wireless mesh protocol; HWMP) from the perspective of transfer reliability, and propose appropriate solutions with a new routing method called HWMP-reliability enhancement to improve the routing reliability of 802.11s-based smart grid mesh networking. A simulation study using ns-3 was conducted to demonstrate the superiorityof the proposed schemes.
7D48C9A2	This article proposes a scalable wavelength-routed optical Network on Chip (NoC) based on the Spidergon topology, named Power-efficient Scalable Wavelength-routed Network-on-chip (PeSWaN). The key idea of the proposed all-optical architecture is the utilization of per-receiver wavelengths in the data network to prevent network contention and the adoption of per-sender wavelengths in the control network to avoid end-point contention. By performing a series of simulations, we study the efficiency of the proposed architecture, its power and energy consumption, and the data transmission delay. Moreover, we compare the proposed architecture with electrical NoCs and alternative ONoC architectures under various traffic patterns.
7DB5CC40	We present a control scheme for a rate scalable video codec. We describe a wavelet based video codec with motion compensation used to reduce temporal redundancy. The prediction error frames are encoded using an embedded zerotree wavelet (EZW) approach which allows data rate scalability. Since motion compensation is used in the algorithm, the duality of the decoded video may decay due to the propagation of errors in the temporal domain. An adaptive motion compensation scheme is proposed to address this problem. We show that using our control scheme the quality of the decoded video can be maintained at any data rate.
80168B22	Generally speaking, rate scalable video systems today are evaluated operationally, meaning that the algorithm is implemented and the rate-distortion performance is evaluated for an example set of inputs. However, in these cases it is difficult to separate the artifacts caused by the compression algorithm and data set with general trends associated with scalability. In this paper, we derive and evaluate theoretical rate-distortion performance bounds for both layered and continuously rate scalable video compression algorithms which use a single motion-compensated prediction (MCP) loop. These bounds are derived using rate-distortion theory based on an optimum mean-square error (MSE) quantizer, and are thus applicable to all methods of intraframe encoding which use MSE as a distortion measure. By specifying translatory motion and using an approximation of the predicted error frame power spectral density, it is possible to derive parametric versions of the rate-distortion functions which are based solely on the input power spectral density and the accuracy of the motion-compensated prediction. The theory is applicable to systems which allow prediction drift, such as the data-partitioning and SNR-scalability schemes in MPEG-2, as well as those with zero prediction drift such as fine granularity scalability MPEG-4. For systems which allow prediction drift we show that optimum motion compensation is a sufficient condition for stability of the decoding system
80DCC6AC	Layer 5 switching-based transparent Web caching intercepts HTTP requests and redirects requests according to their contents. This technique makes the deployment and configuration of a caching system easier and improves its performance by ensuring that non-cacheable HTTP requests bypass the cache servers. We propose a Load Balancing Layer 5 switching-based (LB-L5) Web caching scheme that uses the Layer 5 switching-based technique to support distributed Web caching. We present simulation results that show that LB-L5 outperforms existing Web caching schemes, namely ICP, Cache Digest, and basic L5 transparent Web caching, in terms of cache server workload balancing and response time. LB-L5 is also shown to be more adaptable to high HTTP request intensity than the other schemes.
817736CD	With the introduction of the H.264/AVC video coding standard, significant improvements have recently been demonstrated in video compression capability. The Joint Video Team of the ITU-T VCEG and the ISO/IEC MPEG has now also standardized a Scalable Video Coding (SVC) extension of the H.264/AVC standard. SVC enables the transmission and decoding of partial bit streams to provide video services with lower temporal or spatial resolutions or reduced fidelity while retaining a reconstruction quality that is high relative to the rate of the partial bit streams. Hence, SVC provides functionalities such as graceful degradation in lossy transmission environments as well as bit rate, format, and power adaptation. These functionalities provide enhancements to transmission and storage applications. SVC has achieved significant improvements in coding efficiency with an increased degree of supported scalability relative to the scalable profiles of prior video coding standards. This paper provides an overview of the basic concepts for extending H.264/AVC towards SVC. Moreover, the basic tools for providing temporal, spatial, and quality scalability are described in detail and experimentally analyzed regarding their efficiency and complexity.
5BA87494	In this paper, we propose a linear dependent rate-quantization model for video enhancement layers encoding in H.264/AVC based scalable video coding (SVC). It is noted that the proposed model is applicable for different scalable structures, such as temporal, quality, spatial and combined scalability. Leveraging the base layer information (such as bitrate and quantization parameter), proposed model can accurately predict the number of bit required for the enhancement layer encoding. Such linear model demonstrates the high accuracy for bitrate estimation at enhancement layers, with the average prediction accuracy over 94%. It has the noticeable improvement from the existing works, without requiring additional complexity increase. Meanwhile, proposed model is applied to do the rate control for enhancement layers encoding. Experimental results show that the average bitrate mismatch error can be significantly reduced compared with the existing algorithms.
7FE8D880	Swarm intelligence forms the core of a new class of algorithms inspired by the social behavior of insects that live in swarms. Its attractive features include adaptation, robustness and a distributed, decentralized nature, rendering swarm-based algorithms well-suited for routing in wireless or satellite networks, where it is difficult it implement centralized network control. We propose one such routing algorithm, dubbed adaptive swarm-based distributed routing (adaptive-SDR), which is scalable, robust and suitable to handle large amounts of network traffic, while minimizing delay and packet loss.
7136DB48	In this paper, we present a scalable three dimensional hybrid parallel Delaunay image-to-mesh conversion algorithm (PDR.PODM) for distributed shared memory architectures. PDR.PODM is able to explore parallelism early in the mesh generation process because of the aggressive speculative approach employed by the Parallel Optimistic Delaunay Mesh generation algorithm (PODM). In addition, it decreases the communication overhead and improves data locality by making use of a data partitioning scheme offered by the Parallel Delaunay Refinement algorithm (PDR). PDR.PODM utilizes an octree structure to decompose the initial mesh and to distribute the bad elements to different octree leaves (subregions). A set of independent subregions are selected and refined in parallel without any synchronization among them. In each subregion, a group of threads is assigned to insert or delete multiple points based on the refinement rules offered by PODM. We tested PDR.PODM on Blacklight, a distributed shared memory (DSM) machine in the Pittsburgh Supercomputing Center, and observed a weak scaling speedup of 163.8 and above for up to 256 cores as opposed to PODM whose weak scaling speedup is only 44.7 on 256 cores. The end result is that we can generate 18 million elements per second as opposed to 14 million per second in our earlier work. To the best of our knowledge, PDR.PODM exhibits the best scalability among parallel guaranteed quality Delaunay mesh generation algorithms running on DSM supercomputers.
80C21360	In tree-based hierarchical key management schemes, scalability is achieved by reducing the number of messages exchanged during a rekeying operation. A single server manages the entire tree structure in such schemes. Failure of that server leads to single point failure which interrupts the group communication. In this paper we propose a method to avoid single point failure by distributing user information among set of X number of servers and use (t,X) threshold scheme to reconstruct the tree. The new auxiliary keys and group key are computed partly by the users which reduces number of encryptions required to communicate new set of keys to the remaining group members.
79C99A78	This article presents the progressive evolution of NFV from the initial SDN-agnostic initiative to a fully SDN-enabled NFV solution, where SDN is not only used as infrastructure support but also influences how virtual network functions (VNFs) are designed. In the latest approach, when possible, stateless processing in the VNF shifts from the computing element to the networking element. To support these claims, the article presents the implementation of a flow-based network access control solution, with an SDN-enabled VNF built on IEEE 802.1x, which establishes services as sets of flow definitions that are authorized as the result of an end user authentication process. Enforcing the access to the network is done at the network element, while the authentication and authorization state is maintained at the compute element. The application of this proposal allows the performance to be enhanced, while traffic in the control channel is reduced to a minimum. The SDN-enabled NFV approach sets the foundation to increase the areas of application of NFV, in particular in those areas where massive stateless processing of packets is expected.
781333FF	Scheduling research has increasingly taken the concept of learning into consideration. In general, a worker’s learning effect on a job depends not only on the total processing time of the jobs that he has processed but also on the job’s position. Besides, in the early stage of processing a given set of jobs, the worker is not familiar with the operations, so the learning effect on the jobs scheduled early is not apparent. Based on the above observations, we introduce in this paper a position-weighted learning effect model based on sum-of-logarithm-processing-times and job position for scheduling problems. We provide optimal solutions for the single-machine problems to minimize the makespan and the total completion time, and for the single-machine problem to minimize the sum of weighted completion times, the maximum lateness, and the total tardiness under an agreeable situation. We also solve two special cases of the flowshop problem under the learning model.
800AC70E	We propose a hybrid scheduling scheme which can efficiently support various types of traffic in a cooperative relay system of downlink cellular networks. The hybrid scheduling scheme harmonizes the shared-channel resource allocation scheme and dedicated-channel resource allocation scheme to accommodate various traffic in heterogeneous traffic environments. We show that the proposed scheme significantly improves the system performance in terms of system throughput in heterogeneous traffic environments through the analysis of the system capacity and throughput.
7E26F65E	The existence of the learning effect in many manufacturing systems is undoubted; thus, it is worthwhile that it be taken into consideration during production planning to increase production efficiency. Generally, it can be done by formulating the specified problem in the scheduling context and optimizing an order of jobs to minimize the given time criteria. To carry out a reliable study of the learning effect in scheduling fields, a comprehensive survey of the related results is presented first. It reveals that most of the learning models in scheduling are based on the learning curve introduced by Wright. However, further study about learning itself pointed out that the curve may be an ldquoSrdquo-shaped function, which has not been considered in the scheduling domain. To fill this gap, we analyze a scheduling problem with a new experience-based learning model, where job processing times are described by ldquoSrdquo-shaped functions that are dependent on the experience of the processor. Moreover, problems with other experience-based learning models are also taken into consideration. We prove that the makespan minimization problem on a single processor is NP-hard or strongly NP-hard with the most of the considered learning models. A number of polynomially solvable cases are also provided.
7E8F52BD	We consider the problem of scheduling a set of n preemptable tasks in a system having r resources. Each task has an arbitrary, but known, worst case processing time and a deadline, and may request simultaneous use of a number of resources. A resource can be used either in shared mode or exclusive mode. In this paper, we develop and evaluate algorithms for determining whether or not a set of preemptive tasks is schedulable in such a real-time system, and if so, determining a schedule for it. This scheduling problem is known to be computationally intensive. In many real-time application environments, tasks are scheduled dynamically, and hence the scheduling algorithms used must have low run-time costs. To keep run-time costs low, we propose the use of suboptimal but practical algorithms that employ computationally simple heuristics. The computational complexity of our algorithms for scheduling n tasks in a system having r resources is O(rn2), which is very much lower than that of known optimal algorithms. We report on the results of simulation studies performed on such heuristic preemptive scheduling algorithms and the sensitivity of the performance of the algorithms with respect to various scheduling parameters. These studies show that due to the complexity of the problem, straightforward heuristics do not perform satisfactorily. However, an algorithm that uses combinations of such heuristics in conjunction with limited backtracks works very well.
7F8DBD8E	We propose and analyze a proportional share resource allocation algorithm for realizing real-time performance in time-shared operating systems. Processes are assigned a weight which determines a share (percentage) of the resource they are to receive. The resource is then allocated in discrete-sized time quanta in such a manner that each process makes progress at a precise, uniform rate. Proportional share allocation algorithms are of interest because: they provide a natural means of seamlessly integrating real and non-real-time processing; they are easy to implement; they provide a simple and effective means of precisely controlling the real-time performance of a process; and they provide a natural means of policing so that processes that use more of a resource than they request have no ill-effect on well-behaved processes. We analyze our algorithm in the context of an idealized system in which a resource is assumed to be granted in arbitrarily small intervals of time and show that our algorithm guarantees that the difference between the service time that a process should receive and the service time it actually receives is optimally bounded by the size of a time quantum. In addition, the algorithm provides support for dynamic operations, such as processes joining or leaving the competition, and for both fractional and non-uniform time quanta. As a proof of concept we have implemented a prototype of a CPU scheduler under FreeBSD. The experimental results shows that our implementation performs within the theoretical bounds and hence supports real-time execution in a general purpose operating system.
7C4B92E0	We consider a class of machine scheduling problems in which the processing time of a task is dependent on its starting time in a schedule. On reviewing the literature on this topic, we provide a framework to illustrate how models for this class of problems have been generalized from the classical scheduling theory. A complexity boundary is presented for each model and related existing results are consolidated. We also introduce some enumerative solution algorithms and heuristics and analyze their performance. Finally, we suggest a few interesting areas for future research.
80F25302	In order to apply the results of formal studies of real-time task models, a practitioner must account for the effects of phenomena present in the implementation but not present in the formal model. We study the feasibility and schedulability problems for periodic tasks that must compete for the processor with interrupt handlers - tasks that are assumed to always have priority over application tasks. The emphasis in the analysis is on deadline driven scheduling methods. We develop conditions that solve the feasibility and schedulability problems and demonstrate that our solutions are computationally feasible. Lastly, we compare our analysis with others developed for static priority task systems.
7B24499B	Scheduling plays a vital role in LTE downlink systems with Multiple Input and Multiple Output (MIMO) antennas. We consider the MIMO downlink scheduling problem at the base station (BS) in LTE networks under several practical constraints mandated by the 3GPP standards. We Define a new construct called transmission mode, which denotes a particular choice of MIMO operational mode, precoding matrix, transmission rank, as well as the modulation and coding schemes (MCSs) of up to two codewords and show that both LTE systems require that each scheduled user be served using only one transmission mode in every subframe. We prove that the resulting scheduling problems are NP-hard under both backlogged and finite queue traffic models, and then develop a unified low-complexity greedy algorithm that yields solutions guaranteed to be within 1/2 of the respective optima. Extensive performance evaluation in realistic settings reveals near-optimal performance of our proposed algorithm and that it significantly outperforms the state of the art, especially under the more practical, finite queue model.
78998183	Most scheduling papers consider flexible machining and assembly systems as being independent. In this paper, a heuristic two-level scheduling algorithm for a system consisting of a machining and an assembly subsystem is developed. It is shown that the upper level problem is equivalent to the two machine flow shop problem. The algorithm at the lower level schedules jobs according to the established product and part priorities. Related issues, such as batching, due dates, process planning and alternative routes, are discussed. The algorithm and associated concepts are illustrated on a number of numerical examples.
59A985F6	This paper, as well as coupled paper [2], deal with various aspects of scheduling algorithms dedicated for processing in parallel computing environments. In this paper, for the exemplary problem, namely the flow-shop scheduling problem with makespan criterion, there are proposed original methods for parallel analysis of a solution as well as a group of concentrated and/or distributed solutions, recommended for the use in metaheuristic approaches with single-thread trajectory. Such methods examine in parallel consecutive local sub-areas of the solution space, or a set of distributed solutions called population, located along the single trajectory passed through the space. Supplementary multi-thread search techniques applied in metaheuristics have been discussed in complementary our paper 
789C7E4B	The following type of problem arises in practice: in a node-weighted graph G, find a minimum-weight node set that satisfies certain conditions and, in addition, induces a perfectly matchable subgraph of G. This has led us to study the convex hull of incidence vectors of node sets that induce perfectly matchable subgraphs of a graph G, which we call the perfectly matchable subgraph polytope of G. For the case when G is bipartite, we give a linear characterization of this polytope, i.e., specify a system of linear inequalities whose basic solutions are the incidence vectors of perfectly matchable node sets of G. We derive this result by three different approaches, using linear programming duality, projection, and lattice polyhedra, respectively. The projection approach is used here for the first time as a proof method in polyhedral combinatorics, and seems to have many similar applications. Finally, we completely characterize the facets of our polytope; i.e., we separate the essential inequalities of our linear defining system from the redundant ones.
8023DC2E	Most systems which are required to operate under severe real-time constraints assume that all tasks and their characteristics are known a priori. Scheduling of such tasks can be done statistically. Further, scheduling algorithms operating under such conditions are usually limited to multiprocessor configurations. The authors present a scheduling algorithm which works dynamically and on loosely coupled distributed systems for tasks with hard real-time constraints; i.e., the tasks must meet their deadlines. It uses a scheduling component local to every node and a distributed scheduling scheme which is specifically suited to hard real-time constraints and other timing considerations. Periodic tasks, nonperiodic tasks, scheduling overheads, communication overheads due to scheduling and preemption are all accounted for in the algorithm. Simulation studies are used to evaluate the performance of the algorithm.
8134F61B	In dynamic real-time task systems, tasks that are subject to deadlines are allowed to join and leave the system. In previous work, Stoica et al. and Baruah et al. presented conditions under which such joins and leaves may occur in fair-scheduled uniprocessor systems without causing missed deadlines. In this paper, we extend their work by considering fair-scheduled multiprocessors. We show that their conditions are sufficient on M processors, under any deadline-based Pfair scheduling algorithm, if the utilization of every subset of M − 1 tasks is at most one. Further, for the general case in which task utilizations are not restricted in this way, we derive sufficient join/leave conditions for the PD2 Pfair algorithm. We also show that, in general, these conditions cannot be improved upon without causing missed deadlines.
803B7CFD	Preemptive multitasking, a popular technique for timesharing of computational resources in software-based systems, faces considerable difficulties when applied to partially reconfigurable hardware. In this paper, we propose a cooperative scheduling technique for reconfigurable hardware threads as a feasible compromise between computational efficiency and implementation complexity. We have implemented this mechanism for the multithreaded reconfigurable operating system ReconOS and evaluated its overheads and performance on a prototype.
7EFC0944	We consider finite graphs whose edges are labeled with elements, called colors, taken from a fixed finite alphabet. We study the problem of determining whether there is an infinite path where either (i) all colors occur with a fixed asymptotic frequency, or (ii) there is a constant that bounds the difference between the occurrences of any two colors for all prefixes of the path. These properties can be viewed as quantitative refinements of the classical notion of fair path in a concurrent system, whose simplest form checks whether all colors occur infinitely often. Our notions provide stronger criteria, particularly suitable for scheduling applications based on a coarse-grained model of the jobs involved. In particular, they enforce a given set of priorities among the jobs involved in the system. We show that both problems we address are solvable in polynomial time, by reducing them to the feasibility of a linear program. We also consider two-player games played on finite colored graphs where the goal is one of the above frequency-related properties. For all the goals, we show that the problem of checking whether there exists a winning strategy is Co-NP-complete.
7AB7CE85	This paper presents a generalized formulation of precedence constrained scheduling where the number of dependent tasks which are to be scheduled before the task itself can be scheduled is a variable. This formulation is capable of modeling a number of scheduling and path-finding problems. An algorithm is presented to solve the problem of finding the minimum time schedule. Variants are discussed. One simple variant is shown to be NP-Complete.	
77EDE772	This paper presents a modified harmony search (HS) for the flow shop scheduling problem, with the objective to minimize the maximum completion time of all jobs, i.e. makespan. Firstly, a simple memory consideration rule based on a recombination operator is employed to generate a new harmony. Secondly, an insertion procedure is used as a pitch adjustment rule. Thirdly, a neighborhood local strategy is embedded into the algorithm to enhance the solution quality. To validate the proposed algorithm, various computational tests are established using a set of 60 instances from Taillard Benchmark [1]. The HS algorithm is compared to two constructive heuristics from literature, the NEH heuristic [2], and the stochastic greedy heuristic (SG) [6]. The obtained results show the superiority of the proposed algorithm in terms of solution quality.
7D47A9F7	The problem considered in this paper is the deterministic scheduling of tasks on a set of identical processors. However, the model presented differs from the classical one by the requirement that certain tasks need more than one processor at a time for their processing. This assumption is especially justified in some microprocessor applications and its impact on the complexity of minimizing schedule length is studied. First we concentrate on the problem of nonpreemptive scheduling. In this case, polynomial-time algorithms exist only for unit processing times. We present two such algorithms of complexity O(n) for scheduling tasks requiring an arbitrary number of processors between 1 and k at a time where k is a fixed integer. The case for which k is not fixed is shown to be NP-complete. Next, the problem of preemptive scheduling of tasks of arbitrary length is studied. First an algorithm for scheduling tasks requiring one or k processors is presented. Its complexity depends linearly on the number of tasks. Then, the possibility of a linear programming formulation for the general case is analyzed.
75FC5AC9	Hadoop is a framework for processing large amount of data in parallel with the help of Hadoop Distributed File System (HDFS) and MapReduce framework. Job scheduling is an important process in Hadoop MapReduce. Hadoop comes with three types of schedulers namely FIFO, Fair and Capacity Scheduler. The schedulers are now a plug gable component in the Hadoop MapReduce framework. When jobs have a dependency on an external service like database or Web service may leads to the failure of tasks due to overloading. In this scenario, Hadoop needs to re-run the tasks in another slots. To address this issue, Task Tracker aware scheduling has introduced. This scheduler enables users to configure a maximum load per Task Tracker in the Job Configuration itself. The algorithm will not allow a task to run and fail if the load of the Task Tracker reaches its threshold for the job. Also this scheduler allows the users to select the Task Tracker's per Job in the Job configuration.
7B1B79AE	This paper presents an overview of assignment and sequencing models that are used inthe scheduling of process operations with mathematical programming techniques. Althoughscheduling models are problem specific, there are common features which translate intosimilar types of constraints. Two major categories of scheduling models are identified:single-unit assignment models in which the assignment of tasks to units is known a priori,and multiple-unit assignment models in which several machines compete for the processingof products. The most critical modeling issues are the time domain representation and networkstructure of the processing plant. Furthermore, a summary of the major features of thescheduling model is presented along with computational experience, as well as a discussionon their strengths and limitations.
7535C3B9	We consider a family of problems of scheduling a set of starting time dependent tasks with release times and linearly increasing/decreasing processing rates on a single machine to minimize the makespan. We first present an equivalence relationship between several pairs of problems. Based on this relationship, we show that the makespan problem with arbitrary release times and identical increasing processing rates is strongly NP-complete and the corresponding case with only one non-zero release time is at least NP-complete in the ordinary sense. On the other hand, the makespan problem with arbitrary release times and identical decreasing processing rates is solvable in O(n6 log n) time by a dynamic programming algorithm. Using a different approach, we also show that, when the normal processing times are identical, the makespan problem with arbitrary release times and increasing/decreasing processing rates is strongly NP-complete and the corresponding case with only one non-zero release time is at least NP-complete in the ordinary sense.
7FA0527B	The PD2 Pfair/ERfair scheduling algorithm is the most efficient known algorithm for optimally scheduling periodic tasks on multiprocessors. In this paper, we prove that PD2 is also optimal for scheduling “rate-based” tasks whose processing steps may be highly jittered. The rate-based task model we consider generalizes the widely-studied sporadic task model.
7DE991AC	The paper presents scheduling algorithms in look-ahead reconfigurable multi-processor systems. In multiprocessor systems with message passing, link connection reconfiguration is a very promising alternative to fixed interconnection networks. To eliminate connection reconfiguration time overheads, a new approach called look-ahead dynamic link reconfiguration has been proposed. It consists in preparing link connections in advance in parallel with program execution. An application program is partitioned into sections, which are executed using redundant communication resources. Two algorithms for scheduling parallel program graphs are presented. The first algorithm, based on list scheduling utilizes a genetic heuristic for section partitioning. The second approach combines scheduling and partitioning into one heuristic.
7DBC3C41	The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed.
76B2EE53	In this study, we propose a new scheduling model with position-dependent deterioration, in which the processing time of a job is defined by an increasing function of total weighted normal processing time of jobs prior to it in the sequence, where the weight is position dependent. We show that some single machine scheduling problems remain polynomially solvable under the proposed model, respectively. In addition, we show that some special cases of the flow shop scheduling problems can be optimally solved by polynomial time algorithm, respectively.
7D3CE9E4	Extended T-N Plane Abstraction (E-TNPA) proposed inthis paper realizes work-conserving and efficient optimalreal-time scheduling on multiprocessors relative to the originalT-N Plane Abstraction (TNPA). Additionally a schedulingalgorithm named NVNLF (No Virtual Nodal LaxityFirst) is presented for E-TNPA. E-TNPA and NVNLF relaxthe restrictions of TNPA and the traditional algorithmLNREF, respectively. Arbitrary tasks can be preferentiallyexecuted by both tie-breaking rules and time apportionmentpolicies in accordance with various system requirementswith several restrictions. Simulation results show that E-TNPAsignificantly reduces the number of task preemptionsas compared to TNPA.
7FF76847	This paper presents a new scheduling algorithm that maximizes the performance of a design under resource constraints in high-level synthesis. The algorithm tries to achieve the maximal utilization of resources and the minimal waste of clock slack time. Moreover, it exploits the technique of bit-level chaining to target high-speed designs. The algorithm tries non-integer multiple-cycling and chaining, which allows multiple cycle execution of chained operations, to further increase the performance at the cost of small increase in the complexity of the control unit. Experimental results on several datapath-intensive designs show significant improvement in execution time, over the conventional scheduling algorithms.
766DA98B	Recently learning effects in scheduling have received considerable attention in the literature. All but one paper are based on the learning-by-doing (or autonomous learning) assumption, even though proactive investments in know how (induced learning) are very important from a practical point of view. In this review we first discuss the questions why and when learning effects in scheduling environments might occur and should be regarded from a planning perspective. Afterwards we give a concise overview on the literature on scheduling with learning effects.
7747E43E	This paper describes a heuristic approach for solving the problem of dynamically scheduling tasks in a real-time system where tasks have deadlines and general resource requirements. The crux of our approach lies in the heuristic function used to select the task to be scheduled next. The heuristic function is composed of three weighted factors. These factors explicitly consider information about real-time constraints of tasks and their utilization of resources. Simulation studies show that the weights for the various factors in the heuristic function have to be fine-tuned in order to obtain a degree of success in the range of 75-88 percent of that obtained via exhaustive search. However, modifying the approach to use limited backtracking improves the degree of success substantially to as high as 99.5 percent. This improvement is observed even when the initial set of weights are not tailored for a particular set of tasks. Simulation studies also show that in most cases the schedule determined by the heuristic algorithm is optimal or close to optimal.
7FEF7274	The need for supportingvariety of hard and soft real-time, as well as best effort applications in a multimedia computing environment requires an operating system framework that: (1) enables different schedulers to be employed for different application classes, and (2) provides protection between the various classes of applications. We argue that these objectives can be achieved by hierarchical partitioning of CPU bandwidth, in which an operating system partitions the CPU bandwidth among various application classes, and each application class, in turn, partitions its allocation (potentially using a different scheduling algorithm) among its sub-classes or applications. We present Start-time Fair Queuing (SFQ) algorithm, which enables such hierarchical partitioning. We have implemented a hierarchical scheduler in Solaris 2.4. We describe our implementation, and demonstrate its suitability for multimedia operating systems. 
7B8345BF	Scheduling with learning effect and deteriorating jobs has become more popular. However, most of the research assume that the setup time is negligible or a part of the job processing time. In this paper, we propose a model where the deteriorating jobs, the learning effect, and the setup times are present simultaneously. Under the proposed model, the setup time is past-sequence-dependent and the actual job processing time is a general function of the processing times of the jobs already processed and its scheduled position. We provide the optimal schedules for some single-machine problems.
5F89548A	We consider the implementation of a Pfair real-time scheduler on a symmetric multiprocessor (SMP). Although SMPs are in many ways well-suited for Pfair scheduling, experimental results presented herein suggest that bus contention resulting from the simultaneous scheduling of all processors can substantially degrade performance. To correct this problem, we propose a staggered model for Pfair scheduling that strives to improve performance by more evenly distributing bus traffic over time. Additional simulations and experiments with a scheduler prototype are presented to demonstrate the effectiveness of the staggering approach. In addition, we discuss other techniques for improving performance while maintaining worst-case predictability. Finally, we present an efficient scheduling algorithm to support the proposed model and briefly explain how existing Pfair results apply to staggered scheduling.
7ACE40FE	This paper considers single-machine scheduling problems with simultaneous consideration of due date assignment, past-sequence-dependent (p-s-d) delivery times, and position-dependent learning effects. By p-s-d delivery times, we mean that the delivery time of a job is proportional to the job’s waiting time. Specifically, we study four variants of the problem: (i) the variant of total earliness and tardiness with common due date assignment (referred to as TETDC), (ii) the variant of total earliness and weighted number of tardy jobs with CON due date assignment (referred to as TEWNTDC), (iii) the variant of total earliness and weighted number of tardy jobs with slack due date assignment (referred to as TEWNTDS), and (iv) the variant of weighted number of tardy jobs with different due date assignment (referred to as WNTDD). We derive the structural properties of the optimal schedules and show that the variants TETDC, TEWNTDC and TEWNTDS are all polynomially solvable. Although the complexity status of the variant WNTDD is still open, we show that two special cases of it are polynomially solvable.
7A247C2D	The paper is devoted to some single-machine scheduling problems with variable job processing times. The objectives are to minimize the makespan (i.e., the maximum completion time of all jobs), and to minimize the total completion time. For some special cases, we show that these problems can be solved in polynomial time. For some another special cases of the makespan and the total completion time minimization problems, we prove that an optimal schedule has an V-shape property in terms of processing times. We also propose a heuristic algorithm by utilizing the V-shape property.
7B8594DD	The authors study contention-based bus-control schemes for scheduling processors in using a bit-parallel shared bus. The protocol is designed under the requirements that each processor exhibit a random access behavior, that there be no centralized bus control in the system, and that access be granted in real time. The proposed scheme is based on splitting algorithms used in conventional contention-resolution schemes, and utilizes two-state information obtained from collision detection. Two versions of the bus-control scheme are studied. The static one resolves contentions of N requesting processors in an average of O(logW/2N) iterations, where W is the number of bits in the bit-parallel bus. An adaptive version resolves contentions in an average time that is independent of N. 
81273310	We consider a single-processor scheduling model where the execution time of a task is a decreasing linear function of its starting time. The complexity of the problem of minimizing the number of late tasks remains unknown for a set of tasks with identical due dates. We present an O(n2)-time dynamic programming algorithm for solving this problem.
7DEF560D	This paper presents a new algorithm for scheduling of sporadic task systems with arbitrary deadlines on identica l multiprocessor platforms. The algorithm is based on the concept of semi-partitioned scheduling, in which most task s are fixed to specific processors, while a few tasks migrate across processors. Particularly, we design the algorithm s o that tasks are qualified to migrate only if a task set cannot be partitioned any more, and such migratory tasks migrate from one processor to another processor only once in each period. The scheduling policy is then subject to Earliest Deadline First. Simulation results show that the algorithm delivers competitive scheduling performance to the state- of- the-art, with a smaller number of context switches.
76061C10	Scheduling with learning effects has attracted growing attention of the scheduling research community. A recent survey classifies the learning models in scheduling into two types, namely position-based learning and sum-of-processing-times-based learning. However, the actual processing time of a given job drops to zero precipitously as the number of jobs increases in the first model and when the normal job processing times are large in the second model. Motivated by this observation, we propose a new learning model where the actual job processing time is a function of the sum of the logarithm of the processing times of the jobs already processed. The use of the logarithm function is to model the phenomenon that learning as a human activity is subject to the law of diminishing return. Under the proposed learning model, we show that the scheduling problems to minimize the makespan and total completion time can be solved in polynomial time. We further show that the problems to minimize the maximum lateness, maximum tardiness, weighted sum of completion times and total tardiness have polynomial-time solutions under some agreeable conditions on the problem parameters.
7B54C0EC	This paper investigates the scheduling problem for a single-cell distributed multiple-input multiple-output (d-MIMO) downlink system with a massive number of remote access units (RAUs), N. We first derive the ergodic rate expressions for both the single RAU transmission (SRT) and the cooperative RAU transmission (CRT) modes as functions of long-term channel state information (CSI). Then, greedy scheduling algorithms aiming for maximizing the ergodic sum rate for the massive d-MIMO system using local long-term CSI are proposed. To mitigate intra-cell interference, a two-stage greedy scheduling algorithm (GSA) is developed to further improve the ergodic sum rate. Asymptotic analysis reveals that with infinite N intra-cell interference can be efficiently mitigated. Simulation results verify the derived expressions and demonstrate that the two-stage GSA exhibits a higher ergodic sum rate.
7E30BF29	So far, the Link-status Dependent Scheduling (LDS) Algorithm in HR-WPAN (High Rate Wireless Personal Area Network) mainly focused on the throughput elevation in terms of an entire network. However, in the LDS algorithm the throughput of each DEV (Device) is not sensibly considered for the elevation. For instance in case where the DEV has been connected under the bad channel quality, the LDS algorithm does not clearly provide avoidance of the bad user experience of the UE. From this consideration, we introduce a new algorithm which is Guaranteed Scheduling for Worst-case (GSW) that the throughput of each DEV can be individually guaranteed.
7CE4778B	In this paper, we present surplus fair scheduling (SFS), a proportional-share CPU scheduler designed for symmetric multiprocessors. We first show that the infeasibility of certain weight assignments in multiprocessor environments results in unfairness or starvation in many existing proportional-share schedulers. We present a novel weight readjustment algorithm to translate infeasible weight assignments to a set of feasible weights. We show that weight readjustment enables existing proportional-share schedulers to significantly reduce, but not eliminate, the unfairness in their allocations. We then present surplus fair scheduling, a proportional-share scheduler that is designed explicitly for multiprocessor environments. We implement our scheduler in the Linux kernel and demonstrate its efficacy through an experimental evaluation. Our results show that SFS can achieve proportionate allocation, application isolation and good interactive performance, albeit at a slight increase in scheduling overhead. We conclude from our results that a proportional-share scheduler such as SFS is not only practical but also desirable for server operating systems.
80829446	The services received by a process from a time-sharing operating system can be characterized by a resource count ∑ wiRij where Rij is the number of units of service received by process j from resource i and wi is the cost per unit of the service. Each class of users can be characterized by a policy function which specifies the amount of service a user who belongs to this class should receive as a function of time. Priority changes dynamically as a function of the difference between the service promised to the user by the policy function and the service he actually receives. A scheduling and swapping algorithm which keeps the resource count of each process above its policy function will provide the specified level of service. Overhead can be reduced by avoiding swaps of processes which have received at least this level of service. The algorithm has been implemented in a general purpose operating system, and it has provided significantly better service to interactive and to batch jobs than the previous scheduler.
7C9912E6	In multiuser MIMO-OFDM closed-loop system downlink, a proper scheduling algorithm can be used to extract the multi-user diversity by pouring more space-time-frequency (STF) resources to the user with better channel quality. For these systems, we aim to achieve further improvement without increasing the feedback overhead, and without sacrificing the performance gain the multiuser scheduling can provide. In this paper, a new space-frequency precoding scheme is proposed which makes better use of the additional diversity among different STF resources. In this scheme, the symbol streams at different STF resource are rotated before transmitting. These symbol streams should belong to the same user and have the same modulation and coding (MC) level. The rotation balances the signal to interference plus noise ratio (SINR) of each symbol stream, the average bit error rate (BER) performance is improved. With linear receiver, we proposed to use FFT basis as the rotation. Two cases occurred in space division multiple assess (SDMA) system are discussed. By the simulations in 3GPP long term evaluation (LTE) environment, the proposed scheme is proved to achieve lower BER and higher throughput than the existing systems.
27007EBE	This note considers a problem of minimum length scheduling for a set of messages subject to precedence constraints for switching and communication networks, and shows some improvements upon previous results on the problem.
7B33D9F8	This paper presents an EDF-based algorithm, called Earliest Deadline Deferrable Portion (EDDP), for efficient scheduling of recurrent real-time tasks on multiprocessor systems. The design of EDDP is based on the portioned scheduling technique which classifies each task into a fixed task or a migratable task. A fixed task is scheduled on the dedicated processor without migrations. A migratable task is meanwhile permitted to migrate between the particular two processors. In order to curb the cost of task migrations, EDDP makes at most M -- 1 migratable tasks on M processors. The scheduling analysis derives the condition for a given task set to be schedulable. It is also proven that no tasks ever miss deadlines, if the system utilization does not exceed 65%. Beyond the theoretical analysis, the effectiveness of EDDP is evaluated through simulation studies. Simulation results show that EDDP achieves high system utilization with a small number of preemptions, compared with the traditional EDF-based algorithms.
784A8116	In classical scheduling theory job processing times are constant. However, there are many situations where processing time of a job depends on the starting time of the job in the queue. This paper reviews the rapidly growing literature on single machine scheduling models with time dependent processing times. Attention is focused on linear, piecewise linear and non-linear processing time functions for jobs. We survey known results and introduce new solvable cases. Finally, we identify the areas and give directions where further research is needed.
105F2D6E	This work considers a problem of optimal query processing in heterogeneous and distributed database systems. A global query submitted at a local site is decomposed into a number of queries processed at the remote sites. The partial results returned by the queries are integrated at a local site. The paper addresses a problem of an optimal scheduling of queries that minimizes time spend on data integration of the partial results into the final answer. A global data model defined in this work provides a unified view of the heterogeneous data structures located at the remote sites and a system of operations is defined to express the complex data integration procedures. This work shows that the transformations of an entirely simultaneous query processing strategies into a hybrid (simultaneous/sequential) strategy may in some cases lead to significantly faster data integration. We show how to detect such cases, what conditions must be satisfied to transform the schedules, and how to transform the schedules into the more efficient ones.
80811368	In order to provide efficient support of interactive packet services on downlink dedicated channels (DCH) in a WCDMA FDD (frequency division duplexing) system, we envisage a novel MAC measurement based (MMB) packet scheduling algorithm. The MMB scheduler switches packets between DCH of different rates based on traffic volume measurements of their corresponding serving DCH buffers. Algorithm performance has been evaluated by system simulations in terms of average packet throughput. Numerical results show that the provided MMB packet scheduling algorithm has consistent performance for different traffic sources and various DCH combinations studied, arguing its capabilities of robust handling of multiple services of interactive class and support of elastic access capacity.
7DB1889F	This paper studies an electricity producer's long-term optimality in the case of a multireservoir hydropower system. The model solves the optimal production process and trading strategy of electricity and weather derivatives by maximizing the utility from production and terminal water reservoir level. The optimal trading strategy hedges the rainfall and electricity price uncertainties.
7C179829	The algorithms for solving ATM cell-scheduling problem include first-in-first-out (FIFO), static priority (SPR), dynamically weighted priority scheduling (DWPS) as well as other traditional schemes. However, these traditional algorithms lack flexibility. FIFO and SPR cannot adapt to changes in the cell flow environment. DWPS on the other hand, is more adaptable to changing traffic flow. But if the cell flow changes dramatically, the performance of this method is also not very good. In order to address these issues, we propose the framework of evolvable fuzzy system (EFS). The system is intrinsically evolvable and able to carry out on-line adaptation to meet the desired QoS requirement. The EFS is realizable as a form of evolvable fuzzy hardware (EFH) by means of a reconfigurable fuzzy inference chip (RFIC). With an implementation of the EFS as EFH which carries out intrinsic evolution and on-line adaptation, some open issues pertinent to evolvable hardware (EHW) can be addressed.
7C367647	In this paper we investigate a single machine scheduling problem with time dependent processing times and group technology (GT) assumption. By time dependent processing times and group technology assumption, we mean that the group setup times and job processing times are both increasing functions of their starting times, i.e., group setup times and job processing times are both described by function which is proportional to a linear function of time. We attempt to minimize the makespan with ready times of the jobs. We show that the problem can be solved in polynomial time when start time dependent processing times and group technology are considered simultaneously.
78411C7B	In this note, we show that the main results in the two papers [C.C. Wu, W.C. Lee, Single-machine and flowshop scheduling with a general learning effect model, Computers and Industrial Engineering 56 (2009) 1553–1558, W.C. Lee, C.C. Wu, Some single-machine and m-machine flowshop scheduling problems with learning considerations, Information Sciences 179 (2009) 3885–3892] are incorrect.
7B2C2E92	This paper deals with a single machine scheduling problem with start time dependent job processing times. The job processing times are characterized by decreasing linear functions dependent on their start times. The problem is to find a schedule for which the total weighted completion time is minimized. It is proved that the problem is NP-hard. Some properties of special cases of the general problem are also given. Based on these results, two heuristic algorithms are constructed and their performance is compared.
7F2EB872	Optimal multiprocessor real-time schedulers incur significant overhead for preemptions and migrations. We present RUN, an efficient scheduler that reduces the multiprocessor problem to a series of uniprocessor problems. RUN significantly outperforms existing optimal algorithms with an upper bound of O(log m) average preemptions per job on m processors (≤ than 3 per job in all of our simulated task sets) and reduces to Partitioned EDF whenever a proper partitioning is found.
8136F0E9	For the performance of networked control system is limited to the network resources and compute resources, the scheduling strategy is the key factor. This paper proposed two level schedule strategy: First, the controller and sensor nodes can be configured as event-time hybrid driven mode to improve the utilization rate. Then, considering the error and error difference response, a BP neural network and fuzzy feedback scheduler that shares communication net is designed with the bandwidth constraints. Two different scheduling algorithms with stochastic delay are compared with respectively. Finally, the results of simulation highlights that proposed scheduling strategy can optimize the performance of control loop and is more flexible than the other algorithms in uncertain running conditions.
725C3850	 Mixed-criticality systems emerged with the aim of reconciling safety requirements and efficient use of multi-processor or uniprocessor platforms. On multi-processors, recent works on mixed-criticality have produced impressive results in terms of speed-up factor. But these solutions, based on Pfair-like scheduling algorithms, entail too many preemptions and migrations to be effectively used in real systems. As RUN is an optimal scheduling algorithm that is known to limit this problem, we propose MxC-RUN, an adaptation of RUN to mixed-criticality systems. We redefine RUN's primal servers as modal servers that allocate the overestimated time budget of their higher criticality tasks to execute lower criticality ones. These servers can be handled by RUN without any modification and preserve its performances in terms of preemptions and migrations. MxC-RUN earns a speed-up factor smaller than other multi-processors EDF-based mixed-criticality scheduling algorithms.
78B8F583	This paper deals with single machine scheduling problems with simple linear deterioration in which the processing time of a job is a simple linear function of its execution starting time. The objective is to determine the optimal schedule to minimize the weighted sum of the th is a positive integer number) power of waiting times. It is proved that the general problem can be solved in polynomial time. In addition, for the jobs with weak (strong) parallel chains and a series–parallel digraph precedence constraints, it is also proved that these problems can be solved in polynomial time, respectively.
80066DE7	Scheduling theory holds great promise as a means to a priori validate timing correctness of real-time applications. However, there currently exists a wide gap between scheduling theory and its implementation in operating system kernels running on specific hardware platforms. The implementation of any particular scheduling algorithm introduces overhead and blocking components which must be accounted for in the timing correctness validation process. This paper presents a methodology for incorporating the costs of scheduler implementation within the context of fixed priority scheduling algorithms. Both event-driven and timer-driven scheduling implementations are analyzed. We show that for the timer-driven scheduling implementations the selection of the timer interrupt rate can dramatically affect the schedulability of a task set, and we present a method for determining the optimal timer rate. We analyzed both randomly generated and two well-defined task sets and found that their schedulability can be significantly degraded by the implementation costs. Task sets that have ideal breakdown utilization over 90% may not even be schedulable when the implementation costs are considered. This work provides a first step toward bridging the gap between real-time scheduling theory and implementation realities. This gap must be bridged for any meaningful validation of timing correctness properties of real-time applications.
76FE392E	A mixed-criticality system consists of multiple components with different criticalities. While mixed-criticality scheduling has been extensively studied for the uniprocessor case, the problem of efficient scheduling for the multiprocessor case has largely remained open. We design a fluid model-based multiprocessor mixed-criticality scheduling algorithm, called MC-Fluid, in which each task is executed in proportion to its criticality-dependent rate. We propose an exact schedulability condition for MC-Fluid and an optimal assignment algorithm for criticality-dependent execution rates with polynomial complexity. Since MC-Fluid cannot construct a schedule on real hardware platforms due to the fluid assumption, we propose MC-DP-Fair algorithm, which can generate a non-fluid schedule while preserving the same schedulability properties as MC-Fluid. We show that MC-Fluid has a speedup factor of (1 + v 5)/2 ( 1.618), which is best known in multiprocessor MC scheduling, and simulation results show that MC-DP-Fair outperforms all existing algorithms
78F818A7	Emerging reconfigurable systems attain high performance with embedded optimized cores. For mapping designs on such special architectures, synthesis tools, that are aware of the special capabilities of the underlying architecture are necessary. We propose an algorithm to perform simultaneous scheduling and binding, targeting embedded reconfigurable systems. The algorithm differs from traditional scheduling methods in its capability of efficiently utilizing embedded blocks within the reconfigurable system. The algorithm can be used to implement several other scheduling techniques, such as ASAP, ALAP, and list scheduling. Hence we refer to it as a super-scheduler. The algorithm is a path-based scheduling algorithm. At each step, an individual path from the input DFG is scheduled. The experiments with several DFGs extracted from MediaBench suite indicate promising results. The scheduler presents the capability to perform the trade-off between maximally utilizing the high-performance embedded blocks and exploiting parallelism in the schedule.
80C37553	An investigation is conducted of two protocols belonging to the priority inheritance protocols class; the two are called the basic priority inheritance protocol and the priority ceiling protocol. Both protocols solve the uncontrolled priority inversion problem. The priority ceiling protocol solves this uncontrolled priority inversion problem particularly well; it reduces the worst-case task-blocking time to at most the duration of execution of a single critical section of a lower-priority task. This protocol also prevents the formation of deadlocks. Sufficient conditions under which a set of periodic tasks using this protocol may be scheduled is derived.
7D8236E4	Functional, instruction-based self-testing of microprocessors has emerged as an effective alternative or supplement to other testing approaches, and is progressively adopted by major microprocessor manufacturers. In this paper, we study, for first time, the applicability of functional self-testing on bus-based symmetric multiprocessors (SMP) and the exploitation of SMPs parallelism during testing. We focus on the impact of the memory system architecture and the cache coherency mechanisms on the execution of self-test programs on the processor cores. We propose a generic self-test routines scheduling algorithm aiming at the reduction of the total test application time for the SMP by reducing both bus contention and data cache coherency invalidation. We demonstrate the proposed solutions with detailed experiments in two-core and four-core SMP benchmarks based on a RISC processor core.
7C3D294D	Carrier aggregation (CA) is one of the promising techniques for the further advancements of the third-generation (3G) long-term evolution (LTE) system, referred to as LTE-Advanced. When CA is applied, a well-designed carrier scheduling (CS) scheme is essential to the LTE-Advanced system. Joint user scheduling (JUS) and separated random user scheduling (SRUS) are two straightforward CS schemes. JUS is optimal in performance but with very high complexity, whereas SRUS is contrary. Consequently, the authors propose a novel CS scheme, termed as `separated burst-level scheduling` (SBLS). In SBLS, the connected component carrier (CC) of one user can be changed in burst level, whereas in SRUS, it is fixed. Meanwhile, SBLS limits the users to receive from only one of the CCs simultaneously, which is the same as that in SRUS. In this way, SBLS is expected to achieve higher resource utilisation than SRUS but with acceptable complexity increase. There are two factors that are important to the performance of SBLS, namely the dispatching granularity and the dispatching policy. The authors` analysis is verified by system-level simulations. The simulation results also show that the resultant performance gain of SBLS over SRUS is notable and increasing dispatching granularity will quickly deteriorate the performance of SBLS.
7A73E4A7	This article presents a detailed discussion of LRE-TL (Local Remaining Execution-TL-plane), an algorithm that schedules hard real-time periodic and sporadic task sets with unconstrained deadlines on identical multiprocessors. The algorithm builds upon important concepts such as the TL-plane construct used in the development of the LLREF algorithm (Largest Local Remaining Execution First). This article identifies the fundamental TL-plane scheduling principles used in the construction of LLREF . These simple principles are examined, identifying methods of simplifying the algorithm and allowing it to handle a more general task model. For example, we identify the principle that total local utilization can never increase within any TL-plane as long as a minimal number of tasks are executing. This observation leads to a straightforward approach for scheduling task arrivals within a TL-plane. In this manner LRE-TL can schedule sporadic tasks and tasks with unconstrained deadlines. Like LLREF, the LRE-TL scheduling algorithm is optimal for task sets with implicit deadlines. In addition, LRE-TL can schedule task sets with unconstrained deadlines provided they satisfy the density test for multiprocessor systems. While LLREF has a O(n2) runtime per TL-plane, LRE-TL’s runtime is O(nlog n) per TL-plane.
7DF1D46B	Two important performance metrics in collaborative systems are local and remote response times. Previous analytical and simulation work has shown that these response times depend on three important factors: processing architecture, communication architecture, and scheduling of tasks dictated by these two architectures. We show that it is possible to create a system that improves response times by dynamically adjusting these three system parameters in response to changes to collaboration parameters such as new users joining and network delays changing. We present practical approaches for collecting collaboration parameters, computing multicast overlays, applying analytical models of previous work, preserving coupling semantics during optimizations, and keeping overheads low. Simulations and experiments show that the system improves performance in practical scenarios.
7F5B0762	Supporting uninterrupted services for distributed soft real-time applications is hard in resource-constrained and dynamic environments, where processor or process failures and system workload changes are common. Fault-tolerant middleware for these applications must achieve high service availability and satisfactory response times for client applications. Although passive replication is a promising fault tolerance strategy for resource-constrained systems, conventional client failover approaches are non-adaptive and load-agnostic, which can cause system overloads and significantly increase response times after failure recovery.This paper presents four contributions to the study of passive replication for distributed soft real-time applications. First, it describes how our Fault-tolerant Load-aware and Adaptive middlewaRe (FLARe) dynamically adjusts failover targets at runtime in response to system load fluctuations and resource availability. Second, it describes how FLARe's overload management strategy proactively enforces desired CPU utilization bounds by redirecting clients from overloaded processors. Third, it presents the design and implementation of FLARe's lightweight middleware architecture that manages failures and overloads transparently to clients. Finally, it presents experimental results on a distributed Linux testbed that demonstrate how FLARe adaptively maintains soft real-time performance for clients operating in the presence of failures and overloads with negligible runtime overhead.
7FAB19D6	Real-time transient simulation of large transmission networks requires significant computational power. This paper proposes a field-programmable gate array (FPGA)-based real-time electromagnetic transient simulator. Taking advantage of the inherent parallel architecture, high density, and high clock speed, the real-time Electromagnetic Transients Program (EMTP) is implemented on a single FPGA chip. It is based on a paralleled algorithm that is deeply pipelined, and uses a floating-point arithmetic calculation to achieve high accuracy for simulating fast electromagnetic transients. To validate the new simulator, a sample system with 15 transmission lines using full frequency-dependent modeling is simulated in real time. A timestep of 12 mus has been achieved based on a 12.5-ns clock period with high data throughput. The captured real-time oscilloscope results demonstrate excellent accuracy of the simulator in comparison to the offline simulation of the original system in the Alternative Transients Program version of EMTP.
7F909201	This paper provides two contributions to the study of quality of service (QoS)-enabled middleware that supports the network QoS requirements of distributed real-time and embedded (DRE) systems. First, we describe the design and implementation of NetQoPE, which is a model-driven component middleware framework that shields applications from the details of network QoS mechanisms by (1) specifying per-flow network QoS requirements, (2) performing resource allocation and validation decisions (such as admission control), and (3) enforcing per-flow network QoS at runtime. Second, we evaluate the effort required and flexibility of using NetQoPE to provide network QoS assurance to end-to-end application flows. Our results demonstrate that NetQoPE can provide network-level differentiated performance to each application flow without modifying its programming model or source code, thereby providing greater flexibility in leveraging network-layer mechanisms.
7E1A7A39	The emergence of functional embedded systems such as cell-phones and digital appliances brought up a new issue, building a system supporting both real-time and rich services. One of the solutions is leveraging a hypervisor to integrate an RTOS and a commodity OS into a single device. However, this approach induces the limitation of application deployment; all the high priority tasks should reside in the RTOS.In this paper, we propose a task grain scheduling for a real-time hypervisor, which enables a flexible application deployment between an RTOS and a commodity OS.  We constructed a prototype system with an existing hypervisor, an RTOS, and a commodity OS. We measured some basic overheads, and fixed some tasks which were missing their deadlines using the task grain scheduling to meet their deadlines. The overheads were small and the task grain scheduling achieved a flexible real-time scheduling for the hypervisor based system.
5E138E90	A recent trend has seen the extension of object-oriented middleware. A major advantage components offer over objects is that only the business logic of an application needs to be addressed by a programmer with support services required incorporated into the application at deployment time. This is achieved via components (business logic of an application), containers that host components and are responsible for providing the underlying middleware services required by components and application servers that host containers. Well-known examples of component middleware architectures are Enterprise Java Beans (EJBs) and the CORBA Component model (CCM). Two of the many services available at deployment time in most component architectures are component persistence and atomic transactions. This paper examines, using EJBs, how replication for availability can be supported by containers so that components that are transparently using persistence and transactions can also be made highly available.
74972C61	There is a growing need to provide students with meaningful system-level design (SLD) experiences at the undergraduate level. Relevant SLD skills include the ability to integrate IP from third-party providers, create reusable IP (including appropriate documentation), partition system functionality between software and hardware, and properly integrate real-time functions within an operating system. A senior SLD project was created to provide such an experience for undergraduate students. With cooperation from Xilinx corporation, this single semester course provides students the opportunity to learn SLD skills by creating a single-chip multimedia computer system using FPGAs. Students in this class integrate custom IP and third-party IP into a PowerPC-based system within a single FPGA device. The final product is a real-time multimedia computer system providing both audio and video services. This paper describes the SLD course, beginning by outlining its goals and requirements. Next, the hardware and software infrastructure used by the project is described. Finally, the class schedule is reviewed.
808824F7	In reality, peripheral devices often make a significant contribution to the power consumption of the entire system. An effective energy-efficient scheduling algorithm should consider not only the energy consumption of the processor but also the usages of devices. In this paper, we explore energy-efficient scheduling of periodic real-time tasks in a system with a dynamic-voltage-scaling (DVS) processor and multiple non-DVS system devices. We consider systems that any device used by a task remains operating while the task is active. We propose scheduling algorithms in the management of task preemption to reduce the energy consumption of devices. Simulation results show that our proposed algorithms could not only reduce the number of task preemption significantly but also minimize the energy consumption, compared to earliest-deadline-first scheduling.
7C2230AD	The paper presents a distributed real-time operating system (DRTOS) for hard real-time embedded systems such as automotive control systems. A real-time control application program is usually designed as a set of tasks that cooperate with each other through the mechanism of a real- time operating system. In a distributed embedded control system, location-transparent mechanisms are required because the tasks are allocated to multiple nodes. We have developed a DRTOS that provides location-transparent system calls for task management and inter-task synchronization. The DRTOS is an extension to OSEK OS, which is a standard operating system in the automotive control domain. The DRTOS manages distributed tasks based on the global time supported by the clock synchronization of FlexRay, which is a real-time network based on a TDMA (Time Division Multiple Access) protocol. By using the DRTOS, we can develop a distributed control application program with location-transparent system calls. We can also reallocate the tasks of the application program just by reconfiguration, without rewriting the source code. The worst case response time of a remote system call of the DRTOS is predictable if the FlexRay communication is well configured.
7CC7B191	Timing predictability of service oriented architectures is challenged by their dynamic nature. Systems have to reconfigure their service-based structure to adapt to the changing environmental requirements. The development of dynamic systems that have timing constraints is currently not possible without imposing some bounds and limitations to the structure and operation of the system. This paper identifies the key factors for achieving time-bounded service-based reconfiguration from a system perspective. The key contribution to bypass the possible complexity of the used task model and associated schedulability analysis algorithm is the provided architectural design that separates the composition from the schedulability. The paper also extends a previous service composition algorithm that provided a feasible solution compliant with the application quality of service criteria (QoS). Due to the design based on the separation of concerns, the algorithm is a simple straight forward graph search guided by values related to the application QoS. The generalized algorithm offers a search mechanism guided through n regions going beyond the four regions model of the previous contribution. Results from the previous algorithm and the n regions version are shown to illustrate the advantages and finer grain results of the latter.
802D8BC3	End-to-end predictability of remote operations is essential for many fixed-priority distributed real-time and embedded (DRE) applications, such as command and control systems, manufacturing process control systems, large-scale distributed interactive simulations, and testbeam data acquisition systems. To enhance predictability, the Real-time CORBA specification defines standard middleware features that allow applications to allocate, schedule, and control key CPU, memory, and networking resources necessary to ensure end-to-end quality of service support. This paper provides two contributions to the study of Real-time CORBA middleware for DRE applications. First, we identify potential problems with ensuring predictable behavior in conventional middleware by examining the end-to-end critical code path of a remote invocation and identifying sources of unbounded priority inversions. Experimental results then illustrate how the problems we identify can yield unpredictable behavior in conventional middleware platforms. Second, we present design techniques for ensuring real-time quality of service in middleware. We show how middleware can be redesigned to use nonmultiplexed resources to eliminate sources of unbounded priority inversion. The empirical results in this paper are conducted using TAO, which is widely used and open-source DRE middleware compliant with the Real-time CORBA specification
758811F9	Many application domains (e.g., avionics, telecommunications, and multimedia) require real-time guarantees from the underlying networks, operating systems, and middleware components to achieve their quality of service (QoS) requirements. In addition to providing end-to-end QoS guarantees, applications in these domains must be flexible and reusable. Requirements for flexibility and reusability motivate the use of object-oriented middleware like the Common Object Request Broker Architecture (CORBA). However, the performance of current CORBA implementations is not yet suited for hard real-time systems (e.g., avionics) and constrained latency systems (e.g., teleconferencing). This article describes the architectural features and optimizations required to develop real-time ORB end systems that can deliver end-to-end QoS guarantees to applications. While some operating systems, networks, and protocols now support real-time scheduling, they do not provide integrated solutions. The main thrust of this article is that advances in real-time distributed object computing can be achieved only by systematically pinpointing performance bottlenecks; optimizing the performance of networks, ORB end systems, common services, and applications; and simultaneously integrating techniques and tools that simplify application development.
71095C34	There is increasing demand to extend Object Request Broker (ORB) middleware to support distributed applications with stringent real-time requirements. However, conventional ORE implementations, such as CORBA ORBs, exhibit substantial priority inversion and non-determinism, which makes them unsuitable for applications with deterministic real-time requirements. The paper provides two contributions to the study and design of real-time ORB middleware. First, it illustrates empirically why conventional ORBs do not yet support real-time quality of service. Second, it evaluates connection and concurrency software architectures to identify strategies that reduce priority inversion and non-determinism in real-time CORBA ORBs.
7E5A6EBA	The workshop is a small, high quality and interactive meeting for novel P2P ideas and proposals. On the one hand, collaborative P2P systems aim to benefit from the huge amount of unused resources of desktop computers (content, data storage, cpu, bandwidth) to create the next generation of Internet Infrastructures. On the other hand, P2P is particularly challenging for developing collaborative information systems. In this line, P2P can be used to create social networks and communities, to discover users and information, to aggregate community contents or even as a communication medium for large groups. The Fifth International Workshop on Collaborative Peer-to-Peer Systems has attracted high quality submissions from top research groups around the world. It received 27 submissions from 14 countries and it is creating a vibrant community of researchers in the peer-to-peer topic. After the review process, 10 full papers, and 1 short paper were included in the final program. The high quality submissions have demonstrated the relevance of the topic and the recent trends in the peer-to-peer research community. The workshop was divided in three sessions: algorithms, middleware and security. Every paper addresses a hot topic in peer-to-peer research and relevant researchers in the area presented their recent contributions.
7E755772	An agent-oriented middleware supporting contextaware and adaptable mLearning service provision within an InfoStation-based University network is presented. The InfoStation's middleware architecture facilitating the users' mobile (WiFi) access to services is described. The agents' interaction is explained in detail.
80C907A1	The distributed embedded systems industry is poised to leverage emerging real-time operating systems, such as Inferno Windows CE, EPOC, and Palm OS to support mobile communication applications, such as electronic mail, Internet browsing, and network management. Ideally, these applications can be developed using standard middleware components like CORBA to improve their quality and reduce their cost and cycle time. However, stringent constraints on memory available in embedded systems imposes a severe limit on the footprint of CORBA middleware. This paper provides three contributions to the study and design of small footprint, embedded CORBA middleware. First, we describe the optimizations used to develop the protocol engine and CORBA IDL compiler provided by TAO, which is our real-time CORBA implementation. TAO's IDL compiler produces stubs that can use either compiled and/or interpretive marshalling. Second, we compare the performance and footprint of TAO IDL compiler-generated stubs and skeletons that use compiled and/or interpretive marshalling for a wide range of IDL data types. Third, we illustrate the benefits of the small footprint and efficiency of TAO IDL compiler-generated stubs and skeletons for CORBA object services implemented using TAO. The results comparing the performance of the compiled and interpretive stubs and skeletons indicate that the interpretive stubs and skeletons perform between 75-100% of the compiled stubs and skeletons for a wide range of data types. However the code size for the interpreted stubs and skeletons was between 26-45% and 50-80% of the compiled stubs and skeletons, respectively. These results indicate a positive step towards implementing high performance, small footprint middleware for distributed embedded systems.
78A7BE36	In this work we present a middleware architecture for a mobile peer-to-peer content distribution system. Our architecture allows wireless content dissemination between mobile nodes without relying on infrastructure support. Contents are exchanged opportunistically when nodes are within communication range. Applications access the service of our platform through a publish/subscribe interface and therefore do not have to deal with low-level opportunistic networking issues or matching and soliciting of contents. Our architecture consists of three key components. A content structure that facilitates dividing contents into logical topics and allows for efficient matching of content lookups and downloading under sporadic node connectivity. A solicitation protocol that allows nodes to solicit content meta-information in order to discover contents available at a neighboring node and to download content entries disjointedly from different nodes. An API that allows applications to access the system services through a publish/subscribe interface. In this work we describe the design and implementation of our architecture. We also discuss potential applications and present evaluation results from profiling of our system.
7DFDBAF5	To be an effective platform for performance-sensitive real time systems, commercial-off-the-shelf (COTS) distributed object computing (DOC) middleware must support application quality of service (QoS) requirements end-to-end. However, conventional DOC middleware does not provide this support, which makes it unsuited for applications with stringent latency, determinism, and priority preservation requirements. It is essential, therefore, to develop standards based, COTS DOC middleware that permits the specification, allocation, and enforcement of application QoS requirements end-to-end. The Real-time CORBA and Messaging specifications in the forthcoming CORBA 3.0 standard are important steps towards defining standards based COTS DOC middleware that can deliver end-to-end QoS support at multiple levels in distributed and embedded real time systems. However these specifications still lack sufficient detail to portably configure and control processor, communication, and memory resources for applications with stringent QoS requirements. The paper provides four contributions to research on real time DOC middleware. First, we show how the CORBA 3.0 Real-time and Messaging specifications provide a starting point for addressing the needs of an important class of applications with stringent real time requirements. Second, we show how the CORBA 3.0 specifications are not sufficient to solve all the issues within this application domain. Third, we describe how we have implemented portions of these specifications, as well as several enhancements, using TAO, which is our open-source real time CORBA ORB. Finally, we empirically evaluate the performance of TAO to illustrate how its features address the QoS requirements of certain types of real time applications.
7EC7FF26	Many real-time application domains can benefit from flexible and open distributed architectures, such as those defined by the CORBA specification. CORBA is an architecture for distributed object computing being standardized by the OMG. Although CORBA is well-suited for conventional request/response applications, CORBA implementations are not yet suited for real-time applications due to the lack of key quality of service (QoS) features and performance optimizations. This paper makes three contributions to the design of real-time CORBA systems. First, the paper describes the design of TAO, which is our high-performance, real-time CORBA 2.0-compliant implementation that runs on a range of OS platforms with real-time features including VxWorks, Chorus, Solaris 2.x, and Windows NT. Second, it presents TAO's real-time scheduling service that can provide QoS guarantees for deterministic real-time CORBA applications. Finally, the paper presents performance measurements that demonstrate the effects of priority inversion and non-determinism in conventional CORBA implementations and how these hazards are avoided in TAO.
7DDF8C83	Large scale distributed real time and embedded (DRE) applications are complex entities that are often composed of different subsystems and have stringent Quality of Service (QoS) requirements. These subsystems are often developed separately by different developers increasingly using commercial off-the shelf (COTS) middleware. Subsequently, these subsystems need to be integrated, configured to communicate with each other, and distributed. However, there is currently no standard way of supporting these requirements in existing COTS middleware. While recently emerging component-based middleware provides standardized support for packaging, assembling, and deploying, there is no standard way to provision QoS required by the DRE applications. We have previously introduced a QoS encapsulation model, qoskets, as part of our QuO middleware framework that can dynamically adapt to resource constraints. In this paper we introduce implementing these QoS behaviors as components that can be assembled with other application components. The task of ensuring QoS then becomes an assembly issue. To do so we have componentized our QuO technology instead of integrating QuO into the middleware as a service. To date, we have demonstrated our approach of QoS provisioning in MICO, CIAO, and Boeing’s Prism component middleware. We present experimental results to evaluate the overhead incurred by these QoS provisioning components in the context of CIAO CCM. We use a simulated Unmanned Aerial Vehicle (UAV) application as an illustrative DRE application for the demonstration of QoS adaptations using qosket components.
7F8E640F	To support the quality-of-service (QoS) requirements of embedded multimedia applications off-the-shelf middleware like common object request broker architecture (CORBA) must be flexible, efficient, and predictable. Moreover, stringent memory constraints imposed by embedded system hardware necessitates a minimal footprint for middleware that supports multimedia applications. This paper provides three contributions toward developing efficient object request broker's (ORBs) middleware to support embedded multimedia applications. First, we describe optimization principle patterns used to develop a time and space-efficient CORBA inter-ORB protocol (IIOP) interpreter for the adaptive communication environment (ACE)-ORB (TAO), which is our high-performance, real-time ORB. Second, we describe the optimizations applied to TAO's interface definition language (IDL) compiler to generate efficient and small stubs/skeletons used in TAO's IIOP protocol engine. Third, we empirically compare the performance and memory footprint of interpretive (de)marshaling versus compiled (de)marshaling for a wide range of IDL data types. Applying our optimization principle patterns to TAO's IIOP protocol engine improved its interpretive (de)marshaling performance to the point where it is now comparable to the performance of compiled (de)marshaling. Moreover, our IDL compiler optimizations generate interpreted stubs/skeletons whose footprint is substantially smaller than compiled stubs/skeletons. Our results illustrate that careful application of optimization principle patterns can yield both time and space-efficient standards-based middleware.
79A6D961	MERT is a multi-environment real-time operating system for the Digital Equipment PDP-11/45 and 11/70 computers. It is a structured operating system built on top of a kernel which provides the basic services such as memory management, process scheduling, and trap handling needed to build various operating system environments. Real-time response to processes is achieved by means of preemptive priority scheduling. The file system structure is optimized for real-time response. Processes are built as modular entities with data structures that are independent of all other processes. Interprocess communication is achieved by means of messages, event flags, shared segments, and shared files. Process ports are used for communication between unrelated processes.
785BB087	Software-based reconfiguration of distributed real-time systems is a complex problem with many sides to it ranging from system-wide concerns down to the intrinsic non-robust nature of the specific middleware layer and the used programming techniques. In a completely open distributed system, mixing reconfiguration and real-time is not possible; the set of possible target states can be very large threatening the temporal predictability of the reconfiguration process. Over the last years, middle ware solutions have appeared mainly for general purpose systems where efficient state transitions are sought for, but real-time properties are not considered. One of the few contributions to run-time software reconfiguration in distributed real-time environments has been the iLAND middleware, where the germ of a solution with high potential has been conceived and delivered in practice.1 The key idea has been the fact that a set of bounds and limitations to the structure of systems and to their open nature needs to be imposed in order to come up with practical solutions. In this paper, the authors present the different sides of the problem of software reconfiguration from two complementary middleware perspectives comparing two strategies built inside distribution middleware. We highlight the lessons learned in the iLAND project aimed at service-based reconfiguration and compare it to our experience in the development of distributed real-time Java reconfiguration based on distributed tasks rescheduling. Authors also provide a language view of both solutions. Lastly, empirical results are shown that validate these solutions and compare them on the basis of different programming language realizations.
8115183F	The design phase of any real-time system requires balancing the limited computational resources against the functional requirements and the performance of the application. The optimal design solution can be obtained by solving an optimization problem where the system performance is maximized within the schedulability constraints. In this paper, we provide a procedure that finds the task activation rates maximizing a performance function within the deadline constraints in systems scheduled by fixed priorities. First, we describe the exact feasibility region in the domain of task frequencies. Then, we introduce a procedure that starts by finding an initial solution, and incrementally improves it by using an original branch and bound search, until the global optimum is reached. Experiments show that our algorithm finds the optimal task periods for practical problems with a remarkable speedup, if compared with existing techniques. When the size of the problem makes the global search intractable, the experiments show that the algorithm can still find a high quality solution in the very early steps
7865D947	The steady increase in raw computing power of the processors commonly adopted for distributed real-time systems leads to the opportunity of hosting diverse classes of tasks on the same hardware, for example process control tasks, network protocol stacks and man–machine interfaces.This paper describes how virtualization techniques can be used to concurrently run multiple operating systems on the same physical machine, although they are kept fully separated from the security and execution timing points of view, and still have them exhibit acceptable real-time execution characteristics.With respect to competing approaches, the main advantages of this method are that it requires little or no modifications to the operating systems it hosts, along with a better modularity and clarity of design.
5F02D60B	Workflow manager is a useful tool that brings the power of computational Grid resources to the desktop, and allow them to conveniently put together and run their own scientific workflows. In existing workflow systems, individual tasks wait for input to be available perform computation,and produce output. Behind this, workflow manager automates the data movement from the data generating taskto the data consumption task. This process is referred as file staging. Generally, stage-in, process, and stage-out are serially executed and staging is treated by traditional workflow systems as a trivial step. However, as the data sizeis exponentially increasing and more and more scientific workflows require multiple processing steps to obtain the desired output, we argue that the data movement will possess high portion of overall running time and staging will become a challenging step of scientific workflow systems. In this paper, we propose a task pipelining framework for various e-Science workflow systems. Our system is a flexible and efficient tool to help the workflow systems to overlap the execution of adjacent tasks by enabling the pipelining ofthe intermediate data transfer between the interconnected tasks.
5E4297C6	Although component middleware is increasingly used to develop distributed, real-time and embedded (DRE) systems, it poses new fault-tolerance challenges, such as the need for efficient synchronization of internal component state, failure correlation across groups of components, and configuration of fault-tolerance properties at the component granularity level. This paper makes three contributions to R&D on component-based fault-tolerance. First, it describes the COmponent Replication based on Failover Units (CORFU) component middleware, which provides fail-stop behavior and fault correlation across groups of components treated as an atomic unit in DRE systems. Second, it describes how CORFU’s Components with HEterogeneous State Synchronization (CHESS) module provides mechanisms for real-time aware state transfer and synchronization in CORFU. Third, we empirically evaluate the client failover and group shutdown capabilities of CORFU and its CHESS module and compare/contrast it with existing object-oriented fault-tolerance methods. Our results show that component middleware (1) has acceptable fault-tolerance performance for DRE systems, (2) allows timely recovery while considering failure location, size, and functional topology of the group, and finally (3) eases the burden of application development by providing middleware support for fault-tolerance at the component level.
7ED97825	A new task mode, hardware logic task mode, is presented. Its architecture, schedule and implementation are described with HDL(Hardware Description Language), and the validity of the system has been proved by logic simulation. It has advantage for real-time applications and overheadsaving for operating system, so it is profitable for the controller in the embedded system. The relationship among RTOS (Real-Time Operating System), SoC (System on Chip), VIA (Virtual Interface Architecture) and hardware logic task is also discussed in the paper.
7E2BD844	Real-time operating systems generally depend on some form of priority information for making scheduling decisions. Priorities may take the form of small integers or deadline times, for example, and the priorities indicate the preferred order for execution of the jobs. Unfortunately, most systems suffer from some degree of priority inversion where a high priority job must wait for a lower priority job to execute. The authors consider the nature of the nonpreemptible code sections, called critical sections or critical regions, which give rise to this priority inversion in the context of a soft real-time operating system where average response time for different priority classes is the primary performance metric. An analytical model is described which is used to illustrate how critical regions may affect the time-constrained jobs in a multimedia (soft real-time) task set.
800C8759	Hardware task placements in dynamically reconfigurablelogic need to satisfy different goals such as highplacement efficiency, low fragmentation of the reconfigurable logic, and minimization of routing resources. There have been several placement algorithms proposed for each goal. Nevertheless, the algorithm can only satisfy one goal, which results in poor results in the other goal satisfactions. We propose a novel Multi-Objective Hardware Placement (MOHP) method such that all goals are satisfied and if there are conflicts a good trade off is obtained. MOHP is similar to share-based schedulers that try to adjust resource utilizationsamong different scheduling methods by varying the processorbandwidth. By applying MOHP to some examples, we find thatMOHP approximates the best performance for each assessmentcriterion.
80DD419F	Existing real-time ORB middleware standards such as RT-CORBA do not adequately address the challenges of 1) providing robust performance guarantees portably across different platforms, and 2) managing unpredictable workload. To overcome this limitation, we have developed software called FCS/nORB that integrates a Feedback Control real-lime Scheduling (FCS) service with the nORB small-footprint real-time ORB designed for networked embedded systems. FCS/nORB features feedback control loops that provide real-time performance guarantees by automatically adjusting the rate of remote method invocations transparently to an application. FCS/nORB thus enables real-time applications to be truly portable in terms of real-time performance as well as functionality, without the need for hand tuning. This paper presents the design, implementation, and evaluation of FCS/nORB. Our extensive experiments on a Linux testbed demonstrate that FCS can provide deadline miss ratio and utilization guarantees in face of changes in the platform and task execution times, while introducing a small amount of overhead.
7E3567FA	As real-time systems are moving form centralized architectures to distributed and Internet-based ones, global task scheduling and resource management have been the central issues. However, the current resource management techniques are based on fixed methods and hard to be extended. This paper develops a resource management framework with flexible, reusable and extensible resource management infrastructure by extending the methods provided in the real-time CORBA (RT-CORBA). The application of this framework is described in the design and implementation of an Internet-based distributed real-time system, in which a set of control nodes is distributively connected to the Internet. The system is designed for integrating multiple-platforms into the common protocol of the Internet, which extends the architecture and technology of conventional bus-based or dedicated communication-channel-based distributed systems. The client server of any node can monitor or control the status of other remote nodes in the system. Various optimizations are adopted in order to enhance the real-time performance of this system.
77B88F5B	This paper outlines the design objectives and research goals for HARTOS, a distributed real-time operating system being developed at The University of Michigan. This effort is part of a larger research project to design and implement an experimental distributed real-time system called the Hexagonal Architecture for Real-Time Systems (HARTS). An important feature of HARTS is the use of an intelligent network processor to handle many of the functions relating to communications. The paper focuses on the communications aspects of the operating system and the control software kernel of the network processor. The preliminary version of the kernel provides good support for inter-process communication and distributed control. Its performance has been measured and analyzed and found to be comparable to that of other message passing systems like the V system.
7FD63100	There is increasing demand to extend Object RequestBroker (ORB) middleware to support distributed applications withstringent real-time requirements. However, conventional ORB implementations,such as CORBA ORBs, exhibit substantial priority inversion andnon-determinism, which makes them unsuitable for applicationswith deterministic real-time requirements. This paper providestwo contributions to the study and design of real-time ORB middleware.First, it illustrates empirically why conventional ORBs do notyet support real-time quality of service. Second, it evaluatesconnection and concurrency software architectures to identifystrategies that reduce priority inversion and non-determinismin real-time CORBA ORBs. The results presented in this paperdemonstrate the feasibility of using standard OO middleware likeCORBA to support certain types of real-time applications overthe Internet.
7ABD83C3	In this paper we propose and evaluate a methodology for run-time fast software component migration for application survivability in distributed real-time systems. For fast migration we focus on the two dominant factors; lightweight migration and proactive resource discovery. The former is to minimize the absolute amount of time required for migration and the latter is to provide a destination host information at the time of migration decision. The run-time software component is implemented as Java object whose class is defined by extending the unicast remote server class. The proactive resource discovery consists of Community protocol and associated algorithms. These two ideas have been implemented as a middleware that also provides a real-time job scheduler in JVM (Java Virtual Machine), and a naming server. Our analysis and simulation in a cluster computing environment show that the proactive resource discovery requires very low communication overhead while maintaining high effectiveness in finding available CPU resources. Our implementation and measurement show that run-time component migration based on our approach takes much less time compared to the approach based on reactive resource discovery
5E522BB9	Packet processing systems of forthcoming high-speed network nodes demand extremely high processing rates, but also modularity and easy adaptability due to new or evolving protocols and services. As the fixed architecture and instruction set of current network processors sometimes hinders an efficient implementation of processing tasks, we introduced the MIXMAP architecture [4] that is designed to offer programmability at multiple levels of abstraction. Now we describe the prototypical realization of this architecture showing its feasibility. Our results indicate that up to 170 million packets per second can be processed with this architecture using current FPGAs. By implementing packet processing tasks at register-transfer level and at software level, we validate the architecture's applicability and the benefits of implementing at an appropriate level of abstraction.
7F447BA2	Summary form only given. We propose architecture and a general optimization framework for dynamic, distributed real-time systems. Interesting features of this model include the consideration of adaptive applications and utility functions. We extend by formalizing the corresponding multicriterial optimization problem. As the most difficult part of this problem, we identified the evaluation and comparison of the quality of single allocations and sets of allocations, respectively. To this end, we propose and examine metrics for measuring the goodness of solutions within our general resource management framework. These metrics lay the basis for further work on developing both online and offline algorithms to tackle the general optimization problem and provide an efficient adaptive resource manager for dynamic, distributed real-time systems.
7EFCA914	The recent proliferation of ever smaller and smarter electronic devices, combined with the introduction of wireless communication and mobile software technologies enables the construction of a large variety of pervasive applications, such as home supervision and alarm systems. The inherent complexity of such applications along with their nonexpert clientele raises the necessity for Autonomic Management solutions. Nonetheless, such solutions remain difficult to conceive, as they must deal with the increased volatility, heterogeneity and distribution of the pervasive domain, while ensuring stringent performance and dependability requirements. This paper proposes that reusable support for Autonomic Management solutions be provided by middleware platforms, along with already existing middleware services, such as security and transactions. Following this approach, a service oriented component platform, iPOJO, was extended with elementary Autonomic Management capabilities. These include monitoring and effector touch points, as well as embedded Autonomic Management functions, such as service dependency management. IPOJO is an open source Apache project and has been successfully employed to implement several research projects inthe pervasive domain. This paper presents iPOJO’s extension with reusable Autonomic Management middleware services.
7D31AD48	Real-time software systems are rarely developed once and left to run. They are subject to changes of requirements as the applications they support expand, and they commonly outlive the platforms they were designed to run on. A successful real-time system is duplicated and adapted to a variety of applications - it becomes a product line. Current methods for real-time software development are commonly based on low-level programming languages and involve considerable duplication of effort when a similar system is to be developed or the hardware platform changes. To provide more dependable, flexible and maintainable real-time systems at a lower cost what is needed is a platform-independent approach to real-time systems development. The development process is composed of two phases: a platform-independent phase, that defines the desired system behaviour and develops a platform-independent design and implementation, and a platform-dependent phase that maps the implementation onto the target platform. The last phase should be highly automated. For critical systems, assessing dependability is crucial. The partitioning into platform dependent and independent phases has to support verification of system properties through both phases.
7ED7CD4F	Many real-world distributed, real-time, embedded (ORE) systems, such as multiagent military applications, are built using commercially available operating systems, middleware, and collections of pre-existing software. The complexity of these systems makes it difficult to ensure that they maintain high quality of service (QOS). At design time, the challenge is to introduce coordinated QOS controls into multiple software elements in a non-invasive manner. At run time, the system must adapt dynamically to maintain high QOS in the face of both expected events, such as application mode changes, and unexpected events, such as resource demands from other applications. We describe the design and implementation of a CPU broker for these types of ORE systems. The CPU broker mediates between multiple real-time tasks and the facilities of a real-time operating system: using feedback and other inputs, it adjusts allocations over tune to ensure that high application-level QOS is maintained. The broker connects to its monitored tasks in a non-invasive manner, is based on and integrated with industry-standard middleware, and implements an open architecture for new CPU management policies. Moreover, these features allow the broker to be easily combined with other QOS mechanisms and policies, as part of an overall end-to-end QOS management system. We describe our experience in applying the CPU Broker to a simulated DUE military system. Our results show that the broker connects to the system transparently and allows it to function in the face of run-time CPU resource contention.
5BC89B7B	Efficient and predictable demultiplexing is necessary to provide real-time support for distributed object computing applications developed with CORBA. This paper presents two contributions to the study of demultiplexing for real-time CORBA endsystems. First, we present an empirical study of four CORBA request demultiplexing strategies (linear search, perfect hashing, dynamic hashing, and active demultiplexing) for a range of target objects and operations. Second, we describe our use of the perfect hashing and active demultiplexing strategies to develop a high-performance, real-time object request broker (ORB) called TAO.
7C81A5DC	Increasing quality-of-service requirements pose challenges in both design and optimization for real time CORBA middleware. The authors describe TAO (The Ace ORB), an object request broker that provides end-to-end QoS guarantees. In discussing TAO, we focus on object-adapter and ORB-core optimizations in three key dimensions in high performance, real time ORB end system design: server-side concurrency, memory management, and CORBA request demultiplexing.
5FD1FEB6	The FlexiNet Platform is a Java middleware platform that features a component based internal structure with strong emphasis placed on reflection and introspection at all levels. This allows programmers to tailor the platform for a particular application domain or deployment scenario by assembling strongly typed components. In this paper we give an overview of the FlexiNet architecture, highlighting how its approach differs from other middleware architectures, and illustrate the benefits that result from the new approach.

7A2E21CE	To be an effective platform for performance-sensitive real-time systems, commodity-off-the-shelf (COTS) distributed object computing (DOC) middleware must support application quality of service (QoS) requirements end-to-end. However, conventional COTS DOC middleware does not provide this support, which makes it unsuited for applications with stringent latency, determinism, and priority preservation requirements. It is essential, therefore, to develop standards-based, COTS DOC middleware that permits the specification, allocation, and enforcement of application QoS requirements end-to-end.The real-time CORBA and messaging specifications in the CORBA 2.4 standard are important steps towards defining standards-based, COTS DOC middleware that can deliver end-to-end QoS support at multiple levels in distributed and embedded real-time systems. These specifications still lack sufficient detail, however, to portably configure and control processor, communication, and memory resources for applications with stringent QoS requirements.
5DD3CA36	In this article we present a detailed theoretical analysis of the behavior of our artificial hormone system. The artificial hormone system (AHS) is part of an organic middleware for mapping tasks on an heterogeneous grid of processing elements. The AHS works completely decentral - each processing cell decides for itself if it is best suited for a task and interacts with the other processing cells via ”hormone” messages. As there is no central element the stability of the system can not be controlled by a single processing element, instead the hormone values have to be chosen carefully to guarantee system stability. We will present upper and lower bounds which have to be met to guarantee system stability.
7F7E1DC6	Improving real-time is one of the most essential problems in studies of real-time operating system (RTOS). The time of task context switching is an important factor that affects the real-time of RTOS. This paper concentrates on the improvement of task context switching of one kind of RTOS-OSEK operating system. According to different task states, we apply different context switching strategies to reduce the average time of context switching. This method has been successfully implemented and evaluated in our OSEK compatible operating system-SmartOSEK OS.
7AC411E4	There is increasing demandto extend CORBA middleware to support applications with stringentquality of service (QoS) requirements. However, conventionalCORBA middleware does not define standard features to dynamicallyschedule operations for applications that possess deterministicreal-time requirements. This paper presents three contributionsto the study of real-time CORBA operation scheduling strategies.First, we document our evolution from static to dynamic schedulingfor applications with deterministic real-time requirements. Second,we describe the flexible scheduling service framework in ourreal-time CORBA implementation, TAO, which supports core schedulingstrategies efficiently. Third, we present results from empiricalbenchmarks that quantify the behavior of these scheduling strategiesand assess the overhead of dynamic scheduling in TAO. Our empiricalresults using TAO show that dynamic scheduling of CORBA operationscan be deterministic and can achieve acceptable latency for operations,even with moderate levels of queueing.
7D5EE7EB	Distributed object computing middleware such as CORBA, RMI, and DCOM have gained wide acceptance and has shielded programmers from many tedious and error-prone aspects of distributed programming. In particular, CORBA event service has been used extensively in embedded systems. We propose an aspect oriented approach to develop synchronization code for distributed systems that use event service as the underlying communication middleware. Our approach is to factor out synchronization as a separate aspect, synthesize synchronization code and then compose it with the functional code. We use high-level "global invariants" to specify the synchronization policies which are then automatically translated into synchronization code for the underlying event service. To implement synchronization efficiently using the event service, we propose enhancements to the semantics of the event service. Specifically, we define the notion of condition events and exactly k semantics. Given these enhancements, we describe a synthesis procedure to translate global invariants into synchronization code based on events. We describe the implementation of the enhancements on the Tao's Real-Time Event Service. We present experimental results to demonstrate that the enhanced event service leads to more efficient implementation of synchronization. We feel that our methodology and the enhanced Real-Time Event Service will lead to more confident use of sophisticated synchronization policies in distributed object oriented systems.
75AF2650	The end-to-end performance of pervasive mobile systems is commonly dictated by the availability of resources at the weakest link. However, a number of run-time adaptations or morphing steps can be performed to tune the system performance. In this paper, we present M-ECho, a middleware for system morphing. M-ECho is designed with focus on data streaming applications, specifically in the field of pervasive mobile systems. We consider an autonomous robotics application comprising of a set of cooperating mobile robots to demonstrate and evaluate M-ECho's system morphing capabilities. Optimizations are based upon metrics of average instantaneous power consumption at a single node (local) as well as the power consumed by all participants (global). Experimental results show that M-ECho is able to achieve improved end-to-end performance with its dynamic code morphing techniques.
7D919970	For a secure highly distributed computing environment, we suggest an efficient role based access control using attribute certificate. It reduces management cost and overhead incurred when we change the specification of the role. In this paper, we grouped roles and structured them into the role group relation tree. It results in secure and efficient role updating and distribution. For scalable role specification certificate distribution, multicasting packets are used. We take into account the packet loss and quantify performance enhancements of structuring role specification certificates.
7D37A34B	Role  Based  Access  Control  is  only  a  subset  of  the  security management and distributed systems management. Yet, the characteristics  and  use  of  the  role  objects  in  RBAC  or  Role Based Management (RBM) may differ significantly. In this paper we outline a Role Management Framework  based on the specification of policies and examine its differences and similarities  with  the  RBAC  concepts.  In  particular,  two aspects of roles required in RBM are emphasised: the need for  obligation  policies  which  changes  the  way  roles  are used  within  the  system  and  the  Object  Oriented  role  model which uses inheritance for re-use of the specification rather than implementing set-subset relationships on access rights.
78A428E2	To meet the authorization administration requirements in a distributed computer network environment, this paper extends the role-based access control model with multiple application dimensions and establishes a new access control model ED-RBAC(Extended Role Based Access Control Model) for the distributed environment. We propose an extendable hierarchical authorization assignment framework and design effective role-registering, role-applying and role-assigning protocol with symmetric and asymmetric cryptographic systems. The model can be used to simplify authorization administration in a distributed environment with multiple applications.
58756C0B	PrincipalDomain is an administrative scoping construct for establishing security policies based on the principals invoking object services that may entail objects moving around a network to accomplish their task.   The privileges attached to the principal determines the privileges of those mobile objects, which effectively defines the access control rules for any resource the object might request.  These objects may cooperate by delegating subtasks  to  other  objects.   During  the  process  of  delegation, when one object (initiator) authorizes another object (delegate) to perform some task, the attached privileges might be passed on from initiator to the delegate to accomplish the task.  Support for roles is used to improve manageability by adding an optional level of indirection.  Role-based access control and delegation provides a higher level of granularity than approaches limited only to individuals. In this paper, we describe a proposed  protection  mechanism  based  on  code-executing principals exercising their privileges via role constructs, and building delegation framework over this infrastructure. This mechanism extends current Java security features  to  support principals,  roles  and  delegation.   The framework supports a control API for application developers to specify mechanisms and security policies.
599992D8	The paper presents proposed Security Architecture for Open Collaborative Environment (OCE) being developed in the framework of the Collaboratory.nl (CNL) project with the intent to build a flexible, customer-driven security infrastructure for open collaborative applications. The architecture is based on extended use of emerging Web Services and Grid security technologies combined with concepts from the generic Authentication Authorization and Accounting (AAA) and Role-based Access Control (RBAC) frameworks. The paper describes another proposed solution the Job-centric security model that uses a Job description as a semantic document created on the basis of the signed order (or business agreement) to provide a job-specific context for invocation of the basic OCE security services. Typical OCE use case of policy based access control is discussed in details.
7E11F5FD	The paper describes a security architecture designed to support role based access control for distributed object systems in a large scale, multi-organisational enterprise in which domains are used to group objects for specifying security policies. We use the concept of a role to define access control related to a position within an organisation although our role framework caters for the specification of both authorisation and obligation policies. Access control and authentication is implemented using security agents on a per host basis to achieve a high degree of transparency to the application level. Cascaded delegation of access rights is also supported. The domain based authentication service uses symmetric cryptography and is implemented by replicated servers which maintain minimal state.
79C8533E	Recently research is focused on security policy integration and conflict reconciliation among various healthcare organizations. Problem statement: However, challenging security and privacy risk issues still arisen during sharing sensitive patient data in different large distributed organizations. Though eXtensible Access Control Markup Language (XACML) has a powerful capacity of expression, it does not support all the elements character of RBAC. Thus, it has not been built to manage security in large distributed systems in healthcare domain since each organization may join or leave at runtime. The policy redundancy and conflict resolution are important to resolve redundancy and inconsistencies before security policies can be integrated for healthcare collaboration. Existing approaches did not look at policy redundancy and conflict resolution process based on the types of redundancy and conflict for dynamic set of organizations collaboration. Besides that, a policy integration mechanism in order to generate actual security policy integration is not in well studied. Approach: In this study, we proposed an approach for integrating security XACML policies based on RBAC policy model considering both constraints and meta data information. Besides that, an approach to filter and collect only the required policies from different organizations based on user’s integration requirements is investigated. It is important for us to resolve policy redundancy and conflicts based on the types of policy redundancy and conflicts. Results: From the observation and literature analysis, it can be concluded that our work could provide the maximum confidence for pre-compile a large amount of policies and only return the most similar policies for policy integration. Besides that, our approach proved that the more restrict policy will be generated during the policy integration. Conclusion: Our work can guarantee the completeness as well as consistency of the access control policy. It is recommended that the dynamic constraints such as dynamic Separation Of Duty (SOD) should be considered because we believe this consideration can support dynamic updates and control policies in collaborative environments.
5F7ED447	Our society is increasingly moving towards richer forms of information exchange where mobility of processes and devices plays a prominent role. This tendency has prompted the academic community to study the security problems arising from such mobile environments, and in particular, the security policies regulating who can access the information in question. In this paper we describe a calculus for mobile processes and propose a mechanism for specifying access privileges based on a combination of the identity of the users seeking access, their credentials, and the location from which they seek it, within a reconfigurable nested structure. We define BACIR, a boxed ambient calculus extended with a Distributed Role-Based Access Control mechanism where each ambient controls its own access policy. A process in BACIR is associated with an owner and a set of activated roles that grant permissions for mobility and communication. The calculus includes primitives to activate and deactivate roles. The behavior of these primitives is determined by the process's owner, its current location and its currently activated roles. We consider two forms of security violations that our type system prevents: 1) attempting to move into an ambient without having the authorizing roles granting entry activated and 2) trying to use a communication port without having the roles required for access activated. We accomplish 1) and 2) by giving a static type system, an untyped transition semantics, and a typed transition semantics. We then show that a well-typed program never violates the dynamic security checks.
7E4AB174	The fast evolving workflow technologies facilitate organizations to interact and cooperate with each other to achieve their business goals by process collaborations. Task-role based access control is an important security mechanism to protect data and resources in information systems. However, the traditional centralized authorization and administration mechanism in access control can not satisfy the administrative requirements in process collaboration environments. In this paper, we propose a domain based administration model for task-role based access control (DATRBAC), in which the authorization and administration permissions are distributed to multiple administrative domains and administrative roles. Then we propose the solution to detect and resolve the conflicts between access control policies defined by different administrative roles. We also described the implementation of the model in the PLM product and the experiments based on the practical application data.
7BE4C462	We present an approach that uses special purpose role-based access control (RBAC) constraints to base certain access control decisions on context information. In our approach a context constraint is defined as a dynamic RBAC constraint that checks the actual values of one or more contextual attributes for predefined conditions. If these conditions are satisfied, the corresponding access request can be permitted. Accordingly, a conditional permission is an RBAC permission that is constrained by one or more context constraints. We present an engineering process for context constraints that is based on goal-oriented requirements engineering techniques, and describe how we extended the design and implementation of an existing RBAC service to enable the enforcement of context constraints. With our approach we aim to preserve the advantages of RBAC and offer an additional means for the definition and enforcement of fine-grained context-dependent access control policies.
7EE8A78F	The distributed cooperation environment is applied more and more in many departments and enterprises. But the traditional RBAC96 model can't resolve the access control problems in the distributed cooperation environment. Based on the traditional RBAC96 model, this paper proposes the dRBAC model which adds delegation mechanism contrast to the RBAC96 model. This model can efficiently resolve the access control problem in the distributed cooperation environment.
80D3B5C0	Role engineering, the task of defining roles and associating permissions  to them,  is essential to realize the full benefits of the role-based access control paradigm.  Essentially, there are two basic approaches to accomplish this: the topdown and the bottom-up . The top-down approach relies on a careful analysis of the business processes to define job functions and then specify appropriate roles from them.  While this approach can aid in defining roles more accurately, it is tedious and time consuming since it requires that the semantics of the business processes be well understood. Moreover, it ignores existing permissions within an organization and does not utilize them. On the other hand, the bottomup approach starts with existing permissions and attempts to derive roles from them, thus helping to automate role definition.  In this paper, we present an unsupervised approach called RoleMiner that mines roles from existing userpermission assignments. Since a role is nothing but a set of permissions, when no semantics are available, the task of role mining is essentially that of clustering users that have same (or similar)  permissions.   However,  unlike the traditional applications of data mining that ideally require identification of non-overlapping clusters, roles will have overlapping permission needs and thus permission sets that define roles should be allowed to overlap. It is this distinction from traditional clustering that makes the problem of role mining non-trivial.  Our experiments with real and simulated data sets indicate that our role mining process is quite accurate and efficient
59D0ACAC	Protecting information against unauthorized access is a key issue in information system security. Advanced access control models and mechanisms have now become necessary for applications and systems due to emerging acts, such as the Health Insurance Portability and Accountability Act (HIPAA) and the Sarbanes-Oxley Act. Role-Based Access Control (RBAC) is a viable alternative to traditional discretionary and mandatory access control. RBAC has been shown to be cost effective and is being employed in various application domains on account of its characteristics: rich specification, policy neutrality, separation of duty relations, principle of least privilege, and ease of management. Existing RBAC approaches support time-, content- and purpose-based, as well as context-aware and other forms of access control policies that are useful for developing secure systems. Although considerable amount of effort has been spent on policy specification aspects, relatively much less attention has been paid towards flexible enforcement of various aspects of RBAC approaches. Furthermore, current approaches are inadequate, as many applications and systems require the more dynamic and expressive event pattern constraints. In this thesis, we have focused on several aspects of RBAC, including generalization and enforcement of RBAC, by exploiting and extending a well-established event-based framework that has a solid theoretical foundation. Specifically, we have addressed the following problems and made the following contributions: Enforcement of existing RBAC Approaches: Security mechanisms are required for enforcing security policies. We have provided a flexible event-based technique for enforcing the RBAC standard and other current extensions in a uniform manner using an event framework. We have extended the event specification and detection with interval-based semantics for event operators and alternative actions for active rules. Generalization of RBAC and Snoop: We have generalized RBAC policies with expressive event pattern constraints. We have shown how to model diverse constraints, such as precedence, dependency, non-occurrence, and their combinations, using event patterns that are not available in existing RBAC approaches. Event patterns are event expressions that have simple and complex events as constituent events and they control the state change. Snoop, an event specification language, provides the basis for extensions needed to support the generalized RBAC. The generalization of RBAC using constraints based on event patterns can be accomplished by the extended Snoop. Enforcement of Generalized RBAC: We have shown the modeling and enforcement of generalized RBAC policies using the extended local event detector (LED). We have introduced event registrar graphs for capturing simple and complex event occurrences and keeping track of event patterns. We have also shown how RBAC with expressive event pattern constraints can be enforced using event registrar graphs. When compared to other mechanisms, the proposed event-based enforcement mechanism has the advantage of using the same framework for both policy specification and enforcement. We have briefly explored identification and handling of policy conflicts. Usability in RBAC: We have enhanced the usability of RBAC by adding an intelligent module for discovering roles and guiding (or prompting) the user to acquire appropriate roles for performing operations on objects. This approach relieves the user from the details of role-permission assignment and allows concentrating on their task. We have developed several algorithms for discovering roles, and analyzed their complexity and effectiveness. Novel Applications: We have developed various applications for demonstrating the applicability of the results obtained in this thesis. i) We have shown how role-based security policies can be supported in web gateways using a smart push-pull approach. ii) We have shown how event operators based on interval-based semantics can be utilized for information filtering. iii) We provided an integrated model for advanced data stream applications that supports not only stream processing but also complicated event and rule processing. We have also shown how the integrated model can be utilized for a network fault management system. This thesis is a first step in the direction of bridging the gap that currently exists between policy specification and enforcement. By mapping RBAC policies using a framework (event-based in our case) that can be incorporated with the underlying system in various ways (integrated, layered, wrapper-based, and distributed), we have not only extended RBAC to make it more useful, but also shown how the extended specifications can be mapped and enforced. This combination of specification and enforcement using a common framework forms the core contribution of the thesis.
7F2E99F7	Healthcare is information driven and knowledge driven. Good healthcare depends on making decisions at the right time and place, using the right patient data and applicable knowledge. Communication is of utmost relevance in today's healthcare settings, in that delivery of care, research, and management all depend on sharing information. The proposed system can securely gather, integrate, and display distributed medical information using mobile-agent technology and agent-driven security
77CFD776	The Role Based Access Control (RBAC) model and mechanism have proven to be useful and effective. T his is clear from the many RBAC implementations in commercial products. However, there are many common examples where access decisions must include other factors, in particular, relationships between entities, such as , the user, the object to be accessed, and the subject of the i nformation contained within the object. Such relationships are often not efficiently represented using traditional static se curity attributes centrally administered. Furthermore, the extension of RBAC models to include relationships obscures the fundamental RBAC metaphor. This paper furthers the concept of relationships fo r use in access control, and it shows how relationships can be supported in role based access decisions by using t he Object Management Group's (OMG) Resource Access Decision facility (RAD). This facility allows relat ionship information, which can dynamically change as part o f normal application processing, to be used in access decisions by applications. By using RAD, the access decision logic is separate from application logic. In addition, RAD allows access decision logic from dif ferent models to be combined into a single access decision . Each access control model is thus able to retain its met aphor.
7740926C	This paper describes the design and development of a flexible, customer driven, security infrastructure for open collaborative environments. The experiences were gained within the framework of the collaboratory.nl project. The work is based on extended use of emerging Web services and grid security technologies, combined with concepts from the generic authentication authorization and accounting (AAA) authorisation framework. Basic CNL use cases and functional security requirements are analysed to provide motivation for the proposed job-centric security model. This model describes access control and user- and resource management. The proposed job-centric approach uses a job description as a semantic document that is created on the basis of the signed order (or business agreement). It contains all the information required to run the experiment and also to create/manage the virtual job-based associations of users and resources. The proposed trust relations analysis explains the use of trust anchors in the job-centric security model. In addition, the paper provides implementation details of using XACML and SAML for authorisation assertions and messaging, based on the current CNL implementation
7E7D4978	In this paper, we present static verification of security requirements for CSCW systems using finite-state techniques, i.e., model checking. The coordination and security constraints of CSCW systems are specified using a role based collaboration model. The verification ensures completeness and consistency of the specification given global requirements. We have developed several verification models to check security properties, such as task-flow constraints, information flow or confidentiality, and assignment of administrative privileges. The primary contribution of this paper is a methodology for verification of security requirements during designing collaboration systems.
810701CB	Role-based access control, where object accesses are controlled by roles (or job functions) is a more feasible alternative to traditional access control mechanisms. Constraints play a critical role in realizing and providing finegrained RBAC in diverse domains such as P2P and grid computing. In this paper, we have shown how events and authorization rules are used to provide fine-grained RBAC. First, simple events are identified for the RBAC domain. Second, various event operators for modeling constraints such as precedence, non-occurrence, dependency and their combinations are introduced. Third, how event-based RBAC policies are specified using both simple and complex events are discussed. Finally, how the proposed fine-grained RBAC policies can be exploited for P2P resource management is discussed
7CCF112E	Access control needs to be more flexible and fine-grained to support cooperative tasks and processes performed by dynamic teams. This can be done by applying state-of-the- art role-based access control (RBAC) technology. This paper examines how to integrate RBAC in a team-based organization context and how to apply such access control to hypermedia structures. Based on the analysis of these issues, a team-and-role-based access control model is proposed, which describes various aspects of role-based access control in cooperative hypermedia environments. The model has been implemented in CHIPS, a cooperative hypermedia-based process support system. Application examples demonstrate that its organizational context management and access permission authorization retain the simplicity of RBAC. Our extensions provide effective and flexible access control for managing various kinds of shared workspaces, especially shared process spaces, where access control is not only used for managing security, but also for supporting coordination. 
7AEDF09A	With the boom of Internet Technology, it becomes possible to combine designers from different disciplines into one team to support product design globally. In this paper, a distributed collaborative product design environment is presented to support top-down process oriented product design. In conceptual design stage, the artifact is managed by semantic norm model (SNM). In SNM, the designers can define virtual components at early design stage and instantiate those components at later detailed design stage. By role-based access control (RBAC), different roles with corresponding permissions could be assigned to distributed designers, and the designers could concurrently modify different components of the product relevant to his or her roles. Based on the SNM and RBAC system, a distributed collaborative product design environment is developed and the top-down oriented product design process is demonstrated.
7DA28104	Devising a complete and correct set of roles has been recognized as one of the most important and challenging tasks in implementing role based access control. A key problem related to this is the notion of goodness/interestingness -- when is a role good/interesting? In this paper, we define the role mining problem (RMP) as the problem of discovering an optimal set of roles from existing user permissions. The main contribution of this paper is to formally define RMP, and analyze its theoretical bounds. In addition to the above basic RMP, we introduce two different variations of the RMP, called the δ-approx RMP and the Minimal Noise RMP that have pragmatic implications. We reduce the known "set basis problem" to RMP to show that RMP is an NP-complete problem. An important contribution of this paper is also to show the relation of the role mining problem to several problems already identified in the data mining and data analysis literature. By showing that the RMP is in essence reducible to these known problems, we can directly borrow the existing implementation solutions and guide further research in this direction.
7319FC1E	Several advanced Role based access control (RBAC) models have been developed supporting specific features (i.e.: role hierarchy, separation of duty) to achieve high flexibility. However, integrating additional features also increases their design complexity, and consequently the opportunity for mistakes that may cause information to flow to inappropriate destinations. In this paper, we present a formal technique to model and analyze RBAC using Colored Petri nets (CP-nets) and CPNtools for editing and analyzing CP-nets. Our purpose is to elaborate a CP-net model which describes generic access control structures based on an RBAC policy. The resulting CP-net model can be then composed with different context-specific aspects depending on the application. A significant benefit of CP-nets and, particularly, CPNtools is to provide a graphical representation and an analysis framework that can be used by security administrators to understand why some permissions are granted or not and to detect whether security constraints are violated.
80139700	With the fast development of high speed wireless technologies and the growing population of mobile portable devices, location information is potentially available for access control systems.Such applications are especially meaningful in emergency situations, where quick responses are urgently required for persons to be physically present in a certain place to perform sensitive tasks without conflicting with security policies. In this paper, we investigate this challenging problem and propose a novel Constraint-based Authorization Management Model, which takes the mobile execution of tasks with handheld devices into account. The authorizations are activated by means of Location Based Execution Binding to handle uncertain conditions such as flexible business processes and emergency situations, considering both the user's location and attributes. With the introduced algorithms the model is capable of execution planning to detect and avoid inconsistencies in the security constraints of activities at design and runtime. Finally we propose a system architecture based on Web service technologies and a XACML based syntax for defining the security constraints.
7FDCDF8C	Multi-domain application environments where distributed domains interoperate with each other are becoming a reality in Internet-based enterprise applications. The secure interoperation in a multi- domain environment is a challenging problem. Role- based access control (RBAC) is used for specifying the security requirements of multi-domain applications in this paper. Then, role mapping relationship between domains is described by XML documents. Furthermore, the situations where dynamic role mapping violates separation of duties (SoD) which is one of the three basic security principles for the RBAC model are analyzed in detail, and relevant algorithms to detect the above security problem are designed in this paper.
7DF8F415	The Internet enables connectivity between many strangers: entities that don't know each other. We present the Trust Policy Language (TPL), used to define the mapping of strangers to predefined business roles, based on certificates issued by third parties. TPL is expressive enough to allow complex policies, e.g. non-monotone (negative) certificates, while being simple enough to allow automated policy checking and processing. Issuers of certificates are either known in advance, or provide sufficient certificates to be considered a trusted authority according to the policy. This allows bottom-up, "grass roots" buildup of trust, as in the real world. We extend, rather than replace, existing role based access control mechanisms. This provides a simple, modular architecture and easy migration from existing systems. Our system automatically collects missing certificates from peer servers. In particular this allows use of standard browsers, which pass only one certificate to the server. We describe our implementation, which can be used as an extension of a Web server or as a separate server with interface to applications.
76A59A90	The role metaphor in Role Based Access Control (RBAC) is particularly powerful in its ability to express access policy in terms of the way in which administrators view organizations. Much of the effort in providing administrative tools for RBAC has been devoted to tools for associating users with roles and roles with roles. This paper introduces the concept of an “Object Access Type” and describes the tool “RGP-Admin” for administering associations between roles and permissions using Object Access Types. RGP-Admin is applicable to most RBAC mechanisms and Access Control List mechanisms which support groups. A prototype demonstration of RGPAdmin was developed to illustrate how Object Access Types are used to manage associations between Windows NT groups, representing roles, and file permissions within the Windows NT File System.
5CF44CAD	This paper analyzes and compares role-based access control (RBAC) features supported in the most recent versions of three popular commercial database management systems: Informix Online Dynamic Server Version 7.2, Oracle Enterprise Server Version 8.0 and Sybase Adaptive Server Release 11.5. We categorize RBAC features under three broad areas: user role assignment, support for role relationships and constraints, and assignable privileges. Our finding is that these products provide a sound basis for implementing the basic features of RBAC, although there are significant differences. In particular, Informix restricts users to a single active role at any time, while Oracle and Sybase allow multiple roles to be activated simultaneously as per the user's selection. All three provide support for role hierarchies, but Sybase is the only one to directly support mutual exclusion of roles. 
814B8263	This paper proposes a context-aware security service providing multiple authentications and authorization from a Security Level which is decided dynamically in a context-aware environment. It helps developers build secure services efficiently. A security service in a dynamic environment uses Multi-Attribute Utility Theory and extended Generalized Role-Based Access Control. The system uses attribute values in GRBAC to calculate the Security Level, and extend the GRBAC. We expect this model to be widely used in providing flexible security services in a heterogeneous network.
781BD0DF	Role-based access control (RBAC) models have been successfully implemented in various information systems in recent years. However, the traditional centralized authorization and administration mechanisms in RBAC have several drawbacks in collaborative environments. In this paper, we propose a distributed domain administration of RBAC model, DARBAC, in which the authorization and administration privileges are distributed to multiple administrative domains. Each administrative role is assigned to an administrative domain and can only execute administrative operations within its domain. By introducing the concept of administrative domain and administrative role hierarchy, the DARBAC model can flexibly meet the access control requirements in collaborative environments. We also describe how to implement the model in the PLM product and how to apply the model in a distributed enterprise environment to support cooperative work
5EDB0A93	This paper describes the design and specification of a roles and rights mark-up language (R2ML) that provides a role based access control policy for applications executing in a distributed e-commerce environment. The R2ML language was written using XML and a test trading application (AUTONET) was developed using the Java enterprise environment (J2EE). The aim of the test application was to implement a basic B2B application that would interact with the R2ML language to enforce secure application access control.
7A866AB1	Aiming at the lack of adequate and flexible access control in 3D virtual environment when used in education, according to RBAC model, we present an authority control method which can manage manipulations to all objects, e.g. a variety of assets, avatars, scripts and network communication pipes in 3D virtual environment. The proposed technique overcomes the insufficiency on access control aspect in original platform, and provides more powerful and fexible ability on it. We experiment with the method in OpenSim, a free open source 3D virtual platform, the results show its effectiveness and versatility.
7B269C6F	Role mining from the existing permissions has been widely applied to aid the process of migrating to an RBAC system. While all permissions are treated evenly in previous approaches, none of the work has employed the weights of permissions in role mining to our knowledge, thus providing the motivation for this work. In this paper, we generalize this to the case where permissions are given weights to reflect their importance to the system. The weights can correspond to the property of operations, the sensitive degree of objects, and the attribute of users associated with permissions. To calculate the weight of permissions, we introduce the concept of similarity between both users and permissions, and use a similarity matrix to reinforce the similarity between permissions. Then we create a link between the reinforced similarity and the weight of permissions. We further propose a weighted role mining algorithm to generate roles based on weights. Experiments on performance study prove the superiority of the new algorithm.
84F9647D	 The main objective of this thesis is to define a high level logical language that can express complex security policies within an access control framework. The development of this language is done in three steps. First we define a role based dynamic framework where the state evolution of a service depends on the execution of its functionalities. Next we define an attribute based framework that gives more expressivity in terms of specification of access control conditions and add the notion of workflow that gives an order over the execution of the service functionalities and thus allows the definition of the general behavior of the service. Finally, in order to take into account the collaboration between different services we add a trust negotiation layer that allows each service to define its own exchange policy with respect to other services in the environment. Having such a unified framework facilitates the safety analysis of the security policy since one can take into account the different factors that influence the access control decisions within the same framework. Thus the second objective of this thesis is to study the main access control features such as delegation and separation of duty properties on the one hand and the security features for the communication between the services at the trust negotiation level on the other hand. 
5A5E305C	Despite numerous advantages that cloud computing offer (e.g., Flexibility, elasticity, scalability, etc.), many potential clients are still hesitant to join the cloud due to their security and privacy concerns. Outsourcing the data to a cloud in a multitenant environment brings many security challenges including data leaks, threats, and malicious attacks. The cloud computing platform, virtual servers, and the provider's services are highly dynamic and diverse in nature, making the traditional access control mechanisms (e.g., Firewalls and VLAN etc.) less effective in controlling the unauthorized access to cloud's data and resources. Several access control policies and authorization system have been proposed in literature to defend against cloud security threats. Most of these systems are designed to work with one or more access control policies. However, little work has been done to develop generic access control architecture capable to work with most of the available access control policies. In this paper, we present a new access control architecture using a global resource management system (GRMS) to effectively handle both local and remote access requests. The introduction of GRMS makes our proposed architecture semi distributed at the expense of minimal request-response time. In addition, our proposed architecture works effectively with both peered access control module (PACM) and virtual resource manager (VRM) to protect and manage all resources and services of cloud providers from unauthorized access.
758B37CF	The research communitiy has shown considerable interest in studying access control in single Trusted Operating Systems (TOS). However, interactions among multiple TOSs have attracted relatively little attention. In this paper, we propose a Collaborative Role-Based Access Control (C-RBAC) model for distributed systems in which accesses across system domain boundaries are allowed. Access entities in a TOS vary in time. The changes in the organizational structure of the access entities in one system may influence other cooperating systems. In addition, policyfreeness, domain and rule conflicts are possible. These problems restrict the flexibility and scalability of coordination. We propose drafting a metacomponent to play the role of a coordinator in multi-domain role-based access control. It is then possible to impart flexibility and scalability in a secure fashion. Experimental studies of the proposed model with the Network File System and SELinux system support our conclusion.
0CFD8AA9	In this paper, we extended hierarchial structure of managed object class to support Role-Based Access Control, and described constraint conditions that have support dynamic temporal function as well as statical temporal function established by management process. And we defined about violation notifications should report to manager when rules violate constraint conditions. Also we presented system architecture that support RBAC with MIB(Management Information Base) of ITU-T recommendation. By access control enforcement and decision function, constraint conditions and activated translation procedure of each roles are described, our system presents dynamic temporal property systematically.
77B34D29	Security analysis is a formal verification technique to ascertain certain desirable guarantees on the access control policy specification. Given a set of access control policies, a general safety requirement in such a system is to determine whether a desirable property is satisfied in all the reachable states. Such an analysis calls for the use of formal verification techniques. While formal analysis on traditional Role Based Access Control (RBAC) has been done to some extent, recent extensions to RBAC lack such an analysis. In this paper, we consider the temporal RBAC extensions and propose a formal technique using timed automata to perform security analysis by analyzing both safety and liveness properties. Using safety properties one ensures that something bad never happens while liveness properties show that some good state is also achieved. GTRBAC is a well accepted generalized temporal RBAC model which can handle a wide range of temporal constraints while specifying different access control policies. Analysis of such a model involves a process of mapping a GTRBAC based system into a state transition system. Different reduction rules are proposed to simplify the modeling process depending upon the constraints supported by the system. The effect of different constraints on the modeling process is also studied.
7F9C8A74	Devising a complete and correct set of roles has been recognized as one of the most important and challenging tasks in implementing role based access control. A key problem related to this is the notion of goodness - when is a set of roles good? Recently, the role mining problem (RMP) has been defined as the problem of discovering an optimal set of roles from existing user permissions. Several different objectives for optimality have been proposed. However, one problem with these definitions is that often organizations already have a deployed set of roles and wish to optimize this set. Even if an optimal set of roles is discovered, if this is widely different, it is impossible to simply throw out the deployed roles and start using the new ones as this may disrupt organizational processes and separation of duty constraints that are defined on roles. Essentially, what is missing is taking role migration cost into account when defining optimality, which would allow us to come up with the best suited set of roles.In this paper, we define a fundamentally different Role Mining Problem that takes the problem of deployed roles into account. We define the Minimal Perturbation RMP as the problem of discovering an optimal set of roles from existing user permissions that are similar to the currently deployed roles. In order to do this, we discuss the concept of similarity of roles and propose suitable definitions. Solutions also need to be parameterized to set relative weight of similarity and minimality to find the optimal set. We propose a heuristic solution based on the previously developed FastMiner algorithm that meets these requirements. We demonstrate the effectiveness of the algorithm through our experimental results.
7EC8D1E3	The development of internet has made cooperation among distributed organizations become a reality. Access control in such distributed cooperation environment is a challenge problem as new environment introduces new requirements. Considerable recent works incorporate RBAC into distributed cooperation environment by role mapping; however, role mapping approach has a number of problems, such as security violation problems and access permission leakage. In this paper, we proposed a role-based access control model called RBAC-DC to meet new requirements of distributed cooperation environment. In stead by role mapping, RBAC-DC achieves cooperation by service providing domain providing roles, permissions of those roles and user-role assignment privilege of those roles to service requesting domain. Besides, RBAC-DC disables transitivity of access permissions among domains. RBAC-DC achieves the goal of meeting new requirements of distributed cooperation environment, and has a set of properties compared to role mapping approach, such as maximizing degree of cooperation and more control power.
80BAC20D	Traditional access control disciplines such as RBAC has difficulty in covering open and decentralized multi-centric systems because it has focused on a closed system where all users are known and primarily utilizes a server-side reference monitor within the system. Trust management has relaxed this known user restriction and allowed authorize for strangers based on their credentials. However, trust management has also been found to be lacking because of certain inherent drawbacks with the notion of credential. In this work, a new access control model T&RBAC is presented in this paper. It integrates RBAC and TM. User can be assigned to local roles, also can be assigned to foreign roles based on his credential and local roles. We proof that there is no security constraints in T&RBAC. To some extends, T&RBAC is only a core model and can be extended for specific requirement.
68FAA330	The discretionary access controls (DAC) employed by traditional operating systems only provide system administrators and users with a loose ability to specify the security policies of the system. In contrast, mandatory access controls (MAC) provide a stronger, finer-grained mechanism for specifying and enforcing system security policies. A related security concept called the principle of least authority (POLA) states that subjects should only have access to the specific resources that they absolutely require to function properly at any given time.Although a number of existing projects (Plash and Polaris) seek to provide POLA implementations, these are not enforced using strong MAC. Conversely, existing MAC implementations (SELinux and AppArmor) do not provide rigorous POLA because they do not provide an effective mechanism for dynamic policy modification based on user preferences.This paper presents our solution to fill this void, called the Pluggable User-space Linux Security Environment (PULSE), which implements a MAC enforced, dynamic, user-level POLA implementation. Through the use of user-space plug-ins to specify security policy, PULSE provides a high degree of dynamism, flexibility and usability which is not available in existing security architectures.
59CF8251	An RBAC (Role Based Access Control) based and policy enforcement coordination (RBPEC) model suitable in Internet environment is described. Coordination deals with the interaction between different agents and also between agents and surrounding environments. A set of agents interacting with each other for a common global system task constitutes a coordination group. Coordination is combined with access control in RBPEC model. Role based access control is introduced for security concerns. Coordination policy is enforced in a distributed manner. The model has two advantages of being secure and scalable.
7F77E68B	To ensure correct implementation of complex access control requirements, it is important that the validated and verified requirements are effectively integrated with the rest of the system. It is also important that the system can be validated and verified early in the development process. In this paper we present an integrated, role-based access control model. The model is based on the graphical behavior tree notation, and can be validated by simulation, as well as verified using a model checker. Using this model, access control requirements can be integrated with the rest of the system from the outset, because: a single notation is used to express both access control and functional requirements; a systematic and incremental approach to constructing a formal behavior tree specification can be adopted; and the specification can be simulated and model checked. The effectiveness of the model is evaluated using a case study with distributed access control requirements.	
7B46D51E	The paper shows how role-based access control (RBAC) models could be implemented using CORBA Security service. A configuration of CORBA protection system is defined. We provide definitions of RBACo and RBACl implementations in the framework of CORBA Security and describe what is required from an implementation of CORBA Security service in order to support RBACoRBACs models. 
7AC4D6CE	In the future, a largely invisible and ubiquitous computing infrastructure will assist people with a variety of activities in the home and at work. The applications that will be deployed in such systems will create and manipulate private information and will provide access to a variety of other resources. Securing such applications is challenging for a number of reasons. Unlike traditional systems where access control has been explored, access decisions may depend on the context in which requests are made. We show how the well-developed notion of roles can be used to capture security-relevant context of the environment in which access requests are made. By introducing environment roles, we create a uniform access control framework that can be used to secure context-aware applications. We also present a security architecture that supports security policies that make use of environment roles to control access to resources.
6164F309	The establishment of globalization is driving inter- organizational collaboration towards a necessity. We cannot expect total conformity between organizations nor homogeneous security settings. Nevertheless, each organization, with its own security policies, needs to exchange data. Employees involved in inter-organizational tasks shall require remote access to data hosted by other organizations. Administrating access control policies for those employees creates problems for security officers, particularly for role assignments. Flexibility in extending (or restricting) permissions for roles imported from other organizations is required. In this work, we present an approach based on Description Logic formalisms to create from the inter-organizational agreement a set of bridge rules that in addition to (i) the permissions assigned to a given role from one organization and (ii) the permissions assigned to another role in the other organization, allows security officers to check consistency of the resulting combination of roles from both organizations.
771A3CDF	Access control is used in computer systems to control access to confidential data. In this thesis we focus on access control for dynamic collaborative environments where multiple users and systems access and exchange data in an ad hoc manner. In such environments it is difficult to protect confidential data using conventional access control systems, because users act in unpredictable ways. In this thesis we propose a new access control framework, called Auditbased Compliance Control (AC2). In AC2 user actions are not checked immediately (a-priori), like in conventional access control, but users must account for their actions at a later time (a-posteriori), by providing machinecheckable justification proofs to auditors. The logical proofs are based on policies received from other users, and other logged actions. AC2 has a rich policy language based on first-order logics, and it features an automated audit procedure. AC2 allows users to exchange and access confidential data in an ad hoc manner, and thus collaborate more easily. Applied in a medical setting, for example, doctors would be able to continue their work, regardless of authorization issues such as missing patient consent, and missing or outdated policies. Doctors can deal with these issues at a later time. Although this unconventional approach may seem, at first sight, inappropriate for practical applications, recently a similar design choice has been made for the Dutch national infrastructure for the exchange of electronic health records (AORTA). At the same time we are aware of the fact that it is a big step for organizations to change from a conventional access control mechanism (apriori) to a new mechanism. In this thesis we also take a more conventional approach by proposing two extensions to Role-based Access Control (RBAC) - an existing and widely used access control model. These extensions give users more ways of authorizing and deploying RBAC policy changes, thus favoring dynamic collaboration between users.
7E1DFC66	Advance technologies in decentralized systems are the new building block of today's Internet and provide interoperability among heterogeneous databases. In these environments, interoperation and information sharing are one of the most critical issues. Interoperability enables users to access database in different domain. Furthermore, government, financial and medical institutions more require secure collaboration to share their data with organizations. However secure collaboration has many challenging problem in multi-domain environments. In this paper, we propose secure collaboration to effectively share resource by reconcilement structure. Proposed scheme is based on Role Based Access Control (RBAC) to support flexible control. In decentralized RBAC-based system, the number of roles can be in the hundreds or thousands and they share information among dispersed domains. We present solution which regulates the interoperability.
76C335C7	The focus of this paper is on a specification model for defining security and coordination policies for distributed collaboration and workflow systems. This work is motivated by the objective to build distributed collaboration systems from their high level specifications. We identify here unique requirements for secure collaboration, specifically role admission and activation constraints, separation of duties, dynamic access control, and a model for multiuser participation in a role. We present a role-based model for specifying coordination and dynamic security requirements in collaboration systems. It also supports hierarchical structuring of a large collaboration environment using the concept of activities, which define a naming scope and a protection domain to specify security and coordination policies. We have implemented this specification model in XML and used it to construct the runtime environments for distributed collaboration systems using a policy based middleware.
77C46276	The suitability of Role Based Access Control (RBAC) is being challenged in dynamic environments like healthcare. In an RBAC system, a user's legitimate access may be denied if their need has not been anticipated by the security administrator at the time of policy specification. Alternatively, even when the policy is correctly specified an authorised user may accidentally or intentionally misuse the granted permission. The heart of the challenge is the intrinsic unpredictability of users' operational needs as well as their incentives to misuse permissions. In this paper we propose a novel Budget-aware Role Based Access Control (B-RBAC) model that extends RBAC with the explicit notion of budget and cost, where users are assigned a limited budget through which they pay for the cost of permissions they need. We propose a model where the value of resources are explicitly defined and an RBAC policy is used as a reference point to discriminate the price of access permissions, as opposed to representing hard and fast rules for making access decisions. This approach has several desirable properties. It enables users to acquire unassigned permissions if they deem them necessary. However, users misuse capability is always bounded by their allocated budget and is further adjustable through the discrimination of permission prices. Finally, it provides a uniform mechanism for the detection and prevention of misuses.
7B9A1652	In order to implement role-based controls in operational environments, they must be represented and managed in a secure manner, desirably leveraging other security infrastructure elements. Attribute certification is an emerging technology area, extending authentication- oriented public-key infrastructures (PIUS) to support authorization facilities. It allows a wide range of authorization decision criteria to be managed in a coordinated fashion. In particular, it  offers facilities that can be applied usefully and effectively to manage and delegate role-related attributes within distributed and mutually suspicious computing environments, minimizing unnecessary trust in intermediaries. As such, its definition and adoption affords opportunities for increasing support of role-based policies, Consistent with separation of duties, it allows attribute management to be separated from identity and user management. This paper considers aspects of attribute certification, examines current proposals in the area, and assesses the technology’s value for controlled support of delegation and role-based policies within distributed environments. 
80A7BFD2	Roles are a powerful concept for facilitating distributed systems management. It has been accepted as a good solution for information sharing based on the research of KB AC (role-based access control). Although no explicit roles are declared in traditional CSCW (computer-supported-cooperative work) systems, there are implicit roles granted in the design. Consequently, we may infer that "without roles, there would be no collaboration". There is, however, a lack of specific, comprehensive research on building collaborative systems with role management and on role-based methods that have been successfully used in access control. This paper aims to clarify the basic problems related to the research of role-based collaboration and to propose a basic method to do that research.
7D5C6D1E	E-commerce applications have diverse security requirements ranging from business-to-business over business-to-consumer to consumer-to-consumer types of applications. This range of requirements cannot be handled adequately by one single security model although role-based access controls (RBAC) depict a promising fundament for generic high-level security. Furthermore, RBAC is well researched but rather incompletely realized in most of the current backend as well as business layer systems. Security mechanisms have often been added to existing software causing many of the well-known deficiencies found in most software products. However, with the rise of component-based software development security models can also be made available for reuse. Therefore, we present a general-purpose software framework providing security mechanisms such as authentication, access controls, and auditing for Java software development. The framework is called GAMMA (Generic Authorization Mechanisms for Multi-Tier Applications) and offers multiple high-level security models (including the aforementioned RBAC) that may even be used concurrently to cover such diverse security requirements as found within e-commerce environments.
7E45C5B9	Object-oriented security mechanisms can provide scalable, fine-grained access control in both applications and at the boundary controller. Existing commercial work has neglected fine-grained access control at the boundary controller. Standards organizations have also neglected security management issues that are vital for scalability. This research focuses on role-based access control mechanisms for CORBA and Java RMI. The resulting approach has been tested through a variety of methods and integrated with several other research prototypes. We describe our access control implementations and the results of our testing and integration activities.
7A8D96DF	Recently, the importance of including obligations as part of access control systems for privilege management, for example, in healthcare information systems, has been well recognized. In an access control system, an a posteriori obligation states which actions need to be performed by a user after he has accessed a resource. There is no guarantee that a user will fulfill a posteriori obligations. Not fulfilling these obligations may incur financial loss, or loss of goodwill and productivity to the organization. In this paper, we propose a trust-and-obligation based framework that reduces the risk exposure of an organization associated with a posteriori obligations. We propose a methodology to assign trust values to users to indicate how trustworthy they are with regards to fulfilling their obligations. When access requests that trigger a posteriori obligations are evaluated, the requesting users' trust values and the criticality of the associated obligations are used. Our framework detects and mitigates insider attacks and unintentional damages that may result from violating a posteriori obligations. Our framework also provides mechanisms to determine misconfigurations of obligation policies. We evaluate our framework through simulations and demonstrate its effectiveness.
6E512098	This paper presents a role-based decomposition model for improving concurrency in distributed object-oriented software development environments. In software development process, the target software system consists of a large number of components closely related with one another. This results in potential operation conflicts in cooperative works among the project members. This problem can be much reduced by decomposing the target software system into the relatively independent components. Our system incorporates hierarchical role-based decomposition model to reduce operation conflicts frequently occur in synchronous/asynchronous collaboration. The decomposed components are also used as basic units of the role-based access control. Once the target software system is properly decomposed according to this model, its subsystems can migrate to the other sites, and thus, the components can be evenly distributed over networked computers.
795D8755	With Role Based Access Control (RBAC), each role is associated with a set of operations which a user in that role may perform. The power of RBAC as an access control mechanism is the concept that an operation may theoretically be anything. This is contrasted to other access control mechanisms where bits or labels are associated with information blocks. These bits or labels indicate relatively simple operations, such as, read or write, which can be performed on an information block. Operations in RBAC may be arbitrarily complex, e.g., 'a night surgical nurse can only append surgical information to a patient record from a workstation in the operating theater while on duty in that operating theater from midnight to 8 AM.' A goal for implementing RBAC is to allow operations associated with roles to be as general as possible while not adversely impacting the administrative flexibility or the behavior of applications. 
58918DD9	Recently research is focused on security policy integration and conflict reconciliation among various healthcare organizations. However, challenging security and privacy risks issues still arisen during sharing sensitive patient data in different large distributed organizations. In this paper, we proposed an approach for integrating security policies based on Role-Based Access Control (RBAC) policy model that supports dynamic constraint rules and meta data information, reduces policy redundancy and resolves conflicts based on the types of policy redundancy and conflict. We believe this work can support dynamic updates and control policies in collaborative environments.
2471C250	The research communitiy has shown considerable interest in studying access control in single Trusted Operating Systems (TOS). However, interactions among multiple TOSs have attracted relatively little attention. In this paper, we propose a Collaborative Role-Based Access Control (C-RBAC) model for distributed systems in which accesses across system domain boundaries are allowed. Access entities in a TOS vary in time. The changes in the organizational structure of the access entities in one system may influence other cooperating systems. In addition, policyfreeness, domain and rule conflicts are possible. These problems restrict the flexibility and scalability of coordination. We propose drafting a metacomponent to play the role of a coordinator in multi-domain role-based access control. It is then possible to impart flexibility and scalability in a secure fashion. Experimental studies of the proposed model with the Network File System and SELinux system support our conclusion.
5F1C2EDC	In the Internet-age, the geographical boundaries that have previously impinged upon inter-organization collaborations have become decreasingly important. However, ensuring secure and authorized access to remote services and information resource in a dynamic collaborative environment is a challenging task. According to some recent literatures, trust between users in different security domains or organizations is an effective method to this problem. However, only trust is not enough because of the complexity and burden of authorization. So we integrate role into our trust model to simplify the management of access control. Moreover, in order to make the dynamic collaboration between different domains more secure, we present the constraint to authorization and operations of the users in foreign domains.
5E672001	In this paper, we propose a new manifold representation for visual speech recognition. The developed system consists of three main steps: a) lip extraction from input video data, b) generate the expectation-maximization PCA (EMPCA) manifolds for the entire image sequence and perform manifold interpolation and re-sampling, c) classify the manifolds using a HMM classifier to identify the words described by the lips motions in the input video sequence.
80118637	We extend our earlier work on deep-structured conditional random field (DCRF) and develop deep-structured hidden conditional random field (DHCRF). We investigate the use of this new sequential deep-learning model for phonetic recognition. DHCRF is a hierarchical model in which the final layer is a hidden conditional random field (HCRF) and the intermediate layers are zero-th-order conditional random fields (CRFs). Parameter estimation and sequence inference in the DHCRF are developed in this work. They are carried out layer by layer so that the time complexity is linear to the number of layers. In the DHCRF, the training label is available only at the final layer and the state boundary is unknown. This difficulty is addressed by using unsupervised learning for the intermediate layers and lattice-based supervised learning for the final layer. Experiments on the standard TIMIT phone recognition task show small performance improvement of a three-layer DHCRF over a two-layer DHCRF; both are significantly better than the single-layer DHCRF and are superior to the discriminatively trained tri-phone hidden Markov model (HMM) using identical input features.
0000AB99	In this paper we have developed a novel technique to deal with the problem of feeding a temporal variable speech signal to Multi-Layered Perceptron (MLP), which generally only accept fixed-dimension input pattern, for speech recognition. Instead of using conventional linear or nonlinear interpolation methods, this method is based on the integration of an MLP with Finite-State Vector Quantizer (FSVQ) which is characterized by the ability to memorize the correlations between successive speech feature vectors. FSVQ is designed to map the variable length input pattern into an activation trace on a set of sub-codebooks, each of which corresponds to a 'frame' input unit of the MLP. Experiments show that for the multi-speaker English Alphabet E-set task it can achieve better performance than an MLP with a non linearly interpolated input of fixed dimension. 
7843D7EA	Pattern classification is an instinct in human, and artificial neural networks (ANN) can be its biotical simulation. Therefore, a pattern classification based on Pi calculus is firstly discussed, which use the same basic principles of concurrent computation as neural networks; secondly this paper introduces a method to use Pi calculus for ANN and illustrating equivalence between them. Finally its superiority is also discussed.
7E8C6BD1	We develop and present the deep-structured conditional random field (CRF), a multi-layer CRF model in which each higher layer's input observation sequence consists of the previous layer's observation sequence and the resulted frame-level marginal probabilities. Such a structure can closely approximate the long-range state dependency using only linear-chain or zeroth-order CRFs by constructing features on the previous layer's output (belief). Although the final layer is trained to maximize the log-likelihood of the state (label) sequence, each lower layer is optimized by maximizing the frame-level marginal probabilities. In this deep-structured CRF, both parameter estimation and state sequence inference are carried out efficiently layer-by-layer from bottom to top. We evaluate the deep-structured CRF on two natural language processing tasks: search query tagging and advertisement field segmentation. The experimental results demonstrate that the deep-structured CRF achieves word labeling accuracies that are significantly higher than the best results reported on these tasks using the same labeled training set.
7BD8C65F	Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems.
7D5D90EF	A structured generative model of speech coarticulation and reduction is described with a novel two-stage implementation. At the first stage, the dynamics of formants or vocal tract resonances (VTRs) in fluent speech is generated using prior information of resonance targets in the phone sequence, in absence of acoustic data. Bidirectional temporal filtering with finite-impulse response (FIR) is applied to the segmental target sequence as the FIR filter's input, where forward filtering produces anticipatory coarticulation and backward filtering produces regressive coarticulation. The filtering process is shown also to result in realistic resonance-frequency undershooting or reduction for fast-rate and low-effort speech in a contextually assimilated manner. At the second stage, the dynamics of speech cepstra are predicted analytically based on the FIR-filtered and speaker-adapted VTR targets, and the prediction residuals are modeled by Gaussian random variables with trainable parameters. The combined system of these two stages, thus, generates correlated and causally related VTR and cepstral dynamics, where phonetic reduction is represented explicitly in the hidden resonance space and implicitly in the observed cepstral space. We present details of model simulation demonstrating quantitative effects of speaking rate and segment duration on the magnitude of reduction, agreeing closely with experimental measurement results in the acoustic-phonetic literature. This two-stage model is implemented and applied to the TIMIT phonetic recognition task. Using the N-best (N=2000) rescoring paradigm, the new model, which contains only context-independent parameters, is shown to significantly reduce the phone error rate of a standard hidden Markov model (HMM) system under the same experimental conditions.
800458B9	We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.
78DF895A	Modeling dynamic structure of speech is a novel paradigm in speech recognition research within the generative modeling framework, and it offers a potential to overcome limitations of the current hidden Markov modeling approach. Analogous to structured language models where syntactic structure is exploited to represent long-distance relationships among words , the structured speech model described in this paper makes use of the dynamic structure in the hidden vocal tract resonance space to characterize long-span contextual influence among phonetic units. A general overview is provided first on hierarchically classified types of dynamic speech models in the literature. A detailed account is then given for a specific model type called the hidden trajectory model, and we describe detailed steps of model construction and the parameter estimation algorithms. We show how the use of resonance target parameters and their temporal filtering enables joint modeling of long-span coarticulation and phonetic reduction effects. Experiments on phonetic recognition evaluation demonstrate superior recognizer performance over a modern hidden Markov model-based system. Error analysis shows that the greatest performance gain occurs within the sonorant speech class
7D658A7A	This paper introduces an enhanced phoneme-based myoelectric signal (MES) speech recognition system. The system can recognize new words without retraining the phoneme classifier, which is considered to be the main advantage of phoneme-based speech recognition. It is shown that previous systems experience severe performance degradation when new words are added to a testing dataset. To maintain high accuracy with new words, several improvements are proposed. In the proposed MES speech recognition approach, the raw MES is processed by class-specific rotation matrices to spatially decorrelate the data prior to feature extraction in a preprocessing stage. Then, an uncorrelated linear discriminant analysis is used for dimensionality reduction. The resulting data are classified through a hidden Markov model classifier to obtain the phonemic log likelihoods of the phonemes, which are mapped to corresponding words using a word classifier. An average word classification accuracy of 98.533% is achieved over six subjects. The system offers dramatically improved accuracy when expanding a vocabulary, offering promise for robust large-vocabulary myoelectric speech recognition.
7F4B2665	Traditional acoustic speech recognition accuracies have been shown to deteriorate in highly noisy environments. A secondary information source is exploited using surface myoelectric signals (MES) collected from facial articulatory muscles during speech. Words are classified at the phoneme level using a hidden Markov model (HMM) classifier. Acoustic and MES data was collected while the words "zero" through "nine" were spoken. An acoustic expert classified the 18 formative phonemes in low noise levels [signal-to-noise ratio (SNR) of 17.5 dB] with an accuracy of 99%, but deteriorated to approximately 38% under simulations with SNR approaching 0 dB. A fused acoustic-myoelectric multiexpert system, without knowledge of SNR, improved on acoustic classification results at all noise levels. A multiexpert system, incorporating SNR information, obtained accuracies of 99% at low noise levels while maintaining accuracies above 94% during low SNR (0 dB) simulations. Results improve on previous full word MES speech recognition accuracies by almost 10%
7C2E106F	Many alternative models have been proposed to address some of the shortcomings of the hidden Markov model (HMM), which is currently the most popular approach to speech recognition. In particular, a variety of models that could be broadly classified as segment models have been described for representing a variable-length sequence of observation vectors in speech recognition applications. Since there are many aspects in common between these approaches, including the general recognition and training problems, it is useful to consider them in a unified framework. The paper describes a general stochastic model that encompasses most of the models proposed in the literature, pointing out similarities of the models in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs. In addition, we summarize experimental results assessing different modeling assumptions and point out remaining open questions.
12F4604F	This paper describes a novel speaker-independent speech recognition method, called speaker-consistent parsing", which is based on an intra-speaker correlation called the speaker-consistency principle. We focus on the fact that a sentence or a string of words is uttered by an individual speaker even in a speaker-independent task. Thus, the proposed method searches through speaker variations in addition to the contents of utterances. As a result of the recognition process, an appropriate standard speaker is selected for speaker adaptation. This new method is experimentally compared with a conventional speaker-independent speech recognition method. Since the speaker-consistency principle best demonstrates its effect with a large number of training and test speakers, a small-scale experiment may not fully exploit this principle. Nevertheless, even the results of our small-scale experiment show that the new method significantly outperforms the conventional method. In addition, this framework's speaker selection mechanism can drastically reduce the likelihood map computation.
7EF2B2E3	One of the major drawbacks to using speech as the input to any pervasive environment is the requirement to balance accuracy with the high processing overheads involved. This paper presents an Arabic speech recognition system (called UbiqRec), which address this issue by providing a natural and intuitive way of communicating within ubiquitous environments, while balancing processing time, memory and recognition accuracy. A hybrid approach has been used which incorporates spectrographic information, singular value decomposition, concurrent self-organizing maps (CSOM) and pitch contours for Arabic phoneme recognition. The approach employs separate self-organizing maps (SOM) for each Arabic phoneme joined in parallel to form a CSOM. The performance results confirm that with suitable preprocessing of data, including extraction of distinct power spectral densities (PSD) and singular value decomposition, the training time for CSOM was reduced by 89%. The empirical results also proved that overall recognition accuracy did not fall below 91%.
7DB61D52	Lipreading is an efficient method among those proposed to improve the performance of speech recognition systems, especially in acoustic noisy environments. This paper proposes a simple audio-visual speech recognition (AVSR) system, which could improve the robustness and accuracy of audio speech recognition by integrating the synchronous audio and visual information. We propose a hidden Markov model (HMM) based on the probabilistic principal component analysis (PCA) for the visual-only speech recognition and the visual modality of the audio-visual speech recognition. The probabilistic PCA based HMM directly uses the images which only contain the speaker's mouth region without pre-processing (mouth corner detection, contour marking, etc), and takes probabilistic PCA as the observation probability density function (PDF). Then we integrate these two modalities information (audio and visual) together and obtain a multi-stream hidden Markov model (MSHMM). We found that, without extracting the specialized features before processing, probabilistic PCA could capture the principal components during the training and describe the visual part of the materials. It is also verified by the experiments that the integration of the audio and visual information could help to improve the recognition accuracy even at a low acoustic signal-to-noisy ratio (SNR).
8029ACB7	An improved BP neural network classifier integration method was mainly described, by which using k-means clustering a group of value of weights and thresholds with some differences were gotten, and then as the value of individuals of integrated network to improve the performance of integrated learning, and be successfully applied to non-specific human isolated word speech recognition system. By comparing the experimental result and the traditional Adaboost integration algorithm, the validity of the method was confirmed.
79B47D09	A pre-processing of linear predictive coefficient (LPC) features for preparation of reliable reference templates for the set of words to be recognized using the artificial neural network is presented in this paper. The paper also proposes the use of pitch feature derived from the recorded speech data as another input feature. The Dynamic Time Warping algorithm (DTW) is the back-bone of the newly developed algorithm called DTW fixing frame algorithm (DTW-FF) which is designed to perform template matching for the input preprocessing. The purpose of the new algorithm is to align the input frames in the test set to the template frames in the reference set. This frame normalization is required since NN is designed to compare data of the same length, however same speech varies in their length most of the time. By doing frame fixing, the input frames and the reference frames are adjusted to the same number of frames according to the reference frames. Another task of the study is to extract pitch features using the Harmonic Filter algorithm. After pitch extraction and linear predictive coefficient (LPC) features fixed to a desired number of frames, speech recognition using neural network can be performed and results showed a very promising solution. Result showed that as high as 98% recognition can be achieved using combination of two features mentioned above. At the end of the paper, a convergence comparison between conjugate gradient descent (CGD), Quasi-Newton, and steepest gradient descent (SGD) search direction is performed and results show that the CGD outperformed the Newton and SGD.
82F9BBC3	Voice activity detection (VAD) is an important topic in audio signal processing. Contextual information is important for improving the performance of VAD at low signal-to-noise ratios. Here we explore contextual information by machine learning methods at three levels. At the top level, we employ an ensemble learning framework, named multi-resolution stacking (MRS), which is a stack of ensemble classifiers. Each classifier in a building block inputs the concatenation of the predictions of its lower building blocks and the expansion of the raw acoustic feature by a given window (called a resolution). At the middle level, we describe a base classifier in MRS, named boosted deep neural network (bDNN). bDNN first generates multiple base predictions from different contexts of a single frame by only one DNN and then aggregates the base predictions for a better prediction of the frame, and it is different from computationally-expensive boosting methods that train ensembles of classifiers for multiple base predictions. At the bottom level, we employ the multi-resolution cochleagram feature, which incorporates the contextual information by concatenating the cochleagram features at multiple spectrotemporal resolutions. Experimental results show that the MRS-based VAD outperforms other VADs by a considerable margin. Moreover, when trained on a large amount of noise types and a wide range of signal-to-noise ratios, the MRS-based VAD demonstrates surprisingly good generalization performance on unseen test scenarios, approaching the performance with noise-dependent training.
7FD6EED9	We apply the recently proposed Context-Dependent Deep- Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-text transcription. For single-pass speaker-independent recognition on the RT03S Fisher portion of phone-call transcription benchmark (Switchboard), the word-error rate is reduced from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs, to 18.5%?aa 33% relative improvement. CD-DNN-HMMs combine classic artificial-neural-network HMMs with traditional tied-state triphones and deep-beliefnetwork pre-training. They had previously been shown to reduce errors by 16% relatively when trained on tens of hours of data using hundreds of tied states. This paper takes CD-DNNHMMs further and applies them to transcription using over 300 hours of training data, over 9000 tied states, and up to 9 hidden layers, and demonstrates how sparseness can be exploited. On four less well-matched transcription tasks, we observe relative error reductions of 22¨C28%.
80C3AB1B	We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third-from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%-using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.
5E0BE9BB	We study the problem of how to recognize a person only by his eyebrow based on hidden Markov models (HMM). By experiments on a small-scale eyebrow image database taken from 27 subjects, we show that our HMM-based eyebrow recognition method can achieve the highest accuracy of 92.6%, based on the relation of its accuracy to the number of observation symbols and that of states. Hence, we conclude that human eyebrow can work as a biometric with certain possibility and feasibility.
71BFFE83	This paper presents a way of improving the recognition rate of a typical Hidden Markov Model (HMM)-based Automatic Speech Recognition (ASR) system by integrating the l1 - least absolute deviation (LAD) algorithm and the l0 - least square (LS) algorithm in a framework designed to selectively use them based on the level of impulse noise present in speech signal. We present the overall architecture of the model, as well as experimental results and compare our enhanced noise-robust HMM-based ASR system with state-of-the-art proving the improvements brought by this approach as well as future directions of research.
7D6812CD	JANUS-II is a research system for investigating various issues in speech-to-speech translations and has been implemented for translations in many languages. In this paper, we address the Spanish speech recognition part of JANUS-II. First, we report the bootstrapping and optimization of the recognition system. Then we investigate the difference between push-to-talk and cross-talk dialogs, which are two different kinds of data in our database. We give a detailed noise analysis for the push-to-talk and cross-talk dialogs and present some recognition results for comparison. We have observed that the cross-talk dialogs are harder than the push-to-talk dialogs for speech recognition, because they are more noisy than the latter. Currently, the error rate of our Spanish recognizer is 27% for the push-to-talk test set and 32% for the cross-talk test set.
0B91B818	Czech (like other Slavic languages) is well known for its complex morphology. Text processing (e.g., automatic translation, syntactic analysis...) usually requires unambiguous selection of grammatical categories (so called morphological tag) for every word in a text. Morphological tagging consists of two parts - assigning all possible tags to every word in a text and selecting the right tag in a given context. Project Morče attempts to solve the second part, usually called disambiguation. Using a statistical method based on the combination of a Hidden Markov Model and the AveragedAveraged Perceptron algorithm, a number of experiments have been made exploring different parameter settings of the algorithm in order to obtain the best success rate possible. Final accuracy of Morče on data from PDT 2.0 was 95.431% (results of March 2006). So far, it is the best result for a standalone tagger.
7D030924	Speech recognition is a key element of diverse applications in communication systems, medical transcription systems, security systems etc. However, there has been very little research in the domain of speech processing for African languages, thus, the need to extend the frontier of research in order to port in, the diverse applications based on speech recognition. Hausa language is an important indigenous lingua franca in west and central Africa, spoken as a first or second language by about fifty million people. Speech recognition of Hausa Language is presented in this paper. A pattern recognition neural network was used for developing the system.
7940F51A	Pitch determination is a fundamental problem in speech processing, which has been studied for decades. However, it is challenging to determinate pitch in strong noise because the harmonic structure is corrupted. In this paper, we estimate pitch using supervised learning, where the probabilistic pitch states are directly learned from noisy speech data. We investigate two alternative neural networks modeling pitch state distribution given observations. The first one is a feedforward deep neural network (DNN), which is trained on static frame-level acoustic features. The second one is a recurrent deep neural network (RNN) which is trained on sequential frame-level features and capable of learning temporal dynamics. Both DNNs and RNNs produce accurate probabilistic outputs of pitch states, which are then connected into pitch contours by Viterbi decoding. Our systematic evaluation shows that the proposed pitch tracking algorithms are robust to different noise conditions and can even be applied to reverberant speech. The proposed approach also significantly outperforms other state-of-the-art pitch tracking algorithms.
80BD42AB	Artificial intelligence — one of the most interesting theoretical and applied areas of computer science. There is a wide range of techniques and approaches for creation of artificial intelligence in the applications and real-time control systems. In this paper, we propose a realization of neuro-automata controlling based on neural networks and automaton paradigm and considering an example of how this paradigm could be used in real application.
8179B46E	A robust speech recognition system for videoconference applications is presented based on a microphone array. By means of a microphone array, the speech recognition system is able to know the position of the users and increase the signal-to-noise ratio (SNR) between the desired speaker signal and the interference from the other users. The user positions are estimated by means of the combination of a direction of arrival (DOA) estimation method with a speaker identification system. The beamforming is performed by using the spatial references of the desired speaker and the interference locations. A minimum variance algorithm with spatial constraints working in the frequency domain is used to design the weights of the broadband microphone array. Results of the speech recognition system are reported in a simulated environment with several users asking questions to a geographic data base.
0D759238	This paper considers the robust tracking control problem for output stochastic distributions of dynamic non-Gaussian systems. By using the square root B-spline approximations with modelling errors, a robust constrained tracking control strategy with proportional-integral (PI) structure is investigated for a nonlinear weighting system in the presence of exogenous disturbances. The main objective is to make the output probability density functions (PDFs) to follow a target PDF. An LMI-based PI control algorithm is proposed to track the desired weight dynamics, where the robust peak-to-peak measure is applied to optimize the tracking performance and the state constraints system related to the B-spline expansion can be guaranteed. Rigorous stability and performance analysis is provided for the constrained weight tracking control problem
72C1B620	As for the problem of too long training time of convolution neural network (CNN), this paper proposes a fast training method for CNN in SAR automatic target recognition (ATR). The CNN is divided into two parts: one that contains all the convolution layers and sub-sampling layers is considered as convolutional auto-encoder (CAE) for unsupervised training to extract high-level features; the other that contains fully connected layers is regarded as shallow neural network (SNN) to work as a classifier. The experiment based on MSATR database shows that the proposed method can tremendously reduce the training time with little loss of recognition rate.
0889039D	This paper introduces a method for regularization of HMM systems that avoids parameter overfitting caused by insufficient training data.  Regularization is done by augmenting the EM trainicng method by a penalty term that favors simple and smooth HMM systems.  The penalty term is constructed as a mixture model ofnegative exponential distributions that is assumed to generate the state dependent emission probabilities of the HMMs.  This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HcMM systems. The effect of regularization is demonstrated for contcinuous speech recognition tasks by improving overfitted triphone modelsand by speaker adaptation with limited training data.
659F446A	The paper presents a new variant of parameter estimation methods for discrete hidden Markov models (HMM) in speech recognition. This method makes use of a codeword dependent distribution normalization (CDDN) and a distance weighting by fuzzy contribution in dealing with the problems of robust state modeling in FVQ based modeling. The proposed method is compared with the existing techniques using speaker-independent phonetically balanced isolated word recognition. The results have shown that the recognition rate of the proposed method is improved 4.5% over the conventional NQ based method and the distance weighting to the smoothing of output probability is more efficient than the distance based codeword weighting.
7DEC73B6	This paper presents a statistical approach to developing multimodal recognition systems and, in particular, to integrating the posterior probabilities of parallel input signals involved in the multimodal system. We first derive the performance bounds of multimodal recognition probabilities, and identify the primary factors that influence multimodal recognition performance. We then develop a technique, a members-teams-committee (MTC) recognition approach, designed to optimize accurate recognition during the multimodal integration process. We evaluate these methods using Quickset, a speech/gesture multimodal system, and report evaluation results based on an empirical corpus collected with Quickset. From an architectural perspective, the integration technique presented offers enhanced robustness. It also is premised on more realistic assumptions than previous multimodal systems using semantic fusion. From a methodological standpoint, the evaluation techniques that we describe provide a valuable tool for evaluating multimodal systems.
7EDFCD19	The paper mainly discusses the speech keyword recognition system dealing with the audio streaming media. With the help of the Microsoft Windows Media Format SDK (WMFSDK), a powerful front-end interface module is designed to extract audio stream from different streaming media and convert it to the audio format supported by the speech-recognizer. In order to rapidly spot keywords and reject out-of-vocabulary (OOV) words, the keyword-spotting strategy is put forward based on on-line garbage models. Studies show that this strategy works well in utterance verification. On the utterance verification stage, mixed with multi-confident measures, three classifiers are designed and compared, with the test results proved by different classifiers, the support vector machine (SVM) method is proved superior in performance to the Fisher and neural network (NN) method
5E976DF6	 We have developed two different methods for using auditory, telephone speech to drive the movements of a synthetic face. In the first method, Hidden Markov Models (HMMs) were trained on a phonetically transcribed telephone speech database. The output of the HMMs was then fed into a rule-based visual speech synthesizer as a string of phonemes together with time labels. In the second method, Artificial Neural Networks (ANNs) were trained on the same database to map acoustic parameters directly to facial control parameters. These target parameter trajectories were generated by using phoneme strings from a database as input to the visual speech synthesis The two methods were evaluated through audio-visual intelligibility tests with ten hearing impaired persons, and compared to "ideal" articulations (where no recognition was involved), a natural face, and to the intelligibility of the audio alone. It was found that the HMM method performs considerably better than the audio alone condition (54% and 34% keywords correct respectively), but not as well as the "ideal" articulating artificial face (64%). The intelligibility for the ANN method was 34% keywords correct.
0A18E4E8	This paper describes a text independent continuous speech recognition system, which is built up from many steps. The method of the process is different in all steps, adapted to the particular problems of that level. The whole system is a combination of the application of neural nets and rule based processes, using segment based approach. The segmentation technics is language independent, and all the other processes can be adapted easily to other languages. The recognition works sentence by sentence. At the input the sound pressure is measured, and at the output the half-syllable series (first and second candidates) of the sentences are given. The recognition time of the sentences is about the pronunciation time of them, running the program on a PC 486.
5BB77197	In automatic speech recognition (ASR) systems, hidden Markov models (HMMs) have been widely used for modeling the temporal speech signal. As discussed in Part I, the conventional acoustic models used for ASR have many drawbacks like weak duration modeling and poor discrimination. This paper (Part II) presents a review on the techniques which have been proposed in literature for the refinements of standard HMM methods to cope with their limitations. Current advancements related to this topic are also outlined. The approaches emphasized in this part of review are connectionist approach, explicit duration modeling, discriminative training and margin based estimation methods. Further, various challenges and performance issues such as environmental variability, tied mixture modeling, and handling of distant speech signals are analyzed along with the directions for future research.
7B96DDC7	We study the problem of how to recognize a person only by his eyebrow based on hidden Markov models (HMM). By experiments on a small-scale eyebrow image database taken from 27 subjects, we show that our HMM-based eyebrow recognition method can achieve the highest accuracy of 92.6%, based on the relation of its accuracy to the number of observation symbols and that of states. Hence, we conclude that human eyebrow can work as a biometric with certain possibility and feasibility.
5B5286A0	Binary weights are favored in electronic and optical hardware implementations of neural networks as they lead to improved system speeds. Optical neural networks based on fast ferroelectric liquid crystal binary level devices can benefit from the many orders of magnitudes improved liquid crystal response times. An optimized learning algorithm for all-positive perceptrons is simulated on a limited data set of handwritten digits and the resultant network implemented optically. First, gray-scale and then binary inputs and weights are used in recall mode. On comparing the results for the example data set, the binarized inputs and weights network shows almost no loss in performance.
7DC4D8A8	In this contribution we introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared throughout the paper. Within the first method a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines and results achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English language is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86% recognition rate. As a basis of comparison the similar judgment of human deciders classifying the same corpus at 79.8% recognition rate was analyzed.
7D1C079F	When adaptive arrays are applied to practical problems, the performances of the existing adaptive algorithms are known to degrade substantially in the presence of even slight mismatches between the actual and presumed array responses to the desired signal. Similar types of performance degradation can occur when the signal array response is known precisely but the training sample size is small. In this paper, we propose a novel neural network approach to robust adaptive beamforming. The proposed algorithm is based on explicit modeling of uncertainties in the desired signal array response and a three-layer radial basis function neural network (RBFNN). In the proposed algorithm, the computation of the optimum weight vector is viewed as a mapping problem, which can be modeled using a RBFNN trained with input/output pairs. Our proposed approach offers fast convergence rate, provides excellent robustness against some types of mismatches and makes the mean output array SINR consistently close to the optimal one. Computer simulation results are presented, which show that the proposed algorithm yields significantly better performance as compared with the existing adaptive beamforming algorithms.
5B7FD1FC	Hidden Markov Model (HMM), which is widely used in acoustic modeling, has powerful dynamic time-series modeling capability; Support Vector Machine (SVM) still has strong classification ability when the training samples are limited. This paper proposes an improved speech recognition algorithm based on a hybrid SVM/HMM architecture. We use the algorithm to extract the speech features and apply the features to the Speech Recognition (SR) interface of Microsoft Speech SDK (SAPI) to improve the interface data type. The experimental results show that the recognition rate increases greatly.
623973B2	Speech production knowledge has been used to enhance the phonetic representation and the performance of automatic speech recognition (ASR) systems successfully. Representations of speech production make simple explanations for many phenomena observed in speech. These phenomena can not be easily analyzed from either acoustic signal or phonetic transcription alone. One of the most important aspects of speech production knowledge is the use of articulatory knowledge, which describes the smooth and continuous movements in the vocal tract. In this paper, we present a new articulatory model to provide available information for rescoring the speech recognition lattice hypothesis. The articulatory model consists of a feature front-end, which computes a voicing feature based on a spectral harmonics correlation (SHC) function, and a back-end based on the combination of deep neural networks (DNNs) and hidden Markov models (HMMs). The voicing features are incorporated with standard Mel frequency cepstral coefficients (MFCCs) using heteroscedastic linear discriminant analysis (HLDA) to compensate the speech recognition accuracy rates. Moreover, the advantages of two different models are taken into account by the algorithm, which retains deep learning properties of DNNs, while modeling the articulatory context powerfully through HMMs. Mandarin speech recognition experiments show the proposed method achieves significant improvements in speech recognition performance over the system using MFCCs alone.
5950EF37	The Rosenblatt perceptron was used for handwritten digit recognition. For testing its performance the MNIST database was used. 60,000 samples of handwritten digits were used for perceptron training, and 10,000 samples for testing. A recognition rate of 99.2% was obtained. The critical parameter of Rosenblatt perceptrons is the number of neurons N in the associative neuron layer. We changed the parameter N from 1,000 to 512,000. We investigated the influence of this parameter on the performance of the Rosenblatt perceptron. Increasing N from 1,000 to 512,000 involves decreasing of test errors from 5 to 8 times. It was shown that a large scale Rosenblatt perceptron is comparable with the best classifiers checked on MNIST database (98.9%-99.3%).
7CEF18CC	Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition. This paper first provides a brief overview of graphical models and their uses as statistical models. It is then shown that the statistical assumptions behind many pattern recognition techniques commonly used as part of a speech recognition system can be described by a graph — this includes Gaussian distributions, mixture models, decision trees, factor analysis, principle component analysis, linear discriminant analysis, and hidden Markov models. Moreover, this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph, including many at the acoustic-, pronunciation-, and language-modeling levels. A number of speech recognition techniques born directly out of the graphical-models paradigm are also surveyed. Additionally, this paper includes a novel graphical analysis regarding why derivative (or delta) features improve hidden Markov model-based speech recognition by improving structural discriminability. It also includes an example where a graph can be used to represent language model smoothing constraints. As will be seen, the space of models describable by a graph is quite large. A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model.
7D373DBB	In recent years there has been a significant body of work, both theoretical and experimental, that has established the viability of artificial neural networks (ANN's) as a useful technology for speech recognition. It has been shown that neural networks can be used to augment speech recognizers whose underlying structure is essentially that of hidden Markov models (HMM's). In particular, we have demonstrated that fairly simple layered structures, which we lately have termed big dumb neural networks (BDNN's), can be discriminatively trained to estimate emission probabilities for an HMM. Recently simple speech recognition systems (using context-independent phone models) based on this approach have been proved on controlled tests, to be both effective in terms of accuracy (i.e., comparable or better than equivalent state-of-the-art systems) and efficient in terms of CPU and memory run-time requirements. Research is continuing on extending these results to somewhat more complex systems. In this paper, we first give a brief overview of automatic speech recognition (ASR) and statistical pattern recognition in general. We also include a very brief review of HMM's, and then describe the use of ANN's as statistical estimators. We then review the basic principles of our hybrid HMM/ANN approach and describe some experiments. We discuss some current research topics, including new theoretical developments in training ANN's to maximize the posterior probabilities of the correct models for speech utterances. We also discuss some issues of system resources required for training and recognition. Finally, we conclude with some perspectives about fundamental limitations in the current technology and some speculations about where we can go from here.
80B76035	An overview of a statistical paradigm for speech recognition is given where phonetic and phonological knowledge sources, drawn from the current understanding of the global characteristics of human speech communication, are seamlessly integrated into the structure of a stochastic model of speech. A consistent statistical formalism is presented in which the submodels for the discrete, feature-based phonological process and the continuous, dynamic phonetic process in human speech production are computationally interfaced. This interface enables global optimization of a parsimonious set of model parameters that accurately characterize the symbolic, dynamic, and static components in speech production and explicitly separates distinct sources of the speech variability observable at the acoustic level. The formalism is founded on a rigorous mathematical basis, encompassing computational phonology, Bayesian analysis and statistical estimation theory, nonstationary time series and dynamic system theory, and nonlinear function approximation (neural network) theory. Two principal ways of implementing the speech model and recognizer are presented, one based on the trended hidden Markov model (HMM) or explicitly defined trajectory model, and the other on the state-space or recursively defined trajectory model. Both implementations build into their respective recognition and model-training algorithms a continuity constraint on the internal, production-affiliated trajectories across feature-defined phonological units. The continuity and the parameterized structure in the dynamic speech model permit a joint characterization of the contextual and speaking-style variations manifested in speech acoustics, thereby holding promises to overcome some key limitations of the current speech recognition technology
7ECCA8AC	A topic dependent class (TDC) language model (LM) is a topic-based LM that uses a semantic extraction method to reveal latent topic information from noun-document relation. Then a clustering for a given context is performed to define topics. Finally, a fixed window of word history is observed to decide the topic of the current event through voting in online manner. Previously, we have shown that TDC overperforms several state-of-the-art baselines in terms of perplexity. In this paper we evaluate TDC on automatic speech recognition experiment (ASR) for rescoring task. Experiments on read speech Wall Street Journal (English ASR system) and Mainichi Shimbun (Japanese ASR system) show that TDC LM improves both perplexity and word-error-rate (WER). The result shows that the proposed model gives improvements 3.0% relative on perplexity and 15.2% relative on WER for English ASR system, and 16.4% relative on perplexity and 24.3% relative on WER for Japanese ASR system.
80039747	This paper presents a new algorithm for detecting and tracking multiple moving objects in both outdoor and indoor environments. The proposed method measures the change of a combined color-texture feature vector in each image block to detect moving objects. The texture feature is extracted from DCT frequency domain. An attributed relational graph (ARG) is used to represent each object, in which vertices are associated to an object’s sub-regions and edges represent spatial relations among them. Multiple cues including color, texture, and spatial position are integrated to describe each object’s sub-regions. Object tracking and identification are accomplished by inexact graph matching, which enables us to track partially occluded objects and to cope with object articulation. An ARG adaptation scheme is incorporated into the system to handle the changes in object scale and appearance. The experimental results prove the efficiency of the proposed method.
7EF0AE42	Real-time human face detection and recognition from video sequences in surveillance applications is a challenging task due to the variances in background, facial expression and illumination. The face detection approach is based on modest AdaBoost  algorithm and can achieve fast, accurate face detection that is robust to changes in illumination and background. The detection stage provides good results maintaining a low computational cost. The recognition stage is based on an improved independent components Analysis approach which has been modified to cope with the video surveillance application. In the recognition stage, the Hausdorff distance is used as a similarity measure between a general face model and possible instances of the object within the image. After the integration of the two stages, several improvements are proposed which increase the face detection and recognition rate and the overall performance of the system. The experimental results demonstrate the significant performance improvement using the proposed approach over others. It can be seen that the proposed method is very efficient and has significant value in application.
7F4FBA42	We present a method for the detection of instances of an object class, such as cars or pedestrians, in natural images. Similarly to some previous works, this is accomplished via generalized Hough transform, where the detections of individual object parts cast probabilistic votes for possible locations of the centroid of the whole object; the detection hypotheses then correspond to the maxima of the Hough image that accumulates the votes from all parts. However, whereas the previous methods detect object parts using generative codebooks of part appearances, we take a more discriminative approach to object part detection. Towards this end, we train a class-specific Hough forest, which is a random forest that directly maps the image patch appearance to the probabilistic vote about the possible location of the object centroid. We demonstrate that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets.
81790792	Model-based 3D tracker estimate the position, rotation, and joint angles of a given model from video data of one or multiple cameras. They often rely on image features that are tracked over time but the accumulation of small errors results in a drift away from the target object. In this work, we address the drift problem for the challenging task of human motion capture and tracking in the presence of multiple moving objects where the error accumulation becomes even more problematic due to occlusions. To this end, we propose an analysis-by-synthesis framework for articulated models. It combines the complementary concepts of patch-based and region-based matching to track both structured and homogeneous body parts. The performance of our method is demonstrated for rigid bodies, body parts, and full human bodies where the sequences contain fast movements, self-occlusions, multiple moving objects, and clutter. We also provide a quantitative error analysis and comparison with other model-based approaches.
7FDC9A31	One of the major challenges that visual tracking algorithms face nowadays is being able to cope with changes in the appearance of the target during tracking. Linear sub-space models have been extensively studied recently and are possibly the most popular way of modeling target appearance. Unfortunately, efficiency is one of the limitations of present linear subspace models, and this is a key feature for a good tracker. In this paper we present an efficient procedure for tracking based on a linear subspace model of target appearance (grey levels). A set of motion templates is built from the subspace base, which is used to efficiently compute target motion and appearance parameters. It differs from previous works in that we impose no restrictions on the subspace used for modeling appearance. In the experiments conducted we have built a modular PCA-based face tracker which shows that video-rate tracking performance can be achieved with a non optimized implementation of our algorithm.
7E7285EB	In background subtraction, cast shadows induce silhouette distortions and object fusions hindering performance of high level algorithms in scene monitoring. We introduce a nonparametric framework to model surface behavior when shadows are cast on them. Based on physical properties of light sources and surfaces, we identify a direction in RGB space on which background surface values under cast shadows are found. We then model the posterior distribution of lighting attenuation under cast shadows and foreground objects, which allows differentiation of foreground and cast shadow values with similar chromaticity. The algorithms are completely unsupervised and take advantage of scene activity to learn model parameters. Spatial gradient information is also used to reinforce the learning process. Contributions are two-fold. Firstly, with a better model describing cast shadows on surfaces, we achieve a higher success rate in segmenting moving cast shadows in complex scenes. Secondly, obtaining such models is a step toward a full scene parametrization where light source properties, surface reflectance models and scene 3D geometry are estimated for low-level segmentation.
7FAD2C94	While low-dimensional image representations have been very popular in computer vision, they suffer from two limitations: (i) they require collecting a large and varied training set to learn a low-dimensional set of basis functions, and (ii) they do not retain information about the 3D geometry of the object being imaged. In this paper, we show that it is possible to estimate low-dimensional manifolds that describe object appearance while retaining the geometrical information about the 3D structure of the object. By using a combination of analytically derived geometrical models and statistical learning methods, this can be achieved using a much smaller training set than most of the existing approaches. Specifically, we derive a quadrilinearmanifold of object appearance that can represent the effects of illumination, pose, identity and deformation, and the basis functions of the tangent space to this manifold depend on the 3D surface normals of the objects. We show experimental results on constructing this manifold and how to efficiently track on it using an inverse compositional algorithm.
8075F3AF	Moving cast shadows are a major concern for foreground detection algorithms. Processing of foreground images in surveillance applications typically requires that such shadows have been identified and removed from the detected foreground. This paper presents a novel pixel-based statistical approach to model moving cast shadows of non-uniform and varying intensity. This approach uses the Gaussian mixture model (GMM) learning ability to build statistical models describing moving cast shadows on surfaces. This statistical modeling can deal with scenes with complex and time-varying illumination, and prevent false detection in regions where shadows cannot be detected. Gaussian mixture shadow models (GMSM) are automatically constructed and updated over time, are easily added to GMM architecture for foreground detection, and require only a small number of parameters. Results obtained with different scene types show the robustness of the approach.
7E3F3BFE	Cast shadows induced by moving objects often cause serious problems to many vision applications. We present in this paper an online statistical learning approach to model the background appearance variations under cast shadows. Based on the bi-illuminant (i.e. direct light sources and ambient illumination) dichromatic reflection model, we derive physics-based color features under the assumptions of constant ambient illumination and light sources with common spectral power distributions. We first use one Gaussian mixture model (GMM) to learn the color features, which are constant regardless of the background surfaces or illuminant colors in a scene. Then, we build up one pixel based GMM for each pixel to learn the local shadow features. To overcome the slow convergence rate in the conventional GMM learning, we update the pixel-based GMMs through confidence-rated learning. The proposed method can rapidly learn model parameters in an unsupervised way and adapt to illumination conditions or environment changes. Furthermore, we demonstrate that our method is robust to scenes with few foreground activities and videos captured at low or unsteady frame rates.
80DC8ED6	Illumination models of the image set of an object (e.g., human face) under varying lighting conditions have been either empirically or analytically explored. However, the theoretical dimensionality of video bricks of an object under varying illumination is still unknown. In this paper, we focus on this question concretely and give both analytical and empirical results. We derive the theoretical upper bound of the dimensionality of video bricks by investigating the analytical formula of appearance changes due to motion variables of light sources. Theoretical results show in real-world scenes video bricks of an object under varying illumination could be expressed well by a low-dimensional linear subspace. Empirical results of the principal component analysis on the YaleB Face database and our video database are consistent with the theoretical results completely. The application of the low-dimensional linear models of video bricks is demonstrated by the foreground detection task in visual surveillance with drastic illumination changes.
7D5CF113	Historically, SSD or correlation-based visual tracking algorithms have been sensitive to changes in illumination and shading across the target region. This paper describes methods for implementing SSD tracking that is both insensitive to illumination variations and computationally efficient. We first describe a vector-space formulation of the tracking problem, showing how to recover geometric deformations. We then show that the same vector space formulation can be used to account for changes in illumination. We combine geometry and illumination into an algorithm that tracks large image regions on live video sequences using no more computation than would be required to trade with no accommodation for illumination changes. We present experimental results which compare the performance of SSD tracking with and without illumination compensation.
80AE788F	In this paper, we formulate object tracking in a particle filter framework as a multi-task sparse learning problem, which we denote as Multi-Task Tracking (MTT). Since we model particles as linear combinations of dictionary templates that are updated dynamically, learning the representation of each particle is considered a single task in MTT. By employing popular sparsity-inducing ℓp, q mixed norms (p ∈ {2, ∞} and q = 1), we regularize the representation problem to enforce joint sparsity and learn the particle representations together. As compared to previous methods that handle particles independently, our results demonstrate that mining the interdependencies between particles improves tracking performance and overall computational complexity. Interestingly, we show that the popular L1 tracker [15] is a special case of our MTT formulation (denoted as the L11 tracker) when p = q = 1. The learning problem can be efficiently solved using an Accelerated Proximal Gradient (APG) method that yields a sequence of closed form updates. As such, MTT is computationally attractive. We test our proposed approach on challenging sequences involving heavy occlusion, drastic illumination changes, and large pose variations. Experimental results show that MTT methods consistently outperform state-of-the-art trackers.
7D19FB73	We propose a novel tracking algorithm that can work robustly in a challenging scenario such that several kinds of appearance and motion changes of an object occur at the same time. Our algorithm is based on a visual tracking decomposition scheme for the efficient design of observation and motion models as well as trackers. In our scheme, the observation model is decomposed into multiple basic observation models that are constructed by sparse principal component analysis (SPCA) of a set of feature templates. Each basic observation model covers a specific appearance of the object. The motion model is also represented by the combination of multiple basic motion models, each of which covers a different type of motion. Then the multiple basic trackers are designed by associating the basic observation models and the basic motion models, so that each specific tracker takes charge of a certain change in the object. All basic trackers are then integrated into one compound tracker through an interactive Markov Chain Monte Carlo (IMCMC) framework in which the basic trackers communicate with one another interactively while run in parallel. By exchanging information with others, each tracker further improves its performance, which results in increasing the whole performance of tracking. Experimental results show that our method tracks the object accurately and reliably in realistic videos where the appearance and motion are drastically changing over time.
7DA8D19C	This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed, thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in real time even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outperforms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.
816C1AA9	In this paper, we present a novel re-texturing approach using intrinsic video. Our approach begins with indicating the regions of interests by contour-aware layer segmentation. Then, the intrinsic video (including reflectance and illumination components) within the segmented region is recovered by our weighted energy optimization. After that, we compute the normals for the re-textured region, and the texture coordinates in key frames through our newly developed optimization approach. At the same time, the texture coordinates in non-key frames are optimized by our proposed energy function. Finally, when the target sample texture is specified, the re-textured video is created by multiplying the re-textured reflectance component by the original illumination component within the replaced region. As demonstrated in our experimental results, our method can produce high quality video re-texturing results with preserving the lighting and shading effect of the original video.
7DB09023	Many methods have been introduced to detect shot boundaries, e.g. pixel-by-pixel comparisons and histogram comparisons. But they ignore the problem of illumination variation inherent in the video production process especially in TV news reports. So they often can be confused when the incident illumination varies. The Color Ratio Histogram is proposed as frame content measure to solve the problem. However, it is computationally expensive. Different methods have both advantages and disadvantages, in this paper, the merits of different methods are associated to acquire a better performance. A good compromise between speed and accuracy is achieved. The detection is divided into two steps in which different metrics are used to identify different transitions. The effectiveness of our methods has been validated by experiments on some real-world video sequences.
75A72BF2	The objective of this work was to evaluate the effectiveness of commercial video image fire detection systems for small, cluttered spaces as would be found on Navy ships. The primary goal was to establish an understanding of the performance sensitivity and limitations of the video image detection (VID) systems to various setup and environmental conditions that may occur onboard ship while exposed to a range of flaming and smoldering fire sources and potential nuisance alarm sources. The response of the VID systems was benchmarked against standard fire alarm systems using addressable ionization and photoelectric smoke detectors.
8151AE60	In consumer video conferencing, lighting conditions are usually not ideal thus the image qualities are poor. Lighting affects image quality on two aspects: brightness and skin tone. While there has been much research on improving the brightness of the captured images including contrast enhancement and noise removal (which can be thought of as components for brightness improvement), little attention has been paid to the skin tone aspect. In contrast, it is a common knowledge for professional stage lighting designers that lighting affects not only the brightness but also the color tone which plays a critical role in the perceived look of the host and the mood of the stage scene. Inspired by stage lighting design, we propose an active lighting system which automatically adjusts the lighting so that the image looks visually appealing. The system consists of computer controllable light emitting diode light sources of different colors so that it improves not only the brightness but also the skin tone of the face. Given that there is no quantitative formula on what makes a good skin tone, we use a data driven approach to learn a good skin tone model from a collection of photographs taken by professional photographers. We have developed a working system and conducted user studies to validate our approach.
793127B8	Recently, most background modeling approaches represent distributions of background changes by using parametric models such as Gaussian mixture models. Because of significant illumination changes and dynamic moving backgrounds with time, variations of background changes are hard to be modeled by parametric background models. Moreover, how to efficiently and effectively update parameters of parametric models to reflect background changes remains a problem. In this paper, we propose a novel coarse-to-fine detection theory algorithm to extract foreground objects on the basis of nonparametric background and foreground models represented by binary descriptors. We update background and foreground models by a first-in-first-out strategy to maintain the most recent observed background and foreground instances. As shown in the experiments, our method can achieve better foreground extraction results and fewer false alarms of surveillance videos with lighting changes and dynamic backgrounds in both collected and CDnet 2012 benchmark data sets.
788A8E2C	We present a novel algorithm that exploits joint optimization of representation and classification for robust tracking in which the goal is to minimize the least-squares reconstruction errors and discriminative penalties with regularized constraints. In this formulation, an object is represented by the sparse coefficients of local patches based on an overcomplete dictionary, and a classifier is learned to discriminate the target object from the background. To locate the target object in each frame, we propose a deterministic approach to solve the optimization problem. We show that the proposed algorithm can be considered as a generalization of several tracking methods with effectiveness. To account for appearance change of the target and the background, the classifier is adaptively updated with new tracking results. Compared with the most recent tracking algorithms based on sparse representation, the proposed formulation has more discriminative power due to the use of background information and is much faster due to the use of deterministic optimization. Qualitative and quantitative experiments on a variety of challenging sequences show favorable performance of the proposed algorithm against several state-of-the-art methods.
80D8263D	Segmenting and tracking of objects in video is of great importance for video-based encoding, surveillance, and retrieval. However, the inherent difficulty of object segmentation and tracking is to distinguish changes in the displacement of objects from disturbing effects such as noise and illumination changes. Therefore, in this paper, we formulate a color-based deformable model which is robust against noisy data and changing illumination. Computational methods are presented to measure color constant gradients. Further, a model is given to estimate the amount of sensor noise through these color constant gradients. The obtained uncertainty is subsequently used as a weighting term in the deformation process. Experiments are conducted on image sequences recorded from three-dimensional scenes. From the experimental results, it is shown that the proposed color constant deformable method successfully finds object contours robust against illumination, and noisy, but homogeneous regions.
7D394E2F	This paper describes a novel framework for detection and suppression of properly shadowed regions for most possible scenarios occurring in real video sequences. Our approach requires no prior knowledge about the scene, nor is it restricted to specific scene structures. Furthermore, the technique can detect both achromatic and chromatic shadows even in the presence of camouflage that occurs when foreground regions are very similar in color to shadowed regions. The method exploits local color constancy properties due to reflectance suppression over shadowed regions. To detect shadowed regions in a scene, the values of the background image are divided by values of the current frame in the RGB color space. We show how this luminance ratio can be used to identify segments with low gradient constancy, which in turn distinguish shadows from foreground. Experimental results on a collection of publicly available datasets illustrate the superior performance of our method compared with the most sophisticated, state-of-the-art shadow detection algorithms. These results show that our approach is robust and accurate over a broad range of shadow types and challenging video conditions.
76E3B663	Foreground/background segmentation via change detection in video sequences is often used as a stepping stone in high-level analytics and applications. Despite the wide variety of methods that have been proposed for this problem, none has been able to fully address the complex nature of dynamic scenes in real surveillance tasks. In this paper, we present a universal pixel-level segmentation method that relies on spatiotemporal binary features as well as color information to detect changes. This allows camouflaged foreground objects to be detected more easily while most illumination variations are ignored. Besides, instead of using manually set, frame-wide constants to dictate model sensitivity and adaptation speed, we use pixel-level feedback loops to dynamically adjust our method's internal parameters without user intervention. These adjustments are based on the continuous monitoring of model fidelity and local segmentation noise levels. This new approach enables us to outperform all 32 previously tested state-of-the-art methods on the 2012 and 2014 versions of the ChangeDetection.net dataset in terms of overall F-Measure. The use of local binary image descriptors for pixel-level modeling also facilitates high-speed parallel implementations: our own version, which used no low-level or architecture-specific instruction, reached real-time processing speed on a midlevel desktop CPU. A complete C++ implementation based on OpenCV is available online.
7EC8840E	This paper presents a technique for motion detection that incorporates several innovative mechanisms. For example, our proposed technique stores, for each pixel, a set of values taken in the past at the same location or in the neighborhood. It then compares this set to the current pixel value in order to determine whether that pixel belongs to the background, and adapts the model by choosing randomly which values to substitute from the background model. This approach differs from those based upon the classical belief that the oldest values should be replaced first. Finally, when the pixel is found to be part of the background, its value is propagated into the background model of a neighboring pixel. We describe our method in full details (including pseudo-code and the parameter values used) and compare it to other background subtraction techniques. Efficiency figures show that our method outperforms recent and proven state-of-the-art methods in terms of both computation speed and detection rate. We also analyze the performance of a downscaled version of our algorithm to the absolute minimum of one comparison and one byte of memory per pixel. It appears that even such a simplified version of our algorithm performs better than mainstream techniques.
769D105E	This letter presents a new method for background subtraction and shadow removal for grayscale video sequences. The background image is modeled using robust statistical descriptors, and a noise estimate is obtained. Foreground pixels are extracted, and a statistical approach combined with geometrical constraints are adopted to detect and remove shadows.
7F224147	Road  scene  analysis  is  a  challenging  problem  that has  applications  in  autonomous  navigation  of  vehicles.  An  integral component of this system is the robust detection and tracking of  lane  markings.  It  is  a  hard  problem  primarily  due  to  large appearance variations in lane markings caused by factors such as occlusion (traffic on the road), shadows (from objects like trees), and  changing  lighting  conditions  of  the  scene  (transition  from day  to  night).  In  this  paper,  we  address  these  issues  through a  learning-based  approach  using  visual  inputs  from  a  camera mounted  in  front  of  a  vehicle. We  propose,  (i)  a  pixel-hierarchy feature descriptor to model contextual information shared by lane markings with the surrounding road region, (ii) a robust boosting algorithm to select relevant contextual features for detecting lane markings,  and  (iii)  particle  filters  to  track  the  lane  markings, without  the  knowledge  of  vehicle  speed,  by  assuming  the  lane markings  to  be  static  through  the  video  sequence  and  then learning  the  possible  road  scene  variations  from  the  statistics of tracked model parameters. We investigate the effectiveness of our algorithm on challenging daylight and night-time road video sequences.
801C8D0A	To prevent moving shadows being misclassified as moving objects or parts of moving objects, this paper presents an explicit method for detection of moving cast shadows on a dominating scene background. Those shadows are generated by objects moving between a light source and the background. Moving cast shadows cause a frame difference between two succeeding images of a monocular video image sequence. For shadow detection, these frame differences are detected and classified into regions covered and regions uncovered by a moving shadow. The detection and classification assume plane background and a nonnegligible size and intensity of the light sources. A cast shadow is detected by temporal integration of the covered background regions while subtracting the uncovered background regions. The shadow detection method is integrated into an algorithm for two-dimensional (2-D) shape estimation of moving objects from the informative part of the description of the international standard ISO/MPEG-4. The extended segmentation algorithm compensates first apparent camera motion. Then, a spatially adaptive relaxation scheme estimates a change detection mask for two consecutive images. An object mask is derived from the change detection mask by elimination of changes due to background uncovered by moving objects and by elimination of changes due to background covered or uncovered by moving cast shadows. Results obtained with MPEG-4 test sequences and additional sequences show that the accuracy of object segmentation is substantially improved in presence of moving cast shadows. Objects and shadows are detected and tracked separately.
7FDBF311	In this paper, we propose a graph-based approach for detecting and tracking multiple players in broadcast soccer videos. In the first stage, the position of the players in each frame is determined by removing the non player regions. The remaining pixels are then grouped using a region growing algorithm to identify probable player candidates. A directed weighted graph is constructed, where probable player candidates correspond to the nodes of the graph while each edge links candidates in a frame with the candidates in next two consecutive frames. Finally, dynamic programming is applied to find the trajectory of each player. Experiments with several sequences from broadcasted videos of international soccer matches indicate that the proposed approach is able to track the players reasonably well even under varied illumination and ground conditions.
78D0A0EB	Moving objects segmentation plays a very important role in real-time image analysis. However, as one of the common parts in the natural scenes, shadows severely interfere with the accuracy of moving objects detection in video surveillance. In this paper, we present a novel method for moving cast shadows detection. Based on the analysis of the physical model of moving shadows, we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each moving pixel into foreground object or moving shadow. Intensity constraint and geometric heuristics are imposed to further improve the performance. Experiments on various typical scenes exhibit the robustness of the proposed method. Extensively quantitative evaluation and comparison demonstrate that the proposed method significantly outperforms state-of-the-art methods.
803B72DE	Shot change detection is an essential step in video content analysis. However, automatic shot change detection often suffers from high false detection rates due to camera or object movements. To solve this problem, we propose an approach based on local keypoint matching of video frames. This approach aims to detect both abrupt and gradual transitions between shots without modeling different kinds of transitions. Our experiment results show that the proposed algorithm is effective for most kinds of shot changes.
80A1F632	In this paper, we present a theory for combining the effects of motion, illumination, 3D structure, albedo, and camera parameters in a sequence of images obtained by a perspective camera. We show that the set of all Lambertian reflectance functions of a moving object, at any position, illuminated by arbitrarily distant light sources, lies "close" to a bilinear subspace consisting of nine illumination variables and six motion variables. This result implies that, given an arbitrary video sequence, it is possible to recover the 3D structure, motion, and illumination conditions simultaneously using the bilinear subspace formulation. The derivation builds upon existing work on linear subspace representations of reflectance by generalizing it to moving objects. Lighting can change slowly or suddenly, locally or globally, and can originate from a combination of point and extended sources. We experimentally compare the results of our theory with ground truth data and also provide results on real data by using video sequences of a 3D face and the entire human body with various combinations of motion and illumination directions. We also show results of our theory in estimating 3D motion and illumination model parameters from a video sequence
7DF2F63C	This paper proposes an automatic and robust method to detect human faces from video sequences that combines feature extraction and face detection based on local normalization, Gabor wavelets transform and Adaboost algorithm. The key step and the main contribution of this work is the incorporation of a normalization technique based on local histograms with optimal adaptive correlation (OAC) technique to alleviate a common problem in conventional face detection methods: inconsistent performance due to the sensitivity to illumination variations such as local shadowing, noise and occlusion. This approach uses a cascade of classifiers to adopt a coarse-to-fine strategy to achieve higher detection rate with lower false positives. The experimental results demonstrate a significant performance improvement by local normalization over method without normalizations in real video sequences with a wide range of facial variations in color, position, scale, and varying lighting conditions.
7F4EC590	The paper investigates the interaction between tracking and recognition of human faces from video under a framework proposed earlier (Shaohua Zhou et al., Proc. 5th Int. Conf. on Face and Gesture Recog., 2002; Shaohua Zhou and Chellappa, R., Proc. European Conf. on Computer Vision, 2002), where a time series model is used to resolve the uncertainties in both tracking and recognition. However, our earlier efforts employed only a simple likelihood measurement in the form of a Laplacian density to deal with appearance changes between frames and between the observation and gallery images, yielding poor accuracies in both tracking and recognition when confronted by pose and illumination variations. The interaction between tracking and recognition was not well understood. We address the interdependence between tracking and recognition using a series of experiments and quantify the interacting nature of tracking and recognition.
816A1DCF	This paper provides a comprehensive quantitative comparison of metrics for detecting visual anomalies between two videos that are recorded along same path but at different times by a camera on a patrolling platform. The metrics used in this paper are histogram based metrics, statistic based metrics and pixel differences based metrics. We test the metrics for the detection of mobile and stationary anomalies between videos. The two videos are brought to spatial temporal alignment by a two step process. For each frame in the first video the closest matching frame from the second video is found manually and the matched pair of frames are registered using a feature based registration method. Laws texture kernels are used to extract texture energy measures from the images and nine different metrics are applied to generate a difference image sequence which is followed by thresholding to get a binary image sequence. The binary images are compared with the actual ground truth and the performance of each metric are presented for four videos taken in different environments.
7FF0C0FE	We develop an efficient algorithm to track point features supported by image patches undergoing affine deformations and changes in illumination. The algorithm is based on a combined model of geometry and photometry, that is used to track features as well as to detect outliers in a hypothesis testing framework. The algorithm runs in real time on a personal computer; and is available to the public.
7F020A02	This paper presents an algorithm for computing optical flow, shape, motion, lighting, and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination. The problem is formulated in a manner that subsumes structure from motion, multi-view stereo, and photometric stereo as special cases. The algorithm utilizes both spatial and temporal intensity variation as cues: the former constrains flow and the latter constrains surface orientation; combining both cues enables dense reconstruction of both textured and texture-less surfaces. The algorithm works by iteratively estimating affine camera parameters, illumination, shape, and albedo in an alternating fashion. Results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera
7D5EE484	This paper proposes a method for background modeling and foreground detection in video. This method divides the background into two layers, the dynamic layer and the static layer. An energy descriptor is proposed to analysis the motion state in dynamic layer while a grid filter is proposed to reduce the negative impact of sudden illumination change such as light switching off. Experiment results compared with four typical algorithms show that this method outperforms others in most challenging videos including sudden illumination change and some complex backgrounds.
7EB39671	In this paper, we show how to estimate, accurately and efficiently, the 3D motion of a rigid or non-rigid object, and time-varying lighting in a dynamic scene. This is achieved in an inverse compositional tracking framework with a novel warping function that involves a 2D → 3D → 2D transformation. The method is guaranteed to converge, is able to work with rigid and non-rigid objects, and estimates the lighting and motion from a video sequence. Experimental analysis on multiple face video sequences shows impressive speed-up over existing methods while retaining a high level of accuracy.	
7EA28D97	We propose a new robust head detection algorithm that is capable of handling significantly different conditions in terms of viewpoint, tilt angle, scale and resolution. To this aim, we built a new model for the head based on appearance distributions and shape constraints. We construct a categorical model for hair and skin, separately, and train the models for four categories of hair (brown, red, blond and black) and three categories of skin representing the different illumination conditions (bright, standard and dark). The shape constraint fits an elliptical model to the candidate region and compares its parameters with priors based on human anatomy. The experimental results validate the usability of the proposed algorithm in various video surveillance and multimedia applications.
81592FB1	Recreating the temporal illumination variations of natural scenes has great potential for realistic synthesis of video sequences. In this paper, we present a 3D (model-based) approach that achieves this goal. The approach requires a training sequence to learn the time-varying illumination models, which can then be used for synthesis in another sequence. The motion and illumination parameters in the training sequence are estimated alternately by projecting onto appropriate basis functions of a bilinear space defined in terms of the 3D surface normals of the objects. The motion is represented in terms of 3D translation and rotation of the object centroid in the camera frame, and the illumination is represented using a spherical harmonics linear basis. We show video synthesis results using the proposed approach.
80550AB9	This paper proposes a new way to achieve feature point tracking using the entropy of the image. Sum of Squared Differences (SSD) is widely considered in differential trackers such as the KLT. Here, we consider another metric called Mutual Information (MI), which is far less sensitive to changes in the lighting condition and to a wide class of non-linear image transformation. Since mutual-information is used as an energy function to be maximized to track each points, a new feature selection, which is optimal for this metric, is proposed. Results under various complex conditions are presented. Comparison with the classical KLT tracker are proposed.
76B40489	Facial landmark detection has proved to be a very challenging task in biometrics due to the numerous sources of variation. In this work, we present an algorithm for robust detection of facial component-landmarks. Specifically, we address the variation due to extreme pose and illumination. To achieve robust detection for extreme poses, we use a set of independent pose and landmark specific detectors. Each component-landmark detector is applied independently and the information obtained is used to make inferences about the layout of multiple components. In addition, we incorporate a multi-view representation based on an aspect graph approach. The performance of our algorithm is assessed using data from a publicly available database. The failure rate of our method is lower than that of commercially available software.
80D7C0B1	One basic observation for pedestrian detection in video sequences is that both appearance and motion information are important to model the moving people. Based on this observation, we propose a new kind of features, 3D Haar-like (3DHaar) features. Motivated by the success of Haar-like features in image based face detection and differential-frame based pedestrian detection, we naturally extend this feature by defining seven types of volume filters in 3D space, instead of using rectangle filter in 2D space. The advantage is that it can not only represent pedestrian's appearance, but also capture the motion information. To validate the effectiveness of the proposed method, we combine the 3DHaar with support vector machine (SVM) for pedestrian detection. Our experiments demonstrate the 3DHaar are more effective for video based pedestrian detection.
7F50262E	The paper presents a statistical adaptive realtime background subtraction algorithm that is very robust to moving shadows and dynamic scene environment. The algorithm enhances the previously developed method reported by T. Horprascrt et al. (see Proc. IEEE ICCV'99 Frame-rate Workshop, 1999) by adding adaptation of the model corresponding to a dynamic background using adaptive brightness and color distortion. In addition, we propose a novel "vivacity factor" to measure the activities of foreground objects. It is used to delay the adaptation rate for the area of often-occurring moving foregrounds. Our method provides a solution to real-time moving object and shadow detection in the dynamic background scene of a video stream. We also develop the learning-rate control mechanism that is not addressed by most background subtraction algorithms
75974390	We present a novel method to relight video sequences given known surface shape and illumination. The method preserves fine visual details. It requires single view video frames, approximate 3D shape and standard studio illumination only, making it applicable in studio production. The technique is demonstrated for relighting video sequences of faces
7D3B6BBC	In many real applications traditional superresolution methods fail to provide high-resolution images due to objectionable blur and inaccurate registration of input low-resolution images. In this paper, we present a method of superresolution and blind deconvolution of video sequences and address problems of misregistration, local motion and change of illumination. The method processes the video by applying temporal windows, masking out regions of misregistration, and minimizing a regularized energy function with respect to the high-resolution frame and blurs, where regularization is carried out in both the image and blur domains. Experiments on real video sequences illustrate robustness of the method.
7F7F452D	Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object’s appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination. 
80DF8849	The perception of 2nd-order, texture-contrast-defined motion was studied for apparent-motion stimuli composed of a pair of spatially displaced, simultaneously visible checkerboards. It was found that background-relative, counter-changing contrast provided the informational basis for the perception of 2nd-order apparent motion; motion began where contrast changed toward the contrast value of the background checkerboard and ended where contrast changed away from the background value. The perceived apparent motion was not attributable to either postrectification motion-energy analysis or salience-mapping/feature-tracking mechanisms. Parallel results for 1st-order, luminance-defined motion (H. S. Hock, L. A. Gilroy, & G. Harnett, 2002) suggest that counter-changing activation provides a common basis for the perception of both luminance- and texture-contrast-defined apparent motion.
7E49514F	The pupil detection and tracking is an important step for developing a human-computer interaction system. To develop a human eye-computer interaction system, we examine pupil detection and tracking by image processing techniques. In the image processing techniques, the illumination directly influences the image quality in general. If influences of illumination is little, we can obtain an image of good image quality. The subsequent image processing techniques are expected almost to succeed. In this paper, in order to avoid the influences of illumination, we have tried to combine the hardware constitution of an infrared light-emitting diode (LED) light, a sensitive infrared camera, and an infrared (IR) filter. In the experiment with this hardware constitution, we investigate the effects of the pupil detection and tracking by image processing techniques for a human eye-computer interaction system.
70678280	When an airbag deploys on a rear-facing infant seat, it can injure or kill the infant. When an airbag deploys on an empty seat, the airbag and the money to replace it are wasted. We have shown that video images can be used to determine whether or not to deploy the passenger-side airbag in a crash. Images of the passenger seat, taken from a video camera mounted inside the vehicle, can be used to classify the seat as either empty, containing a rear-facing infant seat, or occupied. Our first experiment used a single, monochrome video camera. The system was automatically trained on a series of test images. Using a principle components (eigenimages) nearest neighbor classifier, it achieved a correct classification rate of 99.5% on a test of 910 images. Our second experiment used a pair of monochrome video cameras to compute stereo disparity (a function of 3D range) instead of intensity images. Using a similar algorithm, the second approach achieved a correct classification rate of 95.1% on a test of 890 images. The stereo technique has the advantage of being less sensitive to illumination, and would likely work best in a real system.
5BB733DA	Graph cuts based interactive segmentation has become very popular over the last decade. In standard graph cuts, the extraction of foreground object in a complex background often leads to many segmentation errors and the parameter λ in the energy function is hard to select. In this paper, we propose an iterated graph cuts algorithm, which starts from the sub-graph that comprises the user labeled foreground/background regions and works iteratively to label the surrounding un-segmented regions. In each iteration, only the local neighboring regions to the labeled regions are involved in the optimization so that much interference from the far unknown regions can be significantly reduced. To improve the segmentation efficiency and robustness, we use the mean shift method to partition the image into homogenous regions, and then implement the proposed iterated graph cuts algorithm by taking each region, instead of each pixel, as the graph node for segmentation. Extensive experiments on benchmark datasets demonstrated that our method gives much better segmentation results than the standard graph cuts and the GrabCut methods in both qualitative and quantitative evaluation. Another important advantage is that it is insensitive to the parameter λ in optimization.
7FAF78B0	A new robust watermarking method, named QIM-NSGA-II, is proposed based on the non-dominated sorting genetic algorithm II (NSGA-II) and quantization index modulation (QIM). The NSGA-II algorithm is utilized to find out the optimal embedding position and adaptive quantization step for embedding watermark into a carrier image in the framework of QIM. In the process of searching an optimal solution, the trade-off between robustness and image fidelity of the watermarked image is represented by the Pareto-Front discovered by NSGA-II. Experiment results show that the proposed scheme has a good robustness against common attacks, such as amplitude scaling, noise, filtering, cropping, JPEG compression.
7E397560	This paper presents the effects of multiscale windowed denoising of spectral signatures before segmentation of hyperspectral images. In the proposed denoising approach it is intended to exploit both spectral and spatial information of the hyperspectral images by using wavelets and principal component analysis. The windowed structure incorporated for this method exploits spatial information by making use of possibly highly correlated pixels. In addition to the proposed method, the segmented PCA is also investigated and compared in the experimental results with a proper modification. In the segmentation process, the K-means and fuzzy-ART algorithms are used. Especially fuzzy-ART is a fast learning network and can be used in high dimensional and high volume data such as hyperspectral images. In the experiments it has been shown that multiscale windowed principal component denoising has positive effects on the segmentation/clustering level.
7D6A2FF8	This paper addresses the problem of object detection and recognition in complex scenes, where objects are partially occluded. The approach presented herein is based on the hypothesis that a careful analysis of visible object details at various scales is critical for recognition in such settings. In general, however, computational complexity becomes prohibitive when trying to analyze multiple sub-parts of multiple objects in an image. To alleviate this problem, we propose a generative-model framework—namely, dynamic tree-structure belief networks (DTSBNs). This framework formulates object detection and recognition as inference of DTSBN structure and image-class conditional distributions, given an image. The causal (Markovian) dependencies in DTSBNs allow for design of computationally efficient inference, as well as for interpretation of the estimated structure as follows: each root represents a whole distinct object, while children nodes down the sub-tree represent parts of that object at various scales. Therefore, within the DTSBN framework, the treatment and recognition of object parts requires no additional training, but merely a particular interpretation of the tree/subtree structure. This property leads to a strategy for recognition of objects as a whole through recognition of their visible parts. Our experimental results demonstrate that this approach remarkably outperforms strategies without explicit analysis of object parts.
7C680F0E	Topology is an important prior in many image segmentation tasks. In this paper, we design and implement a novel graph-based min-cut/max-flow algorithm that incorporates topology priors as global constraints. We show that the optimization of the energy function we consider here is NP-hard. However, our algorithm is guaranteed to find an approximate solution that conforms to the initialization, which is a desirable property in many applications since the globally optimum solution does not consider any initialization information. The key innovation of our algorithm is the organization of the search for maximum flow in a way that allows consideration of topology constraints. In order to achieve this, we introduce a label attribute for each node to explicitly handle the topology constraints, and we use a distance map to keep track of those nodes that are closest to the boundary. We employ the bucket priority queue data structure that records nodes of equal distance and we efficiently extract the node with minimal distance value. Our methodology of embedding distance functions in a graph-based algorithm is general and can also account for other geometric priors. Experimental results show that our algorithm can efficiently handle segmentation cases that are challenging for graph-cut algorithms. Furthermore, our algorithm is a natural choice for problems with rich topology priors such as object tracking.
8064C0D7	This paper proposes a new object representation, called Connected Segmentation Tree (CST), which captures canonical characteristics of the object in terms of the photometric, geometric, and spatial adjacency and containment properties of its constituent image regions. CST is obtained by augmenting the object’s segmentation tree (ST) with inter-region neighbor links, in addition to their recursive embedding structure already present in ST. This makes CST a hierarchy of region adjacency graphs. A region’s neighbors are computed using an extension to regions of the Voronoi diagram for point patterns. Unsupervised learning of the CST model of a category is formulated as matching the CST graph representations of unlabeled training images, and fusing their maximally matching subgraphs. A new learning algorithm is proposed that optimizes the model structure by simultaneously searching for both the most salient nodes (regions) and the most salient edges (containment and neighbor relationships of regions) across the image graphs. Matching of the category model to the CST of a new image results in simultaneous detection, segmentation and recognition of all occurrences of the category, and a semantic explanation of these results.
7E204DC5	We propose a generic grouping algorithm that constructs a hierarchy of regions from the output of any contour detector. Our method consists of two steps, an oriented watershed transform (OWT) to form initial regions from contours, followed by construction of an ultra-metric contour map (UCM) defining a hierarchical segmentation. We provide extensive experimental evaluation to demonstrate that, when coupled to a high-performance contour detector, the OWT-UCM algorithm produces state-of-the-art image segmentations. These hierarchical segmentations can optionally be further refined by user-specified annotations.
8006580F	Graph cut is a popular technique for interactive image segmentation. However, it has certain shortcomings. In particular, graph cut has problems with segmenting thin elongated objects due to the “shrinking bias”. To overcome this problem, we propose to impose an additional connectivity prior, which is a very natural assumption about objects. We formulate several versions of the connectivity constraint and show that the corresponding optimization problems are all NP-hard. For some of these versions we propose two optimization algorithms: (i) a practical heuristic technique which we call DijkstraGC, and (ii) a slow method based on problem decomposition which provides a lower bound on the problem. We use the second technique to verify that for some practical examples DijkstraGC is able to find the global minimum.
7D66ECD1	This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009. Each region is represented by a rich set of image cues (shape, color and texture). We then learn region weights using a max-margin framework. In detection and segmentation, we apply a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis. The proposed approach significantly outperforms the state of the art on the ETHZ shape database(87.1% average detection rate compared to Ferrari et al. 's 67.2%), and achieves competitive performance on the Caltech 101 database.
7D53354D	Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
7E12A1BA	We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.
72286762	Depth estimation and semantic segmentation are two fundamental problems in image understanding. While the two tasks are strongly correlated and mutually beneficial, they are usually solved separately or sequentially. Motivated by the complementary properties of the two tasks, we propose a unified framework for joint depth and semantic prediction. Given an image, we first use a trained Convolutional Neural Network (CNN) to jointly predict a global layout composed of pixel-wise depth values and semantic labels. By allowing for interactions between the depth and semantic information, the joint network provides more accurate depth prediction than a state-of-the-art CNN trained solely for depth prediction [6]. To further obtain fine-level details, the image is decomposed into local segments for region-level depth and semantic prediction under the guidance of global layout. Utilizing the pixel-wise global prediction and region-wise local prediction, we formulate the inference problem in a two-layer Hierarchical Conditional Random Field (HCRF) to produce the final depth and semantic map. As demonstrated in the experiments, our approach effectively leverages the advantages of both tasks and provides the state-of-the-art results.
7046E954	This paper presents a novel variational method for im age segmentation that unifies boundary and region-based information sources under the Geodesic Active Region framework. A statistical analysis based on the Minimum Description Length criterion and the Maximum Likelihood Principle for the observed density function (image histogram) using a mixture of Gaussian elements, indicates the number of the different regions and their intensity properties. Then, the boundary information is determined using a probabilistic edge detector, while the region information is estimated using the Gaussian components of the mixture model. The defined objective function is mini mized using a gradientdescent method where a level set approach is used to implement the resulting PDE system. According to the motion equations, the set of initial curves is propagated toward the segmentation result under the influence of boundary and region-based segmentation forces, and being constrained by a regularity force. The changes of topology are naturally handled thanks to the level set implementation, while a coupled multi-phase propagation is adopted that increases the robustness and the convergence rate by imposing the idea of mutually exclusive propagating curves. Finally, to reduce the required computational cost and the risk of convergence to local minima, a multi-scale approach is also considered. The performance of our method is demonstrated on a variety of real images.
5A16827E	We propose a variational framework for the integration of multiple competing shape priors into level set based segmentation schemes. By optimizing an appropriate cost functional with respect to both a level set function and a (vector-valued) labeling function, we jointly generate a segmentation (by the level set function) and a recognition-driven partition of the image domain (by the labeling function) which indicates where to enforce certain shape priors. Our framework fundamentally extends previous work on shape priors in level set segmentation by directly addressing the central question of where to apply which prior. It allows for the seamless integration of numerous shape priors such that—while segmenting both multiple known and unknown objects—the level set process may selectively use specific shape knowledge for simultaneously enhancing segmentation and recognizing shape.
70B302A3	In recent years, segmentation with graph cuts is increasingly used for a variety of applications, such as photo/video editing, medical image processing, etc. One of the most common applications of graph cut segmentation is extracting an object of interest from its background. If there is any knowledge about the object shape (i.e. a shape prior), incorporating this knowledge helps to achieve a more robust segmentation. In this paper, we show how to implement a star shape prior into graph cut segmentation. This is a generic shape prior, i.e. it is not specific to any particular object, but rather applies to a wide class of objects, in particular to convex objects. Our major assumption is that the center of the star shape is known, for example, it can be provided by the user. The star shape prior has an additional important benefit - it allows an inclusion of a term in the objective function which encourages a longer object boundary. This helps to alleviate the bias of a graph cut towards shorter segmentation boundaries. In fact, we show that in many cases, with this new term we can achieve an accurate object segmentation with only a single pixel, the center of the object, provided by the user, which is rarely possible with standard graph cut interactive segmentation.
7E663631	The objective of image segmentation is to extract meaningful objects. A meaningful segmentation selects the proper threshold values to optimize a criterion using entropy. The conventional multilevel thresholding methods are efficient for bi-level thresholding. However, they are computationally expensive when extended to multilevel thresholding since they exhaustively search the optimal thresholds to optimize the objective functions. To overcome this problem, two successful swarm-intelligence-based global optimization algorithms, cuckoo search (CS) algorithm and wind driven optimization (WDO) for multilevel thresholding using Kapur’s entropy has been employed. For this purpose, best solution as fitness function is achieved through CS and WDO algorithm using Kapur’s entropy for optimal multilevel thresholding. A new approach of CS and WDO algorithm is used for selection of optimal threshold value. This algorithm is used to obtain the best solution or best fitness value from the initial random threshold values, and to evaluate the quality of a solution, correlation function is used. Experimental results have been examined on standard set of satellite images using various numbers of thresholds. The results based on Kapur’s entropy reveal that CS, ELR-CS and WDO method can be accurately and efficiently used in multilevel thresholding problem.
78AF2E9C	In this paper, a modified artificial bee colony (MABC) algorithm based satellite image segmentation using different objective function has been presented to find the optimal multilevel thresholds. Three different methods are compared with this proposed method such as ABC, particle swarm optimization (PSO) and genetic algorithm (GA) using Kapur’s, Otsu and Tsallis objective function for optimal multilevel thresholding. The experimental results demonstrate that the proposed MABC algorithm based segmentation can efficiently and accurately search multilevel thresholds, which are very close to optimal ones examined by the exhaustive search method. In MABC algorithm, an improved solution search equation is used which is based on the bee’s search only around the best solution of previous iteration to improve exploitation. In addition, to improve global convergence when generating initial population, both chaotic system and opposition-based learning method are employed. Compared to other thresholding methods, segmentation results of the proposed MABC algorithm is most promising, and the computational time is also minimized.
79E0D29E	Picture segmentation is expressed as a sequence of decision problems within the framework of a split-and-merge algorithm. First regions of an arbitrary initial segmentation are tested for uniformity and if not uniform they are subdivided into smaller regions, or set aside if their size is below a given threshold. Next regions classified as uniform are subject to a cluster analysis to identify similar types which are merged. At this point there exist reliable estimates of the parameters of the random field of each type of region and they are used to classify some of the remaining small regions. Any regions remaining after this step are considered part of a boundary ambiguity zone. The location of the boundary is estimated then by interpolation between the existing uniform regions. Experimental results on artificial picutres are also included.
7E5B25FF	A method of image segmentation was recently introduced based on defining links between pixels at adjacent levels of a “pyramid” of reduced-resolution versions of the image. This paper studies some of the problems that arise with linked-pyramid segmentation, and proposes a two-stage segmentation process that overcomes these problems.
7A19A32A	In this paper, a prototype delta-sigma ADC is implemented in a 0.18μm 2P5M CMOS process. The input signal sampling capacitors are shared with the front-end DAC capacitors. The sampling frequency is 50MHz and oversampling ratio is 24. The out-of-band peaking is deliberately set to help the stability and to allow larger input signals to be processed by the loop. This modulator achieves 78.2dB peak SNDR and 79.3dB peak SNR while consuming 1.35mW analog and 1.55mW digital power from 1.5V supplies. The major portion of the digital power (1.3mW) is consumed by an overdesigned generic clock generator to provide flexibility in testing with various sampling frequencies. The rest of the digital power (0.25mW) includes the DLL, digital counter, DWA and the comparator. The achieved minimal analog power is the direct result of the extra order of noise shaping, and the elimination of the flash ADC and the typical large capacitive loading that comes with it. The FoM is 210fJ/conversion-step and it can easily be reduced further with redesign (i.e., eliminating the wasted clock generator power).
7532C61B	This paper addresses the problem of semantic segmentation, where the possible class labels are from a predefined set. We exploit top-down guidance, i.e., the coarse localization of the objects and their class labels provided by object detectors. For each detected bounding box, figure-ground segmentation is performed and the final result is achieved by merging the figure-ground segmentations. The main idea of the proposed approach, which is presented in our preliminary work, is to reformulate the figure-ground segmentation problem as sparse reconstruction pursuing the object mask in a nonparametric manner. The latent segmentation mask should be coherent subject to sparse error caused by intra-category diversity; thus, the object mask is inferred by making use of sparse representations over the training set. To handle local spatial deformations, local patch-level masks are also considered and inferred by sparse representations over the spatially nearby patches. The sparse reconstruction coefficients and the latent mask are alternately optimized by applying the Lasso algorithm and the accelerated proximal gradient method. The proposed formulation results in a convex optimization problem; thus, the global optimal solution is achieved. In this paper, we provide theoretical analysis of the convergence and optimality. We also give an extended numerical analysis of the proposed algorithm and a comprehensive comparison with the related semantic segmentation methods on the challenging PASCAL visual object class object segmentation datasets and the Weizmann horse dataset. The experimental results demonstrate that the proposed algorithm achieves a competitive performance when compared with the state of the arts.
754B3DE9	In this paper, we present a single-path multi-bit delta-sigma analog-to-digital converter (ΔΣ ADC) architecture that uses time as a reference for performing multi-bit digital-to-analog conversion in the feedback path. The architecture uses a voltage-controlled oscillator as a multi-bit quantizer. In the feedback path of the ΔΣ ADC, the errors due to component mismatch are avoided by using a single-path for all the levels in a multi-bit digital-to-analog converter (DAC). The technique eliminates the need for feedback DAC architectures with static and dynamic component matching
7674D351	For a block fading n_t x n_r,n_t > n_r MIMO channel, we propose a precoding scheme that achieves both n_tn_rth order diversity as well as rate of n_t symbols per channel use, feeding back B(n_t-1) bits as partial channel state information to the transmitter (CSIT), where 2^B is the number of quantization states available. We establish the optimality of the uniform quantizer which achieves minimum loss in coding gain due to quantization of feedback values of our precoding scheme in comparison to the non-uniform quantization. We also lay down the guidelines for constellation sets with which our precoding scheme can achieve full diversity. We also derive the order of complexity involved in computing the precoder matrix and show that it is independent of the size of constellation sets. We compare our BER results with that of precoding schemes in the literature utilizing full CSIT as well as that with partial CSIT. We also investigate the loss in error rate performance due to imperfect channel knowledge at the receiver and present simulation results to verify our claims.
7E8AFD39	An unsupervised segmentation approach to classification of multispectral image is suggested here in Markov random field (MRF) frame work. This work generalizes the work of Sarkar et al. (2000) on gray value images for multispectral images and is extended for landuse classification. The essence of this approach is based on capturing intrinsic characters of tonal and textural regions of any multispectral image. The approach takes an initially oversegmented image and the original. multispectral image as the input and defines a MRF over region adjacency graph (RAG) of the initially segmented regions. Energy function minimization associated with the MRF is carried out by applying a multivariate statistical test. A cluster validation scheme is outlined after obtaining optimal segmentation. Quantitative evaluation of classification accuracy of test data for three illustrations are shown and compared with conventional maximum likelihood procedure. Comparison of the proposed methodology with a recent work of texture segmentation in the literature has also been provided. The findings of the proposed method are found to be encouraging.
81482B20	A simple technique has been suggested to obtain optimal segmentation based on tonal and textural characteristics of an image using the Markov random field (MRF) model. The technique takes an initially over segmented image as well as the original image as its inputs and defines an MRF over the region adjacency graph (RAG) of the initially segmented regions. A tonal-region based segmentation technique due to Kartikeyan and Sarkar (1989) has been used for initial segmentation. The energy function has been defined over the first order cliques of the MRF. The essence of this approach is primarily based on quantitative values of the second order statistics, on region characteristics and consequently deciding upon the action of merging neighboring regions using the F-statistic. The effectiveness of our approach is demonstrated with wide variety of real life examples viz., indoor, outdoor and satellite and a comparison of its output with that of a previous work in the literature has been provided.
7D9F3E81	We present a new image segmentation algorithm based on a tree-structured binary MRF model. The image is recursively segmented in smaller and smaller regions until a stopping condition, local to each region, is met. Each elementary binary segmentation is obtained as the solution of a MAP estimation problem, with the region prior modeled as an MRF. Since only binary fields are used, and thanks to the tree structure, the algorithm is quite fast, and allows one to address the cluster validation problem in a seamless way. In addition, all field parameters are estimated locally, allowing for some spatial adaptivity. To improve segmentation accuracy, a split-and-merge procedure is also developed and a spatially adaptive MRF model is used. Numerical experiments on multispectral images show that the proposed algorithm is much faster than a similar reference algorithm based on "flat" MRF models, and its performance, in terms of segmentation accuracy and map smoothness, is comparable or even superior.
7F47189C	Automatic three-dimensional (3-D) segmentation of the brain from magnetic resonance (MR) scans is a challenging problem that has received an enormous amount of attention lately. Of the techniques reported in the literature, very few are fully automatic. In this paper, we present an efficient and accurate, fully automatic 3-D segmentation procedure for brain MR scans. It has several salient features; namely, the following. 1) Instead of a single multiplicative bias field that affects all tissue intensities, separate parametric smooth models are used for the intensity of each class. 2) A brain atlas is used in conjunction with a robust registration procedure to find a nonrigid transformation that maps the standard brain to the specimen to be segmented. This transformation is then used to: segment the brain from nonbrain tissue; compute prior probabilities for each class at each voxel location and find an appropriate automatic initialization. 3) Finally, a novel algorithm is presented which is a variant of the expectation-maximization procedure, that incorporates a fast and accurate way to find optimal segmentations, given the intensity models along with the spatial coherence assumption. Experimental results with both synthetic and real data are included, as well as comparisons of the performance of our algorithm with that of other published methods.
7D92ECD1	Accurately segmenting and quantifying structures is a key issue in biomedical image analysis. The two conventional methods of image segmentation, region-based segmentation, and boundary finding, often suffer from a variety of limitations. Here the authors propose a method which endeavors to integrate the two approaches in an effort to form a unified approach that is robust to noise and poor initialization. The authors' approach uses Green's theorem to derive the boundary of a homogeneous region-classified area in the image and integrates this with a gray level gradient-based boundary finder. This combines the perceptual notions of edge/shape information with gray level homogeneity. A number of experiments were performed both on synthetic and real medical images of the brain and heart to evaluate the new approach, and it is shown that the integrated method typically performs better when compared to conventional gradient-based deformable boundary finding. Further, this method yields these improvements with little increase in computational overhead, an advantage derived from the application of the Green's theorem.
7DF6E3D8	In recent years, many image segmentation approaches have been based on Markov random fields (MRFs). The main assumption of the MRF approaches is that the class parameters are known or can be obtained from training data. In this paper the authors propose a novel method that relaxes this assumption and allows for simultaneous parameter estimation and vector image segmentation. The method is based on a tree structure (TS) algorithm which is combined with Besag's iterated conditional modes (ICM) procedure. The TS algorithm provides a mechanism for choosing initial cluster centers needed for initialization of the ICM. The authors' method has been tested on various one-dimensional (1-D) and multidimensional medical images and shows excellent performance. In this paper the authors also address the problem of cluster validation. They propose a new maximum a posteriori (MAP) criterion for determination of the number of classes and compare its performance to other approaches by computer simulations.
7CF618D4	A class of constraint-satisfaction neural networks (CSNNs) is proposed for solving the problem of medical image segmentation, which can be formulated as a constraint-satisfaction problem (CSP). A CSNN consists of a set of objects, a set of labels for each object, a collection of constraint relations linking the labels of neighboring objects, and a topological constraint describing the neighborhood relationship among various objects. Each label for a particular object indicates one possible interpretation for that object. The CSNN can be viewed as a collection of neurons that interconnect with each other. The connections and the topology of a CSNN are used to represent the constraints in a CSP. The mechanism of the neural network is to find a solution that satisfies all the constraints in order to achieve a global consistency. The final solution outlines segmented areas and simultaneously satisfies all the constraints. This technique has been applied to medical images, and the results show that the, method is a very promising approach to image segmentation,.
7DC6766A	This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.
7EB30A18	We present a probabilistic framework namely, multiscale generative models known as dynamic trees (DT), for unsupervised image segmentation and subsequent matching of segmented regions in a given set of images. Beyond these novel applications of DTs, we propose important additions for this modeling paradigm. First, we introduce a novel DT architecture, where multilayered observable data are incorporated at all scales of the model. Second, we derive a novel probabilistic inference algorithm for DTs, structured variational approximation (SVA), which explicitly accounts for the statistical dependence of node positions and model structure in the approximate posterior distribution, thereby relaxing poorly justified independence assumptions in previous work. Finally, we propose a similarity measure for matching dynamic-tree models, representing segmented image regions, across images. Our results for several data sets show that DTs are capable of capturing important component-subcomponent relationships among objects and their parts, and that DTs perform well in segmenting images into plausible pixel clusters. We demonstrate the significantly improved properties of the SVA algorithm, both in terms of substantially faster convergence rates and larger approximate posteriors for the inferred models, when compared with competing inference algorithms. Furthermore, results on unsupervised object recognition demonstrate the viability of the proposed similarity measure for matching dynamic-structure statistical models.
7E4F7C77	Semantic segmentation and object detection are nowadays dominated by methods operating on regions obtained as a result of a bottom-up grouping process (segmentation) but use feature extractors developed for recognition on fixed-form (e.g. rectangular) patches, with full images as a special case. This is most likely suboptimal. In this paper we focus on feature extraction and description over free-form regions and study the relationship with their fixed-form counterparts. Our main contributions are novel pooling techniques that capture the second-order statistics of local descriptors inside such free-form regions. We introduce second-order generalizations of average and max-pooling that together with appropriate non-linearities, derived from the mathematical structure of their embedding space, lead to state-of-the-art recognition performance in semantic segmentation experiments without any type of local feature coding. In contrast, we show that codebook-based local feature coding is more important when feature extraction is constrained to operate over regions that include both foreground and large portions of the background, as typical in image classification settings, whereas for high-accuracy localization setups, second-order pooling over free-form regions produces results superior to those of the winning systems in the contemporary semantic segmentation challenges, with models that are much faster in both training and testing.
7D2A29EF	Parametric image segmentation consists of finding a label field that defines a partition of an image into a set of nonoverlapping regions and the parameters of the models that describe the variation of some property within each region. A new Bayesian formulation for the solution of this problem is presented, based on the key idea of using a doubly stochastic prior model for the label field, which allows one to find exact optimal estimators for both this field and the model parameters by the minimization of a differentiable function. An efficient minimization algorithm and comparisons with existing methods on synthetic images are presented, as well as examples of realistic applications to the segmentation of Magnetic Resonance volumes and to motion segmentation.
8220B77E	Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.
7D1D8D7A	The solution of the segmentation problem requires a mechanism for partitioning the image array into low-level entities based on a model of the underlying image structure. A piecewise-smooth surface model for image data that possesses surface coherence properties is used to develop an algorithm that simultaneously segments a large class of images into regions of arbitrary shape and approximates image data with bivariate functions so that it is possible to compute a complete, noiseless image reconstruction based on the extracted functions and regions. Surface curvature sign labeling provides an initial coarse image segmentation, which is refined by an iterative region-growing method based on variable-order surface fitting. Experimental results show the algorithm's performance on six range images and three intensity images.
80B5F3F2	We present an algorithm that integrates multiple region segmentation maps and edge maps. It operates independently of image sources and specific region-segmentation or edge-detection techniques. User-specified weights and the arbitrary mixing of region/edge maps are allowed. The integration algorithm enables multiple edge detection/region segmentation modules to work in parallel as front ends. The solution procedure consists of three steps. A maximum likelihood estimator provides initial solutions to the positions of edge pixels from various inputs. An iterative procedure using only local information (without edge tracing) then minimizes the contour curvature. Finally, regions are merged to guarantee that each region is large and compact. The channel-resolution width controls the spatial scope of the initial estimation and contour smoothing to facilitate multiscale processing. Experimental results are demonstrated using data from different types of sensors and processing techniques. The results show an improvement over individual inputs and a strong resemblance to human-generated segmentation.
7FB1ABBD	Suppose a set of arbitrary (unlabeled) images contains frequent occurrences of 2D objects from an unknown category. This paper is aimed at simultaneously solving the following related problems: 1) unsupervised identification of photometric, geometric, and topological properties of multiscale regions comprising instances of the 2D category, 2) learning a region-based structural model of the category in terms of these properties, and 3) detection, recognition, and segmentation of objects from the category in new images. To this end, each image is represented by a tree that captures a multiscale image segmentation. The trees are matched to extract the maximally matching subtrees across the set, which are taken as instances of the target category. The extracted subtrees are then fused into a tree union that represents the canonical category model. Detection, recognition, and segmentation of objects from the learned category are achieved simultaneously by finding matches of the category model with the segmentation tree of a new image. Experimental validation on benchmark data sets demonstrates the robustness and high accuracy of the learned category models when only a few training examples are used for learning without any human supervision.
7FD6645C	Among the existing texture segmentation methods, those relying on Markov random fields have retained substantial interest and have proved to be very efficient in supervised mode. The use of Markov random fields in unsupervised mode is, however, hampered by the parameter estimation problem. The recent solutions proposed to overcome this difficulty rely on assumptions about the shapes of the textured regions or about the number of textures in the input image that may not be satisfied in practice. In this paper, an evolutionary approach, selectionist relaxation, is proposed as a solution to the problem of segmenting Markov random field modeled textures in unsupervised mode. In selectionist relaxation, the computation is distributed among a population of units that iteratively evolves according to simple and local evolutionary rules. A unit is an association between a label and a texture parameter vector. The units whose likelihood is high are allowed to spread over the image and to replace the units that receive lower support from the data. Consequently, some labels are growing while others are eliminated. Starting with an initial random population, this evolutionary process eventually results in a stable labelization of the image, which is taken as the segmentation. In this work, the generalized Ising model is used to represent textured data. Because of the awkward nature of the partition function in this model, a high-temperature approximation is introduced to allow the evaluation of unit likelihoods. Experimental results on images containing various synthetic and natural textures are reported.
795EDA89	The task of segmenting an image and that of estimating properties of image regions may be highly interdependent. The goal of segmentation is to partition the image into regions with more or less homogeneous properties; but the processes which estimate these properties should be confined within individual regions. A cooperative, iterative approach to segmentation and property estimation is defined; the results of each process at a given iteration are used to adjust the other process at the next iteration. A linked pyramid structure provides a framework for this process iteration. This hierarchical structure ensures rapid convergence even with strictly local communication between pyramid nodes.
7B2EC90E	The ISODATA clustering algorithm is investigated for analysis of a one-dimensional feature space. The algorithm is an iterative thresholding scheme for numbers of classes restricted to two, so that it always terminates in one-dimensional two-class case. For a number of classes larger than two, ISODATA can be applied to requantize images into a specified number of gray levels; in this case, reasonable data compression can be achieved without significant image distortion.
77D078A3	Image segmentation is a subfield of image analysis whose potential for applications has stimulated both practical and theoretical research, particularly in the last decade. A selection of papers is reviewed to give an idea of the main lines of attack that are being pursued at present.
7E7DADAE	A statistical model to segment clinical magnetic resonance (MR) images in the presence of noise and intensity inhomogeneities is proposed. Inhomogeneities are considered to be multiplicative low-frequency variations of intensities that are due to the anomalies of the magnetic fields of the scanners. The measurements are modeled as a Gaussian mixture where inhomogeneities present a bias field in the distributions. The piecewise contiguous nature of the segmentation is modeled by a Markov random field (MRF). A greedy algorithm based on the iterative conditional modes (ICM) algorithm is used to find an optimal segmentation while estimating the model parameters. Results with simulated and hand-segmented images are presented to compare performance of the algorithm with other statistical methods. Segmentation results with MR head scans acquired from four different clinical scanners are presented.
80D36529	Image contour detection is fundamental to many image analysis applications, including image segmentation, object recognition and classification. However, highly accurate image contour detection algorithms are also very computationally intensive, which limits their applicability, even for offline batch processing. In this work, we examine efficient parallel algorithms for performing image contour detection, with particular attention paid to local image analysis as well as the generalized eigensolver used in Normalized Cuts. Combining these algorithms into a contour detector, along with careful implementation on highly parallel, commodity processors from Nvidia, our contour detector provides uncompromised contour accuracy, with an F-metric of 0.70 on the Berkeley Segmentation Dataset. Runtime is reduced from 4 minutes to 1.8 seconds. The efficiency gains we realize enable high-quality image contour detection on much larger images than previously practical, and the algorithms we propose are applicable to several image segmentation approaches. Efficient, scalable, yet highly accurate image contour detection will facilitate increased performance in many computer vision applications.
783112DD	The paper presents a novel variational method for supervised texture segmentation. The textured feature space is generated by filtering the given textured images using isotropic and anisotropic filters, and analyzing their responses as multi-component conditional probability density functions. The texture segmentation is obtained by unifying region and boundary based information as an improved Geodesic Active Contour Model. The defined objective function is minimized using a gradient-descent method where a level set approach is used to implement the obtained PDE. According to this PDE, the curve propagation towards the final solution is guided by boundary and region based segmentation forces, and is constrained by a regularity force. The level set implementation is performed using a fast front propagation algorithm where topological changes are naturally handled. The performance of our method is demonstrated on a variety of synthetic and real textured frames.
716C42F4	This paper applies shuffling to digital watermarking and data hiding. The data embedding capacity in the multimedia source generally varies significantly from one part of the source to another. Sequential embedding is very sensitive to noise which may cause synchronization problem; the common but conservative solution via partitioning an image into large segments and embedding only one bit per segment is wasteful of the data embedding capacity. This paper shows how random shuffling can be used to equalize the uneven distribution of embedding capacity. The effectiveness of random shuffling is demonstrated by analysis and experiments.
808E9FB4	In this paper, the development and application of a fast algorithm for segmentation of textured images is discussed. It is based on Markov random fields as a method of feature extraction. We present a post-processing algorithm which increases the classification accuracy of an initial pixel-by-pixel scheme. The algorithm employs a majority decision concept to counteract the misclassification caused by multiple textures in a computational window. The method is then extended to yield a high speed algorithm which combines pixel and region classification, affording large computational savings. Experiments for both synthetic and real images, yielding accurate results, are reported.
7CA2BEFD	The motion compensation is one of the most important mechanisms in the context of video coding, allowing the efficient coding of temporal information. Video coding standards have been using entropy coding for motion information which is more prone to transmission errors than fixed-length coding. In this paper, we propose a joint source-channel decoding (JSCD) scheme for robustly decoding motion vectors, enabling the traditional usage of motion compensation in noisy channel scenarios while still maintaining a good coding efficiency. At the encoder, motion vectors are simply coded using scalar quantization enabling scalable video coding setups. The decoder then tries to recover corrupted motion vectors by means of a maximum-a-posteriori (MAP) estimation. This scheme works in open-loop mode, not requiring feedback from the decoder, which is equally important for noisy channel environments. Because some of the coding complexity is brought from the encoder to the decoder, this scheme can be employed in nontraditional applications like video streaming from low power mobile peers.
7588D026	Arbitrary domains represent one of the most difficult areas for image classification algorithms to categorize effectively. Inconsistent features require a computationally expensive multipartite approach to search for possible underlying structures within datasets. This paper proposes a new approach to the problem by applying a self-developed, non-linear, multi-scale image segmentation method to identify and extract prominent regions among several visual features expressing color, texture and layout properties. Integrating this method with the Layered Self-Organizing Map has achieved a simple yet powerful multifaceted Artificial Neural Network classifier for mixed domains which has improved abstract classification precision when compared against unsegmented classification methods.
7E3786A0	We developed a binarization approach to handle a large variety of images, from scanned flatbed images to images acquired by mobile phone cameras. The binarization is targeted at creating layers of binary images for processing by OCR engines. The layers are classified spatially and by intensity and color. First textual pixels are classified by a text operator. The text kernel is then segmented by intensity/color levels and layout analysis techniques to create regions of similar text. Finally, adaptive binarization is applied to each region to obtain superior binary images. Our experimental results show the advantages of our method over local binarization methods.
5E07E2EE	Semantic understanding of environments is an important problem in robotics in general and intelligent autonomous systems in particular. In this paper, we propose a semantic segmentation algorithm which effectively fuses information from images and 3D point clouds. The proposed method incorporates information from multiple scales in an intuitive and effective manner. A late-fusion architecture is proposed to maximally leverage the training data in each modality. Finally, a pairwise Conditional Random Field (CRF) is used as a post-processing step to enforce spatial consistency in the structured prediction. The proposed algorithm is evaluated on the publicly available KITTI dataset [1] [2], augmented with additional pixel and point-wise semantic labels for building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence regions. A per-pixel accuracy of 89.3% and average class accuracy of 65.4% is achieved, well above current state-of-the-art
7E040242	In this paper, heterogeneous clutter models are used to describe polarimetric synthetic aperture radar (PolSAR) data. The KummerU distribution is introduced to model the PolSAR clutter. Then, a detailed analysis is carried out to evaluate the potential of this new multivariate distribution. It is implemented in a hierarchical maximum likelihood segmentation algorithm. The segmentation results are shown on both synthetic and high-resolution PolSAR data at the X- and L-bands. Finally, some methods are examined to determine automatically the “optimal” number of segments in the final partition.
7C1006EF	We propose a variational framework for the integration of multiple competing shape priors into level set based segmentation schemes. By optimizing an appropriate cost functional with respect to both a level set function and a (vector-valued) labeling function, we jointly generate a segmentation (by the level set function) and a recognition-driven partition of the image domain (by the labeling function) which indicates where to enforce certain shape priors. Our framework fundamentally extends previous work on shape priors in level set segmentation by directly addressing the central question of where to apply which prior. It allows for the seamless integration of numerous shape priors such that—while segmenting both multiple known and unknown objects—the level set process may selectively use specific shape knowledge for simultaneously enhancing segmentation and recognizing shape.
81167D77	We propose a new multiphase level set framework for image segmentation using the Mumford and Shah model, for piecewise constant and piecewise smooth optimal approximations. The proposed method is also a generalization of an active contour model without edges based 2-phase segmentation, developed by the authors earlier in T. Chan and L. Vese (1999. In Scale-Space'99, M. Nilsen et al. (Eds.), LNCS, vol. 1682, pp. 141–151) and T. Chan and L. Vese (2001. IEEE-IP, 10(2):266–277). The multiphase level set formulation is new and of interest on its own: by construction, it automatically avoids the problems of vacuum and overlap; it needs only log n level set functions for n phases in the piecewise constant case; it can represent boundaries with complex topologies, including triple junctions; in the piecewise smooth case, only two level set functions formally suffice to represent any partition, based on The Four-Color Theorem. Finally, we validate the proposed models by numerical results for signal and image denoising and segmentation, implemented using the Osher and Sethian level set method.
7E51F583	Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We present motivation and detailed technical description of the basic combinatorial optimization framework for image segmentation via s/t graph cuts. After the general concept of using binary graph cut algorithms for object segmentation was first proposed and tested in Boykov and Jolly (2001), this idea was widely studied in computer vision and graphics communities. We provide links to a large number of known extensions based on iterative parameter re-estimation and learning, multi-scale or hierarchical approaches, narrow bands, and other techniques for demanding photo, video, and medical applications.
76BE721F	In this paper, we make two contributions to the field of level set based image segmentation. Firstly, we propose shape dissimilarity measures on the space of level set functions which are analytically invariant under the action of certain transformation groups. The invariance is obtained by an intrinsic registration of the evolving level set function. In contrast to existing approaches to invariance in the level set framework, this closed-form solution removes the need to iteratively optimize explicit pose parameters. The resulting shape gradient is more accurate in that it takes into account the effect of boundary variation on the object’s pose.Secondly, based on these invariant shape dissimilarity measures, we propose a statistical shape prior which allows to accurately encode multiple fairly distinct training shapes. This prior constitutes an extension of kernel density estimators to the level set domain. In contrast to the commonly employed Gaussian distribution, such nonparametric density estimators are suited to model aribtrary distributions.We demonstrate the advantages of this multi-modal shape prior applied to the segmentation and tracking of a partially occluded walking person in a video sequence, and on the segmentation of the left ventricle in cardiac ultrasound images. We give quantitative results on segmentation accuracy and on the dependency of segmentation results on the number of training shapes.
7617B343	Building on recent progress in modeling filter response statistics of natural images we integrate a statistical model into a variational framework for image segmentation. Incorporated in a sound probabilistic distance measure the model drives level sets toward meaningful segmentations of complex textures and natural scenes. Since each region comprises two model parameters only the approach is computationally efficient and enables the application of variational segmentation to a considerably larger class of real-world images. We validate the statistical basis of our approach on thousands of natural images and demonstrate that our model outperforms recent variational segmentation methods based on second-order statistics.
7F0CAE4C	In this paper, a prototype delta-sigma ADC is implemented in a 0.18μm 2P5M CMOS process. The input signal sampling capacitors are shared with the front-end DAC capacitors. The sampling frequency is 50MHz and oversampling ratio is 24. The out-of-band peaking is deliberately set to help the stability and to allow larger input signals to be processed by the loop. This modulator achieves 78.2dB peak SNDR and 79.3dB peak SNR while consuming 1.35mW analog and 1.55mW digital power from 1.5V supplies. The major portion of the digital power (1.3mW) is consumed by an overdesigned generic clock generator to provide flexibility in testing with various sampling frequencies. The rest of the digital power (0.25mW) includes the DLL, digital counter, DWA and the comparator. The achieved minimal analog power is the direct result of the extra order of noise shaping, and the elimination of the flash ADC and the typical large capacitive loading that comes with it. The FoM is 210fJ/conversion-step and it can easily be reduced further with redesign (i.e., eliminating the wasted clock generator power).
5A8BF9CF	Computer aided diagnosis of breast cancers often relies on automatic image analysis of histopathology images. The automatic region segmentation in breast cancer is challenging due to: i) large regional variations, and ii) high computational costs of pixel-wise segmentation. Deep convolutional neural network (CNN) is proven to be an effective method for image recognition and classification. However, it is often computationally expensive. In this paper, we propose to apply a fast scanning deep convolutional neural network (fCNN) to pixel-wise region segmentation. The fCNN removes the redundant computations in the original CNN without sacrificing its performance. In our experiment it takes only 2.3 seconds to segment an image with size 1000 × 1000. The comparison experiments show that the proposed system outperforms both the LBP feature-based and texton-based pixel-wise methods.
58B34D41	 A ΔΣ ADC using an LSB-first quantizer (LSBFQ) is proposed. LSBFQ are energy-efficient ADCs for processing signals with low activity, and have been proposed as standalone quantizers for sensor and biomedical applications. Since the quantizers in highly oversampled multibit ΔΣ ADC process signals with low average activity, the LSBFQ is an ideal quantizer solution. In order to avoid clocking the LSBFQ at a rate much faster than the rest of the ΔΣ ADC, it is proposed that the quantizer be provided a fixed number of comparison cycles, then be interrupted regardless of whether the conversion has fully completed. This is acceptable because for high oversampling ratios (OSR), the average code change is small and an N-bit conversion can usually be completed in fewer than N comparison cycles. In the rare cases that the quantizer is interrupted early, it injects slightly more quantization noise into the loop filter, which is filtered and shaped with little impact on signal-to-noise and distortion ratio (SNDR). Simulation results demonstrate that for high OSR, an LSBFQ achieves higher resolution and lower capacitor switching energy than a conventional SAR ADC using the same number of comparator bitcycles.
76854C77	Applying computer vision technology to IR (Infra-Red) images for UAV (Unmanned Aerial Vehicle) applications is difficult due to its characteristics which differ from common image processing. By combining visual categorization with low level IR image processing, this paper presents a framework for automatic labeling of IR images in probabilistic manner. We extract the features which contain temperature, texture and orientation information from the IR image, model visual categories by the distribution of features in terms of an extended visual vocabulary, and categorize IR image segments probabilistically. The proposed framework is demonstrated in experiments with high labeling accuracy, for near IR images of urban terrain taken from 100 feet altitude.
77F59E87	An ultrasonic diagnostic imaging apparatus and method are provided for segmenting signals from nonlinear targets such as microbubble contrast agents. Received echo signals are separated into their constituent linear and nonlinear components by a Doppler filter using pulse inversion separation. A threshold level is derived from an estimate made of the contributions to the echo signal from linear scattering and noise. Echo signals which exceed the threshold level are segmented as nonlinear (microbubble-originating) signals and displayed as such, whereas signals not exceeding the threshold are suppressed in the image display.
7A17AE87	This paper is about the development of the image segmentation algorithm for the industrial measurement system. Specifically, the problem of segmentation of textile yarn images is considered. The algorithm developed for yarn hairiness analyzer is introduced. It aims at extracting single fibers protruding from the yarn core. The algorithm is a region growing-based approach where the growth of the region is guided and constrained by the coherence enhancing diffusion filter. Results of the proposed method are presented and compared with the results provided by the traditional clustering approaches and recent, well-established segmentation methods. The comparison proves that the proposed segmentation algorithm provides high quality results and significantly outperforms other methods in number of fibers extracted from the background.
7B781C61	Image segmentation is a fundamental problem in computer vision. Despite many years of research, general purpose image segmentation is still a very challenging task because segmentation is inherently ill-posed. Among different segmentation schemes, graph theoretical ones have several good features in practical applications. It explicitly organizes the image elements into mathematically sound structures, and makes the formulation of the problem more flexible and the computation more efficient. In this paper, we conduct a systematic survey of graph theoretical methods for image segmentation, where the problem is modeled in terms of partitioning a graph into several sub-graphs such that each of them represents a meaningful object of interest in the image. These methods are categorized into five classes under a uniform notation: the minimal spanning tree based methods, graph cut based methods with cost functions, graph cut based methods on Markov random field models, the shortest path based methods and the other methods that do not belong to any of these classes. We present motivations and detailed technical descriptions for each category of methods. The quantitative evaluation is carried by using five indices – Probabilistic Rand (PR) index, Normalized Probabilistic Rand (NPR) index, Variation of Information (VI), Global Consistency Error (GCE) and Boundary Displacement Error (BDE) – on some representative automatic and interactive segmentation methods.
793A15A9	For the past decade, many image segmentation techniques have been proposed. These segmentation techniques can be categorized into three classes, (1) characteristic feature thresholding or clustering, (2) edge detection, and (3) region extraction. This survey summarizes some of these techniques. In the area of biomedical image segmentation, most proposed techniques fall into the categories of characteristic feature thresholding or clustering and edge detection.
7B97D449	In this work, we study ladar images of man-made objects in outdoor scenes. Our objective is to separate man-made objects from background. We explore ways to segment images from different modalities of ladar and integrate results to improve segmentation. We use planar surface fitting to segment the range image. The background usually cannot be fit into planar segments while man-made objects usually yield planar segments. The intensity image is segmented by using image statistics. Both surface fitting and statistical methods can be applied to the velocity component for segmentation. We combine these segmentation maps to generate a composite segmentation map. The final result shows strong resemblance to manual segmentation. These results can be fed into a cuing system for further processing, or into a higher level, knowledge-based system for scene interpretation.
76070AB6	Image segmentation is a process to divide an image into segments with uniform and homogeneous attributes such as graytone or texture. An image segmentation problem can be casted as a Constraint Satisfaction Problem (CSP) by interpreting the process as one of assigning labels to pixels subject to certain spatial constraints. A class of Constraint Satisfaction Neural Networks (CSNNs), different from the conventional algorithms, is proposed for image segmentation. In the network, each neuron represents one possible label of an object in a CSP and the interconnections between the neurons constitutes the constraints. In the context of image segmentation, each pixel in an n × n image can be considered as an object, i.e. there are n2 objects in the CSP. Suppose that each object is to be assigned one of m labels. Then, the CSNN consists of n × n × m neurons which can be conceived as a three-dimensional (3D) array. The connections and the topology of the CSNN are used to represent the constraints in a CSP. The initial condition for this network is set up by Kohonen's self-organizing feature map. The mechanism of the CSNN is to find a solution that satisfies all the constraints in order to achieve a global consistency. The final solution outlines segmented areas and simultaneously satisfies the given constraints. From our extensive experiments, the results show that this CSNN method is a very promising approach for image segmentation. Due to its network structure, it lends itself admirably to parallel implementation and is potentially faster than conventional image segmentation algorithms.
75958369	This paper presents an iterated region merging-based graph cuts algorithm which is a novel extension of the standard graph cuts algorithm. Graph cuts addresses segmentation in an optimization framework and finds a globally optimal solution to a wide class of energy functions. However, the extraction of objects in a complex background often requires a lot of user interaction. The proposed algorithm starts from the user labeled sub-graph and works iteratively to label the surrounding un-segmented regions. In each iteration, only the local neighboring regions to the labeled regions are involved in the optimization so that much interference from the far unknown regions can be significantly reduced. Meanwhile, the data models of the object and background are updated iteratively based on high confident labeled regions. The sub-graph requires less user guidance for segmentation and thus better results can be obtained under the same amount of user interaction. Experiments on benchmark datasets validated that our method yields much better segmentation results than the standard graph cuts and the Grabcut methods in either qualitative or quantitative evaluation.
763D12D3	This paper reviews recent developments in the use of iterative (or “relaxation”) methods in image analysis. Applications of these methods include histogram modification, noise cleaning, edge and curve detection, thinning, angle detection, template matching, and region labelling. These applications are briefly described, and references are given to papers and reports in which more detailed discussions and examples can be found.
766406D1	This paper describes some attempts to segment textured black and white images by detecting clusters of local feature values and partitioning the feature space so as to separate these clusters. This approach often works well for multispectral images, using a point's spectral signature as its feature vector. The results for grayscale images, using vectors of local feature values, are not quite as good. Factors affecting the performance of this approach are discussed, and methods of improving the results are suggested.
763D50E9	A new methodological approach to digital image processing applied to the particular case of gray-level image segmentation is introduced. The method is based on a modified and simplified version of classifier systems. The labeling function is implemented as a spatially structured set of binary-coded production rules. The labeling is iteratively modified using a distributed genetic algorithm. Results are presented which illustrate both the mechanisms underlying the functioning of the method and its performance on natural images. The relationships between this approach and other related techniques are discussed and it is shown that it compares favorably with these.
75EF76FB	This paper describes a procedure for segmenting imagery using digital methods and is based on a mathematical-pattern recognition model. The technique does not require training prototypes but operates in an "unsupervised" mode. The features most useful for the given image to be segmented are retained by the algorithm without human interaction, by rejecting those attributes which do not contribute to homogeneous clustering in N-dimensional vector space. The basic procedure is a K-means clustering algorithm which converges to a local minimum in the average squared intercluster distance for a specified number of clusters. The algorithm iterates on the number of clusters, evaluating the clustering based on a parameter of clustering quality. The parameter proposed is a product of between and within cluster scatter measures, which achieves a maximum value that is postulated to represent an intrinsic number of clusters in the data. At this value, feature rejection is implemented via a Bhattacharyya measure to make the image segments more homogeneous (thereby removing "noisy" features); and reclustering is performed. The resulting parameter of clustering fidelity is maximized with segmented imagery resulting in psychovisually pleasing and culturally logical image segments.
80DD9E38	Traditional transition region extraction methods depend much on the clip limits Llow and Lhigh. In which methods Llow and Lhigh can not often be obtained correctly from real images, which will result in incorrect extraction of transition region and finally bad quality of segmentation. A novel gradient threshold-based transition region extraction method (GT-TREM) is presented. Transition regions can steadily be extracted by GT-TREM. Experimental results demonstrated the robustness and effectiveness of the algorithm.
811A0FC5	In this paper we study the implementation issues of ultrawideband orthogonal frequency division multiplexing (UWB-OFDM) communication systems. Like narrowband OFDM, it is desirable to accomplish all modulation and demodulation process digitally in the base band. Designing such a transmitter and receiver for UWB-OFDM signal requires very fast and high-resolution digital-to-analog (D/A) and analog-to-digital (A/D) converters that operate on a very large frequency band. A modified version of sigma-delta modulator, which we call an N-tone sigma-delta, can be used for this purpose. This new structure introduces N zeros at N properly selected frequencies in the quantization noise spectrum and can be used anytime there are gaps in the spectrum of the transmitted signal. A digital transmitter and receiver for UWB-OFDM signal is proposed using this structure and its performance is studied in multipath fading channel.
7B3CFEFE	Human motion capture embeds rich detail and style which is difficult to generate with competing animation synthesis technologies. However, such recorded data requires principled means for creating responses in unpredicted situations, for example reactions immediately following impact. This paper introduces a novel technique for incorporating unexpected impacts into a motion capture-driven animation system through the combination of a physical simulation which responds to contact forces and a specialized search routine which determines the best plausible re-entry into motion library playback following the impact. Using an actuated dynamic model, our system generates a physics-based response while connecting motion capture segments. Our method allows characters to respond to unexpected changes in the environment based on the specific dynamic effects of a given contact while also taking advantage of the realistic movement made available through motion capture. We show the results of our system under various conditions and with varying responses using martial arts motion capture as a testbed.
76AB37EE	Human motions are the product of internal and external forces, but these forces are very difficult to measure in a general setting. Given a motion capture trajectory, we propose a method to reconstruct its open-loop control and the implicit contact forces. The method employs a strategy based on randomized sampling of the control within user-specified bounds, coupled with forward dynamics simulation. Sampling-based techniques are well suited to this task because of their lack of dependence on derivatives, which are difficult to estimate in contact-rich scenarios. They are also easy to parallelize, which we exploit in our implementation on a compute cluster. We demonstrate reconstruction of a diverse set of captured motions, including walking, running, and contact rich tasks such as rolls and kip-up jumps. We further show how the method can be applied to physically based motion transformation and retargeting, physically plausible motion variations, and referencetrajectory- free idling motions. Alongside the successes, we point out a number of limitations and directions for future work. 
7BC98C56	In this paper we learn the skills required by real-time physics-based avatars to perform parkour-style fast terrain crossing using a mix of running, jumping, speed-vaulting, and drop-rolling. We begin with a single motion capture example of each skill and then learn reduced-order linear feedback control laws that provide robust ex- ecution of the motions during forward dynamic simulation. We then parameterize each skill with respect to the environment, such as the height of obstacles, or with respect to the task parameters, such as running speed and direction. We employ a continuation process to achieve the required parameterization of the motions and their affine feedback laws. The continuation method uses a predictor-corrector method based on radial basis functions. Lastly, we build control laws specific to the sequential composition of dif- ferent skills, so that the simulated character can robustly transition to obstacle clearing maneuvers from running whenever obstacles are encountered. The learned transition skills work in tandem with a simple online step-based planning algorithm, and together they robustly guide the character to achieve a state that is well-suited for the chosen obstacle-clearing motion. 
76087296	This paper describes a new method for acquiring physically realistic hand manipulation data from multiple video streams. The key idea of our approach is to introduce a composite motion control to simultaneously model hand articulation, object movement, and subtle interaction between the hand and object. We formulate video-based hand manipulation capture in an optimization framework by maximizing the consistency between the simulated motion and the observed image data. We search an optimal motion control that drives the simulation to best match the observed image data. We demonstrate the effectiveness of our approach by capturing a wide range of high-fidelity dexterous manipulation data. We show the power of our recovered motion controllers by adapting the captured motion data to new objects with different properties. The system achieves superior performance against alternative methods such as marker-based motion capture and kinematic hand motion tracking.
7A0A57F6	Educational/entertainment robots are usually designed to perform various gesture motions for nonverbal communication or interaction with humans[1]. This paper proposed a robot motion programming methodology by applying three-level robot motion hierarchical structure and a gesture variation method that can easily apply emotional attributes (Joy, Normal or Sad) to a robot's gesture. Experiments and evaluation tests are conducted with a graphical robot motion simulator and a real robot named Engkey, which is developed to carry out several tasks for interactive English education in elementary school or interactive games for elderly people.
7F84C700	In this paper, a unified tuning rule of the motion profiles for less vibration with flexible system is proposed. For the motion profiles including trapezoidal velocity profile, S and AS curve, the tuning rule is formulated using a Laplace-domain approach. By placing zeros of the motion profiles on poles of the system, which is so-called pole-zero cancellation, the vibration mode of the system can be eliminated, which results in obtaining shorter settling time. Through the Laplace-domain approach, conditions for reducing the residual vibration are derived systematically and the proposed approach can illustrate the characteristics of the motion profiles with ease. The effectiveness of the proposed tuning rule will be illustrated and verified by simulation with a XY stage with flexural structure.
8048EA2F	This paper presents a navigation system for truck-trailer combinations that enables accurate maneuvering to a target position, either assisted or automated. Functional and collision-free motion is achieved by a combination of laser scanner-based navigation and multimodal motion planning. The navigation system uses laser range data to recognize target objects and calculates corresponding target positions. The grid-based multi-dimensional path planner generates a collision-free minimum-cost trajectory for truck-trailer combinations with up to five degrees of freedom. The multimodal planner decides whether the approach is planned as a single backward motion, as a maneuvering path with reversal points or as a long distance approach using free navigation and virtual tracks. Optimized planning algorithms are used for each case. The complete system has been successfully tested using a combination of a Mercedes-Benz truck and a full trailer under varying environmental conditions. The test application was an assistance system for backward driving under a swap body -a very challenging application due to its strict accuracy requirements. The results promise high benefits for this and further applications.
7F6F0AE7	The question addressed in this paper is: just what do we need to know about a process in order to control it? With active disturbance rejection, perhaps we don't need to know as much as we were told. In fact, it is shown that the unknown dynamics and disturbance can be actively estimated and compensated in real time and this makes the feedback control more robust and less dependent on the detailed mathematical model of the physical process. In this paper we first examine the basic premises in the existing paradigms, from which it is argued that a paradigm shift is necessary. Using a motion control metaphor, the basis of such a shift, the active disturbance rejection control, is introduced. Stability analysis and applications are presented. Finally, the characteristics and significance of the new paradigm are discussed
814F7917	The improvement of active disturbance rejection control over the existing industry controller is benchmarked with the state of the art industrial automation equipment, using the common PLC, drive, and the mechanical transmissions of belt, gear, and direct coupling. All together 168 tests are run at various line speeds, different levels of pulse-like torque disturbance, and with various viscous friction. The results are tabulated for comparison, in terms of integrated absolute error and maximum error observed, and the RMS and peak torque required from the drive motor. In the worst case scenarios, across all variations of mechanical configuration, we observe the performance improvement from just under 20% to 290%, with comparable control efforts. Together with simplicity in the control algorithm itself, the ease of tuning and operation, such improvement establishes ADRC as a viable replacement of the existing industry controller in manufacturing.
77453B84	The motion relationship model of all axes involved in CNC hobbing machines is established. A single pitch adjustment method based on motion coupling relationship in gear hobbing is proposed. With position adjustment of only 0 to 1 pitch, the same motion coupling relationship during the whole machining process can be guaranteed. Though automatic programming system based on SINUMERIK 840D, the coupled motion control and adjustment in CNC hobbing process is realized. And practical application in CNC hobbing machines shows that this method can reduced adjustment time effectively during process, improving processing efficiency. 
7D683D03	This study concerns with the resonance problems in motion control, typically described in a two-inertia system model as compliance between the motor and load. We reformulate the problem in the framework of active disturbance rejection control (ADRC), where the resonance is assumed unknown and treated as disturbance, estimated and mitigated. This allows the closed-loop bandwidth to go well beyond the resonant frequency, which is quite difficult with the existing methods. In addition, such level of performance is achieved with minimum complexity in the controller design and tuning: no parameter estimation or adaptive algorithm is needed, and the controller is tuned by adjusting one parameter, namely, the bandwidth of the control loop. It is also shown that the proposed solution applies to both the velocity and position control problems, and the performance is monitored at both motor side and load side, with the latter case much more indicative of true quality of the control system, and the fact that ADRC offers an effective and practical motion control solution, in the presence of unknown resonant frequency within the bandwidth of the control system.
77F37071	Autonomous drive of wheeled mobile robot (WMR) needs implementing velocity and path tracking control subject to complex dynamical constraints. Conventionally, this control design is obtained by analysis and synthesis of the WMR system. This paper presents the dual heuristic programming (DHP) adaptive critic design of the motion control system that enables WMR to achieve the control purpose simply by learning through trial. The design consists of an adaptive critic velocity neuro-control loop and a posture neuro-control loop. The neural weights in the velocity neuro-controller (VNC) are corrected with the DHP adaptive critic method. The designer simply expresses the control objective with a utility function. The VNC learns by sequential optimization to satisfy the control objective. The posture neuro-controller (PNC) approximates the inverse velocity model of WMR so as to map planned positions to desired velocities. Supervised drive of WMR in variant velocities supplies training samples for the PNC and VNC to setup the neural weights. In autonomous drive, the learning mechanism keeps improving the PNC and VNC. The design is evaluated on an experimental WMR. The excellent results make it certain that the DHP adaptive critic motion control design enables WMR to develop the control ability autonomously.
7D18C437	This paper studies high performance robust motion control of linear motors that have a negligible electrical dynamics. A discontinuous projection based adaptive robust controller (ARC) is constructed. Since only output signal is available for measurement, an observer is first designed to provide exponentially convergent estimates of the unmeasurable states. This observer has an extended filter structure so that on-line parameter adaptation can be utilized to reduce the effect of the possible large nominal disturbances. Estimation errors that come from initial state estimates and uncompensated disturbances are effectively dealt with via certain robust feedback at each step of the ARC backstepping design. The resulting controller achieves a guaranteed output tracking transient performance and a prescribed final tracking accuracy. In the presence of parametric uncertainties only, asymptotic output tracking is also achieved. The scheme is implemented on a precision epoxy core linear motor. Experimental results are presented to illustrate the effectiveness and the achievable control performance of the proposed scheme.
7A8312C6	In this paper, we propose a new method to concatenate two dynamic full-body motions such as punches, kicks, and flips by using the angular momentum as a cue. Through the observation of real humans, we have identified two patterns of angular momentum that make the transition of such motions efficient. Based on these observations, we propose a new method to concatenate two full-body motions in a natural manner. Our method is useful for applications where dynamic, full-body motions are required, such as 3D computer games and animations. 
5C639134	In this paper, we propose an interactive motion synthesis technique that synthesizes a continuous motion sequence from given elementary motions. A user can either specify the execution timings of motions or execute motions in a sequence with automatically determined execution timings. Our method is based on a previous approach that determined the appropriate synthesis method and blending range for each pair of sequential motions, while considering the constraints between the foot and ground in the elementary motions to prevent foot sliding. However, using only the foot-ground constraints to determine the blending range may generate unnatural motions such as nonsmooth, too fast, or too slow transitions. Moreover, simple motion blending with a regular weight function can generate unnatural motions. To solve these problems, we have introduced an optimal blending range and a weight function, which are determined for each blending segment for the upper and lower body. We also introduced extensions for applying this method to interactive character control. We have successfully applied our method to both animation generation and interactive character control.
7DC92338	This paper presents a dynamic motion control technique for human-like articulated figures in a physically based character animation system. This method controls a figure such that the figure tracks input motion specified by a user. When environmental physical input such as an external force or a collision impulse are applied to the figure, this method generates dynamically changing motion in response to the physical input. We have introduced comfort and balance control to compute the angular acceleration of the figure's joints. Our algorithm controls the several parts of a human-like articulated figure separetely through the minimum number of degrees-of-freedom. Using this approach, our algorithm simulates realistic human motions at efficient computational cost. Unlike existing dynamic simulation systems, our method assumes that input motion is already realistic, and is aimed at dynamically changing the input motion in real-time only when unexpected physical input is applied to the figure. As such, our method works efficiently in the framework of current computer games.
7EBB3587	Multiple view 3D video reconstruction of actor performance captures a level-of-detail for body and clothing movement which is time-consuming to produce using existing animation tools. In this paper we present a framework for concatenative synthesis from multiple 3D video sequences according to user constraints on movement, position and timing. Multiple 3D video sequences of an actor performing different movements are automatically constructed into a surface motion graph which represents the possible transitions with similar shape and motion between sequences without unnatural movement artifacts. Shape similarity over an adaptive temporal window is used to identify transitions between 3D video sequences. Novel 3D video sequences are synthesized by finding the optimal path in the surface motion graph between user specified key-frames for control of movement, location and timing. The optimal path which satisfies the user constraints whilst minimizing the total transition cost between 3D video sequences is found using integer linear programming. Results demonstrate that this framework allows flexible production of novel 3D video sequences which preserve the detailed dynamics of the captured movement for an actress with loose clothing and long hair without visible artifacts.
7EB88694	Human motion tracking is an important problem in computer vision. Most prior approaches have concentrated on efficient inference algorithms and prior motion models; however, few can explicitly account for physical plausibility of recovered motion. The primary purpose of this work is to enforce physical plausibility in the tracking of a single articulated human subject. Towards this end, we propose a full-body 3D physical simulation-based prior that explicitly incorporates motion control and dynamics into the Bayesian filtering framework. We consider the human’s motion to be generated by a “control loop”. In this control loop, Newtonian physics approximates the rigid-body motion dynamics of the human and the environment through the application and integration of forces. Collisions generate interaction forces to prevent physically impossible hypotheses. This allows us to properly model human motion dynamics, ground contact and environment interactions. For efficient inference in the resulting high-dimensional state space, we introduce exemplar-based control strategy to reduce the effective search space. As a result we are able to recover the physically-plausible kinematic and dynamic state of the body from monocular and multi-view imagery. We show, both quantitatively and qualitatively, that our approach performs favorably with respect to standard Bayesian filtering methods.
80A15418	Precision motion control applications require high accuracy, fast dynamics and robustness performances. The success is dependent on not only advanced technology in actuators and measuring instruments that provide physical realization hardwares, but also innovative control approaches that contribute to performances improvement. This paper presents a design and realization framework of active disturbance rejection strategy for precision motion control problems. The approach deals with the external nonlinear friction, working load and other uncertainties of a motion control system, and the internal plant dynamics variations to be generalized disturbances. Such disturbances are estimated by a unique extended state observer in real time and actively compensated for by a proportional derivative controller. Moreover, the feedforward control is usually involved in the framework to improve the dynamic performances of the motion control system. For purpose of physical realization of the control algorithm, applications of general DSP and FPGA technologies are introduced to simplify and improve the realization. Finally, a precision motion control example is given to explain the design procedure and industrial application. The initial success provides a new robust motion control design approach.
7B6C0AD0	This paper describes some initial steps toward the development of more natural control strategies for free motion of robot arms. The standard lumped parameter dynamical model of an open kinematic chain is shown to be stabilizable by linear feedback, after nonlinear gravitational terms have been cancelled. A new control algorithm is proposed and is shown to drive robot joint positions and velocites asymptotically toward arbitrary time-varying reference trajectories.
7DE41CC7	I.McIC is a single-chip MPEG-2 video encoder for consumer storage applications. It supports both intra- and inter-coding mode to achieve bit rates from 5-15 Mb/s. It contains a recursive motion estimator, a programmable buffer/bit-rate controller, and a temporal noise-reduction stage. The resulting IC has 4.5/spl times/10/sup 6/ transistors and measures 192 mm/sup 2/ in a 0.5-/spl mu/m process. I.McIC was designed using mainly high-level synthesis tools. High-throughput fixed MPEG functions are performed by dedicated hardware. The remainder is performed in software by an embedded application-specific instruction-set processor with downloadable microcode to suit the IC for different applications of video coding.
8115D397	Addresses the question of how to achieve steady motion at very low velocities using proportional-derivative (PD) control. Most prior work in control has used friction models which depend only on the current value of velocity. This type of analysis indicates that stick-slip can be avoided only through velocity feedback. The tribology literature, however, indicates that friction also depends on the history of motion. By including this dependence, a second regime of stable motion is revealed which is associated with position feedback gains above a critical value. Two experimentally-based dynamic friction models are compared using a linearized stability analysis. In accord with experiment, a state variable friction model exhibits asymptotically stable motion for any system stiffness (position feedback gain) exceeding a critical value. This property is not exhibited by a time-lag friction model.
7FC41A02	Dimensional and perturbation analysis are applied to the problem of stick slip encountered during the motion of machines. The friction model studied is motivated by current tribological results and is appropriate for lubricated metal contacts. The friction model incorporates Coulomb, viscous, and Stribeck friction with frictional memory and rising static friction. Through dimensional analysis an exact model of the nonlinear system can be formed using five parameters rather than ten, greatly facilitating study and explicitly revealing the interaction of parameters. By converting the system of differential equations into a set of integrations, the perturbation technique makes approximate analysis possible where only numerical techniques had been available before. The analysis predicts the onset of stick slip as a function of plant and controller parameters; these results are compared with experimental data.
7D2827B3	The quality of field-rate conversion improves significantly with motion-compensation techniques. It becomes possible to interpolate new fields at their correct temporal position. This results in smooth motion portrayal without loss of temporal resolution. However, motion vectors are not always valid for every pixel or object in an image. Therefore, visible artifacts occur wherever such wrong vectors are used on the image. One effective method to solve this problem is the use of non-linear filtering. In this method, a wrongly interpolated pixel is either substituted or averaged with neighbouring pixels. We introduce and evaluate a new and very robust upconversion algorithm which is based on the non-linear filtering approach. It is unique in that it estimates motion vector reliability and uses this information to control the filtering process. This algorithm outperforms others in its class, especially when we have complex image sequences.
8000BAFF	In modern manufacturing, the design of multi-axis motion controllers for high-speed/high-precision and complex machining is becoming more critical. In the past, controllers for each axis were, in general, designed individually to obtain desirable tracking accuracy. Although some advanced motion control algorithms have been developed recently, they are mostly applicable to single-axis or biaxial systems. In particular, the design of two-axis integrated control, which includes: 1) feedback loops; 2) feedforward loops; and 3) the cross-coupled control (CCC), achieves significantly improved performance. However, its applications are limited to basic linear and circular contour commands for two-axis systems only. In this paper, system analysis for the multi-axis integrated control structure is proposed in order to achieve tracking and contouring accuracy simultaneously. With the derived contouring error transfer function (CETF), the integrated control design for multi-axis motion systems can be formulated simply as a single-input-single-output (SISO) design. Thus, a robust design with desirable performance for multi-axis motion systems can then be achieved straightforwardly. Moreover, by estimating the contouring error vector, the proposed multi-axis integrated control can be widely applied to general contour commands. Experimental results on a three-axis CNC machining center indicate that both tracking and contouring accuracy are simultaneously improved by applying the proposed control design.
803CD316	Recently, feedforward controllers like zero phase error tracking controllers (ZPETC) and cross-coupled controllers (CCC) have been developed to effectively reduce tracking error and contouring error, respectively. This paper proposes an integrated controller which combines ZPETC and CCC to achieve both tracking and contouring accuracy. Furthermore, studies indicate that ZPETC and CCC can be designed separately in the present integrated control design. In the provided experimental setup with a servo table, an optimal ZPETC and a robust CCC based on the contouring error transfer function (CETF) were designed to achieve desirable frequency responses and stability. Experimental results show that the proposed integrated controller renders significantly improved accuracy in both tracking and contouring.
7D2E4346	A simple robust autodisturbance rejection controller (ADRC) in linkspace is proposed to realize high precision tracking control of a general 6 degrees of freedom (DOF) Stewart platform in this paper. In practice, the performance of the controlled system is limited by how to select the high-quality differential signal in the presence of disturbances and measurement noise. Moreover, unmodeled nonlinear friction provides degradation on the motion precision. So, a nonlinear tracking differentiator in the feedforward path and an extended states observer in the feedback path are designed to obtain high quality differential signal and the real action component of unknown disturbance signals including nonlinear friction without a precise mathematical model. The nonlinear PD (proportional derivative) controller is used to synthesize the control action to give a superior performance. Extensive simulations and experimental results are presented to verify the effectiveness and ease of engineering implementation of the proposed method. The developed ADRC controller is simple and directly intuitive to the practitioners.
7FDCAA77	In this paper, we propose a new complete closed-form formula for asymmetric motion profiles of long, medium, and short distances, which enables easy manipulation of jerks in arrival time for effectively reducing the acceleration and thus the overshoot. The design parameter named jerk ratio is newly introduced to scale down the jerks during the deceleration period so that the velocity profile is in an asymmetric S-curve. Owing to the jerk ratio, the motion formulas are remarkably simplified in analytic forms for short, medium, and long distances. The effectiveness of the proposed approach will be illustrated with the sledge movement control in an optical data storage system by experiments.
81D6B65F	This paper analyzes a disturbance observer with a focus on parameter variations. The parameter variations are the inertia variation and the variation of torque coefficient. Conventionally, their nominal values are designed not as control parameters but the same values as the actual ones. However, disturbance observer is able to include the effect of phase compensation by properly selecting the nominal model parameters. In this paper, these nominal model parameters are actively designed to achieve the phase compensation. This paper proposes the design method of observer to achieve phase compensation based on disturbance observer. Compared with implementing phase compensator, the control system is simple and easy to design. The selection of these parameters has some restriction. When the nominal model parameters are much separated from the actual ones, the stability of the control system deteriorates. This paper focuses on the inertia variation and neglects the variation of torque coefficient. Furthermore, the stability of position and force control systems is analyzed. The validity of the proposed design method and the effect of phase compensation are verified by some experimental results.
7DAF24B9	The tuning method of controllers can be used for effectively determining the overall performance of positioning systems. In particular, this method is highly effective in the case of high-speed and high-accuracy positioning systems. In this paper, a sliding-mode controller that uses one of the well-known approaches of robust control methodology is designed for high-speed positioning systems that require a high-accuracy performance. A performance-tuning method based on a disturbance observer (DOB) structure is also proposed. First, a generalized disturbance attenuation framework named robust internal-loop compensator (RIC) is introduced, and a sliding-mode controller based on a Lyapunov redesign is analyzed in the RIC framework. Then, the DOB properties of the sliding-mode controller are presented, and it is shown that the performance of the closed-loop system with a sliding-mode controller can be tuned up by using the structural characteristics of the DOB. These results make the design of an enhanced sliding-mode controller possible. Finally, the proposed algorithm is experimentally verified and discussed with two positioning systems. Experimental results show the effectiveness and the robustness of the proposed scheme.
7D5A3C38	This paper investigates the effects of two control algorithms on high-performance point-to-point motions. The emphasis here is to overcome challenges in precision positioning of high-acceleration tables in the presence of significant external disturbances and exited vibration: an A-type of iterative learning control (ILC) (A-ILC) algorithm for repetitive motions and a look-ahead finite impulse response (FIR) filter plus sliding-mode control (SMC) for nonrepetitive motions. The model-free convergence condition and the fastest converging parameter equation for A-ILC are given in the frequency domain. Then, the FIR coefficients are decided through the ILC results and modified to eliminate the friction effect. Experimental studies demonstrate that both the algorithms perform well and the FIR-SMC algorithm is robust in various experimental scenarios which include high acceleration (of 73.7 m/s2 or about 7.5 g), model parameters, and disturbance deviations from the position, velocity, and acceleration at which the ILC (and, hence, FIR) is trained.
7FE0E917	Recently, in the motion control area, one of the most challenging problems has been synchronization control of multiple motion axes or drivers. Unfortunately, the majority of the previous approaches have not fully addressed the synchronization problem when the system performs a complex motion. In this paper, a novel synchronized design of the high-precision motion control system is presented. The basic idea is to introduce the coupling and synchronization factors into the definition of the synchronization error. Then, a new quadratic performance index incorporating the synchronization errors of the multiple motion axes is introduced so that the resulting control law generates the cross-coupling control action, and as a consequence, improved tracking and synchronization performance can be obtained. The key to the success of the new design is to ensure that each motor tracks its desired trajectory while synchronizing motion with others. Computer simulations and real-time experiments on a servo system with two permanent-magnet linear motors demonstrate the effectiveness of the method
81486F5E	In this paper, effort is devoted to point-to-point motion control for the high-acceleration positioning table driven by linear motors. New performance indices for point-to-point motion are first introduced. Then, a cascaded controller is designed. It consists of an inner loop velocity PI controller and an outer loop position P plus A-type iterative learning controller. In the subsequence, the convergence analysis in both time and frequency domains is given. The control strategy has the property that only the position signal is required in the outer loop. With the common motion profiles, i.e., T-curve and S-curve, and partial knowledge of the system, point-to-point motion experiments are carried out. The maximum acceleration of the motion profile is 0.04 (about 4.07 g), and the maximum velocity is 0.4 mm/ms. Experimental results illustrate that the proposed controller can greatly improve the performance, and the S-curve motion profile has advantages over the T-curve one.
7F7A4161	The computerized numerical control machine tool is a highly integrated mechatronic system in manufacturing processes. However, uncertainties degrade its motion accuracy. These include modeling errors, parameter variation, friction, and measurement errors that are present in either linear or nonlinear nature. In this paper, a state-space disturbance observer was successfully applied to servomotors to estimate and compensate for the uncertainties of parameter variation and current measurement problems, in the velocity and current loops, respectively. Furthermore, an autotuning procedure was developed accordingly to identify the varied parameters of the motor. Furthermore, by implementing the present servomotor systems in high-precision machine tools, the nonlinear friction compensation was adopted to reduce the slip-stick effect in contouring motion. Experimental results indicate that the roundness error has been significantly reduced from 13.3 to 2.0 ¿m by applying the proposed approaches.
7E7D0801	A mix locally recurrent neural network was used to create a proportional-integral-derivative (PID)-like neural network nonlinear adaptive controller for uncertain multivariable single-input/multi-output system. It is composed of a neural network with no more than three neural nodes in hidden layer, and there are included an activation feedback and an output feedback, respectively, in a hidden layer. Such a special structure makes the exterior feature of the neural network controller able to become a P, PI, PD, or PID controller as needed. The closed-loop error between directly measured output and expected value of the system is chosen to be the input of the controller. Only a group of initial weights values, which can run the controlled closed-loop system stably, are required to be determined. The proposed controller can update weights of the neural network online according to errors caused by uncertain factors of system such as modeling error and external disturbance, based on stable learning rate. The resilient back-propagation algorithm with sign instead of the gradient is used to update the network weights. The basic ideas, techniques, and system stability proof were presented in detail. Finally, actual experiments both of single and double inverted pendulums were implemented, and the comparison of effectiveness between the proposed controller and the linear optimal regulator were given.
80B301CB	Most video watermarking algorithms embed the watermark in I-frames, but refrain from embedding in P- and B-frames, which are highly compressed by motion compensation. However, P-frames appear more frequently in the compressed video and their watermarking capacity should be exploited, despite the fact that embedding the watermark in P-frames can increase the video bit rate significantly. This paper gives a detailed overview of a common approach for embedding the watermark in I-frames. This common approach is adopted to use P-frames for video watermarking. We show that by limiting the watermark to nonzero-quantized AC residuals in P-frames, the video bit-rate increase can be held to reasonable values. Since the nonzero-quantized AC residuals in P-frames correspond to nonflat areas that are in motion, temporal and texture masking are exploited at the same time. We also propose embedding the watermark in nonzero quantized AC residuals with spatial masking capacity in I-frames. Since the locations of the nonzero-quantized AC residuals is lost after decoding, we develop a watermark detection algorithm that does not depend on this knowledge. Our video watermark detection algorithm has controllable performance. We demonstrate the robustness of our proposed algorithm to several different attacks.
813EE6E7	Media streaming over wireless links is a challenging problem due to both the unreliable, time-varying nature of the wireless channel and the stringent delivery requirements of media traffic. In this paper, we use joint control of packet scheduling at the transmitter and content-aware playout at the receiver, so as to maximize the quality of media streaming over a wireless link. Our contributions are twofold. First, we formulate and study the problem of joint scheduling and playout control in the framework of Markov decision processes. Second, we propose a novel content-aware adaptive playout control, that takes into account the content of a video sequence, and in particular the motion characteristics of different scenes. We find that the joint scheduling and playout control can significantly improve the quality of the received video, at the expense of only a small amount of playout slowdown. Furthermore, the content-aware adaptive playout places the slowdown preferentially in the low-motion scenes, where its perceived effect is lower.
7DC6401A	A methodology for exact robot motion planning and control that unifies the purely kinematic path planning problem with the lower level feedback controller design is presented. Complete information about a freespace and goal is encoded in the form of a special artificial potential function, called a navigation function, that connects the kinematic planning problem with the dynamic execution problem in a provably correct fashion. The navigation function automatically gives rise to a bounded-torque feedback controller for the robot's actuators that guarantees collision-free motion and convergence to the destination from almost all initial free configurations. A formula for navigation functions that guide a point-mass robot in a generalized sphere world is developed. The simplest member of this family is a space obtained by puncturing a disk by an arbitrary number of smaller disjoint disks representing obstacles. The other spaces are obtained from this model by a suitable coordinate transformation. Simulation results for planar scenarios are provided.
81208EA3	This paper studies the high performance robust motion control of an epoxy core linear motor, which has negligible electrical dynamics due to the fast response of the electrical subsystem. A discontinuous projection based adaptive robust controller (ARC) is first constructed. The controller theoretically guarantees a prescribed transient performance and final tracking accuracy in general, while achieving asymptotic tracking in the presence of parametric uncertainties. A desired compensation ARC scheme is then presented, in which the regressor is calculated using the reference trajectory information only. The resulting controller has several implementation advantages such as less online computation time, reduced effect of measurement noise, a separation of robust control design from parameter adaptation, and a faster adaptation rate. Both schemes are implemented and compared on an epoxy core linear motor. Extensive comparative experimental results are presented to illustrate the effectiveness and the achievable control performance of the two ARC designs.
7E6DD7FD	The zero phase error tracking controller (ZPETC) in motion control, as proposed by Tomizuka (1987) , renders the desirable zero phase error, but with a limited gain response. Moreover, a ZPETC, which is basically in a feedforward control structure, is very sensitive to modeling error. To improve the tracking accuracy of the ZPETC, this paper presents an optimal ZPETC design with a concise polynomial digital prefilter (DPF). The parameters of this well-designed DPF are obtained through the derived L/sub 2/-norm optimization. By cascading the developed DPF to the ZPETC, the resultant optimal ZPETC greatly improves the bandwidth of the tracking control systems while maintaining the zero phase error. Compared with other optimal approaches, the present design leads to much simpler procedures and fewer computations. Furthermore, the proposed optimal ZPETC can be adequately implemented as an adaptive ZPETC by including real-time estimation technique to cope with the external load perturbation and parameter variation. Compared with the other adaptive approaches, the optimal concept is used in the present adaptive ZPETC, and it also renders more accurate results because of its improved magnitude response. Experimental results on a DC servo table with different controllers indicate that when there is no loading, the present optimal ZPETC achieves the best tracking performance. Moreover, the adaptive ZPETC achieves the most satisfactory results when an external load is applied.
7CF3BBE4	An optimal feedforward controller which makes a hybrid usage of the shift (q) and delta (/spl delta/) operators is proposed for high-speed and high-precision digital motion control systems. Uncancellable discrete-time zeros arising from sampling the continuous-time plant at high rates, which make the mathematical inverse unstable, are handled in a natural way. The controller is optimized to have good performance in both low and high frequency ranges, and it is able to handle uncancellable discrete-time zeros in the right half plane. The optimization problem is generalized to an H/sup /spl infin// problem. Convex minimization is used to find the solution to the optimization problem. Simulation results and experiments carried out on an MC-510V Matsuura vertical machining center show superior performance of the proposed optimal hybrid feedforward controller.
7E863340	The design and realization of an online learning motion controller for a linear motor is presented, and its usefulness is evaluated. The controller consists of two components: (1) a model-based feedback component, and (2) a learning feedforward component. The feedback component is designed on the basis of a simple second-order linear model, which is known to have structural errors. In the design, an emphasis is placed on robustness. The learning feedforward component is a neural-network-based controller, comprised of a one-hidden-layer structure with second-order B-spline basis functions. Simulations and experimental evaluations show that, with little effort, a high-performance motion system can be obtained with this approach.
7D68075E	In this paper, a modified preview control technique is proposed to compensate packet loss in a wireless tracking control system, where future reference signals over a finite horizon can be previewed. In order to utilize future reference information for the controller design, the system model is augmented with a reference generator whose states are the future reference signals. As a response to the packet loss that occurs in the wireless network, the preview control technique is modified by employing Bernoulli variables to represent packet loss in both controller-actuator and sensor-controller channels. The Bernoulli packet loss model, along with tracking errors and control inputs, is included in a quadratic cost function, and the optimal controller gain that minimizes the cost function is obtained by dynamic programming. A modified Kalman filter considering packet loss is utilized for full-state estimation and state feedback control. The choice of preview horizon is discussed and the performance of the proposed controller is verified by simulation and experimental results.
7CF35A1A	Motion control is now recognized as a key technology in mechatronics. The robustness of motion control will be represented as a function of stiffness and a basis for practical realization. Target of motion is parameterized by control stiffness which could be variable according to the task reference. However, the system robustness of motion always requires very high stiffness in the controller. The paper shows that control of acceleration realizes specified motion simultaneously with keeping the robustness very high. The acceleration is a bridge to connect such robustness and variable stiffness. For practical applications, a technique to estimate disturbance is introduced to make motion controller to be an acceleration controller. Motion control of flexible structure and identification of mechanical parameters are also described.
7FC87A7B	Precision positioning machines are required to run with higher speed and higher accuracy. The high running speed can cause strong excitations on the machine structure, which results in severe structure vibration and long settling time. This paper presents a low-vibration motion profile generation method to reduce the residual vibration. The acceleration profile is designed by using a level-shifted sinusoidal waveform to have an s-shape in order to control its change rate. Simulation and experimental studies showed that in comparison with conventional trapezoidal profile and s-curve profile, the residual vibration by using the proposed motion profile can be reduced significantly.
7E9BDB91	This paper presents a predictive gain scheduler for path tracking control in a networked control system with variable delay. The controller uses the plant model to predict future position and find the amount of travel possible with the global path as a constraint. Based on variable network conditions and vehicle trajectory's curvature the vehicle is allowed to travel farther on the current control signal while the vehicle trajectory matches the path constraint. This method uses path specific characteristics to evaluate the effectiveness of each generated control signal. By scheduling the gain on the control signal the vehicle tracking performance is maintained with an increase in network delay. The tracking time is decreased compared to other methods since the proposed control method allows the controller to look ahead and thus evaluate predicted effect of each control signal before scaling it. The proposed method is compared with existing delay compensation methods through simulation.
77A48E06	This paper describes an algorithm for automatically adapting existing simulated behaviors to new characters. Animating a new character is difficult because a control system tuned for one character will not, in general, work on a character with different limb lengths, masses, or moments of inertia. The algorithm presented here adapts the control system to a new character in two stages. First, the control system parameters are scaled based on the sizes, masses, and moments of inertia of the new and the original characters.  Then a subset of the parameters is fine-tuned using a search process based on simulated annealing.  To demonstrate the  effectiveness of this approach, we animate the running motion of a woman, child, and imaginary character by modifying the control system for a man. We also animate the bicycling motion of a second imaginary character by modifying the control system for a man. We evaluate the results of this approach by comparing the motion of the simulated human runners with video of an actual child and with data for men, women, and children in the literature.  In addition to adapting a control system for a new model, this approach can also be used to adapt the control system in an on-line fashion to produce a physically realistic metamorphosis from the original to the new model while the morphing character is  performing  the behavior.   We  demonstrate this on-line adaptation with a morph from a man to a woman over a period of twenty seconds.
75743E64	This paper describes the application of spacetime constraints to creating transitions between segments of human body motion. The motion transition generation uses a combination of spacetime constraints and inverse kinematic constraints to generate seamless and dynamically plausible transitions between motion segments. We use a fast recursive dynamics formulation which makes it possible to use spacetime constraints on systems with many degrees of freedom, such as human figures. The system uses an interpreter of a motion expression language to allow the user to manipulate motion data, break it into pieces, and reassemble it into new, more complex, motions. We have successfully used the system to create basis motions, cyclic data, and seamless motion transitions on a human body model with 44 degrees of freedom.
7C2DE318	Techniques from the image and signal processing domain can be successfully applied to designing, modifying, and adapting animated motion. For this purpose, we introduce multiresolution motion filtering, multitarget motion interpolation with dynamic timewarping, waveshaping and motion displacement mapping. The techniques are well-suited for reuse and adaptation of existing motion data such as joint angles, joint coordinates or higher level motion parameters of articulated figures with many degrees of freedom. Existing motions can be modified and combined interactively and at a higher level of abstraction than conventional systems support. This general approach is thus complementary to keyframing, motion capture, and procedural animation.
7F636B38	Multi-hypothesis prediction extends motion compensation with one prediction signal to the linear superposition of several motion-compensated prediction signals with the result of increased coding efficiency. The multiple hypotheses in this paper are blocks in past decoded frames. These blocks are referenced by individual motion vectors and picture reference parameters incorporating long-term memory motion-compensated prediction. In this work, we at most employ two hypotheses similar to B-frames. However, they are obtained from the past. Due to the increased rate for the motion vectors, rate-constrained coder control is utilized. For this scheme, we demonstrate the coding efficiency of multi-hypothesis prediction in combination with variable block size and long-term memory and present bit-rate savings up to 32% when compared to standard variable block size prediction without long-term memory motion compensation.
8158ED0D	This paper describes the design and implementation of a model-reference adaptive motion controller for a differential-drive mobile robot. This controller uses absolute position information to modify control parameters in real time to compensate for motion errors. Robot motion errors are classified into internal and external errors. A cross-coupling control method is used to compensate for the internal errors that can be detected by wheel encoders. The adaptive controller provides compensation for external errors. The adaptive controller is analyzed, and its stability and convergence are discussed. Experiments are conducted to evaluate the control system and the results show significant improvements over conventional controllers.
7E60084C	A high-level controller for a SMALL workcell (Sawyer-motor assembly workcell) is presented. It allows multiple independent process to share the robot modules and workspace of a single workcell, by planning and scheduling collision-free paths for module motions in real time. Planning may involve moving around other modules, stationary or in motion, waiting for motions to complete, or displacing obstructing idle modules. The controller handles any number of robots in simultaneous motion along paths consisting of multiple rectilinear segments. Collision avoidance and path-planning are addressed in two dimensions, with extensions to three dimensions briefly discussed. Design considerations for the algorithms involve throughput, protection, fairness, and starvation considerations. Capabilities to allow independent processes to cooperate without interference are provided.
761A235C	A computer-controlled vehicle that is part of a mobile nursing robot system is described. The vehicle applies a motion control strategy that attempts to avoid slippage and minimize positionerrors. A cross-coupling control algorithm that guarantees a zero steady-state orientation error(assuming no slippage) is proposed and a stability analysis of the control system is presented.Results of experiments performed on a prototype vehicle verify the theoretical analysis.
75380138	Linear motors offer several advantages over their rotary counterparts in many precision manufacturing applications requiring linear motion; linear motors can achieve a much higher speed and have the potential of gaining a higher load positioning accuracy due to the elimination of mechanical transmission mechanisms. However, these advantages are obtained at the expense of added difficulties in controlling such a system. Specifically, linear motors are more sensitive to disturbances and parameter variations. Furthermore, certain types of linear motors such as the iron core are subject to significant nonlinear effects due to periodic cogging force and force ripple. To address all these issues, the recently proposed adaptive robust control (ARC) strategy is applied and a discontinuous projection-based ARC controller is constructed. In particular, based on the special structures of various periodic nonlinear forces, design models consisting of known basis functions with unknown weights are used to approximate those unknown nonlinear forces. On-line parameter adaptation is then utilized to reduce the effect of various parametric uncertainties such as unknown weights, inertia, and motor parameters while certain robust control laws are used to handle the uncompensated uncertain nonlinearities effectively for high performance. The resulting ARC controller achieves a guaranteed transient performance and a guaranteed final tracking accuracy in the presence of both parametric uncertainties and uncertain nonlinearities. In addition, in the presence of parametric uncertainties, the controller achieves asymptotic output tracking. Extensive simulation results are shown to illustrate the effectiveness of the proposed algorithm.
761D052C	To suppress two-dimensional load sway caused by the horizontal boom motion of a rotary crane, both horizontal and vertical boom motions are generally used. However, it would be more energy efficient and safer if a control scheme using only horizontal boom motion could be developed, eliminating the need for any boom vertical motion. In addition, if we can suppress load sway without the need to measure it, cost reduction of sensors can be achieved. Furthermore, the use of simple velocity trajectory patterns such as a trapezoidal velocity pattern and an S-curve acceleration/deceleration pattern, which are widely used in industrial automation systems, may provide cost-effective implementation of controllers. This paper presents a simple model of rotary crane dynamics that includes only significant centrifugal and Coriolis force terms. This simple model allows analytical solutions of the differential equations of the model to be derived. Thus, S-curve trajectory that considers residual vibration suppression without sensing it, using only horizontal boom motion, can be generated by solving only algebraic equations numerically. The effectiveness of the proposed method is demonstrated by numerical simulations and experimental results.
78E1A850	Programming with graphical languages implements a completely new type of man-machine interface. Conventional text-based programming is an inherently linear process that forces engineers and scientists to think and express their ideas in terms constrained by the programming language. The ability to visualise graphically a process or algorithm allows them, however, to express their ideas in a more intuitive, natural way. This is true, particularly for parallel processes. LabVIEW is such an icon-based graphical programming system that provides a powerful alternative for scientific and engineering programming - it offers the significant productivity gains of a graphical environment with no sacrifice in performance or flexibility. Typical scientific applications include process control, automation, instrumentation, motion control, simulation, and a number of other technical disciplines.
7E346C88	This paper presents, to the best of our knowledge, the first instance of real-time human-robot interaction using motion capture (mocap) data obtained from fully wireless, on-body sensor networks. During the learning phase, data for motion such as waving of the hands, standing on a leg, performing sit-ups and squats is captured from a human strapped with the Orient motion capture specks. Key features are extracted from the captured motion data using unsupervised learning algorithms. During subsequent interactions with the robot, the motion of the operator, speckled with Orients, is classified and the robot selects to play the closest motion. This approach is particularly useful in situations where the robot operates a well defined vocabulary of motion, and the advantages are the real-time interaction and the rapidity (in a matter of minutes) in programming new behaviour compared to a heuristics-based approach. This paper compares the performances of three unsupervised learning algorithms: c-means, k-means and Expectation Maximisation (EM) for the four motion scenarios. Nine best candidates for the three learning algorithms for each of the four motion scenarios were selected in the Webots robot simulator and then transferred to the real robot. Metrics were defined for each motion scenario and their performances compared for the three learning algorithms. In all the cases the motions were able to be imitated; c-means was the best, followed closely by the k-means algorithms, and the reasons have been analysed.
80F79A33	In this paper, we propose a new method for simulating reactive motions for running or walking human figures. The goal is to generate realistic animations of how humans compensate for large external forces and maintain balance while running or walking. We simulate the reactive motions of adjusting the body configuration and altering footfall locations in response to sudden external disturbance forces on the body. With our proposed method, the user first imports captured motion data of a run or walk cycle to use as the primary motion. While executing the primary motion, an external force is applied to the body. The system automatically calculates a reactive motion for the center of mass and angular momentum around the center of mass using an enhanced version of the linear inverted pendulum model. Finally, the trajectories of the generalized coordinates that realize the precalculated trajectories of the center of mass, zero moment point, and angular momentum are obtained using constrained inverse kinematics. The advantage of our method is that it is possible to calculate reactive motions for bipeds that preserve dynamic balance during locomotion, which was difficult using previous techniques. We demonstrate our results on an application that allows a user to interactively apply external perturbations to a running or walking virtual human model. We expect this technique to be useful for human animations in interactive 3D systems such as games, virtual reality, and potentially even the control of actual biped robots.
78DB1C7A	Block matching (BM) motion estimation plays a very important role in video coding. In a BM approach, image frames in a video sequence are divided into blocks. For each block in the current frame, the best matching block is identified inside a region of the previous frame, aiming to minimize the sum of absolute differences (SAD). Unfortunately, the SAD evaluation is computationally expensive and represents the most consuming operation in the BM process. Therefore, BM motion estimation can be approached as an optimization problem, where the goal is to find the best matching block within a search space. The simplest available BM method is the full search algorithm (FSA) which finds the most accurate motion vector through an exhaustive computation of SAD values for all elements of the search window. Recently, several fast BM algorithms have been proposed to reduce the number of SAD operations by calculating only a fixed subset of search locations at the price of poor accuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC) optimization is proposed to reduce the number of search locations in the BM process. In our algorithm, the computation of search locations is drastically reduced by considering a fitness calculation strategy which indicates when it is feasible to calculate or only estimate new search locations. Since the proposed algorithm does not consider any fixed search pattern or any other movement assumption as most of other BM approaches do, a high probability for finding the true minimum (accurate motion vector) is expected. Conducted simulations show that the proposed method achieves the best balance over other fast BM algorithms, in terms of both estimation accuracy and computational cost.
7FE10E4E	For human motion analysis and recognition, the current trend has extended from the tracking and recognition of hand gesture to pose estimation, body motion analysis and body language recognition and from the appearance based to 3D human model based. Since the major part of the human body language is performed by the upper limbs, and also for the sake of reducing the complexity, this paper only models the upper limbs on human and uses articulated model to reconstruct the 3D pose. It has been proved that it is feasible and reliable to detect the hands and eyes by means of stereo vision. Further, their 3D coordinates can be estimated and serve as approximate constraints. Based on camera perspective model in the font view image, integrating the approximate constrains and the skeleton length scales, this paper shows the precise 3D coordinates of each joint can be obtained iteratively from coarse to fine by hierarchical quasi-Voronoi.
75BCD53A	Motion estimation (ME) consists of three main steps, including spatial-temporal prediction, integer-pel ME and fractional-pel ME. However, we find that video sequences (especially high resolution sequences) can be encoded efficiently even without integer-pel ME.
787CD4CB	The sensitivity to error of a video bitstream often changes as the amount of motion within a scene changes. Taking this into account, a scheme that attempts to optimise error robustness by varying the amount of adaptive intra refresh blocks is presented. Simulation results demonstrate the benefits of the proposed technique.
5A2B3A00	Although the recent advances in the sparse representations of images have achieved outstanding denosing results, removing real, structured noise in digital videos remains a challenging problem. We show the utility of reliable motion estimation to establish temporal correspondence across frames in order to achieve high-quality video denoising. In this paper, we propose an adaptive video denosing framework that integrates robust optical flow into a nonlocal means (NLM) framework with noise level estimation. The spatial regularization in optical flow is the key to ensure temporal coherence in removing structured noise. Furthermore, we introduce approximate K-nearest neighbor matching to significantly reduce the complexity of classical NLM methods. Experimental results show that our system is comparable with the state of the art in removing AWGN, and significantly outperforms the state of the art in removing real, structured noise.
785F285C	This paper focuses on motion estimation engine design in future high-efficiency video coding (HEVC) encoders. First, a methodology is explained to analyze hardware implementation cost in terms of hardware area, memory size and memory bandwidth for various possible motion estimation engine designs. For 11 different configurations, hardware cost as well as the coding efficiency are quantified and are compared through a graphical analysis to make design decisions. It has been shown that using smaller block sizes (e.g. 4 × 4) imposes significantly larger hardware requirements at the expense of modest improvements in coding efficiency. Secondly, based on the analysis on various configurations, one configuration is chosen and algorithm improvements are presented to further reduce hardware implementation cost of the selected configuration. Overall, the proposed changes provide 56 × on-chip bandwidth, 151 × off-chip bandwidth, 4.3 × core area and 4.5 × on-chip memory area savings when compared to the hardware implementation of the HM-3.0 design.
7D94F61E	In this paper, we present efficient hardware implementation of multiplication free one-bit transform (MF1BT) based and constraint one-bit transform (C-1BT) based motion estimation (ME) algorithms, in order to provide low bit-depth representation based full search block ME hardware for real-time video encoding. We used a source pixel based linear array (SPBLA) hardware architecture for low bit depth ME for the first time in the literature. The proposed SPBLA based implementation results in a genuine data flow scheme which significantly reduces the number of data reads from the current block memory, which in turn reduces the power consumption by at least 50% compared to conventional 1BT based ME hardware architecture presented in the literature. Because of the binary nature of low bit-depth ME algorithms, their hardware architectures are more efficient than existing 8 bits/pixel representation based ME architectures.
7C6AFD8C	A multiplication-free one-bit transform (1BT) for low-complexity block-based motion estimation is presented in this letter. A novel filter kernel is utilized to construct the 1BT of image frames using addition and shift operations only. It is shown that the proposed approach provides the same motion estimation accuracy at macro-block level and even better accuracy for smaller block sizes compared to previously proposed 1BT methods. Because the proposed 1BT approach does not require multiplication operations, it can be implemented in integer arithmetic using addition and shifts only, reducing the computational complexity, processing time, as well as power consumption
7D994902	A full-search based block-matching algorithm for motion estimation has a major problem of significant computational load. To solve this problem, extensive research in fast-motion estimation algorithms have been carried out. However, most of them have some degradation in the predicted image from the reduced computation. To decrease the amount of significant computation of the full-search algorithm, we propose a fast block-matching algorithm based on an adaptive matching scan and representative pixels without any degradation of the predicted image. By using Taylor series expansion, we obtain the representative pixels and show that the block-matching errors from the reference block and candidate blocks are proportional to the block complexity. With the derived result, we propose a fast full-search algorithm with adaptive scan direction in block matching. Experimentally, our proposed algorithm is very efficient in terms of computational speedup, and is the fastest among all the conventional full-search algorithms. Therefore, our algorithm is useful in VLSI implementation of video encoders for real-time encoding.
7E2C2125	We present a fast motion estimation algorithm using only binary representation, which is desirable for both embedded system and hardware implementation with parallel architectures. The key algorithm distinction is that only the high-frequency spectrum is used. Our experimental results show that it provides excellent performance at both low and high bit rates. Because of its binary-only representation, the proposed algorithm offers low computational complexity and low memory bandwidth consumption. For multimedia-embedded system design, we further investigated specific implementation techniques for several well-known hardware platforms including Intel x86 processors, single-instruction multiple-data processors, and systolic array circuit design. The systolic array architecture requires only single memory access for both the reference and current frames from the on-chip memory. Such an implementation provides an optimized solution with great throughput, while the quality is maintained. Finally, we show that our binarization methods are closely coupled to the accuracy of binary motion estimation algorithms. The binarization and coding efficiencies can be improved using various filters and binarization methods.
81290EA3	In this paper, we propose an efficient and practical algorithm to dynamically adapt the Lagrange multipliers for each macroblock based on the context of the neighboring or upper layer blocks to improve rate-distortion performance. Our method improves the accuracy for the detection of true motion vectors as well as the most efficient encoding modes for luma, which are used for deriving the motion vectors, and modes for chroma. Simulation results for H.264/advanced video coding video demonstrate that our method reduces bit rate significantly and achieves peak signal-to-noise ratio gain over those of the joint model (JM) software for all sequences tested, with negligible extra computational cost. The improvement is particularly significant for high motion high-resolution videos. This paper describes our work that led to our Joint Video Team adopted contribution (included in software JM 12.0 onward), collectively known as context adaptive Lagrange multiplier (CALM).
7D2C98AF	Motion estimation using the one-bit transform (1BT) was proposed by Natarajan, Bhaskaran and Konstantinides (see ibid., vol.7, p.702-06, 1997) to achieve large computation reduction. However, it degrades the predicted image by almost 1 dB as compared with full search. We propose a modification to the 1BT by adding conditional local searches. Simulation results show that the proposed modification improves the peak signal-to-noise ratio (PSNR) significantly at the expense of slightly increased computational complexity. A variant of the proposed modification called the multiple-candidate two-step search (M2SSFS) is found to be particularly good for high quality, high bit rate video coding. In the MPEG-1 simulation, its PSNR is within 0.1 dB from that of full search at bit rates higher than 1 Mbit/s with a computation reduction factor of ten.
7D376C35	Two algorithms for block motion estimation that produce performance similar to that of exhaustive search but with computation reduced by a factor of 8 or 16 are presented. The algorithms are based on motion-field and pixel subsampling. A subsampled motion field is first determined by estimating the motion vectors for a fraction of the blocks. The motion vectors for these blocks are determined by using only a fraction of the pixels at any searched location and by alternating the pixel subsampling patterns with the searched locations. The subsampled motion field is then interpolated so that a motion vector is determined for each block of pixels. The algorithms are more robust than previously proposed fast algorithms and both can easily be combined with a hierarchical search strategy. One of the algorithms is fully compatible with MPEG-I.
8124B098	A fast block-matching algorithm for motion estimation is presented. It is based on a logarithmic step where, in each search step, only four locations are tested. For a motion displacement of w pels/frame, this technique requires 5+4 log/sub 2/w computations to locate the best match. Using sequences of CIF standard pictures, the interframe motion compensated prediction error with this technique is compared to the other fast methods. The computational complexity of this algorithm is also compared against those methods.
7F19B44E	In the search for lower bit rate image compression and representation, a new video motion estimation technique (VMET), that considers video object translation, as well as rotation, and planar multilayering, is described. This new concept uses a modified multipopulation coevolutionary genetic algorithm (MMCGA), that receives the video objects of segmented reference images, and outputs the corresponding motion and layer information, using object and layer genotypes. Genetic operation strategies of reproduction, crossover, mutation, and dominance are applied recurrently in order to create successive generations of genomes with much better fitness, until convergence, or the maximum allowed number of generations is reached. For the increase of prediction accuracy and convergence speed, a lifetime fitness strategy is used. Simulations with synthetic images have shown very encouraging results with the proposed video motion estimation technique, which competes favorably with respect to the conventional algorithms in accuracy, effectiveness, robustness, simplicity and speed.
7EDF3670	Motion estimation (ME) is the most computationally intensive part of a video coding system. Therefore it is very important to reduce its computational complexity. In this paper, a novel all-binary approach for reducing the computational complexity of sub-pixel accurate ME is proposed. An efficient hardware architecture for the proposed all-binary sub-pixel accurate motion estimation approach is also presented. The proposed hardware architecture has significantly low hardware complexity and therefore very low power consumption. It can process 720p video frames at 30 fps in a pipelined fashion together with the integer ME hardware. Therefore, it can be used in real-time low power video coding systems required by many mobile consumer electronics devices.
80383EF2	Variable Block Size Motion Estimation (VBSME) is one of the most important features of state-of-theart video encoders. In the H.264/AVC encoder, the computational complexity of integer motion estimation is about 75%. Therefore, reducing this complexity is one of the key points to provide low power video encoding. In this paper, a reconfigurable bit plane matching based VBSME method and a runtime reconfigurable hardware architecture are proposed to allow low-power consumer electronic devices to make a trade-off between power requirements and motion estimation (ME) accuracy. The proposed ME method is the only low complexity ME algorithm proposed in the literature so far that can provide compatible ME accuracy for lower block sizes compared to the sum of absolute difference (SAD) criterion. A new data path for the computation of the matching criterion in the proposed hardware architecture which has a fully arithmetic structure is proposed to improve the previously utilized LUT based architectures by having a fully arithmetic structure.
806C82D4	For real-time video encoder, low-complexity video encoding algorithms are required in many applications. This paper provides an early termination scheme to make the conventional binary block motion estimation algorithm significantly faster. The difference between the intensity of original frame and its filtered version is used as an input to something in this paper. With video codec, the proposed scheme shows improved visual quality while reducing complexity. Experimental results are provided to compare the performance of the proposed scheme with the conventional algorithms.
79993A43	Many modified three-step search (TSS) algorithms have been studied for the speed up of computation and improved error performance over the original TSS algorithm. In this work, an efficient and fast TSS algorithm is proposed, which is based on the unimodal error search assumption (UESA), error surface properties, the matching error threshold and the partial sum of the matching error. For the search strategy, we propose a new and efficient search method, which shows a good performance in terms of the computational reduction and the prediction error compared with other search algorithms. Also, we add half-stop algorithms to the above algorithm with little degradation of the predicted image quality while obtaining more computational reduction. One of them is based on the assumption that if a small amount of motion compensation error is produced, we can consider the matching block as a matched block and the motion vector as a global one. The other removes the computational redundancy by stopping the useless calculation of the matching error in a matching block. With the added algorithms, we can reduce significantly the computation for the motion vector with a small degradation of the predicted image quality with a proper threshold. Experimentally, it is shown that the proposed algorithm is very efficient in terms of the speed up of the computation and error performance compared with other conventional modified TSS algorithms.
778575D0	In this paper, a new fast encoding algorithm based on an efficient motion estimation (ME) process is proposed to accelerate the encoding speed of the scalable video coding standard. Through analysis of the ME process performed in the enhancement layer, we discovered that there are redundant MEs and some MEs can simply be unified at the fully overlapped search range (FOSR). In order to make the unified ME more efficient, we theoretically derive a skip criterion to determine whether the computation of rate-distortion cost can be omitted. In the proposed algorithm, the unnecessary MEs are removed and a unified ME with the skip criterion is applied in the FOSR. Simulation results show that the proposed algorithm achieves computational savings of approximately 46% without coding performance degradation when compared with the original SVC encoder.
7A08C2AC	Motion estimation is a critical yet computationally intensive task for video encoding. In this paper, we present an enhancement over a normalized partial distortion search (NPDS) algorithm to further reduce block matching motion estimation complexity while retaining video fidelity. The novelty of our algorithm is that, in addition to the halfway-stop technique in NPDS, a dual-halfway-stop (DHS) method, which is based on a dynamic threshold, is proposed, so that block matching is not performed against all matching candidates. An adaptive search range (ASR) mechanism based on inter block distortion further constrains the searching process. Simulation results show that the proposed algorithm has a remarkable computational speedup when compared to that of full search and NPDS algorithms. Particularly, it requires less computation by 92-99% and encounters an average of only 0.08 dB PSNR video degradation when compared to that of full search. The speedup is also very significant when compared to that of fast motion estimation algorithms. This paper describes our work that led to our joint video team (JVT) adopted contribution (included in software JM 10.1 onwards) as well as later enhancements, collectively known as simplified and unified multi-hexagon search (SUMH), a simplified fast motion estimation.
7DD42915	To reduce the amount of computations for a full search (FS) algorithm for fast motion estimation, we propose a new and fast FS motion estimation algorithm. The computational reduction of our FS motion estimation algorithm comes from fast elimination of impossible motion vectors. We obtain faster elimination of inappropriate motion vectors using efficient matching units from localization of a complex area in image data. In this paper, we show three properties in block matching of motion estimation. We suggest two fast matching scan algorithms: one from adaptive matching scan and the other from fixed dithering order. Experimentally, we remove the unnecessary computations by about 30% with our proposed algorithm compared with the conventional fast FS algorithms.
7A14B919	The large amount of bandwidth that is required for the transmission or storage of digital videos is the main incentive for researchers to develop algorithms that aim at compressing video data (digital images) whilst keeping their quality as high as possible. Motion estimation algorithms are used for video compression as they reduce the memory requirements of any video file while maintaining its high quality. Block matching has been extensively utilized in compression algorithms for motion estimation. One of the main components of block matching techniques is search methods for block movements between consecutive video frames whose aim is to reduce the number of comparisons. One of the most effective searching methods that yield accurate results but is computationally very expensive is the Full Search algorithm. Researchers try to develop fast search motion estimation algorithms to reduce the computational cost required by full-search algorithms. In this research, the authors present a new fast search algorithm based on the hierarchical search approach, where the number of searched locations is reduced compared to the Full Search. The original image is sub-sampled into additional two levels. The Full Search is performed on the highest level where the complexity is relatively low. The Enhanced Three-Step Search Algorithm and a new proposed searching algorithm are used in the consecutive two levels. The results show that by using the standard accuracy measurements and the standard set of video sequences, the performance of the proposed hierarchal search algorithm is close to the Full Search with 83.4% reduction in complexity and with a matching quality over 98%.
81723710	Motion estimation is the most time consuming part in H.264/AVC. In this paper, according to computing redundancy of UMHexagonS algorithm, the method of motion vector distribution prediction is proposed and combines with designed patterns to achieve adaptive sub-regional searching. Simulation results show that the proposed motion estimation scheme achieves reducing 21.48% motion estimation encoding time with a good rate-distortion performance compared to UMHexagonS algorithm in JM18.4. The proposed algorithm improves the performance of real-time encoding.
7F3728BE	In this paper, an approach of kinetic parameter estimation and real-time pose tracking for 3D moving objects is investigated. The main work includes two folds: Firstly, an extended kalman filter (EKF) is designed to estimate the kinetic parameter with a hybrid eye to hand/eye in hand multi-camera vision system. Secondly, a scheme of dynamic feature selection is proposed. One of the main innovations in this paper is that the maximum inscribed circle of the feature set involved in estimation is proposed to be the criterion of feature selection. Simulation results demonstrate that the accuracy of estimation can be obviously improved by using this strategy.
7F6330A0	H.264 adopts Variable Block Size Motion Estimation and Mode Decision with Rate-Distortion Optimization to improve video quality and decrease bit rate. However,these techniques increase the computation complexity and the encoding time enormously. An efficient macroblock level fast mode decision algorithm is proposed in this paper. Macroblocks which are likely to choose a large block size during mode decision are detected early and those smaller size modes are rejected for these macroblocks. Experimental results show that 40.52% encoding time is saved with only 0.01dB PSNR drop on average.
58D5DB9C	We present an efficient search technique which minimizes the computations necessary for estimating the motion in video-sequences by the block matching method. We also discuss the theoretical basis for conducting such a reduced search by our technique. We then present two algorithms which employ the proposed technique for estimating the motion typical of video-conferencing environment. Next, the results of computer simulations on a real video-sequence are included which demonstrate the effectiveness of the proposed technique. Finally, the results of a study of statistical properties of block motion-compensated frame difference signals are also summarized, to assist in future choice of a coding strategy for such signals.
7EA690CD	Motion estimation (ME) is an important part of modern video coding systems to exploit temporary redundancy in a video. Motion estimation is typically per-formed firstly with integer-pixel accuracy and then at sub-pixel accuracy, which includes half-pixel and quarter-pixel accuracy. When sophisticated fast integer-pixel accuracy motion-search algorithms are used to decrease the number of search points for integer-pixel motion search, quarter-pixel motion search becomes another important processing bottleneck in the encoding process. The conventional method is to search 8 half-pixel positions around the motion vector (MV) obtained from integer-pixel motion search, then do motion search in the same way on 8 quarter-pixel positions around the MV obtained from the half-pixel motion search, therefore, in total, 16 search points are needed. The proposed algorithm, named sub-optimal quarter-pixel inter-prediction algorithm (SQIA), successfully optimizes the quarter-pixel motion search part and improves the processing speed with low PSNR penalty.
8029D552	A fast inter-mode determination algorithm based on the macro-block (MB) tracking scheme and rate-distortion (RD) cost is proposed for the H.264/AVC video standard in which residual prediction is composed of intra-modes and inter-modes. In addition to intra-mode prediction, 8 block types exist for the best coding gain based on rate-distortion (RD) optimization in the inter-mode prediction. This scheme gives rise to exhaustive computations (search) in the coding procedure. To reduce the computational load of the inter-mode search at the inter-frame, we propose a new inter-mode determination algorithm based on the rate-distortion (RD) cost of the neighborhood MB that is tracked for the current MB in the previous frame. Based on the MB tracking scheme, an efficient sequential mode search approach is presented. We verify the performance of the proposed scheme through comparative analysis of experimental results using JM reference software.
7EE25013	Motion estimation represents the most computationally intensive task for all efficient motion compensated compression standards. This fact, despite the several efforts aiming at reducing its complexity, still constitutes a serious obstacle for obtaining the highest quality results theoretically achievable by the standard. This is particularly evident when critical conditions occur, such as when very large search windows are needed to have efficient motion prediction for sequences containing large displacements. In this paper we present a new block motion estimation technique, based on the combination of the tracing of trajectories obtained from the already coded motion field and a genetic heuristic search. This technique can be applied to any group of picture structure of any block based compression standard, with a complexity reduction factor up to more than two orders of magnitude at optimal coding results.
7F899BA8	In spite of their advantages, region-based approaches present a heavy burden in the required amount of information to describe region contours. To avoid these limitations, a new region-based motion estimation and compensation strategy is proposed, which allows the operation on arbitrary shaped regions and the ability to reconstruct them without any contour information. The strategy is enhanced by the use of multivector motion estimation and compensation. Results showing the quality and advantages of the strategy are provided.
6E8D4190	We present an efficient fine granular scalable video compression scheme which supports a fast bit rate adaptation independent of the encoder. The proposed scheme generates an embedded bitstream for each frame or, by appropriate multiplexing, for each group of picture. This rate-scalability is supported by an embedded bitstream which allows decoding at multiple rates, or to be more specific at virtually any rate. Drift removing techniques based on intra refresh or feedback mechanisms are presented. We show the potential of this video codec for variable bit rate channels as well as in combination with an unequal erasure protection scheme for error robust and efficient transmission over packet erasure channels. Possible enhancements of the presented coding schemes are discussed.
7F568FCB	Current digital video representations emphasize compression efficiency, lacking some of the flexibility required for interactive manipulation of digital bitstreams. We present a video representation which can encompass both space and time, providing a temporally coherent description of video sequences. The video sequence is segmented into its component objects, and the trajectory of each object throughout the sequence is described parametrically, according to a spatiotemporal motion model. Since the motion model is a continuous function of time, the video representation becomes frame-rate independent and the temporal resolution a user-definable parameter. i.e. the traditional sequence of frames, with the temporal structure hardcoded into the bitstream at the time of production, is replaced by a collection of scene snapshots assembled on the fly by the decoder. This enables random access and temporal scalability, the major building blocks for interactivity.
7FEE278F	This paper describes a method for calibrating a stereo-camera using its pure translational motion. The left and right cameras translate in several directions simultaneously. For each camera and each translation, an image sequence is obtained, feature points being extracted and tracked. An epipole is estimated for each image sequence and calibration is carried out using the relation between known translations and estimated epipoles. After finding calibration and rotation matrices of each camera, 3D reconstruction of feature points is performed. Reconstructed points are used to find translational relation between both cameras. The method was tested with real image data, and experimental results show that it is applicable in practice.
5F72D6F8	Motion-compensated temporal filtering is an essential ingredient of recently developed wavelet-based scalable video coding schemes. Lifting implementation of these decompositions represents a versatile tool for spatio-temporal optimizations and numerous improvements have thus been proposed. In this paper, we propose an alternative structure for the temporal prediction in the 5/3 filterbank. It significantly reduces the ghosting artefacts in the temporal approximation subband frames, providing a higher quality scalability and improved compression performance, for an equivalent complexity.
814D5E6A	Motion compensation with redundant-wavelet multihypothesis, in which multiple predictions that are diverse in transform phase contribute to a single motion estimate, is deployed into the fully scalable MC-EZBC video coder. The bidirectional motion-compensated temporal-filtering process of MC-EZBC is adapted to the redundant-wavelet domain, wherein transform redundancy is exploited to generate a phase-diverse multihypothesis prediction of the true temporal filtering. Noise not captured by the motion model is substantially reduced, leading to greater coding efficiency. In experimental results, the proposed system exhibits substantial gains in rate-distortion performance over the original MC-EZBC coder for sequences with fast or complex motion.
7DE7EAF9	In video transcoding, pre-encoded frames may be arbitrarily dropped to freely adjust the video to meet the network and client requirements. Since transcoding is carried out in real-time, incoming motion vectors are reused to reduce the transcoding latency. In this paper, we propose a new motion vector composition scheme for arbitrarily dropping any frame from incoming video bit-stream comprising I, B and P frames. The transcoded bit-stream retains the I-B-P frame structure. Experimental results are presented and compared to show the efficacy of the proposed scheme
80619D3E	Due to the considerable computational complexity of full-search (ITS) in motion estimation, many suboptimal but fast block-matching algorithms (BMAs) have been developed. Among them, the diamond search (DS) series is the most promising method. To further reduce complexity and improve performance, we propose a modified diamond-search (MODS) algorithm for rapid block matching based on the well-known DS algorithm. A novel fine granularity halfway-stop (FGHS) method based on a dynamic block distortion threshold is also proposed. To avoid being trapped in local optima, unlike some small DS methods, MODS adaptively starts with a relatively large search pattern for high motion blocks which are automatically determined via the first block matching distortion. The threshold is obtained via a linear model utilizing already computed distortion statistics. Experiments show that the proposed algorithm achieves less search points with no significant PSNR degradation when compared to that of FS and other fast BMAs.
7D66B455	In this study sub-pixel motion estimation via one-bit transform is proposed. Low-bit resolution representations exist in literature for motion estimation. However sub-pixel motion estimation employing these kinds of transforms has not tried until now. One-bit (1BT) transform based motion estimation can give inaccurate result for block size of 8times8. Half and quarter pixel motion vectors can be calculated using one-bit transform. In this work higher performance is achieved computing half and quarter pixel motion vectors utilizing one-bit transform than pure one-bit motion estimation
7DC8035D	Multiple block size motion estimation is adopted in the latest JVT/H.264 video coding standard to achieve higher coding efficiency. However, full exhaustive search of all block sizes is computational intensive with motion estimation complexity increasing linearly with the number of block sizes allowed. We propose a fast integer motion estimation method for multiple block size motion estimation (FMBME) which reduces computation significantly. Experimental result shows that, compared with full search, the proposed method can have a speed up factor of four with bit-rate increases within 1%.
5B26EB0A	In several multimedia applications, image and video coding plays a significant role. In many applications, sceneries such as broadcast services over satellite and terrestrial channels, digital video storage, wires and wireless conversational services and digital video communication are used. It is not possible to store full digital video without processing. Video compression reduces the data used to represent digital video images and is a combination of spatial image compression and temporal motion compensation. The main aim of our research is to develop an efficient video compression system. The proposed system consists of three steps. At first, wavelet decomposition is applied to the I–frame and the resulting coefficients are quantised using the listless SPECK (LSK) algorithm. In the next step, motion estimation is done using adaptive rood search with the spatio–temporal correlation method (ARS–ST) and it calculates the distance between the P–frame and I–frame blocks. In the final step, the difference between the original and the predicted P–frame is evaluated and is known as residual. To produce good quality predictive frames, this residual is transmitted along with motion vectors (MVs) and to decrease its size, it is coded using LSK. The proposed video compression technique uses different videos for assessment and by determining the PSNR values, compression efficiency is estimated. By comparing the proposed system with existing systems, it is seen that our system effectively compresses videos with remarkable PSNR measurements.
79B9874E	We combine predictive hexagonal pattern and partial distortion searches with the recently proposed constrained one-bit transform-based motion estimation scheme to reduce the computational load of the motion estimation process. Furthermore, the kernel used to obtain the one-bit images is simplified. Experimental results show significant reduction of the number of average search points, with only a slight loss in motion estimation accuracy.
7AC9CBBA	We introduce a variable block size motion estimation architecture that is adaptive to the full search (FS) and the three-step search (3SS) algorithms. Early termination, intensive data reuse, pipelined datapath with bit serial execution, and memory access management tailored to the search patterns of the FS and 3SS form key features of the architecture. The design was synthesized using Synopsys Design Compiler and 45nm standard cell library technology. The architecture sustains real-time CIF format with an operational frequency as low as 17.6MHz and consumes 1.98 mW at this clock rate. This architecture with its 500MHz peak operational frequency provides the end-user with the flexibility of choosing between video quality and throughput based on power consumption and processing speed constraints.
7E02C748	In the search for lower bit rate image compression and representation, a new video motion estimation technique (VMET), that considers video object translation, as well as rotation, and planar multilayering, is described. This new concept uses a modified multipopulation coevolutionary genetic algorithm (MMCGA), that receives the video objects of segmented reference images, and outputs the corresponding motion and layer information, using object and layer genotypes. Genetic operation strategies of reproduction, crossover, mutation, and dominance are applied recurrently in order to create successive generations of genomes with much better fitness, until convergence, or the maximum allowed number of generations is reached. For the increase of prediction accuracy and convergence speed, a lifetime fitness strategy is used. Simulations with synthetic images have shown very encouraging results with the proposed video motion estimation technique, which competes favorably with respect to the conventional algorithms in accuracy, effectiveness, robustness, simplicity and speed.
8009AFA1	Superresolution is the process of combining information from multiple subpixel-shifted low-resolution images to form a high-resolution image. It works quite well under ideal conditions but deteriorates rapidly with inaccuracies in motion estimates. We model the original high-resolution image as a Markov random field (MRF) with a discontinuity adaptive regularizer. Given the low-resolution observations, an estimate of the superresolved image is obtained by using the iterated conditional modes (ICM) algorithm, which maximizes the local posterior conditional probability sequentially. The proposed method not only preserves edges but also lends robustness to errors in the estimates of motion and blur parameters. We derive theoretically the neighborhood structure for the posterior distribution in the presence of warping, blurring, and downsampling operations and use this to effectively reduce the overall computations. Results are given on synthetic as well as real data to validate our method.
7D6AA12C	We present an adaptive quarter-pel (Qpel) motion estimation (ME) method for H.264/AVC. Instead of applying Qpel ME to all macroblocks (MBs), the proposed method selectively performs Qpel ME in an MB level. In order to reduce the bit rate, we also propose a motion vector (MV) encoding technique that adaptively selects a different variable length coding (VLC) table according to the accuracy of the MV. Experimental results show that the proposed method can achieve about 3% average bit rate reduction.
76899030	In video coding, research is focused on the development of fast motion estimation (ME) algorithms while keeping the coding distortion as small as possible. It has been observed that the real world video sequences exhibit a wide range of motion content, from uniform to random, therefore if the motion characteristics of video sequences are taken into account before hand, it is possible to develop a robust motion estimation algorithm that is suitable for all kinds of video sequences. This is the basis of the proposed algorithm. The proposed algorithm involves a multistage approach that includes motion vector prediction and motion classification using the characteristics of video sequences. In the first step, spatio-temporal correlation has been used for initial search centre prediction. This strategy decreases the effect of unimodal error surface assumption and it also moves the search closer to the global minimum hence increasing the computation speed. Secondly, the homogeneity analysis helps to identify smooth and random motion. Thirdly, global minimum prediction based on unimodal error surface assumption helps to identify the proximity of global minimum. Fourthly, adaptive search pattern selection takes into account various types of motion content by dynamically switching between stationary, center biased and, uniform search patterns. Finally, the early termination of the search process is adaptive and is based on the homogeneity between the neighboring blocks.Extensive simulation results for several video sequences affirm the effectiveness of the proposed algorithm. The self-tuning property enables the algorithm to perform well for several types of benchmark sequences, yielding better video quality and less complexity as compared to other ME algorithms. Implementation of proposed algorithm in JM12.2 of H.264/AVC shows reduction in computational complexity measured in terms of encoding time while maintaining almost same bit rate and PSNR as compared to Full Search algorithm.
7013E392	A cooperative motion estimation (ME) scheme using a modified Particle Swarm Optimization (PSO) algorithm is presented. The proposed algorithm is based on a multi-swarm PSO model where a swarm of PSO particles is defined for each macroblock (MB) in the frame. Motion estimation is then performed in a cooperative manner concurrently for all the MBs in the frame. Cooperation between neighboring MBs during the motion estimation process is allowed through a communication step to exchange information about the motion vectors found so far in the estimation process. This synergic relationship between the swarms of adjacent MBs allows refining the motion search and leads to both a faster convergence of the PSO process and an improvement in the resulting motion vectors. Several techniques are also proposed to improve the search capacity and computational complexity of the PSO iterations. A novel PSO initialization scheme that exploits the existing temporal correlation is proposed to remove dependency between adjacent MBs. A fitness function history preservation mechanism is also presented to prevent redundant repeated calculations of the fitness function of a given search point by the PSO particles which dramatically decreases the computational complexity. Moreover, the maximum allowed velocity of the particles is adaptively varied during the PSO iterative process which provides a balance between search exploration and exploitation. The proposed scheme exhibits a high level of data parallelism since it is capable of performing motion estimation for all the MBs of the frame in parallel rather than serially. As a result, the presented algorithm is amenable to parallel processing techniques. In this paper, a multicore implementation of our proposed algorithm is performed using the MATLAB® Parallel Computing Toolbox™ (PCT). Extensive simulations are performed to analyze the performance of the presented algorithm. It is found that the presented scheme provides improvements in terms of accuracy and computational complexity as compared to conventional fast motion estimation techniques and two state-of-the-art PSO-based ME schemes. An analysis of the parallel performance shows that the presented scheme is highly scalable and that the parallel efficiency increases with the increase in video resolution. The multicore implementation of the proposed algorithm using MATLAB could achieve a speedup of 6.21 on eight CPU cores for high-definition (HD) video sequences. The multicore performance of the proposed scheme is also compared with existing parallel algorithms in the literature and is shown to give superior results.
0BE47B0F	Because motion estimation represents a major computational load in typical vide encoding systems, there has been extensive research into fast motion estimation techniques. Given the nature of the process, two major classes of complexity reduction techniques have been proposed. These seek to speed up search times by (i) reducing the cost of each matching operation or (ii) reducing the number of points considered in the search region. In fast matching (FM) techniques, a typical approach is to compute the cost function based on a subset of pixels in a block. In fast search (FS) approaches, the complexity reduction comes from restricting the number of points in the search region, based on fixed rules or on initialization based on motion vectors already computed for other blocks or the previous frame. In this paper we use as a baseline algorithm the initialize- refine technique which belongs to the FS class. We concentrate on the case of real time software video encoding, which allows the flexibility of using variable complexity algorithms. Thus, we modify our baseline algorithm using a Lagrange multiplier approach similar to that of which allows us to explicitly take into account the trade-offs between search complexity and residual frame energy. Furthermore, we combine this algorithm with a novel fast matching method for SAD estimation which allows us to estimate the SAD based on successive subsets of pixels in a particular block. This method naturally possesses computational scalability because we can stop the framework and gives us one more degree of freedom to control the complexity/residual energy trade-off. We show that the combined algorithm achieves reductions of around 25 percent in computation time with respect to the original algorithm without SAD estimation. These results are further improved by designing a test structure that is optimized for typical sequences and where test for an early termination of the matching process are only included if they are though to be worthwhile in terms of the overall complexity. 
75C8DB1C	This paper proposes a video encoder with improved visual quality achieved through more accurate motion estimation and perceptual quantization. A quadtree-based variable size block motion estimation scheme jointly optimizes both motion vector coding and residual coding by minimizing the actual rate subject to a constraint on overall distortion. The residual coder includes perceptual quantization and direct block coding. Since the residual is encoded directly without any transform, the encoder achieves comparable speed with MPEG. The encoded video outperforms that of MPEG in terms of both PSNR and visual quality.
7DCE018B	Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case.
7AA5BE88	We  consider  the  generic  regularized  optimization  problem. Efron, Hastie, Johnstone and ibshirani [Ann.Statist.32(2004) 407–499] have shown that for the LASSO—that is, if L is squared error loss and J 1  norm of the optimal coefficient path is piecewise linear, that is,   is piecewise constant. We derive a general characterization of the properties of (loss L ,  enalty J )pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation  of   he  full  regularized  coefficient  paths.  We  investigate  the  nature of efficient path following  lgorithms which arise. We use our results to suggest robust versions of the LASSO for regression and  lassification, and to develop new, efficient algorithms for existing problems in the literature, including  ammen and van de Geer’s locally adaptive regression splines.
79AAFCF5	The application of the method of fundamental solutions to the Cauchy problem in three-dimensional isotropic linear elasticity is investigated. The resulting system of linear algebraic equations is ill-conditioned and therefore, its solution is regularized by employing the first-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both under- and equally-determined Cauchy problems in a piece-wise smooth geometry. The convergence, accuracy, and stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
815F61B2	Speech production errors characteristic of dysarthria are chiefly responsible for the low accuracy of automatic speech recognition (ASR) when used by people diagnosed with it. A person with dysarthria produces speech in a rather reduced acoustic working space, causing typical measures of speech acoustics to have values in ranges very different from those characterizing unimpaired speech. It is unlikely then that models trained on unimpaired speech will be able to adjust to this mismatch when acted on by one of the currently well-studied adaptation algorithms (which make no attempt to address this extent of mismatch in population characteristics).In this work, we propose an interpolation-based technique for obtaining a prior acoustic model from one trained on unimpaired speech, before adapting it to the dysarthric talker. The method computes a ‘background’ model of the dysarthric talker's general speech characteristics and uses it to obtain a more suitable prior model for adaptation (compared to the speaker-independent model trained on unimpaired speech). The approach is tested with a corpus of dysarthric speech acquired by our research group, on speech of sixteen talkers with varying levels of dysarthria severity (as quantified by their intelligibility). This interpolation technique is tested in conjunction with the well-known maximum a posteriori (MAP) adaptation algorithm, and yields improvements of up to 8% absolute and up to 40% relative, over the standard MAP adapted baseline.
7B1E0639	In this paper, the application of the method of fundamental solutions to the Cauchy problem associated with two-dimensional Helmholtz-type equations is investigated. The resulting system of linear algebraic equations is ill-conditioned and therefore its solution is regularized by employing the first-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both smooth and piecewise smooth geometries. The convergence and the stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
79DB33C5	Widespread use of large-vocabulary continuous speech recognition systems has recently occurred, encouraging the application of speech recognition techniques to various problems. One of the factors that adversely affect the performance of speech recognition systems is a mismatch between the acoustic properties of the speech of the system user and the acoustic model. The speech of young or middle-aged adults is generally used in constructing the acoustic model. Thus, a mismatch occurs between the model and the acoustic properties of the speech of the elderly, which may degrade the recognition rate. In this study, a large-scale elderly speech database (200 sentences ×301 subjects) is used to train the acoustic model, and the resulting elderly acoustic model is evaluated by using a large-vocabulary continuous speech recognition system. In the experiments, the word recognition rate was improved by 3 to 5% compared to the recognition results of an acoustic model trained by young or middle-aged adult speech, namely, by the JNAS speech database (150 sentences ×260 subjects, average 28.6 years). It is also verified experimentally that the recognition rate is further improved in speaker adaptation to elderly speech by making use of an acoustic model trained by elderly speech.
7E7C9F6F	Cochlear implants can provide partial restoration of hearing, even with limited spectral resolution and loss of fine temporal structure, to severely deafened individuals. Studies have indicated that background noise has significant deleterious effects on the speech recognition performance of cochlear implant patients. This study investigates the effects of noise on speech recognition using acoustic models of two cochlear implant speech processors and several predictive signal-processing-based analyses. The results of a listening test for vowel and consonant recognition in noise are presented and analyzed using the rate of phonemic feature transmission for each acoustic model. Three methods for predicting patterns of consonant and vowel confusion that are based on signal processing techniques calculating a quantitative difference between speech tokens are developed and tested using the listening test results. Results of the listening test and confusion predictions are discussed in terms of comparisons between acoustic models and confusion prediction performance.
75C78FFC	Up to now, more and more online sites have started to allow their users to build the social relationships. Take the Last.fm for example (which is a popular music-sharing site), users can not only add each other as friends, but also join online interest groups where they shall meet people with common tastes. Therefore, in this environment, users might be interested in not only receiving item recommendations (such as music), but also getting friend suggestions so they might put them in the contact list, and group recommendations that they could consider joining. To support such demanding needs, in this paper, we propose a unified framework that provides three different types of recommendation in a single system: recommending items, recommending groups and recommending friends. For each type of recommendation, we in depth investigate the contribution of fusing other two auxiliary information resources (e.g., fusing friendship and membership for recommending items, and fusing user-item preferences and friendship for recommending groups) for boosting the algorithm performance. More notably, the algorithms were developed based on the matrix factorization framework in order to achieve the ideal efficiency as well as accuracy. We performed experiments with two large-scale real-world data sets that contain users’ implicit interaction with items. The results revealed the effective fusion mechanism for each type of recommendation in such implicit data condition. Moreover, it demonstrates the respective merits of regularization model and factorization model: the factorization is more suitable for fusing bipartite data (such as membership and user-item preferences), while the regularization model better suits one mode data (like friendship). We further enhanced the friendship’s regularization by integrating the similarity measure, which was experimentally proven with positive effect.
7F8267B3	The regularization parameter choice is a fundamental problem in Learning Theory since the performance of most supervised algorithms crucially depends on the choice of one or more of such parameters. In particular a main theoretical issue regards the amount of prior knowledge needed to choose the regularization parameter in order to obtain good learning rates. In this paper we present a parameter choice strategy, called the balancing principle, to choose the regularization parameter without knowledge of the regularity of the target function. Such a choice adaptively achieves the best error rate. Our main result applies to regularization algorithms in reproducing kernel Hilbert space with the square loss, though we also study how a similar principle can be used in other situations. As a straightforward corollary we can immediately derive adaptive parameter choices for various kernel methods recently studied. Numerical experiments with the proposed parameter choice rules are also presented.
7D7B6717	In subspace identification methods, the system matrices are usually estimated by least squares, based on estimated Kalman filter state sequences and the observed inputs and outputs. For a finite number of data points, the estimated system matrix is not guaranteed to be stable, even when the true linear system is known to be stable. In this paper, stability is imposed by using regularization. The regularization term used here is the trace of a matrix which involves the dynamical system matrix and a positive (semi) definite weighting matrix. The amount of regularization can be determined from a generalized eigenvalue problem. The data augmentation method of Chui and Maciejowski (1996) is obtained by using specific choices for the weighting matrix in the regularization term.
7ECE76F2	It is well known that there is a dynamic relationship between cerebral blood flow (CBF) and cerebral blood volume (CBV). With increasing applications of functional MRI, where the blood oxygen-level-dependent signals are recorded, the understanding and accurate modeling of the hemodynamic relationship between CBF and CBV becomes increasingly important. This study presents an empirical and data-based modeling framework for model identification from CBF and CBV experimental data. It is shown that the relationship between the changes in CBF and CBV can be described using a parsimonious autoregressive with exogenous input model structure. It is observed that neither the ordinary least-squares (LS) method nor the classical total least-squares (TLS) method can produce accurate estimates from the original noisy CBF and CBV data. A regularized total least-squares (RTLS) method is thus introduced and extended to solve such an error-in-the-variables problem. Quantitative results show that the RTLS method works very well on the noisy CBF and CBV data. Finally, a combination of RTLS with a filtering method can lead to a parsimonious but very effective model that can characterize the relationship between the changes in CBF and CBV.
8118E97E	In this paper we present a novel geometric framework called geodesic active fields for general image registration. In image registration, one looks for the underlying deformation field that best maps one image onto another. This is a classic ill-posed inverse problem, which is usually solved by adding a regularization term. Here, we propose a multiplicative coupling between the registration term and the regularization term, which turns out to be equivalent to embed the deformation field in a weighted minimal surface problem. Then, the deformation field is driven by a minimization flow toward a harmonic map corresponding to the solution of the registration problem. This proposed approach for registration shares close similarities with the well-known geodesic active contours model in image segmentation, where the segmentation term (the edge detector function) is coupled with the regularization term (the length functional) via multiplication as well. As a matter of fact, our proposed geometric model is actually the exact mathematical generalization to vector fields of the weighted length problem for curves and surfaces introduced by Caselles-Kimmel-Sapiro. The energy of the deformation field is measured with the Polyakov energy weighted by a suitable image distance, borrowed from standard registration models. We investigate three different weighting functions, the squared error and the approximated absolute error for monomodal images, and the local joint entropy for multimodal images. As compared to specialized state-of-the-art methods tailored for specific applications, our geometric framework involves important contributions. Firstly, our general formulation for registration works on any parametrizable, smooth and differentiable surface, including nonflat and multiscale images. In the latter case, multiscale images are registered at all scales simultaneously, and the relations between space and scale are intrinsically being accounted for. Second, this method is, to the best of our knowledge, the first reparametrization invariant registration method introduced in the literature. Thirdly, the multiplicative coupling between the registration term, i.e. local image discrepancy, and the regularization term naturally results in a data-dependent tuning of the regularization strength. Finally, by choosing the metric on the deformation field one can freely interpolate between classic Gaussian and more interesting anisotropic, TV-like regularization.
8113BCCD	This work proposes a subspace approach that regularizes and extracts eigenfeatures from the face image. Eigenspace of the within-class scatter matrix is decomposed into three subspaces: a reliable subspace spanned mainly by the facial variation, an unstable subspace due to noise and finite number of training samples, and a null subspace. Eigenfeatures are regularized differently in these three subspaces based on an eigenspectrum model to alleviate problems of instability, overfitting, or poor generalization. This also enables the discriminant evaluation performed in the whole space. Feature extraction or dimensionality reduction occurs only at the final stage after the discriminant assessment. These efforts facilitate a discriminative and a stable low-dimensional feature representation of the face image. Experiments comparing the proposed approach with some other popular subspace methods on the FERET, ORL, AR, and GT databases show that our method consistently outperforms others.
80794964	Estimating a reliable and stable solution to many problems in signal processing and imaging is based on sparse regularizations, where the true solution is known to have a sparse representation in a given basis. Using different approaches, a large variety of regularization terms have been proposed in literature. While it seems that all of them have so much in common, a general potential function which fits most of them is still missing. In this paper, in order to propose an efficient reconstruction method based on a variational approach and involving a general regularization term (including most of the known potential functions, convex and nonconvex), we deal with i) the definition of such a general potential function, ii) the properties of the associated “proximity operator” (such as the existence of a discontinuity), and iii) the design of an approximate solution of the general “proximity operator” in a simple closed form. We also demonstrate that a special case of the resulting “proximity operator” is a set of shrinkage functions which continuously interpolate between the soft-thresholding and hard-thresholding. Computational experiments show that the proposed general regularization term performs better than ℓp -penalties for sparse approximation problems. Some numerical experiments are included to illustrate the effectiveness of the presented new potential function.
8068D18D	In enclosed environments where robots are deployed, the observed speech signal is smeared due to reverberation. This degrades the performance of the automatic speech recognition (ASR). Thus, hands-free speech recognition for human-machine communication is a difficult task. Most speech enhancement techniques used to address this problem enhance the contaminated waveform independent from that of the ASR. However, this approach does not necessarily improve ASR performance. In this paper, we expand the conventional spectral subtraction-based (SS) technique to deal with reverberation. In our proposed approach, the dereverberation parameters of SS are optimized to improve the likelihood of the acoustic model and not just the waveform signal. The system is capable of adaptively fine-tuning these parameters jointly with acoustic model training for effective use in ASR application. We have experimented using real reverberant data collected from an operational robot. Moreover, we also evaluated with reverberant data corrupted with environmental and robot internal noise. Experimental results show that the proposed method significantly improves the recognition performance over conventional approach.	
2526A3A8	This paper describes a hands-free speech recognition technique based on acoustic model adaptation to reverberant speech. In hands-free speech recognition, the recognition accuracy is degraded by reverberation, since each segment of speech is affected by the reflection energy of the preceding segment. To compensate for the reflection signal we introduce a frame-by-frame adaptation method adding the reflection signal to the means of the acoustic model. The reflection signal is approximated by a first-order linear prediction from the observation signal at the preceding frame, and the linear prediction coefficient is estimated with a maximum likelihood method by using the EM algorithm, which maximizes the likelihood of the adaptation data. Its effectiveness is confirmed by word recognition experiments on reverberant speech.
24D39B30	Acoustic modeling in speech recognition uses very little knowledge of the speech production process. At many levels our models continue to model speech as a surface phenomenon. Typically, hidden Markov model (HMM) parameters operate primarily in the acoustic space or in a linear transformation thereof; state-to-state evolution is modeled only crudely, with no explicit relationship between states, such as would be afforded by the use of phonetic features commonly used by linguists to describe speech phenomena, or by the continuity and smoothness of the production parameters governing speech. This survey article attempts to provide an overview of proposals by several researchers for improving acoustic modeling in these regards. Such topics as the controversial Motor Theory of Speech Perception, work by Hogden explicitly using a continuity constraint in a pseudo-articulatory domain, the Kalman filter based Hidden Dynamic Model, and work by many groups showing the benefits of using articulatory features instead of phones as the underlying units of speech, will be covered.
1881F04A	In recent years, the number of studies investigating new directions in speech modeling that goes beyond the conventional HMM has increased considerably. One promising approach is to use Bayesian Networks (BN) as speech models. Full recognition systems based on Dynamic BN as well as acoustic models using BN have been proposed lately. Our group at ATR has been developing a hybrid HMM/BN model, which is an HMM where the state probability distribution is modeled by a BN, instead of commonly used mixtures of Gaussian functions. In this paper, we describe how to use the hybrid HMM/BN acoustic models, especially emphasizing some design and implementation issues. The most essential part of HMM/BN model building is the choice of the state BN topology. As it is manually chosen, there are some factors that should be considered in this process. They include, but are not limited to, the type of data, the task and the available additional information. When context-dependent models are used, the state-level structure can be obtained by traditional methods. The HMM/BN parameter learning is based on the Viterbi training paradigm and consists of two alternating steps - BN training and HMM transition updates. For recognition, in some cases, BN inference is computationally equivalent to a mixture of Gaussians, which allows HMM/BN model to be used in existing decoders without any modification. We present two examples of HMM/BN model applications in speech recognition systems. Evaluations under various conditions and for different tasks showed that the HMM/BN model gives consistently better performance than the conventional HMM.
7B1AA3CE	This paper proposes a new spatial regularization of Fisher linear discriminant analysis (LDA) to reduce the overfitting due to small size sample (SSS) problem in face recognition. Many regularized LDAs have been proposed to alleviate the overfitting by regularizing an estimate of the within-class scatter matrix. Spatial regularization methods have been suggested that make the discriminant vectors spatially smooth, leading to mitigation of the overfitting. As a generalized version of the spatially regularized LDA, the proposed regularized LDA utilizes the non-uniformity of spatial correlation structures in face images in adding a spatial smoothness constraint into an LDA framework. The region-dependent spatial regularization is advantageous for capturing the non-flat spatial correlation structure within face image as well as obtaining a spatially smooth projection of LDA. Experimental results on public face databases such as ORL and CMU PIE show that the proposed regularized LDA performs well especially when the number of training images per individual is quite small, compared with other regularized LDAs.
81717C8A	The research of Mongolian speech recognition technology start comparatively late and it is still in its primary stage. In this paper, we optimized the basic resources of Mongolian speech recognition system, and we also improved the acoustic model of Mongolian speech recognition system, and this is most important. In this paper, we realized continuous HMM Gaussian mixture model and multiple data stream SCHMM model on the basis of context dependent phonetic model and decision tree method. And we compared the two models in performances. Finally, a large quantity of experiments have been taken to the testing set with HTK as an experimental platform by applying trigram language model and acoustic model which is composed of context dependent phonetic model, decision tree method and multiple data stream SCHMM model. We found system performance has been optimized, and system recognition accuracy rates of word and sentence have been greatly improved.
7EB6E8D3	Theoretical results related to properties of a regularized recursive algorithm for estimation of a high dimensional vector of parameters are presented and proved. The recursive character of the procedure is proposed to overcome the difficulties with high dimension of the observation vector in computation of a statistical regularized estimator. As to deal with high dimension of the vector of unknown parameters, the regularization is introduced by specifying a priori non-negative covariance structure for the vector of estimated parameters. Numerical example with Monte-Carlo simulation for a low-dimensional system as well as the state/parameter estimation in a very high dimensional oceanic model is presented to demonstrate the efficiency of the proposed approach.
84349968	The optimization problems associated with various regularization techniques for supervised learning from data (e.g., weight-decay and Tikhonov regularization) are described in the context of Reproducing Kernel Hilbert Spaces. Suboptimal solutions expressed by sparse kernel models with a given upper bound on the number of kernel computational units are investigated. Improvements of some estimates obtained in Comput. Manag. Sci., vol. 6, pp. 53-79, 2009 are derived. Relationships between sparseness and generalization are discussed.
7BD96528	A useful strategy to deal with complex classification scenarios is the “divide and conquer” approach. The mixture of experts (MoE) technique makes use of this strategy by jointly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weighs their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, in the case of high dimensional data. In general, subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using regularization. Experiments using artificial and real-world datasets provide evidence that the proposed method improves the classical MoE technique, in terms of accuracy and sparseness of the solution. Furthermore, our results indicate that the advantages of the proposed technique increase with the dimensionality of the data.
7CC97970	Training a classifier with good generalization capability is a major issue for pattern classification problems. A novel training objective function for Radial Basis Function (RBF) network using a localized generalization error model (L-GEM) is proposed in this paper. The localized generalization error model provides a generalization error bound for unseen samples located within a neighborhood that contains all training samples. The assumption of the same width for all dimensions of a hidden neuron in L-GEM is relaxed in this work. The parameters of RBF network are selected via minimization of the proposed objective function to minimize its localized generalization error bound. The characteristics of the proposed objective function are compared with those for regularization methods. For weight selection, RBF networks trained by minimizing the proposed objective function consistently outperform RBF networks trained by minimizing the training error, Tikhonov Regularization, Weight Decay or Locality Regularization. The proposed objective function is also applied to select center, width and weight in RBF network simultaneously. RBF networks trained by minimizing the proposed objective function yield better testing accuracies when compared to those that minimizes training error only.
79F45FF9	The  vast  majority  of  automatic  speech  recognition systems  use  Hidden  Markov  Models  (HMMs)  as  the  underlying acoustic  model.  Initially  these  models  were  trained  based  on the maximum likelihood criterion. Significant performance gains have been obtained by using discriminative training criteria, such as  maximum  mutual  information  and  minimum  phone  error. However,  the  underlying  acoustic  model  is  still  generative,  with the associated constraints on the state and transition probability distributions, and classification is based on Bayes’ decision rule. Recently, there has been interest in examining discriminative, or direct, models for speech recognition. This paper briefly reviews the  forms  of  discriminative  models  that  have  been  investigated. These  include  maximum  entropy  Markov  models,  hidden  con- ditional  random  fields  and  conditional  augmented  models.  The relationships between the various models and issues with applying them  to  large  vocabulary  continuous  speech  recognition  will  be discussed.	
804433CC	Regularization looks for an interpolating function which is close to the data and also "smooth". This function is obtained by minimizing an error functional which is the weighted sum of a "fidelity term" and a "smoothness term". However, using only one set of weights does not guarantee that this function will be the MAP estimate. One has to consider all possible weights in order to find the MAP function. Also, using only one combination of weights makes the algorithm very sensitive to the data. The solution suggested here is through the Bayesian approach: a probability distribution over all weights is constructed and all weights are considered when reconstructing the function or computing the expectation of a linear functional on the function space.
7CEE7EDE	In this paper, we propose a novel order-recursive training algorithm for kernel-based discriminants which is computationally efficient. We integrate this method in a hybrid HMM-based speech recognition system by translating the outputs of the kernel-based classifier into class-conditional probabilities and using them instead of Gaussian mixtures as production probabilities of a HMM-based decoder for speech recognition. The performance of the described hybrid structure is demonstrated on the DARPA resource management (RMI) corpus
7F8F53F9	In this paper, the method of fundamental solutions is applied to solve some inverse boundary value problems associated with the steady-state heat conduction in an anisotropic medium. Since the resulting matrix equation is severely ill-conditioned, a regularized solution is obtained by employing the truncated singular value decomposition, while the optimal regularization parameter is chosen according to the L-curve criterion. Numerical results are presented for both two- and three-dimensional problems, as well as exact and noisy data. The convergence and stability of the proposed numerical scheme with respect to increasing the number of source points and the distance between the fictitious and physical boundaries, and decreasing the amount of noise added into the input data, respectively, are analysed. A sensitivity analysis with respect to the measure of the accessible part of the boundary and the distance between the internal measurement points and the boundary is also performed. The numerical results obtained show that the proposed numerical method is accurate, convergent, stable and computationally efficient, and hence it could be considered as a competitive alternative to existing methods for solving inverse problems in anisotropic steady-state heat conduction.
828B43F4	We investigate both theoretically and numerically the so-called invariance property, see e.g. Sun and Ma (2015a,b), of the solution of boundary value problems associated with the anisotropic heat conduction equation (or Laplace–Beltrami’s equation) in two dimensions with respect to elementary transformations of the solution domain, e.g. dilations or contractions. We also show that the standard method of fundamental solutions (MFS) does not satisfy the invariance property. Motivated by these reasons, we introduce, in a natural manner, a modified version of the MFS that remains invariant under elementary transformations of the solution domain and is referred to as the invariant MFS (IMFS). Five two-dimensional examples are thoroughly investigated to assess the numerical accuracy, convergence and stability of the proposed IMFS, in conjunction with the Tikhonov regularization method (Tikhonov and Arsenin, 1986) and Morozov’s discrepancy principle (Morozov, 1966), for Laplace–Beltrami’s equation with perturbed boundary conditions.
7D4E550B	The application of the method of fundamental solutions to the Cauchy problem for steady-state heat conduction in two-dimensional functionally graded materials (FGMs) is investigated. The resulting system of linear algebraic equations is ill-conditioned and, therefore, regularization is required in order to solve this system of equations in a stable manner. This is achieved by employing the zeroth-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both smooth and piecewise smooth geometries. The convergence and the stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
7BD64644	The reconstruction of solutions in statistical inverse problems in Hilbert spaces requires regularization, which is often based on a parametrized family of proposal estimators. The choice of an appropriate parameter in this family is crucial. We propose a modification of the classical discrepancy principle as an adaptive parameter selection. This varying discrepancy principle evaluates the misfit in some weighted norm, and it also has an incorporated emergency stop. These ingredients allow the order optimal reconstruction when the solution owns nice spectral resolution. Theoretical analysis is accompanied with numerical simulations, which highlight the features of the proposed varying discrepancy principle.
770EB885	Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie for the selection of groups of correlated variables. To investigate on the statistical properties of this scheme and in particular on its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combination of elements ({\em features}) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ``{\em elastic-net representation}'' of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed by Zou and Hastie 
79F1432A	The aim of this article is to propose a procedure to cluster functional observations in a subspace of reduced dimension. The dimensional reduction is obtained by constraining the cluster centroids to lie into a subspace which preserves the maximum amount of discriminative information contained in the original data. The model is estimated by using penalized least squares to take into account the functional nature of the data. The smoothing is carried out within the clustering and its amount is adaptively calibrated. A simulation study shows how the combination of these two elements, feature-extraction and automatic data-driven smoothing, improves the performance of clustering by reducing irrelevant and redundant information in the data. The effectiveness of the proposal is demonstrated by an application to a real dataset regarding a speech recognition problem. Implementation details of the algorithm together with a computer code are available in the online supplements.
80F92BE9	We investigate the classical integrability of the Alday-Arutyunov-Frolov model, and show that the Lax connection can be reduced to a simpler 2 x 2 representation. Based on this result, we calculate the algebra between the L-operators and find that it has a highly non-ultralocal form. We then employ and make a suitable generalization of the regularization technique proposed by Maillet for a simpler class of non-ultralocal models, and find the corresponding r- and s-matrices. We also make a connection between the operator-regularization method proposed earlier for the quantum case, and the Maillet's symmetric limit regularization prescription used for non-ultralocal algebras in the classical theory
7FD65373	We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.
79F8471E	The proportional odds model (POM) is commonly used in regression analysis to predict the outcome for an ordinal response variable. The maximum likelihood estimation (MLE) approach is typically used to obtain the parameter estimates. The likelihood estimates do not exist when the number of parameters, p, is greater than the number of observations n. The MLE also does not exist if there are no overlapping observations in the data. In a situation where the number of parameters is less than the sample size but p is approaching to n, the likelihood estimates may not exist, and if they exist they may have quite large standard errors. An estimation method is proposed to address the last two issues, i.e. complete separation and the case when p approaches n, but not the case when p>n. The proposed method does not use any penalty term but uses pseudo-observations to regularize the observed responses by downgrading their effect so that they become close to the underlying probabilities. The estimates can be computed easily with all commonly used statistical packages supporting the fitting of POMs with weights. Estimates are compared with MLE in a simulation study and an application to the real data
7B37D69B	We consider efficient construction of nonlinear solution paths for general ℓ1-regularization. Unlike the existing methods that incrementally build the solution path through a combination of local linear approximation and recalibration, we propose an efficient global approximation to the whole solution path. With the loss function approximated by a quadratic spline, we show that the solution path can be computed using a generalized Lars algorithm. The proposed methodology avoids high-dimensional numerical optimization and thus provides faster and more stable computation. The methodology also can be easily extended to more general regularization framework. We illustrate such flexibility with several examples, including a generalization of the elastic net and a new method that effectively exploits the so-called “support vectors” in kernel logistic regression.
7D3C2BD7	In this study , we introduce a path-follo wing algorithm for L 1 regularized general- ized linear mo dels. The L 1 regularization pro cedure is useful esp ecially because it, in e ect, selects variables according to the amoun t of penalization on the L 1 norm of the coecien ts, in a manner less greedy than forw ard selection/bac kward deletion. The GLM path algorithm ecien tly computes solutions along the entire regularization path using the predictor-corrector metho d of con vex-optimization. Selecting the step length of the regularization parameter is critical in con trolling the overall accuracy of the paths; we suggest intuitiv e and  exible strategies for choosing appropriate values. We demonstrate the implemen tation with several sim ulated and real datasets.
799E0C6C	Multimedia event detection (MED) is one of the most important branches of multimedia content analysis. Current research work on MED focuses mainly on detecting specific events, such as sport events, news events and suspicious events, which is far from achieving a complicated and generic MED due to the fact that these events usually contain a lot of visual attributes, such as objects, scenes and human actions. Being different from visual features, visual attributes are hidden classes to event detectors and event classifiers. Hence, proper representation of these visual attributes could be helpful in building a sophisticated and generic MED. In this paper, we use Gaussian mixture model (GMM) for representing video events with the motivation that the individual component densities of GMM could model some underlying hidden visual attributes and propose a ℓ2-regularized logistic Gaussian mixture regression approach, which is also called LLGMM classifier, for a more generic and complicated MED. We also propose an efficient iterative algorithm, which uses gradient descent, a standard convex optimization method, to solve the objective function of LLGMM. Finally, extensive experiments are conducted on the challenging TRECVID MED 2012 development dataset. The results demonstrate the effectiveness of the proposed LLGMM classifier for MED.
7881B8BF	This paper proposes a regularized locality preserving discriminant analysis (RLPDA) approach for facial feature extraction and recognition. The RLPDA approach decomposes the eigenspace of the locality preserving within-class scatter matrix into three subspaces, i.e., the face space, the noise space and the null space, and then regularizes the three subspaces differently according to their predicted eigenvalues. As a result, the proposed approach integrates discriminative information in all of the three subspaces, de-emphasizes the effect of the eigenvectors corresponding to the small eigenvalues, and meanwhile suppresses the small sample size problem. Extensive experiments on ORL face database, FERET face subset and UMIST face database illustrate the effectiveness of the proposed approach.
7FC0C78E	It is well-known that the applicability of linear discriminant analysis (LDA) to high-dimensional pattern classification tasks such as face recognition (FR) often suffers from the so-called "small sample size" (SSS) problem arising from the small number of available training samples compared to the dimensionality of the sample space. In this paper, we propose a new LDA method that effectively addresses the SSS problem using a regularization technique. In addition, a scheme of expanding the representational capacity of the face database is introduced to overcome the limitation that the LDA based algorithms require at least two samples per class available for learning. Extensive experimentation performed on the FERET database indicates that the proposed methodology outperforms traditional methods such as eigenfaces and direct LDA in a number of SSS setting scenarios.
7DA92E38	It is well-known that the applicability of both linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) to high-dimensional pattern classification tasks such as face recognition (FR) often suffers from the so-called “small sample size” (SSS) problem arising from the small number of available training samples compared to the dimensionality of the sample space. In this paper, we propose a new QDA like method that effectively addresses the SSS problem using a regularization technique. Extensive experimentation performed on the FERET database indicates that the proposed methodology outperforms traditional methods such as Eigenfaces, direct QDA and direct LDA in a number of SSS setting scenarios.
7B0C0D3F	We discuss the low-energy analysis of models involving quarks and four-fermion couplings. The relation with QCD and with other models of mesons and meson plus quarks at low energies is discussed. A short description of how the heat-kernel expansion can be used to get regularization independent information, is given
7AB5539E	In this paper we present a new algorithm to recover a sparse signal from a noisy register. The algorithm assumes a new prior distribution for the sparse signal that consists of a mixture of a narrow and a broad Gaussian both with zero mean. A penalty term which favors solutions driven from this model is added to the usual error cost function and the resultant global cost function is minimized by means of a gradient-type algorithm. A condition is derived for the step-size parameter in order to ensure convergence. In the paper we also propose a method (based on the Expectation-Maximization algorithm) to update the mixture parameters. The estimation of the sparse signal and the optimization of the Gaussian mixture are combined in the proposed algorithm: in each iteration a new signal estimate and a new model (which approximates the distribution of the new estimate) are obtained. In this way, the proposed method can be used without any statistical knowledge about the signal. Simulation experiments show that the accuracy of the proposed method is competitive with classical statistical detectors with a lower computational load.
7B2D3282	In probabilistic speech recognition it is often interesting to evaluate the contribution of the language model and that of the acoustic model. We propose an information theoretical approach which takes into account the interaction between the two sources of information. Experimental results are presented concerning the IBM prototype real-time recognizer of the Italian language based on a 20,000-word vocabulary.
7685C69E	Traditional adaptive beamformers have robustness just for specific error condition. They suffer performance degradation in the presence of multiple errors such as sample covariance matrix estimation error and steering vector mismatch. In this article, a new robust adaptive beamforming algorithm based on jointly estimating the covariance matrix and steering vector mismatch is proposed to overcome both the problems of sample covariance errors and steering vector mismatch. The theoretical covariance matrix is estimated based on the shrinkage method. Subsequently, the difference between the actual and presumed steering vectors is estimated in the sense of that the output signal-to noise plus interference ratio (SINR) is maximized and then is used to obtain the actual steering vectors. The proposed algorithm is preferable to traditional ones in the condition of multiple errors. Both simulation results and performance analysis are presented that illustrated the effectiveness and superiority of the proposed method. 	
7D48EFBF	It is important for intelligent robots to detect the location of user even though the user is not captured by vision cameras. Sound source localization is a technique to detect the direction of sound source using multiple microphones. It enables the robot to respond to the user's call. Sometimes, a robot operates in echoic room, but the performance of sound source localization in echoic room is very poor. In this work, a verification method for reverberation is proposed and tested how much it improves the performance of sound source localization in reverberant environment.
793F7FB3	This paper introduces the reader to several recent developments in worst-case identification motivated by various issues of modelling of systems from data for the purpose of robust control design. Many aspects of identification in and ℓ1 are covered including algorithms, convergence and divergence results, worst-case estimation of uncertainty models, model validation and control relevancy issues.
78C2C88B	In this paper, we study the problem of accurately estimating the location of indoor wireless devices which, due to its vast application areas, has continued to generate much interest both in the academia and industry. Specifically, we consider the location estimation problem and develop non-hybrid Bayesian location estimators for when received signal strength (RSS) and/or time of arrival (TOA) measurements between the target node (TN) and designated beacon nodes (BNs) in the network are obtainable. However, RSS- and TOA-based location estimators can be inaccurate when the RF-characteristics are not stable and time synchronization between the TN and BNs is not set up properly, respectively. To mitigate the disadvantages of these two location estimators, we propose the hybrid Bayesian location estimators. We show, through the results of extensive simulation experiments conducted that in comparison with the non-hybrid estimators, the hybrid Bayesian location estimators are more robust in the localization environment where RSS/TOA measurement precision varies.
784358DD	The identification of the signal subspace is a very important first step for most hyperspectral algorithms. In this letter, we investigate the important problem of identifying the hyperspectral signal subspace by minimizing the mean squared error (MSE) between the true signal and an estimate of the signal. Since it is dependent on the true signal, the MSE is uncomputable in practice, and so we propose a method based on Stein's unbiased risk estimator that provides an unbiased estimate of the MSE. The resulting method is simple and fully automatic, and we evaluate it using both simulated and real hyperspectral data sets. Experimental results show that our proposed method compares well to recent state-of-the-art subspace identification methods.
76C7F9B8	A single-channel blind dereverberation algorithm is proposed in this letter for distant-talking speech recognition. The proposed method is based on spectral subtraction (SS) method, in which the spectrum of a late reverberant signal is estimated using a delayed and attenuated version of the reverberant signal. Through some assumptions, the conventional SS method regards the attenuation weight as a constant that is a function of reverberation time. However, these assumptions are not valid in real situations, and the ideal weight varies with the frame. Therefore, in the proposed method, the variable weight sequence is estimated using Viterbi decoding scheme based on the reverberation model. This weight sequence is then substituted for the fixed weight in the conventional SS method without explicitly estimating the reverberation time. The proposed method performs better than the conventional SS method in both isolated word recognition and connected digit recognition experiments in reverberant environments.
80E14FD9	We have developed a time-varying, parallel- cascade system identification algorithm to separate joint stiffness into intrinsic and reflex components at each point in time throughout rapid movements. The components are identified using an iterative algorithm in which intrinsic and reflex dynamics are identified using separate time-varying (TV) techniques based on ensemble methods. An ensemble of input-output records having the same TV behavior is acquired and used to identify the system dynamics as impulse response functions at time increments corresponding to the sampling interval. Simulation studies showed that the time-varying, parallel-cascade algorithm performed well under realistic conditions with 99.9% VAF between simulated and predicted torque. To evaluate the performance of the algorithm under realistic conditions we applied it to an ensemble of experimental data acquired under stationary conditions. Results demonstrated that the TV estimates converged to those of the established time-invariant algorithm and allowed us to determine how variance of the TV estimates varied with the number of realizations in the ensemble.
80B8BE55	Myoelectric signals (MESs) from the speaker's mouth region have been successfully shown to improve the noise robustness of automatic speech recognizers (ASRs), thus promising to extend their usability in implementing noise-robust ASR. In the recognition system presented herein, extracted audio and facial MES features were integrated by a decision fusion method, where the likelihood score of the audio-MES observation vector was given by a linear combination of class-conditional observation log-likelihoods of two classifiers, using appropriate weights. We developed a weighting process adaptive to SNRs. The main objective of the paper involves determining the optimal SNR classification boundaries and constructing a set of optimum stream weights for each SNR class. These two parameters were determined by a method based on a maximum mutual information criterion. Acoustic and facial MES data were collected from five subjects, using a 60-word vocabulary. Four types of acoustic noise including babble, car, aircraft, and white noise were acoustically added to clean speech signals with SNR ranging from -14 to 31 dB. The classification accuracy of the audio ASR was as low as 25.5%. Whereas, the classification accuracy of the MES ASR was 85.2%. The classification accuracy could be further improved by employing the proposed audio-MES weighting method, which was as high as 89.4% in the case of babble noise. A similar result was also found for the other types of noise.
8038EDA0	Ultra-wideband (UWB) communication is envisaged to be deployed in indoor environments, where the noise distribution is decidedly non-Gaussian. A critical challenge for impulse radio UWB is the synchronization of nanosecond-long pulses. In this paper, we propose two acquisition schemes which are robust to uncertainties in the noise distribution and which operate directly on samples taken at symbol-rate. The only channel state information that the algorithms need can be obtained by estimation of an aggregate channel gain, avoiding channel-related complications. Following Huber's M-estimates for the Gaussian mixture noise model, the proposed robust acquisition systems outperform their traditional counterparts designed according to the Gaussian noise assumption. Necessary modifications for operation under multiple access interference are also introduced. Theoretical and simulation-based performance evaluations that reflect the asymptotic variance, normalized mean-square error and the bit error rates demonstrate the gains offered by the robust procedures.
779236C5	Novel theoretical probability density functions (PDF) of electromagnetic fields inside reverberation chambers operating in a “good-but-imperfect” regime have been recently reported. The present work reports on the application and assessment of these PDFs using a non-conventional type of reverberation chamber, namely the Vibrating Intrinsic Reverberation Chamber (VIRC). A vector network analyzer was used in order to measure the complex field components. An electrically short dipole antenna was used as a receiving antenna. Five thousand frequency points were taken ranging from 200MHz (undermoded regime) to 4 GHz (overmoded regime), so one measurement every 760 kHz was performed. For each frequency, 200 samples of the real and imaginary part of the field were measured. Measurements confirm the fact that the novel PDFs are able to describe the occurrence of anomalous statistics in the VIRC.
7F433A10	In this work we present a comparative analysis of the performance of two recently proposed algorithms for signal subspace identification (SSI) and dimensionality reduction (DR) in hyperspectral data. Such SSI algorithms are robust to the presence of rare signal components and are particularly suitable when DR is adopted as a pre-processing step in small target detection applications.
783093ED	Room reverberation poses various deleterious effects on performance of automatic speech systems. Speaker identification (SID) performance, in particular, degrades rapidly as reverberation time increases. Reverberation causes two forms of spectro-temporal distortions on speech signals: i) self-masking which is due to early reflections and ii) overlap-masking which is due to late reverberation. Overlap-masking effect of reverberation has been shown to have a greater adverse impact on performance of speech systems. Motivated by this fact, this study proposes a blind spectral weighting (BSW) technique for suppressing the reverberation overlap-masking effect on SID systems. The technique is blind in the sense that prior knowledge of neither the anechoic signal nor the room impulse response is required. Performance of the proposed technique is evaluated on speaker verification tasks under simulated and actual reverberant mismatched conditions. Evaluations are conducted in the context of the conventional GMM-UBM as well as the state-of-the-art i-vector based systems. The GMM-UBM experiments are performed using speech material from a new data corpus well suited for speaker verification experiments under actual reverberant mismatched conditions, entitled MultiRoom8. The i-vector experiments are carried out with microphone (interview and phonecall) data from the NIST SRE 2010 extended evaluation set which are digitally convolved with three different measured room impulse responses extracted from the Aachen impulse response (AIR) database. Experimental results prove that incorporating the proposed blind technique into the standard MFCC feature extraction framework yields significant improvement in SID performance under reverberation mismatch.
7F725D0F	With the advent and abundance of wireless devices and the confined spaces in which they are installed, the exposure dominance of the direct path and the scattered path needs to be studied. Wireless connectivity and reliability of wireless routers installed in industrial environments, deployment of RFID tags on concrete structures for structural monitoring, safe operation of various sensors coexisting in ad-hoc networks are examples of wireless systems that are encountered every day. The metallic nature of the installation environment reflects EM energy which can constructively or destructively aid the performance of wireless systems. When there is a line of sight between the transmitter and the receiver, the direct path seems to dominate and the received power varies as square of the distance. When the environment is reflective, the multipath contribution is dominant over the direct path even for small distances of separation. In this paper, a method to measure the distance at which the direct and the scattered path are equal is proposed and measurements in both frequency domain and time domain are presented.
67A8F1E7	In this paper, we propose a distant-talking speaker recognition method using a reverberation model with various artificial room impulse responses. These artificial room impulse responses with different speaker and microphone positions, room sizes, and reflection coefficients of walls and convoluted with clean speech are used to train an artificial reverberation speaker model. This artificial reverberation model is also combined with a reverberation speaker model trained with room impulse responses measured in real environments. Speaker identification performance using a combination of the two reverberation speaker models achieved a relative error reduction rate of 50.0% and 78.4% compared with that using a reverberation model trained with real-world room impulse responses and a clean speech model, respectively.
0189BE75	This report describes the research performed on behalf of the Office of Naval Research (grant # N00014-01-D-0043 Delivery Order 0007) on the following: The feasibility of enhancing active target detection using environmentally adaptive reverberation nulling techniques has been demonstrated with experimental data and related simulations.
8417C059	In this paper, we propose to address the moving average (MA) parameters estimation issue based only on noisy observations and without any knowledge on the variance of the additive stationary white Gaussian measurement noise. For this purpose, the MA process is approximated by a high-order AR process and its parameters are estimated by using an errors-in-variables (EIV) approach, which also makes it possible to derive the variances of both the driving process and the additive white noise. The method is based on the Frisch scheme. One of the main difficulties in this case is to evaluate the minimal AR-process order that must be considered to have a "good" approximation of the MA process. To this end, we propose a way based on K-means method. Simulation results of the proposed method are presented and compared to existing MA-parameter estimation approaches.
7D9E8BB7	This paper deals with the identification and estimation of dynamic games when players' beliefs about other players' actions are biased, i.e., beliefs do not represent the probability distribution of the actual behavior of other players conditional on the information available. First, we show that a exclusion restriction, typically used to identify empirical games, provides testable nonparametric restrictions of the null hypothesis of equilibrium beliefs. Second, we prove that this exclusion restriction, together with consistent estimates of beliefs at several points in the support of the special state variable (i.e., the variable involved in the exclusion restriction), is sufficient for nonparametric point-identification of players' payoffs and belief functions. The consistent estimates of beliefs at some points of support may come either from an assumption of unbiased beliefs at these points in the state space, or from available data on elicited beliefs for some values of the state variables. Third, we propose a simple two-step estimation method. We illustrate our model and methods using both Monte Carlo experiments and an empirical application of a dynamic game of store location by retail chains. The key conditions for the identification of beliefs and payoffs in our application are the following: (a) the previous year's network of stores of the competitor does not have a direct effect on the profit of a firm, but the firm's own network of stores at previous year does affect its profit because the existence of sunk entry costs and economies of density in these costs; and (b) firms' beliefs are unbiased in those markets that are close, in a geographic sense, to the opponent's network of stores, though beliefs are unrestricted, and potentially biased, for unexplored markets which are farther away from the competitors' network. Our estimates show significant evidence of biased beliefs. Furthermore, imposing the restriction of unbiased beliefs generates a substantial attenuation bias in the estimate of competition effects. 
780B9201	The space-frequency-polarization (SFP) beam-former using an electromagnetic vector sensor was studied in this paper. An improved robust beamformer algorithm was put forward which was based on Capon and joint constraint. As the three domains joint information was used, the SFP beamformer substantially outperforms any two-domain joint beamformer. Finally, Simulation results verified the effectiveness of the SFP beamformer and the SFP robust beamformer.
7375F18E	Multi-objective samples are powerful and versatile summaries of large data sets. For a set of keys and associated values , a weighted sample taken with respect to f allows us to approximate segment-sum statistics, for any subset H of the keys, with statistically-guaranteed quality that depends on sample size and the relative weight of H. When estimating Sum(g;H) for , however, quality guarantees are lost. A multi-objective sample with respect to a set of functions F provides for each  the same statistical guarantees as a dedicated weighted sample while minimizing the summary size.We analyze properties of multi-objective samples and present sampling schemes and meta-algortithms for estimation and optimization while showcasing two important application domains. The first are key-value data sets, where different functions f∈F applied to the values correspond to different statistics such as moments, thresholds, capping, and sum. A multi-objective sample allows us to approximate all statistics in F. The second is metric spaces, where keys are points, and each is defined by a set of points C with fx being the service cost of x by C, and Sum(f;X) models centrality or clustering cost of C. A multi-objective sample allows us to estimate costs for each. In these domains, multi-objective samples are often of small size, are efficiently to construct, and enable scalable estimation and optimization. We aim here to facilitate further applications of this powerful technique. 
5B25222B	Pedro Almodóvar's oeuvre encompasses a collection of themes and references to his own back catalogue, all the while referencing many other well-known directors. Broken Embraces (2009) refers to and re-enacts scenes from Women on the Verge of a Nervous Breakdown (1988). Yet the references to the latter in Broken Embraces fail to enchant, instead creating a fragmentation of the past and ultimately shifting from a breakdown to being simply broken. The relationship between these films demonstrates the foibles inherent in reconstructing films within films, which relates to Baudrillard's recognition that cinema is constantly plagiarizing, recopying and remaking itself.
5B91FE8D	The sidelobe canceller (SLC) is widely used for the array processing to suppress interferences located in the sidelobe of the radiation pattern. In the presence of steering vector mismatch, the performance of SLC may suffer from severe degradation due to desired components cancellation. In this paper, a modified approach is developed to improve the robustness against the direction of arrival (DOA) mismatch. The principle is to exclude the desired signal from auxiliary channels based on effective estimation of the steering vector. By expressing it as a linear combination of the basic vectors of a properly chosen orthogonal matrix, the steering vector is forced not to converge to any of interferences. Numerical simulations demonstrate the superiority of the modified method over the traditional SLC techniques.
16E86574	A practical model to compute shallow‐water boundary reverberation is described. Normal modes are used to calculate the acoustic energy propagating from the source to the scattering area, and from the scattering area to the receiver. At the scattering patch each mode is decomposed into up‐ and down‐going waves, then ray‐mode analogies and empirical scattering functions can be used to compute the scattered energy. The method was first described by Bucker and Morris [J. Acoust. Soc. Am. 44, 827–828 (1968)], and papers which first appeared in the Chinese literature. Their work is extended here by using group velocities to obtain the travel times for each mode pair, and by further developing the ray‐mode analogy. The effects of summing the modes coherently or incoherently and of including the time spreading due to the modal group velocities are examined. This paper deals with the range‐independent monostatic case, although the technique is extendible to bistatic geometries and range‐dependent environments. Calculations show excellent agreement with some ray‐based models, and, using the Lambert bottom‐scattering coefficient as the only adjustable parameter, good agreement is obtained with some measured shallow‐water reverberation.
5ED02184	Shallow water reverberation in the high sub-kHz to low-kHz range is modeled using ray theoretic algorithms. The algorithms assume multiple-forward/single-backscatter along with reciprocity in the propagation paths. The environments modeled are assumed to be range dependent in bathymetry, bottom loss, and bottom scattering strengths, and range independent water column, wind velocity, and surface scattering strengths. The model is capable of simulating bistatic, and monostatic geometries. The output of the model is reverberation as function of time for specified beams (either idealized look directions, or realizations of the beam pattern of an array). Comparisons of predicted reverberation versus time with measured data for shallow water sites off the south coast of the United States are presented.
76E8ECB3	To deal with the frequent failure of GPS in urban areas, vision-based localization methods such as visual odometry (VO) have been popular in recent years. However, VO still suffers from the problem of drift. In this paper, a novel Turning Point Filtering (TPF) algorithm is proposed to restrain the VO's drift by inducing the constraint from a concise road map. Different from the traditional road map based methods, we believe the simple “edge-node” road map is not enough to well model the true trajectory of the vehicle. Therefore our method does not enforce the corrected trajectory exactly on the edges of the map. A flexible turning point filtering mechanism is designed under a particle filter framework to well balance the information from the VO and the road map. The method features making reliable corrections only on the turning points of the trajectory, which adds little additional computation burden to VO. Experiments on various datasets including KITTI and the data acquired in our campus demonstrate the outperformance of our method.
792B8379	A robust 2-D vehicle-positioning system based on constraint propagation on a data horizon is proposed. Using asynchronous unreliable absolute positions and reliable proprioceptive measurements, the proposed method outputs the bounds of the pose at any desired rate. A bounded error approach is used to propagate the imprecision of the input data to the final pose estimation. Measurements are represented by intervals taking errors into account. Integrity is assumed on proprioceptive measurements error bounds. In opposition, the bounds set on exteroceptive positions are not required to be guaranteed. Indeed, a relaxed intersection of constraints is applied to achieve robustness, given a maximum ratio of erroneous positions. This approach is implemented for an automotive vehicle equipped with a GPS receiver and a CAN gateway. An real experimental validation is carried out with a dataset including wrong GPS measurements and the crossing of a tunnel.
7C037874	Learning-based super-resolution algorithms synthesize a high-resolution image based on learning patch pairs of low- and high-resolution images. However, since a low-resolution patch is usually mapped to multiple high-resolution patches, unwanted artifacts or blurring can appear in super-resolved images. In this paper, we propose a novel approach to generate a high quality, high-resolution image without introducing noticeable artifacts. Introducing robust statistics to a learning-based super-resolution, we efficiently reject outliers which cause artifacts. Global and local constraints are also applied to produce a more reliable high-resolution image. Experimental results demonstrate that the proposed algorithm can synthesize higher quality, higher-resolution images compared to the existing algorithms.
78BBB3FB	In this paper, we propose a new method based on a coherent source spectral estimation for solving the permutation problem of frequency-domain blind source separation (BSS). It combines the robustness of the State Coherence Transform (SCT) to recursively estimate a smooth phase spectrum associated with each source and the precision of the inter-frequency correlation to solve for a correct permutation. Namely, the TDOAs estimated by the SCT are used to constrain the permutation correction process in order to force the resulting filters to be coherent across frequency. This intrinsic interconnection between the TDOA information and the spectral correlation makes the new approach robust even when the signal is short in duration and spatial aliasing is substantial. Experimental results show that the proposed method is able to drastically reduce the number of permutation errors for three sources recorded in a short time block using microphones with large spacing.
78F75DBA	This paper presents a multisource sound localization method based on the generalized cross-correlation (GCC) method weighted by the phase transform (PHAT) and a novel multisource speech tracking method consisting of voice activity detection (VAD) and K-means clustering algorithm for binaural robot audition. The standard K-means clustering algorithm was improved for the purpose of multisource speech tracking by adding two additional steps. Experiments conducted on the SIG-2 humanoid robot in a real environment show that our method can track multiple speakers in real-time with tracking error below 4.35°.
7DFB72A7	The main goal of this paper is to describe and validate a specific hybrid dynamical model for NASA’s Recoverable Computer System subjected to simulated random upsets. The system is in closed-loop with a Boeing 737 simulation model which is flying at cruising altitude. The validation is based on a controlled experiment where the computer upsets are injected into the system at a desired rate, and the effects on the output tracking performance of a simulated aircraft are directly observed and quantified. The hybrid model consists of a jump-linear system driven by a finite-state machine with a Markov input. The jump-linear system models the switched dynamics of the closed-loop due to the presence of controller recoveries, the finite-state machine models the recovery logic, and the Markov input acts as an upset generator. Stability analysis is performed on the hybrid model using existing techniques. The paper supplies the additional performance theory for analyzing the tracking error mean output power. The results are compared against experimental data, showing good agreement.
8119272D	For cooperative force-reflecting teleoperation over networks, conventional passivity-based approaches have limited applicability due to nonpassive slave–slave interactions and irregular communication delays imposed by networks. In this paper, a small gain framework for stability analysis design of cooperative network-based force reflecting teleoperator systems is developed. The framework is based on a version of weak input-to-output practical stability (WIOPS) nonlinear small gain theorem that is applicable to stability analysis of large-scale network-based interconnections. Based on this result, we design a cooperative force-reflecting teleoperator system which is guaranteed to be stable in the presence of multiple network-induced communication constraints by appropriate adjustment of local control gains and/or force-reflection gains. Experimental results are presented that confirm the validity of the proposed approach.
800171EF	A finite element (FE) analysis of the field test excavation in Welland Clay is performed using an anisotropic soil behavior model. This paper describes the model, FE formulation, and transient effective stress stability analysis, and compares FE results with the field measurements. The analysis reflects the postexcavation decrease in the factor of safety with time and predicts the failure of the slope along the observed failure surface. The parametric study shows that the time to failure is a function of the pore pressure boundary conditions at the excavation surface and affects the transient factor of safety. Key words: constitutive relations, excavations, finite element, plasticity models, pore pressure, soil anisotropy, stability.
7D0A41E4	Numerical methods used for solving differential equations should be chosen with great care. Not considering numerical aspects such as stability, consistency and wellposed-ness results in erroneous solutions, which in turn will result in incorrect judgments. One of the most important aspects that should be considered is the stability of the numerical method. In this paper, we discuss stability problems of some of the so far proposed finite difference methods for solving the anisotropic diffusion equation, a second order parabolic equation. This equation is used in a variety of applications in physics and image processing. Here, we focus on its usage in formulating brain tumor growth using the Diffusion Weighted Imaging (DWI) technique. Our study shows that the commonly used chain rule method to discretize the diffusion equation is unstable. We propose a new 3D stable discretization method with its stability conditions to solve the diffusion equation. The new method uses directional discretization and forward differences. We also extend standard discretization method to 3D. The theoretical and practical comparisons of the three methods both on synthetic and real patient data show that while chain rule model is always unstable and standard discretization is unstable in theory, our proposed directional discretization is stable both in theory and practice.
7EF7F8EF	This paper presents a method of designing a modified repetitive control system (MRCS) based on a two-dimensional (2D) continuous-discrete hybrid model. First, a 2D continuous-discrete hybrid model of MRCS is established. Next, the design of the feedback controller of the MRCS is converted to the stabilization problem of the 2D hybrid system with a static-state feedback. Then, by using a 2D Lyapunov method, a sufficient stability condition for the 2D hybrid system is derived in terms of a linear matrix inequality. Furthermore, the result is extended to handle a plant with a time-varying uncertainty. These conditions can be used to obtain the parameters of a feedback controller and a low-pass filter in the MRCS. The validity of the method is demonstrated by numerical examples.
7E62BB1F	This paper provides new LMI-based conditions for the design of H2 gain scheduled controllers for rational linear parameter varying systems (LPV). Such systems are equivalently recast as affine descriptor LPV systems. Based on this, new sufficient LMI-based conditions for H2 performance analysis are proposed. These conditions can be turned to a finite set of LMIs and allow the use of parameter-dependent Lyapunov functions. Accordingly, new LMI conditions for the H2 gain scheduled controller synthesis problem are given. A numerical example highlights the effectiveness of the proposed conditions.
7C28A993	We investigate the problem of stabilizing energy level sets for Euler-Lagrange systems subject to virtual holonomic constraints. We present an energy level set stabilization technique with a guaranteed domain of attraction which preserves the invariance of the constraint manifold. As an illustration of the theory, we present a controller which swings up the Pendubot system while guaranteeing that the unactuated link does not fall over during transient.
76F43CE7	In this work a tachyonization of the ΛCDM model for a spatially flat Friedmann-Robertson-Walker space-time is proposed. A tachyon field and a cosmological constant are considered as the sources of the gravitational field. Starting from a stability analysis and from the exact solutions for a standard tachyon field driven by a given potential, the search for a large set of cosmological models which contain the ΛCDM model is investigated. By the use of internal transformations two new kinds of tachyon fields are derived from the standard tachyon field, namely, a complementary and a phantom tachyon fields. Numerical solutions for the three kinds of tachyon fields are determined and it is shown that the standard and complementary tachyon fields reproduces the CDM model as a limiting case. The standard tachyon field can also describe a transition from an accelerated to a decelerated regime, behaving as an inflaton field at early times and as a matter field at late times. The complementary tachyon field always behaves as a matter field. The phantom tachyon field is characterized by a rapid expansion where its energy density increases with time. 
76C7F9B8	A single-channel blind dereverberation algorithm is proposed in this letter for distant-talking speech recognition. The proposed method is based on spectral subtraction (SS) method, in which the spectrum of a late reverberant signal is estimated using a delayed and attenuated version of the reverberant signal. Through some assumptions, the conventional SS method regards the attenuation weight as a constant that is a function of reverberation time. However, these assumptions are not valid in real situations, and the ideal weight varies with the frame. Therefore, in the proposed method, the variable weight sequence is estimated using Viterbi decoding scheme based on the reverberation model. This weight sequence is then substituted for the fixed weight in the conventional SS method without explicitly estimating the reverberation time. The proposed method performs better than the conventional SS method in both isolated word recognition and connected digit recognition experiments in reverberant environments.
765F037A	XML is an extremely nifty format. Computers can easily parse XML data, yet humans can also understand it. By adopting XML, we can take advantage of the scores of tools that work on arbitrary XML documents. Common tasks like editing, validation, transformations, and queries become just a matter of selecting and applying the right tool.
801597A0	The REMOS (REverberation MOdeling for Speech recognition) concept for reverberation-robust distant-talking speech recognition, introduced in “Distant-talking continuous speech recognition based on a novel reverberation model in the feature domain” (A. Sehr , in Proc. Interspeech, 2006, pp. 769-772) for melspectral features, is extended to logarithmic melspectral (logmelspec) features in this contribution. Thus, the favorable properties of REMOS, including its high flexibility with respect to changing reverberation conditions, become available in the more competitive logmelspec domain. Based on a combined acoustic model consisting of a hidden Markov model (HMM) network and a reverberation model (RM), REMOS determines clean-speech and reverberation estimates during recognition. Therefore, in each iteration of a modified Viterbi algorithm, an inner optimization operation maximizes the joint density of the current HMM output and the RM output subject to the constraint that their combination is equal to the current reverberant observation. Since the combination operation in the logmelspec domain is nonlinear, numerical methods appear necessary for solving the constrained inner optimization problem. A novel reformulation of the constraint, which allows for an efficient solution by nonlinear optimization algorithms, is derived in this paper so that a practicable implementation of REMOS for logmelspec features becomes possible. An in-depth analysis of this REMOS implementation investigates the statistical properties of its reverberation estimates and thus derives possibilities for further improving the performance of REMOS. Connected digit recognition experiments show that the proposed REMOS version in the logmelspec domain significantly outperforms the melspec version. While the proposed RMs with parameters estimated by straightforward training for a given room are robust to a mismatch of the speaker-microphone distance, their performance significantly decreases if they are used in a room with substantially different conditions. However, by training multi-style RMs with data from several rooms, good performance can be achieved across different rooms.	
7D8AF82F	Automatic speech recognition (ASR) in reverberant environments is a challenging task. Most dereverberation techniques address this problem through signal processing and enhances the reverberant waveform independent from the speech recognizer. In this paper, we propose a novel scheme to perform dereverberation in relation with the likelihood of the back-end ASR system. Our proposed approach effectively selects the dereverberation parameters, in the form of multiband scale factors, so that they improve the likelihood of the acoustic model. Then, the acoustic model is retrained using the optimal parameters. During the recognition phase, we implement additional optimization of the parameters. By using Gaussian mixture model (GMM), the process for selecting the scale factors become efficient. Moreover, we remove the dependency of the adopted dereverberation technique on the room impulse response (RIR) measurement, by using an artificial RIR generator and selecting based on the acoustic likelihood. Experimental results show significant improvement in recognition performance with the proposed method over the conventional approach.	
7F5E409B	Distant acquisition of acoustic signals in an enclosed space often produces reverberant components due to acoustic reflections in the room. Speech dereverberation is in general desirable when the signal is acquired through distant microphones in such applications as hands-free speech recognition, teleconferencing, and meeting recording. This paper proposes a new speech dereverberation approach based on a statistical speech model. A time-varying Gaussian source model (TVGSM) is introduced as a model that represents the dynamic short time characteristics of nonreverberant speech segments, including the time and frequency structures of the speech spectrum. With this model, dereverberation of the speech signal is formulated as a maximum-likelihood (ML) problem based on multichannel linear prediction, in which the speech signal is recovered by transforming the observed signal into one that is probabilistically more like nonreverberant speech. We first present a general ML solution based on TVGSM, and derive several dereverberation algorithms based on various source models. Specifically, we present a source model consisting of a finite number of states, each of which is manifested by a short time speech spectrum, defined by a corresponding autocorrelation (AC) vector. The dereverberation algorithm based on this model involves a finite collection of spectral patterns that form a codebook. We confirm experimentally that both the time and frequency characteristics represented in the source models are very important for speech dereverberation, and that the prior knowledge represented by the codebook allows us to further improve the dereverberated speech quality. We also confirm that the quality of reverberant speech signals can be greatly improved in terms of the spectral shape and energy time-pattern distortions from simply a short speech signal using a speaker-independent codebook.	
7EF3FC6E	In this paper, the static noise margin (SNM) of FinFET static random access memory (SRAM) cells operating in the subthreshold region was investigated using an analytical solution of 3-D Poisson's equation. An analytical SNM model for subthreshold FinFET SRAM was demonstrated and validated by 3-D technology computer-aided design (TCAD) mixed-mode simulations. When compared with bulk SRAM, the standard 6T FinFET cell showed larger nominal READ SNM (RSNM), better variability immunity, and lesser temperature sensitivity of cell stability. Furthermore, examination of the stabilities of several novel independently controlled gate FinFET SRAM cells by using the proposed SNM model showed significant nominal RSNM improvements in these novel cells. However, the write ability is found to be degraded, which thus becomes an important concern for certain configurations in the subthreshold region. The result obtained indicates that the READ/WRITE word line voltage control technique is more effective than transistor sizing in improving the stability and write ability of the FinFET subthreshold SRAM. Furthermore, the impacts of process-induced variations on cell stability were also assessed. When compared with RSNM, it was found that WRITE SNM is more susceptible to process variations. While 6T is not a viable candidate for subthreshold SRAM, and 8T/10T cells must be used in bulk CMOS, the present analysis established the potential of 6T FinFET cells for subthreshold SRAM applications.
8004D3FA	nternal model control (IMC) is a well-known and effective control scheme. However, when unstable processes are concerned, the original IMC structure cannot be directly used for control system implementation. In this paper, a new scheme called partial internal model control (PIMC) is proposed, which is capable of controlling both stable and unstable processes. In PIMC, a process model is expressed as the sum of the stable and antistable parts and only the stable part of the process model is used as the internal model. The process stable part is canceled by the internal model and the remaining antistable part is stabilized and controlled with a primary controller, which is usually a PID-type regulator when the antistable part is of a low order. Various properties of a PIMC system such as internal stability and robust stability are analyzed. The design of PIMC is discussed in detail. Various simulation examples are included for illustration and a real-time implementation on a motor system is presented.
7752272C	Improvising on the ideas of Leslie and Gower as well as Holling and Tanner, a new prey-predator model that yields solutions processing either stable limit cycle or stable equilibrium is presented. Certain additional positive features of the model are pointed out. A stability analysis of the equilibrium point is also carried out.
7612EFDC	Magnetic and structural properties of multilayer FeSiN/ceramic films produced by dual-type sputtering were investigated. Laminating of FeSiN with ceramic layers enhanced not only the frequency response of the permeability μ, but also the value of μ. These soft magnetic properties of as-deposited films were found to be strongly related to the substrate temperature Tsub, but were independent of the kind of ceramic used. Multilayered FeSiN/AIN films deposited at Tsub=300°C exhibited a large μ (at 5 MHz) of about 5,000 and a saturation induction Bs of more than 17 kG. SEM observation suggested that the soft magnetic properties of FeSiN-base multilayers arise from the decrease in magnetic anisotropy of the assembly of magnetically interacting small grains, and also from the (111) plane orientation of the grains. Most multilayered films exhibited greater thermal resistance to aging up to 600°C, because the ceramic layers prevent the growth of  grains.
779626F1	In this paper, we present a method in suppressing speech degradation affecting human-robot communication in real reverberant environment condition. The novelty of the proposed method is the mechanism to indirectly incorporate language information in the enhancement process. This is achieved by considering the effects of the acoustic energy transfer of two consecutive words. The proposed method is categorized into two namely, a one-time computation of the smearing coefficients (offline mode) and the actual enhancement (online mode). In the offline mode, word pair smearing coefficients reflective of the inter-word energy transfer within a word pair are calculated and stored, creating a pool of smearing priors database. Then, the online robust enhancement process integrates this information to the conventional framewise enhancement method. In theory, the proposed method outperforms the conventional framewise-only enhancement since it is able to dynamically update the enhancement parameters based on the actual acoustic energy transfer between successive words during testing. Experiments using a humanoid robot inside a reverberant room confirms the effectiveness of the proposed method. Our method renders human-robot speech communication robust to the effect of reverberation as opposed to the conventional method.	
63413671	A three-dimensional finite difference transport model appropriate for the coastal environment is developed for the solution of the three-dimensional convection-diffusion equation. A higher order upwind scheme is used for the convective terms of the convection-diffusion equation, to minimize the numerical diffusion. The validity of the numerical model is verified through a test problem, whose exact solutions are known.
77E84575	For cooperative force-reflecting teleoperation over networks, conventional passivity-based approaches have limited applicability due to nonpassive slave–slave interactions and irregular communication delays imposed by networks. In this paper, a small gain framework for stability analysis design of cooperative network-based force reflecting teleoperator systems is developed. The framework is based on a version of weak input-to-output practical stability (WIOPS) nonlinear small gain theorem that is applicable to stability analysis of large-scale network-based interconnections. Based on this result, we design a cooperative force-reflecting teleoperator system which is guaranteed to be stable in the presence of multiple network-induced communication constraints by appropriate adjustment of local control gains and/or force-reflection gains. Experimental results are presented that confirm the validity of the proposed approach.
5D8F6C5F	We report on the stability analysis of the Rolling SLIP (R-SLIP) model, a sagittal-plane and point-mass model with a compliant leg and rolling ground contact. Owing to its asymmetric morphology, it has four different operation modes according to the range of the initial mass speed and leg configuration. Using the return map analysis, the asymmetric stability property of the R-SLIP model was investigated and compared with that of the SLIP model. The analysis results reveal that the R-SLIP model has better but more complicated stability characteristics than the SLIP model. The analysis results also suggest that the R-SLIP model operates better when it has a soft to hard stiffness change. In addition, the effect of the torsion spring position was also investigated.
032C4ECC	Fluid flow equations and their approximation by finite differences, including advection equations, compressible Euler equations, potential flow, and incompressible Navier-Stokes equations are introduced. Stability analysis of finite difference methods for initial boundary value problems is outlined. Finite element methods for simple steady flows and for unsteady flows are presented. Moving finite element methods are sketched.
7F371D2F	For polytopic matrices and polytopic systems we introduce the concept of diagonal stability with respect to a p-norm, 1 ≤ p ≤ ∞. We address both the discrete-time and the continuous-time cases. We prove the equivalence between this algebraic concept and the following properties of uncertain linear dynamics described by polytopic models: · the existence of diagonal Lyapunov functions defined by weighted p-norm; · the existence of contractive sets shaped by weighted p-norm, which are invariant with respect to the system trajectories. We also discuss the problem of the best rate for the Lyapunov function decrease and, respectively, for the invariant set contraction. A numerical procedure has been developed to support this exploration.
7E1BD1FB	This paper presents the application of black-box terminal characterization models (BBTC) for the simulation and analysis of distributed power electronics systems composed by commercial power electronics converters and filter modules. The models are shown to effectively predict the transient and steady state response of interconnected distributed power systems. They are also used to predict the small-signal stability between source and load converters. All simulation results are validated with experimental data.
2285CC00	This chapter focuses on the macroeconomic framework which guides the analyses of the book. We shall begin with an overview that describes how the major components of the framework relate to one another. The constraints (or ‘conservation laws’) that are imposed by the pool of fluid capital will then be discussed, followed by discussions of the determination of asset values and aggregate demand and aggregate supply. The chapter concludes with a description of the conditions which define macroeconomic equilibrium.
80970C93	In this paper, a five-dimensional vector-host epidemic model with temporary immunity is studied. Applying autonomous convergence theorem, the basic reproduction number is proved to be a sharp threshold which completely determines the global dynamics and the outcome of the disease. If the reproduction number is less than or equal to one, the disease-free equilibrium is globally asymptotically stable in the feasible region and the disease always dies out. If the reproduction number is greater than one, a unique endemic equilibrium is globally asymptotically stable in the interior of the feasible region and the disease will persist at the endemic equilibrium if it is initially present.
7E1541D0	This paper presents experimental results of formation control problems of omnidirectional mobile robots using distributed nonlinear model predictive control (NMPC). Two main objectives are (i) to maintain a desired flexible formation pattern and (ii) to follow a reference path. Both pose errors and formation errors are included into a local objective function, which is minimized at each update time. In the formation control, the curvilinear abscissa s has been used as a coupling term with neighboring robots. The strategy in such a way that the exchange of the most recent optimal state trajectory between coupled subsystems has been employed. The distinct features of NMPC are that constraints can be explicitly accommodated, as well as nonlinear and time-varying systems can be easily handled. Experiments with three omnidirectional mobile robots are presented to illustrate the validity of our proposed method.
684788E1	This paper presents a knowledge-based system to interpret laser radar (ladar) images. The objective of this research is to detect and recognize man-made objects in outdoor scenes. Our system applies themultisensor fusion approach to multiple ladar modalities to improve both segmentation and interpretation. The segmentation modules are written in C. The knowledge-based interpretation system is constructed usingKEE and Lisp. Low-level attributes of image segments (regions) are computed by the segmentation modules and then converted to theKEE format. The interpretation system applies forward chaining in a bottom-up fashion to derive object-level interpretation from input generated by low-level processing and segmentation modules. The interpretation modules detect man-made objects from the background using low-level attributes. Segments are grouped into objects, which are then classified into predefined categories (vehicles, ground, etc.). The efficiency of the interpretation system is enhanced by transferring nonsymbolic processing tasks to a concurrent service manager (program). Experimental results using ladar data are presented.
7D790564	This paper addresses blind dereverberation techniques based on the inherent characteristics of speech signals. Two challenging issues for speech dereverberation involve decomposing reverberant observed signals into colored sources and room transfer functions (RTFs), and making the inverse filtering robust as regards acoustic and system noise. We show that short-time speech characteristics are very important for this task, and that multi-channel linear prediction (MCLP) is a useful tool for achieving robust inverse filtering. As examples, we detail three recently proposed robust dereverberation methods. By assuming the source to be a small order autoregressive process, we can present an efficient source estimation method that reduces late reverberation reflections of the reverberation using multi-step linear prediction. By exploiting the time-varying nature of the speech signals, we can also develop a method that can estimate both the source and the inverse filters of the RTFs. Furthermore, we can achieve high quality speech dereverberation by formulating the problem as a likelihood maximization problem using a statistical speech model that represents the spectral characteristics of short-time speech segments including harmonicity.	
7D2B0237	The repressilator is a regulatory cycle of n genes where each gene represses its successor in the cycle: [see text]. The system is modelled by ODEs for an arbitrary number of identical genes and arbitrarily strong repressor binding. A detailed mathematical analysis of the dynamical behavior is provided for two model systems: (i) a repressilator with leaky transcription and single-step cooperative repressor binding, and (ii) a repressilator with auto-activation and cooperative regulator binding. Genes are assumed to be present in constant amounts, transcription and translation are modelled by single-step kinetics, and mRNAs as well as proteins are assumed to be degraded by first order reactions. Several dynamical patterns are observed: multiple steady states, periodic and aperiodic oscillations corresponding to limit cycles and heteroclinic cycles, respectively. The results of computer simulations are complemented by a detailed and complete stability analysis of all equilibria and of the heteroclinic cycle.
7C68239C	Within the framework of ecological systems analysis, the potential of niche formation plays an essential role, particularly in relation to the conditions of permanence or extinction of individual species. In this realm the model developed by Greve [Helgoländer Meeresuntersuchungen 49, (1995) 811–820] in his study of possibilities of predicting marine populations represents an interesting approach. This model, which consists of a system of coupled differential equations applies an age differentiation with complex relations between the populations. The purpose of this paper is to perform a qualitative analysis of this model. The main questions are the conditions of extinction, boundedness and permanence as well as the conditions of the existence and uniqueness of stationary solutions. Our analysis provides both a notion of the model's behaviour and a mathematical evaluation of this model. The preliminary results are positive and encouraging.
78325576	The infinite-range ferromagnetic Ashkin-Teller model is examined. The phase diagram as well as the order of the transitions are obtained from the stability analysis of the solutions of the stationary conditions for the free-energy functional. The equations for the free energy, magnetizations, specific heat, and susceptibility are discussed and solved numerically for special values of the coupling strengths.
7CBCE1D4	The transition to convection in the Rayleigh-Bénard problem at small Knudsen numbers is studied via a linear temporal stability analysis of the compressible ``slip-flow'' problem. Considering a power-law (variable hard-sphere) model of interaction our analysis indicates that for sufficiently large Froude numbers softer potentials result in less unstable systems. At small Froude numbers this trend is reversed, i.e., the softer interaction potentials correspond to a more unstable response. These results are discussed in terms of the opposing mechanisms of thermal expansion and compressibility. We carry out an asymptotic expansion for small temperature differences and establish the principle of exchange of stabilities for this limit. A singularity appears in this limit when compressibility effects are dominant.
5A4C26A5	It is shown that in a certain range of physical parameters a proper choice of the artificial viscosity parameters can lead to double-mode pulsation of RR Lyrae models. Here we present one such model and exhibit its double-mode behavior both through the stability analysis of the periodic limit cycles and through straightforward numerical integrations.
774D2E7B	In this paper, we consider hysteresis problem which is nonsmooth, nonlinear, and resided in piezoelectric actuators (PEA). A new discrete-time hysteretic model is introduced which is inspired by the famous Bouc-Wen model. We try to fuse the new model with the adaptive control techniques, where the unknown parameters in the model are not needed to be identified. The control algorithm guarantees the stability of system. Experimental results verify the effectiveness of the new model and the proposed algorithm.
7E5AC86C	We study the stability of coupled oscillators motivated by the Frenkel-Kontorova (FK) model. The FK model describes a chain of classical particles coupled to their neighbors and subject to a periodic on-site potential. The open-loop system of the FK model represents interconnected oscillators that have locally stable or unstable equilibrium points. We reveal the stability of the coupled system in the presence of linear and nonlinear particle interactions, respectively, and verify the results by numerical simulations. The result applies to physical systems such as atomic-scale friction whose dynamics is described by the FK model.
7E21FF68	This paper introduces speech dereverberation based on a time-varying Gaussian source model (GSM) and investigates its behavior to provide a better perspective on solving the dereverberation problem. GSM is a generalization of the autocorrelation codebook (ACC) that has recently been shown to enable us to achieve high quality speech dereverberation with only a few seconds' observation. Based on GSM, the speech dereverberation is formulated as a likelihood maximization problem with multi-channel linear prediction, where the reverberant speech signal is transformed into one that is probabilistically more like clean speech. For investigation purposes, the autocorrelation matrix of the GSM is first decomposed into energy, vocal tract filter, and excitation signal features by adopting an autoregressive GSM (ARGSM), and then analyzed based on experiments. They reveal that the energy feature in the models plays a major role in reducing the reverberation components. It is also shown that the other spectral features in the models further contribute to the recovery of the short-time characteristics of the dereverberated signals.
8169EC25	Stability analysis of a fuzzy control system has been one of the main topics of fuzzy control. A Petri net, representing a discretized fuzzy control system, has been applied to the stability analysis. A theory of asymptotic stability has been derived for the approximated control system. However, this theory guarantees the stability of nominal behavior of a fuzzy control system. This paper presents a new stability analysis of a fuzzy control system using a generalized fuzzy Petri net model. The rigorous behavior of the fuzzy system can be analyzed by the generalized model. Neural network realization of the Petri net model for the stability analysis is also given.
5EB46B66	Mental health nurses will play an important role in the administration of the routine outcome measures currently being implemented across Australia, including the Health of the Nation Outcome Scales (HoNOS). Prior research has implied that sources of information may be responsible in part for the mixed reliability and validity of the HoNOS. This study examines which sources of information clinicians use when making a HoNOS rating. Twenty-one mental health clinicians that had been using the HoNOS routinely for 2 years were surveyed to determine the sources of information they used when making a rating. In addition, 12 specific HoNOS ratings were reviewed to obtain data about 'actual' sources utilized. More than half of all information used when completing a HoNOS rating was obtained from interviewing the client or from direct observation of the client. The main secondary sources used included medical records and consulting with family and carers and other staff. Collateral information from general practitioners and police was used in only a small percentage of cases. There was high variability amongst clinicians with regard to how much each source was used. Training mental health workers in routine use of the HoNOS should encourage clinicians to use a range of sources of information when making a rating.
80D08EDA	OpenMusic (OM) is a domain-specific visual programming language designed for computer-aided music composition. This language based on Common Lisp allows composers to develop functional processes generating or transforming musical data, and to execute them locally by demand-driven evaluations. As most historical computer-aided composition environments, OM relies on a transformational declarative paradigm, which is hard to conciliate with reactive data-flow (an evaluation scheme more adequate to the development of interactive systems). We propose to link these two evaluation paradigms in the same and consistent visual programming framework.We establish a denotational semantics of the visual language, which gives account for its demand-driven evaluation mechanism and the incremental construction of programs. We then extend this semantics to enable reactive computations in the functional graphs.The resulting language merges data-driven executions with the existing demand-driven mechanism. A conservative implementation is proposed.We show that the incremental construction of programs and their data-driven and demand-driven evaluations can be smoothly integrated in the visual programming workflow. This integration allows for the propagation of changes in the programs, and the evaluation of graphically designed functional expressions as a response to external events, a first step in bridging the gap between computer-assisted composition environments and real-time musical systems.
7FA0A8BC	Based on cognitive load theory, two experiments investigated the conditions under which audiovisual-based instruction may be an effective or an ineffective instructional technique. Results from Experiment 1 indicated that visual with audio presentations were superior to equivalent visual-only presentations. In this experiment, neither the auditory nor the visual material could be understood in isolation. Both sources of information were interrelated and were essential to render the material intelligible. In contrast, Experiment 2 demonstrated that a non-essential explanatory text, presented aurally with similar written text contained in a diagram, hindered learning. This result was obtained because when compared to a diagram only format, the aural material was unnecessary and therefore created a redundancy effect. Differences between groups were stronger when information was high in complexity. It was concluded that the effectiveness of multimedia instruction depends very much on how and when auditory information is used.
7C9BB964	It is hypothesized that instructional materials that use dual-mode presentation techniques (e.g., auditory text and visual diagrams) can result in superior learning to equivalent, single-modality formats (e.g., visual text and visual diagrams). This modality effect may be attributed to an effective expansion of working memory. The authors explore the effect from a cognitive-load perspective. Using a variety of instructional materials, the authors found in 3 experiments that participants studying materials incorporating audio text and visual diagrams or tables performed better than those studying a conventional, visual-only format. These results were obtained only for instructions with a high intellectual content. In light of these findings, the central role of cognitive load in instructional design is highlighted, and the implications for multimedia instruction are discussed.
7B067698	The relative variogram has been employed as a tool for correcting a simple kind of nonstationarity, namely that in which local variance is proportional to local mean squared. In the past, this has been linked in a vague way to the lognormal distribution, although if {Zt; t ∈ D}is strongly stationary and normal over a domain D,then clearly {exp (Zt); t ∈ D}will stillbe stationary, but lognormal. The appropriate link is made in this article through a universal transformation principle. More general situations are considered, leading to the use of a “scaled variogram.”
809A37E0	In this paper, a new method based on multi-layered dynamic neural group network for analyzing event series is introduced. By the embedded multiple parallel structures, the new method can identify the character patterns contained in the event series. Then, a selective evaluation strategy is applied to integrate the different pattern clusters and predict the event in the next step. The aim is to generate the complex dynamic behaviors about the controlled system. The fundamental concepts and framework of this method are explained in detail. The effectiveness of our approach is demonstrated on the Internet-based telerobot soccer system by simulation experiments. The results are compared to those based on static neural group network. It is showed that, the telerobot can produce the predictive behaviors with high accuracy under the control of multi-layered dynamic neural group network. The proposed method could properly increase the local-autonomy of telerobot and maintain the stability of system. The conclusions and future work are described in the end.
76327740	Parigot's λμ-calculus, a system for computational reasoning about classical proofs, serves as a foundation for control operations embodied by operators like Scheme's callcc. We demonstrate that the call-by-value theory of the λμ-calculus contains a latent theory of delimited control, and that a known variant of λμ which unshackles the syntax yields a calculus of composable continuations from the existing constructs and rules for classical control. To relate to the various formulations of control effects, and to continuation-passing style, we use a form of compositional program transformations which preserves the underlying structure of equational theories, contexts, and substitution. Finally, we generalize the call-by-name and call-by-value theories of the λμ-calculus by giving a single parametric theory that encompasses both, allowing us to generate a call-by-need instance that defines a calculus of classical and delimited control with lazy evaluation and sharing.
7E69CEE3	There are only a few garbage collection algorithms that have been designed to operate over massive object stores. These algorithms operate at two levels, locally via incremental collection of small partitions and globally via detection of cross partition garbage, including cyclic garbage. At each level there is a choice of collection mechanism. For example, the PMOS collector employs tracing at the local level and reference counting at the global level. Another approach implemented in the Thor object database uses tracing at both levels. In this paper we present two new algorithms that both employ reference counting at the local level. One algorithm uses reference counting at the higher level and the other uses tracing at the higher level. An evaluation strategy is presented to support comparisons between these four algorithms and preliminary experiments are outlined.
782EB1EB	We present the &mu; -calculus, a syntax for &lambda;-calculus + control operators exhibiting symmetries such as program/context and call-by-name/call-by-value. This calculus is derived from implicational Gentzen's sequent calculus LK, a key classical logical system in proof theory. Under the Curry-Howard correspondence between proofs and programs, we can see LK, or more precisely a formulation called LK&mu; , as a syntax-directed system of simple types for &mu; -calculus. For &mu; -calculus, choosing a call-by-name or call-by-value discipline for reduction amounts to choosing one of the two possible symmetric orientations of a critical pair. Our analysis leads us to revisit the question of what is a natural syntax for call-by-value functional computation. We define a translation of &lambda;&mu;-calculus into &mu; -calculus and two dual translations back to &lambda;-calculus, and we recover known CPS translations by composing these translations.
37CBAE52	The dangers that future climate change poses to physical, biological, and economic systems are accounted for in analyses of risk and increasingly figure in decision-making about responses to climate change. Yet the potential cultural and social impacts of climate change have scarcely been considered. In this article we bring the risks climate change poses to cultures and social systems into consideration through a focus on places—those local material and symbolic contexts that give meaning and value to peoples' lives. By way of examples, the article reviews evidence on the observed and projected impacts of climate change on the Arctic and Pacific island atoll nations. It shows that impacts may result in the loss of many unique natural and cultural components of these places. We then argue that the risk of irreversible loss of places needs to be factored into decision-making on climate change. The article then suggests ways forward in decision-making that recognizes these non-market and non-instrumental metrics of risk, based on principles of justice and recognition of individual and community identity.
5B26E764	Building on his highly successful textbook on C++, David Yevick provides a concise yet comprehensive one-stop course in three key programming languages, C++, Java and Octave (a freeware alternative to MATLAB). Employing only public-domain software, this book presents a unique overview of numerical and programming techniques, including object-oriented programming, elementary and advanced topics in numerical analysis, physical system modelling, scientific graphics, software engineering and performance issues. Compact, transparent code in all three programming languages is applied to the fundamental equations of quantum mechanics, electromagnetics, mechanics and statistical mechanics. Uncommented versions of the code that can be immediately modified and adapted are provided online for the more involved programs. This compact, practical text is an invaluable introduction for students in all undergraduate- and graduate-level courses in the physical sciences or engineering that require numerical modelling, and also a key reference for instructors and scientific programmers.
09CE86DE	In the call-by-value lambda-calculus solvable terms have been characterised by means of call-by-name reductions, which is disappointing and requires complex reasonings. We introduce the value-substitution lambda-calculus, a simple calculus borrowing ideas from Herbelin and Zimmerman’s call-by-value λCBV calculus and from Accattoli and Kesner’s substitution calculus λsub. In this new setting, we characterise solvable terms as those terms having normal form with respect to a suitable restriction of the rewriting relation.
7EFE6622	Two strategies of randomized search, namely adaptive cluster covering (ACCO), and adaptive cluster covering with descent (ACD), are introduced and positioned in the group of the global optimization techniques. Several algorithms based on these new strategies are compared with other techniques of global randomized search in terms of effectiveness, efficiency and reliability. The other techniques include two versions of multistart, two versions of the controlled random search (CRS2 and CRS4) and the canonical genetic algorithm. Thirteen minimization problems including two parameter identification problems (for a flexible membrane mirror model and a hydrologic model) are solved. The algorithm ACCO, and a version of CRS4 algorithm (Ali and Storey 1994) show the highest efficiency, effectiveness and reliability. The second new algorithm, ACD, is in some runs very efficient and effective, but its reliability needs further improvement.
5CC5E166	The aim of this paper is to demonstrate the actual status of the usage of certain "exceptional" expressions in English: when/where-clauses in defining sentences and if-clauses which serve as subject complements in sentences such as The only way she'd believe it is if she heard it from my lips. While these sentences have been criticized as nonstandard by some grammarians,current corpus-based studies reveal that they are used more often than expected because of several pragmatic properties. The following discussion shows that the corpus-based descriptions and pragmatic interpretation of these expressions may contribute to a better understanding of them and shed some light on the description of definitions in EFL dictionaries.
5FD89745	Learning video concept detectors from social media sources, such as Flickr images and YouTube videos, has the potential to address a wide variety of concept queries for video search. While the potential has been recognized by many, and progress on the topic has been impressive, we argue that two key questions, i.e., What visual tagging source is most suited for selecting positive training examples to learn video concepts? and What strategy should be used for selecting positive examples from tagged sources?, remain open. As an initial attempt to answer the two questions, we conduct an experimental study using a video search engine which is capable of learning concept detectors from social media, be it socially tagged videos or socially tagged images. Within the video search engine we investigate six strategies of positive examples selection. The performance is evaluated on the challenging TRECVID benchmark 2011 with 400 hours of Internet videos. The new experiments lead to novel and nontrivial findings: (1) tagged images are a better source for learning video concepts from the web, (2) selecting tag relevant examples as positives for learning video concepts is always beneficial and it can be done automatically and (3) the best source and strategy compare favorably against several present-day methods.
7E13614A	This paper focuses on data collected from sites that have been identified through literature as cities that have utilised cultural industries as a means to create economic development, increased tourism and a 'better place to live'. Community values and the impact of cultural/creative industries on a community are discussed and it will reveal key links and outcomes of interviews from local government practitioners and institution managers from Spain and Canada. The scoping research contributes to the understanding of city revitalisation strategy through culture and arts and the impact of Local Government Authorities around the influence of cultural industries on a place. It directly contributes to: the practices undertaken by Councils; the influence of Council policies and procedures on institutions; tools, procedures, activities utilised as public organisations to consult with their community to make decisions; and how these improve the amenity of the city.
756A4550	Previously developed sphere-of-control (SOC) concepts are used to develop a scenario for post-process recovery. An information structure provides the recovery boundary around the effects of the usage of a resource. Rules for building this structure are addressed, and the capability for searching backward in time is identified to determine the source of error and possible recovery strategies.
7C309DB6	Mental health nurses will play an important role in the administration of the routine outcome measures currently being implemented across Australia, including the Health of the Nation Outcome Scales (HoNOS). Prior research has implied that sources of information may be responsible in part for the mixed reliability and validity of the HoNOS. This study examines which sources of information clinicians use when making a HoNOS rating. Twenty-one mental health clinicians that had been using the HoNOS routinely for 2 years were surveyed to determine the sources of information they used when making a rating. In addition, 12 specific HoNOS ratings were reviewed to obtain data about 'actual' sources utilized. More than half of all information used when completing a HoNOS rating was obtained from interviewing the client or from direct observation of the client. The main secondary sources used included medical records and consulting with family and carers and other staff. Collateral information from general practitioners and police was used in only a small percentage of cases. There was high variability amongst clinicians with regard to how much each source was used. Training mental health workers in routine use of the HoNOS should encourage clinicians to use a range of sources of information when making a rating.
1149CB67	This paper uncovers the specific properties of “which NP do”, “that NP do” and “this NP do” constructions. Based on a thorough analysis of electronic corpora, it lays bare a factitive meaning shared by the various forms of the verb do, provided the latter takes some complementation (verbal in the case of auxiliary do, nominal in the case of transitive do). In “which NP do”, “that NP do” and “this NP do” constructions notably, this underlying semantic unity appears to blur the boundary between lexical and grammatical occurrences of the verb do, thus obscuring the syntactic differences between the lexical verb and the auxiliary.
7DEBD3B9	The concept of syntactical duality is central in logic. In particular, the duality defined by classical negation, or more syntactically by left and right in sequents, has been widely used to relate logic and computations. We study the proof/test duality proposed by Girard in his 1999 paper on the meaning of logical rules. In detail, starting from the notion of “test” proposed by Girard, we develop a notion of test for intuitionistic logic and we give a complete deductive system whose computational interpretation is the target language of the call-by-value and call-by-name continuation passing style translations.
7C7788C3	Determining transmission electron microscope specimen thickness is an essential prerequisite for carrying out quantitative microscopy. The convergent beam electron diffraction method is highly accurate but provides information only on the small region being probed and is only applicable to crystalline phases. Thickness mapping with an energy filter is rapid, maps an entire field of view and can be applied to both crystalline and amorphous phases. However, the thickness map is defined in terms of the mean free path for energy loss (lambda), which must be known in order to determine the thickness. Convergent beam electron diffraction and thickness mapping methods were used to determine lambda for two materials, Si and P91 steel. These represent best- and worst-case scenario materials, respectively, for this type of investigation, owing to their radically different microstructures. The effects of collection angle and the importance of dynamical diffraction contrast are also examined. By minimizing diffraction contrast effects in thickness maps, reasonably accurate (+/-15%) values of lambda were obtained for P91 and accuracies of +/-5% were obtained for Si. The correlation between the convergent beam electron diffraction-derived thickness and the log intensity ratios from thickness maps also permits estimation of the thickness of amorphous layers on the upper and lower surfaces of transmission electron microscope specimens. These estimates were evaluated for both Si and P91 using cross-sectional transmission electron microscopy and were found to be quite accurate.
7ED41F05	Voice activity detection (VAD) algorithms have become an integral part of many of the standardized wireless cellular and personal communications systems (PCS). We present a comparative study of the performance of three proposed VAD algorithms under various acoustical background noise conditions. We also propose new ideas to enhance the performance of a VAD algorithm in wireless PCS speech applications.
7B5F66BD	A robust algorithm for voice activity detection (VAD) is presented. It defines a likelihood ratio test (LRT) involving multiple and independent observations of the bispectra. The proposed VAD provides significant improvements in speech/pause discrimination when compared to standardised and recently reported VADs.
75280E8A	The author describes a voice activity detector (VAD) that can operate reliably in SNRs down to 0 dB and detect most speech at -5 dB. The detector applies a least-squares periodicity estimator to the input signal, and triggers when a significant amount of periodicity is found. It does not aim to find the exact talkspurt boundaries and, consequently, is most suited to speech-logging applications where it is easy to include a small margin to allow for any missed speech. The author discusses the problem of false triggering on nonspeech periodic signals and shows bow robustness to these signals can be achieved with suitable preprocessing and postprocessing.
7B3D178B	Simple active noise-canceling systems (such as noise canceling headphones) use a feedback mechanism whereby the signal recorded by a microphone placed near the loudspeaker is phase-inverted and sent back through the loudspeaker via a feedback filter. The feedback filter is designed to achieve a high gain at low frequencies (for best noise reduction) while maintaining closed-loop stability under various conditions. To design the feedback filter, fairly ad-hoc techniques were traditionally used under the broad denomination of "loop-shaping" until the advent in the 1980s of optimal Hinfin algorithms. The design technique outlined in this letter uses a cepstral domain approach, where gain and phase constraints take a convenient linear form, and linear programming to design the unique optimal feedback filter
7D10FBFF	Currently, there are technology barriers inhibiting speech processing systems that work in extremely noisy conditions from meeting the demands of modern applications. This letter presents a new voice activity detector (VAD) for improving speech detection robustness in noisy environments and the performance of speech recognition systems. The algorithm defines an optimum likelihood ratio test (LRT) involving multiple and independent observations. The so-defined decision rule reports significant improvements in speech/nonspeech discrimination accuracy over existing VAD methods that are defined on a single observation and need empirically tuned hangover mechanisms. The algorithm has an inherent delay that, for several applications, including robust speech recognition, does not represent a serious implementation obstacle. An analysis of the overlap between the distributions of the decision variable shows the improved robustness of the proposed approach by means of a clear reduction of the classification error as the number of observations is increased. The proposed strategy is also compared to different VAD methods, including the G.729, AMR, and AFE standards, as well as recently reported algorithms showing a sustained advantage in speech/nonspeech detection accuracy and speech recognition performance.
80BCCB77	In this letter, we propose a novel approach to voice activity detection (VAD) based on the modified maximum a posteriori (MAP) criterion conditioned on the voice activity decision made in the previous frame. To exploit the inter-frame correlation of voice activity, the probability of the voice presence conditioned on both the observed spectrum and the voice activity decision in the previous frame is employed instead of the conventional strategy that depends only on the current observation. The proposed conditional MAP criterion incorporating temporal correlations leads to two separate thresholds for the likelihood ratio test (LRT) depending on the previous VAD result. Experimental results show that the VAD based on the proposed conditional MAP criterion outperforms the VAD based on the conventional MAP criterion under various noise environments.
7DFD48D8	In this paper, a new method for voiced/nonvoiced detection based on epoch extraction is proposed. Zero-frequency filtered speech signal is used to extract the instants of significant excitation (or epochs). The robustness of the method to extract epochs in the voiced regions, even with small amount of additive white noise, is used to distinguish voiced epochs from random instants detected in nonvoiced regions. The main feature of the proposed method is that it uses the strength of glottal activity as against using the periodicity of the signal. Performance of the proposed algorithm is studied on TIMIT and CMU ARCTIC databases, for two different noise types, white and vehicle noise from the NOISEX database, at different signal-to-noise ratios (SNRs). The proposed method performs similar or better than the popular normalized crosscorrelation based voiced/nonvoiced detection used in the open source utility wavesurfer, especially at lower SNRs.
7F31EB2F	Discrete Fourier transforms are derived which allow the use of nonequally spaced time-domain samples. It is shown that the use of equal spacings in the logarithmic time and frequency domains provides a very efficient transform algorithm. The applicability of this algorithm for the analysis of systems with moderate dynamic behavior over several frequency decades is demonstrated by examples. An error analysis is given.
785CCF61	The instantaneous frequency of a signal may be uniquely defined from the analytical signal. However, for a Gaussian signal, this frequency has an infinite variance and is useless. Instead, amplitude-weighted instantaneous frequencies are often used. Here it is shown that if powers of the envelope Ak(t), k = 1, 2, 3,.. are used as weight factors, the first power minimizes the variance for a narrow-band signal.
805FB923	This correspondence presents a statistical analysis of frequency estimation using state-variable balancing for a single sinusoid in the presence of additive noise at high signal-to-noise ratios. The calculated variance is compared to the performance of the frequency estimation using linear prediction and the result is validated by simulations.
7EE0AE0C	A stand-alone noise suppression algorithm is presented for reducing the spectral effects of acoustically added noise in speech. Effective performance of digital speech processors operating in practical environments may require suppression of noise from the digital wave-form. Spectral subtraction offers a computationally efficient, processor-independent approach to effective digital speech analysis. The method, requiring about the same computation as high-speed convolution, suppresses stationary noise from speech by subtracting the spectral noise bias calculated during nonspeech activity. Secondary procedures are then applied to attenuate the residual noise left after subtraction. Since the algorithm resynthesizes a speech waveform, it can be used as a pre-processor to narrow-band voice communications systems, speech recognition systems, or speaker authentication systems.
7F66E47F	This paper presents a robust algorithm for a voice activity detector (VAD) based on generalized autoregressive conditional heteroscedasticity (GARCH) filter, variance gamma distribution (VGD), and adaptive threshold function. GARCH models are new statistical methods that are used especially in economic time series. There is a consensus that speech signals exhibit variances that change through time. GARCH models are a popular choice to model these changing variances. A speech signal is assumed to have a VGD because the VGD has heavier tails than the Gaussian distribution (GD). The distribution of noise signal is assumed to be Gaussian. In proposed method, heteroscedasticity will be modeled by GARCH, and then the parameters of the distributions will be estimated recursively. Finally, hard detection is the result of comparing a multiple observation likelihood ratio test (MOLRT) with an adaptive threshold function. The simulation results show that the proposed VAD is able to operate down to -5 dB and in nonstationary environments
7F5323F7	This paper presents a new method for voice activity detection (VAD) based on the autoregressive-generalized autoregressive conditional heteroscedasticity (AR-GARCH) model. The speech signal is modeled as an AR-GARCH process in the time domain, and the likelihood ratio is computed and compared to a threshold. The time-varying variance of the speech signal needed for computing the likelihood function under speech presence hypothesis, is estimated using the AR-GARCH model. The model parameters are estimated using a novel technique based on the recursive maximum likelihood (RML) estimation. The variance of the additive noise, a critical issue in designing a VAD, is estimated using the improved minima controlled recursive averaging (IMCRA) method, which is properly modified to be applicable to noise variance estimation in the time domain. The performances of the VAD and the parameter estimation method are examined under several conditions. Experimental results indicate the robustness of the AR-GARCH based VAD both to noise variations and low signal-to-noise ratio (SNR) conditions.
804AF37B	Traditionally, voice activity detection algorithms are based on any combination of general speech properties such as temporal energy variations, periodicity, and spectrum. This paper describes a novel statistical method for voice activity detection using a signal-to-noise ratio measure. The method employs a low-variance spectrum estimate and determines an optimal threshold based on the estimated noise statistics. A possible implementation is presented and evaluated over a large test set and compared to current modern standardized algorithms. The evaluations indicate promising results with the proposed scheme being comparable or favorable over the whole test set.
7D1784A0	Voice-based interaction generates a lot of interest for natural human-robot interaction. Auditory information from microphones is an essential clue for a robot's attention to a person. Fault detection of microphones is required in order to improve the reliability of the voice-based human-robot interaction. This paper proposes a new approach for real-time fault detection in a microphone array in conversation without a calibration signal and a known sound source position by the intercorrelation of features in voice activity detection. The approach is successfully applied to a six-microphone system, and experimental results show an average fault detection of 97.6%.
7B40D0BB	In this paper, we present a scheme to detect significantly overlapping transients buried in white Gaussian noise. A nonuniform M-band wavepacket decomposition algorithm using M-band, translation-invariant wavelet transform (NMTI) is developed, and its application to transient signal detection is discussed. The robustness of the NMTI-based detector is illustrated.
8123CF86	In this paper, by using the properties of the higher order statistics (HOS) of speech and noise signals, we develop an improved voice activity detection (VAD) scheme. The proposed scheme employs the logarithm of the kurtosis of the LPC residual of a speech signal and is shown to be more effective and efficient in detecting active speech in medium to low signal-to-noise ratio (SNR) conditions without being unduly affected by the variations in the signal energy. To overcome the inability of the HOS in detecting unvoiced speech, another metric (the low band to full band energy ratio) is introduced. Depending on the estimated mean SNR, the proposed scheme works adaptively in two modes: a simple mode using only the SNR, and an enhanced mode using the HOS, the low band to full band energy ratio and the SNR. This scheme is capable of avoiding unnecessary computations, while maintaining the same performance as that working only in the enhanced mode. Simulations results are presented to demonstrate the effectiveness of the proposed voice activity detection scheme.
7EEDE9E5	In this paper, we present a time domain aperiodicity, periodicity, and pitch (APP) detector that estimates 1) the proportion of periodic and aperiodic energy in a speech signal and 2) the pitch period of the periodic component. The APP system is particularly useful in situations where the speech signal contains simultaneous periodic and aperiodic energy, as in the case of breathy vowels and some voiced obstruents. The performance of the APP system was evaluated on synthetic speech-like signals corrupted with noise at various levels of signal-to-noise ratio (SNR) and on three different natural speech databases that consist of simultaneously recorded electroglottograph (EGG) and acoustic data. When compared on a frame basis (at a frame rate of 2.5 ms) the results show excellent agreement between the periodic/aperiodic decisions made by the APP system and the estimates obtained from the EGG data (94.43% for periodicity and 96.32% for aperiodicity). The results also support previous studies that show that voiced obstruents are frequently manifested with either little or no aperiodic energy, or with strong periodic and aperiodic components. The EGG data were used as a reference for evaluating the pitch detection algorithm. The ground truth was not manually checked to rectify or exclude incorrect estimates. The overall gross error rate in pitch prediction across the three speech databases was 5.67%. In the case of synthetic speech-like data, the estimated SNR was found to be in close proportion to the actual SNR, and the pitch was always accurately found regardless of the presence of any shimmer or jitter.
8096C0E7	A new fusion method for voice activity detection in additive nonstationary noise is suggested. A performance study of the methods: fusion, the geometrically adaptive energy level, periodicity measure, and zero crossings rates, is presented. The new method is shown to operate reliably down to -5 dB SNR.
76747C20	Voice activity detection (VAD) plays an important role on the performance of speech processing systems in adverse environments. Recently, statistical model-based VADs have demonstrated impressive performance. The study presents a novel decision test (named likelihood ratio sign test, LRST) for VAD by using sign test and Neyman-Pearson criterion to improve the performance of statistical model-based VAD. The proposed LRST is derived based on the likelihood ratios (LRs) calculated from multiple independent observations by incorporating the long-term speech information into the decision rule. An implementation of the LRST VAD is introduced by defining the LRST over a sliding window and calculating the LRs based on complex Gaussian distribution for an input signal. For experiments, the multiple-observation LRT (MO-LRT) VAD based on multiple observations is used as a reference owing to its outstanding performance compared with conventional VADs. The experimental results show that the proposed approach outperforms the MO-LRT VAD in various noise environments.
76D3AB89	The performance of traditional voice activity detectors significantly deteriorates in the presence of highly nonstationary noise and transient interferences. One solution is to incorporate a video signal which is invariant to the acoustic environment. Although several voice activity detectors based on the video signal were recently presented, merely few detectors which are based on both the audio and the video signals exist in the literature to date. In this paper, we present an audio-visual voice activity detector and show that the incorporation of both audio and video signals is highly beneficial for voice activity detection. The algorithm is based on a supervised learning procedure, and a labeled training data set is considered. The algorithm comprises a feature extraction procedure, where the features are designed to separate speech from nonspeech frames. Diffusion maps is applied separately and similarly to the features of each modality and builds a low dimensional representation. Using the new representation, we propose a measure for voice activity which is based on a supervised learning procedure and the variability between adjacent frames in time. The measures of the two modalities are merged to provide voice activity detection based on both the audio and the video signals. Experimental results demonstrate the improved performance of the proposed algorithm compared to state-of-the-art detectors.
75377BCA	Glottal activity is an important aspect of speech production that results in voiced speech, and localizing such regions for computing various parameters of the excitation source is useful in many speech processing applications. The aim of this paper is to investigate the ability of Empirical Mode Decomposition (EMD) and its noise assisted variants, in characterizing glottal activity from the speech signal. A pair of consecutive Intrinsic Mode Functions (IMFs), obtained from the decomposition is found to reflect the periodic nature of different voiced regions of the speech signal. This IMF pair is utilized to construct a signal, named the Glottal Intrinsic Mode Function (GIMF), which represents most of the voiced speech regions. To measure the capability of the GIMF in representing the glottal activity, it is applied to the tasks of Glottal Activity Detection (GAD), pitch frequency (F0) tracking and detecting pitch markers. The results ascertain the capability of EMD in localizing Glottal activity within a small subset of IMFs, and suggest the possibility of accurately extracting source-information from voiced speech with simple signal processing procedures.
754B0F91	This paper describes a method to detect repeating segments in an audio signal by using dynamic time warping algorithm. The proposed framework extracts features from frames of the audio by Mel frequency cepstral coefficients. The features extracted from the audio clip of the chorus were matched against the features of the whole clip by dynamic time warping. The number of matches found was determined by self similarity matrix. The experimental results indicate that the minimum distance matches between query and reference clip is successfully achieved. The proposed scheme was tested in a database of audio signals and the experimental results are encouraging. The proposed scheme was implemented and tested using a database of audio signals with accuracy up to 98%.
7F5ADE63	Speech coding is one of the major degradation involved in building the speech systems in mobile environment. In this paper, we are exploring the effect of low bit rate speech coding on the accuracy of detection of epochs. Epoch is referred as the instant of significant excitation of the vocal-tract system during production of speech. Many speech applications depend on the the accurate estimation of the epoch locations. Epoch extraction from speech signal is challenging due to time-varying characteristics of the excitation source and vocal-tract system. For determining the epochs, two recently developed accurate methods (i) zero frequency filter (ZFF) (ii) Dynamic programming projected phase slope (DYPSA) are used. Most of the epoch extraction methods except ZFF method, attempt to remove the characteristics of the vocal-tract system, in order to emphasize the excitation characteristics in the residual. ZFF method extracts the epoch locations directly from the speech signals using impulse like nature of excitation. Speech coders used in this study are GSM full rate (ETSI 06.10), CELP (FS-1016), and MELP (TI 2.4 kbps). Performance of epoch extraction methods is evaluated using CMU-Arctic data using the epoch locations from electro-glottograph as reference.
815E3783	Human whistle could be a way to perform activation of different kind of devices, for example turn on and off a light in a smart room. Therefore, in this paper a human whistle detection and frequency estimation system is presented. Further, an investigation of human whistling and a robust non-linear feature extraction is presented. A system for robust performance due to sensor change and various noise situations is proposed using these features. Experiments in various noise situations are conducted.
7EA7521D	An improved voice activity detection (VAD) based on the radial basis function neural network (RBF NN) and continuous wavelet transform (CWT) for speech recognition system is presented in the paper. The input speech signal is analyzed in the form of fixed size window by using Mel-frequency cepstral coefficients (MFCC). Within the windowed signal, the proposed RBF-CWT VAD algorithm detects the speech/ non-speech signal using the RBF NN. Once the interchange of speech to non-speech or vice versa occurred, the energy changes of the CWT coefficients are calculated to localize the final coordination of the starting/ending speech points. Instead of classifying the speech signal using the MFCC at the frame-level which easily capture lots of undesired noise encountered by the conventional VAD with the binary classifier, the proposed RBF NN with the aid of CWT analyzes the transformation of the MFCC at the window-level that offers a better compensation to the noisy signal. The simulation results shows an improvement on the precision of the speech detection and the overall ASR rate particularly under the noisy circumstances compared to the conventional VAD with the zero-crossing rate, short-term signal energy and binary classifier.
7AB9A301	In speech communication systems, the microphone signals are degraded by reverberation and ambient noise. The reverberant speech can be separated into two components, namely, an early speech component that includes the direct path and some early reflections, and a late reverberant component that includes all the late reflections. In this paper, a novel algorithm to simultaneously suppress early reflections, late reverberation and ambient noise is presented. A multi-microphone minimum mean square error estimator is used to obtain a spatially filtered version of the early speech component. The estimator constructed as a minimum variance distortionless response (MVDR) beamformer (BF) followed by a postfilter (PF). Three unique design features characterize the proposed method. First, the MVDR BF is implemented in a special structure, named the nonorthogonal generalized sidelobe canceller (NO-GSC). Compared with the more conventional orthogonal GSC structure, the new structure allows for a simpler implementation of the GSC blocks for various MVDR constraints. Second, In contrast to earlier works, RETFs are used in the MVDR criterion rather than either the entire RTFs or only the direct-path of the desired speech signal. An estimator of the RETFs is proposed as well. Third, the late reverberation and noise are processed by both the beamforming stage and the PF stage. Since the relative power of the noise and the late reverberation varies with the frame index, a computationally efficient method for the required matrix inversion is proposed to circumvent the cumbersome mathematical operation. The algorithm was evaluated and compared with two alternative multichannel algorithms and one single-channel algorithm using simulated data and data recorded in a room with a reverberation time of 0.5 s for various source-microphone array distances (1-4 m) and several signal-to-noise levels. The processed signals were tested using two commonly used objective measures, namely perceptual evaluation of speech quality and log-spectral distance. As an additional objective measure, the improvement in word accuracy percentage of an acoustic speech recognition system is also demonstrated.
7DAA7F69	This paper introduces two new frequency domain overdetermined blind source separation (BSS) algorithms: Inter-frequency Correlation with Microphone Diversity (ICMD), and ICA with Triggered Principal component analysis (ITP). In the first, we consider different sets of microphones, where in each set the number of microphones and sources are equal. In the second, we extract principal components from an overdetermined mixture to form a determined mixture for separation. Both techniques utilize inter-frequency correlation to align permutations via energy profiles. Both monitor the condition number of an inter-frequency cross-correlation matrix of the normalized de-mixed signals' envelopes to determine if separation has failed for the current ICA input configuration; if so, the input configuration is revised and efficiently realigned to produce a better mixture for separation. The complexities and performances of these algorithms are examined in both simulations and a real-room measurement, with three and five sources. They are also compared to other recent frequency domain BSS algorithms for benchmarking purposes. Results show that generally, ICMD and ITP show similar performance with each other and with one of the benchmarking algorithms. However, ICMD is more computationally efficient.
0AE7B73F	Automatic Speaker recognition (ASR) is a pattern recognition problem that involves the process of automatically recognizing the speakers from their voices. Password protected or secured speaker recognition system gives an extra security to the system where a person is not only identified by his voice but also needs to utter a particular password correctly in order to access the system. In this paper we present a new modeling scheme for a password protected speaker recognition system by cascading a modified RBF Neural Network using Kalman filtering approach (ANN) and a Dynamic Time Warping (DTW) model. We propose to use TESPAR (Time Encoded Signal Processing And Recognition) features for Speaker recognition and MFCC Coefficients for password recognition. The key problem is to define the TESPAR alphabet used for the TESPAR coding process. In this paper we propose an approach to generate this alphabet using the Kohenen Neural Networks in a Vector Quantization process. For the recognition process a modified RBF Neural Network using Extended Kalman filtering approach (ANN) is used. MFCC Coefficients using DTW based approach is used to verify the password related information. The combined model has been tested on 20 speakers with an overall accuracy of 98.82%.
7C0324A9	The beauty of human speech lies in the complexity of the different sounds that can be produced by a few tubes and muscles. This intricacy, however, makes speech processing a challenging task. One defining characteristic of speech is its pitch. Detecting this Pitch or equivalently, fundamental frequency detection of a speech signal is important in many speech applications. Pitch detectors are used in vocoders, speaker identification and verification systems and also as aids to the handicapped. Because of its importance many solutions to detect pitch has been proposed both in time and frequency domains. One such solution is pitch detection is by using Autocorrelation method and Average Magnitude Difference Function (AMDF), method which are analyses done in the time domain and the other is detecting the harmonic nature in the frequency domain. This paper gives the implementation results of the pitch period estimated in the time and frequency domains for vowel and fricative speech sounds, both for male and female speakers.
7722C102	The task of query-by-example spoken term detection (QbE-STD) is to find a spoken query within spoken audio data. Current state-of-the-art techniqu es assume zero prior knowledge about the language of the audio data, and thus explore dynamic time warping (DTW) based techniques for the QbE-STD task. In this paper, we use a variant of DTW based algorithm referred to as non-segmental DTW (NS-DTW), with a computational upper bound of and analyze the performance of QbE-STD with Gaussian posteriorgrams obtained from spectral and temporal features of the speech signal. The results show that frequency domain linear prediction cepstral coef fi cients, which capture the temporal dynamics of the speech signal, can be used as an alternative to traditional spectral parameters such as linear prediction cepstral coefficients, perceptual linear prediction cepstral coef fi cients and Mel-frequency cepstral coefficients. We also introduce another variant of NS-DTW called fast NS-DTW (FNS-DTW) which uses reduced feature vectors for search. With a reduction factor of , we show that the computational upper bound for 
81671F57	This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus
7FDB6778	In this work, we propose a novel scheme to re-estimate the linear predictive parameters in sparse speech coding. The idea is to estimate the optimal truncated impulse response that creates the given sparse coded residual without distortion. An all-pole approximation of this impulse response is then found using a least square approximation. The all-pole approximation is a stable linear predictor that allows a more efficient reconstruction of the segment of speech. The effectiveness of the algorithm is proved in the experimental analysis.
8319BFE4	Traditional and fuzzy cluster analyses are applicable to variables whose values are uncorrelated. Hence, in order to cluster time series data which are usually serially correlated, one needs to extract features from the time series, the values of which are uncorrelated. The periodogram which is an estimator of the spectral density function of a time series is a feature that can be used in the cluster analysis of time series because its ordinates are uncorrelated. Additionally, the normalized periodogram and the logarithm of the normalized periodogram are also features that can be used. In this paper, we consider a fuzzy clustering approach for time series based on the estimated cepstrum. The cepstrum is the spectrum of the logarithm of the spectral density function. We show in our simulation studies for the typical generating processes that have been considered, fuzzy clustering based on the cepstral coefficients performs very well compared to when it is based on other features.
7776875F	This paper addresses the problem of automatically detecting infant crying sounds. Infant crying sounds show the distinct and regular time-frequency patterns that include a clear harmonic structure and a unique melody. Therefore, extracting appropriate features to properly represent these characteristics is important in achieving a good performance. In this paper, we propose weighted segment-based two-dimensional linear-frequency cepstral coefficients to characterize the time-frequency patterns within a long-range segment of the target signal. A Gaussian mixture model is adopted to statistically represent the crying and non-crying sounds, and test sounds are classified by using a likelihood ratio test. Evaluation of the proposed feature extraction method on a database of several hundred crying and non-crying sound clips yields an average equal error rate of 4.42% in various noisy environments, showing over 20% relative improvements compared to conventional feature extraction methods.
7E3F7F89	Dialog-act tagging is one of the hot topics in processing human-human conversation. In this paper, we introduce a novel model to predict and tag the dialog-act, in which Markov decision process (MDP) is utilized to predict the dialog-act sequence instead of using traditional dialog-act based n-gram, and Support Vector Machine (SVM) is employed to classify the dialog-act for each utterance. The predicting result of MDP and the classifying result of SVM are integrated as the final tagging. The experimental results have shown that our approach outperforms the traditional method.
7CF8DE0F	The conventional model of the linear prediction analysis suffers from difficulties in estimating vocal tract characteristics of high-pitched speakers. This paper shows that for voiced speech the vocal tract characteristics can be estimated accurately by homomorphic deconvolution in the autocorrelation domain. The speech autocorrelation function used by linear prediction is actually an 'aliased' version of that of the vocal tract system impulse response. This aliasing occurs due to the periodic nature of voiced speech. By using cepstrum analysis, the effect of this periodicity is eliminated from the autocorrelation function which is also periodic with the same periodicity as speech itself. The formant frequencies estimated using the deconvolved autocorrelation sequences of the system impulse response are found to be accurate by more than an order of magnitude when compared with the conventional linear prediction. The accuracy of formant estimation is verified on synthetic vowels for a wide range of pitch periods. The validity of the proposed method is also examined by inspecting the estimated spectral envelopes of real speech spoken by a female child.
805E033C	We discuss techniques for voice activity detection (VAD) for voice over Internet Protocol (VoIP). VAD aids in saving the bandwidth requirement of a voice session, thereby increasing the bandwidth efficiently. We compare the quality of speech, level of compression and computational complexity for three time-domain and three frequency-domain VAD algorithms. Implementation of time-domain algorithms is computationally simple. However, better speech quality is obtained with the frequency-domain algorithms. A comparison of the merits and demerits along with the subjective quality of speech after removal of silence periods is presented for all the algorithms. A quantitative measurement of speech quality for different algorithms is also presented.
7CA89D80	An  algorithm  is  presented  for  the  estimation  of  the  fundamental  frequency of  speech  or musical   sounds.   It   is   based   on   the   well-known   autocorrelation   method   with   a   number   of modifications  that  combine  to  prevent  errors. The  algorithm  has  several  desirable  features.  Error rates are about three times lower than the best competing methods, as evaluated over a database of speech  recorded  together  with  a  laryngograph  signal.  There  is  no  upper  limit  on  the  frequency search  range,  so  the  algorithm  is  suited  for  high-pitched  voices  and  music.  The  algorithm  is relatively  simple  and  may  be  implemented  efficiently  and  with  low  latency,  and  it  involves  few parameters that must be tuned. It is based on a signal model ~ periodic signal ! that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing. 
7E1B495B	Currently, there are technology barriers inhibiting speech processing systems working under extreme noisy conditions. The emerging applications of speech technology, especially in the fields of wireless communications, digital hearing aids or speech recognition, are examples of such systems and often require a noise reduction technique operating in combination with a precise voice activity detector (VAD). This paper presents a new VAD algorithm for improving speech detection robustness in noisy environments and the performance of speech recognition systems. The algorithm measures the long-term spectral divergence (LTSD) between speech and noise and formulates the speech/non-speech decision rule by comparing the long-term spectral envelope to the average noise spectrum, thus yielding a high discriminating decision rule and minimizing the average number of decision errors. The decision threshold is adapted to the measured noise energy while a controlled hang-over is activated only when the observed signal-to-noise ratio is low. It is shown by conducting an analysis of the speech/non-speech LTSD distributions that using long-term information about speech signals is beneficial for VAD. The proposed algorithm is compared to the most commonly used VADs in the field, in terms of speech/non-speech discrimination and in terms of recognition performance when the VAD is used for an automatic speech recognition system. Experimental results demonstrate a sustained advantage over standard VADs such as G.729 and adaptive multi-rate (AMR) which were used as a reference, and over the VADs of the advanced front-end for distributed speech recognition.
76DFEEE9	An effective voice activity detection (VAD) algorithm is proposed for improving speech recognition performance in noisy environments. The proposed speech/pause discrimination method is based on a hard-decision clustering approach built on a set of subband log-energies and noise prototypes that define a cluster. Detecting the presence of speech (a new cluster) is achieved using a basic sequential algorithm scheme (BSAS) according to a given “distance” (in this case, geometrical distance) and a suitable threshold. The accuracy of the Cluster VAD (ClVAD) algorithm lies in the use of a decision function defined over a multiple-observation (MO) window of averaged subband log-energies and a suitable noise subspace model defined in terms of prototypes. In addition, the reduced computational cost of the clustering approach makes it adequate for real-time applications, i.e. speech recognition. An exhaustive analysis is conducted on the Spanish SpeechDat-Car databases in order to assess the performance of the proposed method and to compare it to existing standard VAD methods. The results show improvements in detection accuracy over standard VADs such as ITU-T G.729, ETSI GSM AMR and ETSI AFE and a representative set of recently reported VAD algorithms for noise robust speech processing.
7B245CA0	Nowadays, the accuracy of speech processing systems is strongly affected by acoustic noise. This is a serious obstacle regarding the demands of modern applications. Therefore, these systems often need a noise reduction algorithm working in combination with a precise voice activity detector (VAD). The computation needed to achieve denoising and speech detection must not exceed the limitations imposed by real time speech processing systems. This paper presents a novel VAD for improving speech detection robustness in noisy environments and the performance of speech recognition systems in real time applications. The algorithm is based on a Multivariate Complex Gaussian (MCG) observation model and defines an optimal likelihood ratio test (LRT) involving multiple and correlated observations (MCO) based on a jointly Gaussian probability distribution (jGpdf) and a symmetric covariance matrix. The complete derivation of the jGpdf-LRT for the general case of a symmetric covariance matrix is shown in terms of the Cholesky decomposition which allows to efficiently compute the VAD decision rule. An extensive analysis of the proposed methodology for a low dimensional observation model demonstrates: (i) the improved robustness of the proposed approach by means of a clear reduction of the classification error as the number of observations is increased, and (ii) the trade-off between the number of observations and the detection performance. The proposed strategy is also compared to different VAD methods including the G.729, AMR and AFE standards, as well as other recently reported algorithms showing a sustained advantage in speech/non-speech detection accuracy and speech recognition performance using the AURORA databases.
78881FBC	This paper proposes a noise robust voice activity detection (VAD) technique called PARADE (PAR based Activity DEtection) that employs the periodic component to aperiodic component ratio (PAR). Conventional noise robust features for VAD are still sensitive to non-stationary noise, which yields variations in the signal-to-noise ratio, and sometimes requires a priori noise power estimations, although the characteristics of environmental noise change dynamically in the real world. To overcome this problem, we adopt the PAR, which is insensitive to both stationary and non-stationary noise, as an acoustic feature for VAD. By considering both periodic and aperiodic components simultaneously in the PAR, we can mitigate the effect of the non-stationarity of noise. PARADE first estimates the fundamental frequencies of the dominant periodic components of the observed signals, decomposes the power of the observed signals into the powers of its periodic and aperiodic components by taking account of the power of the aperiodic components at the frequencies where the periodic components exist, and calculates the PAR based on the decomposed powers. Then it detects the presence of target speech signals by estimating the voice activity likelihood defined in relation to the PAR. Comparisons of the VAD performance for noisy speech data confirmed that PARADE outperforms the conventional VAD algorithms even in the presence of non-stationary noise. In addition, PARADE is applied to a front-end processing technique for automatic speech recognition (ASR) that employs a robust feature extraction method called SPADE (Subband based Periodicity and Aperiodicity DEcomposition) as an application of PARADE. Comparisons of the ASR performance for noisy speech show that the SPADE front-end combined with PARADE achieves significantly higher word accuracies than those achieved by MFCC (Mel-frequency Cepstral Coefficient) based feature extraction, which is widely used for conventional ASR systems, the SPADE front-end without PARADE, and other standard noise robust front-end processing techniques (ETSI ES 202 050 and ETSI ES 202 212). This result confirmed that PARADE can improve the performance of front-end processing for ASR.
79E74BEE	A new rescoring method for spoken term detection (STD) is proposed. Phoneme-based close-matching techniques have been used because of their ability to detect out-of-vocabulary (OOV) queries. To improve the accuracy of phoneme-based techniques, rescoring techniques have been used to accurately re-rank the results from phoneme-based close-matching; however, conventional rescoring techniques based on an utterance verification model still produce many false detection results. To further improve the accuracy, in this study, several features representing the “naturalness” (or “abnormality”) of duration of phonemes/syllables in detected candidates of a keyword are proposed. These features are incorporated into a conventional rescoring technique using logistic regression. Experimental results with a 604-hour Japanese speech corpus indicated that combining the rhythmic features achieved a further relative error reduction of 8.9% compared to a conventional rescoring technique.