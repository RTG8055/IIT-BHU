0BD946B6	Building accurate models from a small amount of available training data can sometimes prove to be a great challenge. Expert domain knowledge can often be used to alleviate this burden. Parameter Sharing is one such important form of domain knowledge. Graphical models like HMMs, DBNs and Module Networks use different forms of Parameter Sharing to reduce the variance in the parameter estimates. The goal of this paper is to present a theoretical approach for learning in presence of several other types of Parameter Related Domain Knowledge that go beyond the ones in the above models. First, we introduce a General Parameter Sharing Framework that describes the models just mentioned, but allows for much finer grained parameter sharing assumptions. In this framework, we present sound procedures for parameter learning from both a Frequentist and a Bayesian point of view, from both complete and incomplete data, in the case where a domain expert specifies in advance the structure of the graphical model, and the subsets of parameters to be shared. Second, we describe a hierarchical extension of this framework based on Parameter Sharing Trees. Finally we present algorithms for using domain knowledge that specifies that certain groups of parameters share certain properties. In particular, we consider two kinds of constraints: first kind states certain groups of parameters share the same aggregate probability mass and second kind states the ratio of the parameters is preserved (shared) in several groups. As an example, we derive a novel form of parameter sharing for Bayesian Multinetworks.
7EBEC735	An efficient method for computing the discrete cosine transform (DCT) is proposed. Based on direct decomposition of the DCT, the recursive properties of the DCT for an even length input sequence is derived, which is a generalization of the radix 2 DCT algorithm. Based on the recursive property, a new DCT algorithm for an even length sequence is obtained. The proposed algorithm is very structural and requires fewer computations when compared with others. The regular structure of the proposed algorithm is suitable for fast parallel algorithm and VLSI implementation.
59E5C607	In this paper I explore the psychology of ritual performance and present a simple graphical model that clarifies several issues in William Irons’s theory of religion as a “hard-to-fake” sign of commitment. Irons posits that religious behaviors or rituals serve as costly signals of an individual’s commitment to a religious group. Increased commitment among members of a religious group may facilitate intra-group cooperation, which is argued to be the primary adaptive benefit of religion. Here I propose a proximate explanation for how individuals are able to pay the short-term costs of ritual performance to achieve the long-term fitness benefits offered by religious groups. The model addresses three significant problems raised by Irons’s theory. First, the model explains why potential free-riders do not join religious groups even when there are significant net benefits that members of religious groups can achieve. Second, the model clarifies how costly a ritual must be to achieve stability and prevent potential free-riders from joining the religious group. Third, the model suggests why religious groups may require adherents to perform private rituals that are not observed by others. Several hypotheses generated from the model are also discussed.
7B13384D	 This paper reviews some of the key statistical ideas that are encountered when trying to find empirical support to causal interpretations and conclusions, by applying statistical methods on experimental or observational longitudinal data. In such data, typically a collection of individuals are followed over time, then each one has registered a sequence of covariate measurements along with values of control variables that in the analysis are to be interpreted as causes, and finally the individual outcomes or responses are reported. Particular attention is given to the potentially important problem of confounding. We provide conditions under which, at least in principle, unconfounded estimation of the causal effects can be accomplished. Our approach for dealing with causal problems is entirely probabilistic, and we apply Bayesian ideas and techniques to deal with the corresponding statistical inference. In particular, we use the general framework of marked point processes for setting up the probability models, and consider posterior predictive distributions as providing the natural summary measures for assessing the causal effects. We also draw connections to relevant recent work in this area, notably to Judea Pearl's formulations based on graphical models and his calculus of so-called do-probabilities. Two examples illustrating different aspects of causal reasoning are discussed in detail.
7E19344D	A novel algorithm to convert the discrete cosine transform (DCT) to skew-circular convolutions is presented. The motivation for developing such an algorithm is the fact that VLSI implementation of distributed arithmetic is very efficient for computing convolutions. It is also shown that the inverse DCT (IDCT) can be computed using the same building blocks which are used for computing the DCT. A DCT/IDCT processor can be designed to compute either the DCT or the IDCT depending on a 1-b control signal.
7CE629CE	Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.
772CE3A1	Most interactive graphical applications that use direct manip- ulation are built with low-level libraries such as Xlib because the graphic and interaction models of higher-level toolkits such as Motif are not extensible. This results in high de- sign, development and maintenance costs and encourages the development of stereotyped applications based on buttons, menus and dialogue boxes instead of direct manipulation of the applications objects. There have been several attempts to provide high level tools for building such applications, including popular toolkits such as Garnet [26], Unidraw [33], Fresco [21, 32] and Open- Inventor [28]. Unfortunately, these tools are not adapted to the development of sophisticated graphical editors because of their lack of extensibility: In this article we argue that these drawbacks come from the fact that high-level toolkits rely on a visualization model to manage interaction. We introduce a model that uses several graphical layers to separate the graphic entities involved in visualization from those involved in feedback and interaction management. We describe the implementation of this Multi- Layer Model and we show how it can take advantage of soft- ware and hardware graphic extensions to provide good per- formance. We also show how it supports multiple input de- vices and simplifies the description of a wide variety of in- teraction styles. Finally, we describe our experience in using this model to implement a set of editors for a professional an- imation system. 
7FA005E7	This letter proposes a fast radix-q algorithm to compute type-IV discrete cosine transform (DCT) of the length qlambda, where q is an odd positive integer. The proposed fast radix-q algorithm has merits in computational complexity, parallelism, and numerical stability over existing algorithms. Furthermore, the fast radix-q algorithm is used to develop the fast mixed-radix type-II/ type-IV DCT algorithm for composite lengths.
77C93D55	The modified discrete cosine transform (MDCT) and modified discrete sine transform (MDST) are employed in subband/transform coding schemes as the analysis/synthesis filter banks based on the concept of time domain aliasing cancellation (TDAC). Princen, Bradley and Johnson defined two types of the MDCT, specifically, for an evenly stacked and oddly stacked analysis/synthesis systems. The MDCT is the basic processing component in the international audio coding standards and commercial products for high-quality audio compression. Almost all existing audio coding systems have used the complex-valued or real-valued FFT algorithms, and the DCT/DST of type IV (DCT-IV/DST-IV) for the fast MDCT computation. New fast and efficient algorithm for a unified forward and inverse MDCT/MDST computation in the oddly stacked system is proposed. It is based on the DCT/DST of types II and III (DCT-II/DST-II, DCT-III/DST-III), and the real arithmetic is used only. Corresponding generalized signal flow graph is regular, structurally simple and enables to compute MDCT/MDST and their inverses in general for any N divisible by 4 (N being length of a data sequence). Consequently, the new fast algorithm can be adopted for the MDCT computation in the current audio coding standards such as MPEG family (MPEG-1, MPEG-2, MPEG-2 Advanced Audio Coding and MPEG-4 audio), and in commercial products (proprietary audio coding algorithms) such as Sony MiniDisc/ATRAC/ATRAC2/SDDS digital audio coding systems, the AT&T Perceptual Audio Coder (PAC) or Lucent Technologies PAC/Enhanced PAC/Multichannel PAC, and Dolby Labs AC-3 digital audio compression algorithm. Besides the new fast algorithm has some interesting properties, it provides an efficient implementation of the forward and inverse MDCT computation for layer III in MPEG audio coding, where the length of data blocks. Especially, for the AC-3 algorithm, it is shown how both the proposed new MDCT/MDST algorithm and existing fast algorithms/computational architectures for the discrete sinusoidal transforms computation of real data sequences such as the DCT-IV/DST-IV, generalized discrete Fourier transform of type IV (DFT-IV) and generalized discrete Hartley transform of type IV (DHT-IV) can be used for the fast alternate or simultaneous (on-line) MDCT/MDST computation by simple pre- and post-processing of data sequences.
80E6A82C	An efficient direct method for the computation of a length-N discrete cosine transform (DCT) given two adjacent length-(N/2) DCT coefficients, is presented. The computational complexity of the proposed method is lower than the traditional approach for lengths N>8. Savings of N memory locations and 2N data transfers are also achieved.
7F9A8948	Existing recommender systems provide an elegant solution to the information overload in current digital libraries such as the Internet archive. Nowadays, the sensors that capture the user's contextual information such as the location and time are become available and have raised a need to personalize recommendations for each user according to his/her changing needs in different contexts. In addition, visual documents have richer textual and visual information that was not exploited by existing recommender systems. In this paper, we propose a new framework for context-aware recommendation of visual documents by modeling the user needs, the context and also the visual document collection together in a unified model. We address also the user's need for diversified recommendations. Our pilot study showed the merits of our approach in content based image retrieval.
7E68A6E2	The discrete cosine transform (DCT) is often computed from a discrete Fourier transform (DFT) of twice or four times the DCT length. DCT algorithms based on identical-length DFT algorithms generally require additional arithmetic operations to shift the phase of the DCT coefficients. It is shown that a DCT of odd length can be computed by an identical-length DFT algorithm, by simply permuting the input and output sequences. Using this relation, odd-length DCT modules for a prime factor DCT are derived from corresponding DFT modules. The multiplicative complexity of the DCT is then derived in terms of DFT complexities.
7F77E0BA	The discrete cosine transform (DCT) and the discrete sine transform (DST) have found wide applications in speech and image processing, as well as telecommunication signal processing for the purpose of data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new recursive algorithms for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design recursive algorithms for the 2-D DCT and the 2-D DST. These new structured recursive algorithms are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other recursive algorithms. Finally, we propose two parallel algorithms for accelerating the computation.
7D10C675	In this paper, we first propose an efficient algorithm for computing one-dimensional (1-D) discrete cosine transform (DCT) for a signal block, given its two adjacent subblocks in the DCT domain and then introduce several algorithms for the fast computation of multidimensional (m-D) DCT with size N/sub 1//spl times/N/sub 2//spl times/.../spl times/N/sub m/ given 2/sup m/ subblocks of DCT coefficients with size N/sub 1//2/spl times/N/sub 2//2/spl times/.../spl times/N/sub m//2, where N/sub i/(i=1,2,...,m) are powers of 2. Obviously, the row-column method, which employs the most efficient algorithms along each dimension, reduces the computational complexity considerably, compared with the traditional method, which employs only the one-dimensional (1-D) fast DCT and inverse DCT (IDCT) algorithms. However, when m/spl ges/2, the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms, has lower computational complexity than the row-column method. Besides, we propose a direct method by dividing the data into 2/sup m/ parts for independent fast computation, in which only two steps of r-dimensional (r=1,2,...,m) IDCT and additional multiplications and additions are required. If all the dimensional sizes are the same, the number of multiplications required for the direct method is only (2/sup m/-1)/m2/sup m-1/ times of that required for the row-column method, and if N/spl ges/2/sup 2m-1/, the computational efficiency of the direct method is surely superior to that of the traditional method, which employs the most efficient multidimensional DCT/IDCT algorithms.
7FE3364C	Recently, a fast radix-q algorithm for an efficient computation of the type-IV discrete cosine transform (DCT-IV) has been proposed in , where q is an odd positive integer. In particular, based on the proposed fast algorithm, optimized efficient 3-, 5-, and 9-point scaled DCT-IV (SDCT-IV) modules have been derived in . As a response, an improved efficient optimized 9-point scaled DCT-IV (SDCT-IV) module in terms of the arithmetic complexity is presented. The improved optimized efficient 9-point SDCT-IV module requires 17 multiplications, 53 additions, and three shifts. Consequently, the arithmetic complexity of extended fast mixed-radix DCT-IV algorithm for composite lengths is also significantly improved.
7C5AAA1F	Therapy processes are complex dynamical systems where several variables are constantly interacting with each other. In general, the underlying mechanisms are difficult to assess. Our approach is to identify the dependency structure of relevant variables within the therapy process using interaction graphs. These are instruments for multivariate time series which are based on the analysis of partial spectral coherences. We used interaction graphs in order to investigate the therapy process of a multimodal therapy concept for fibromyalgia patients. Our main hypothesis was that self-efficacy plays a central role in the therapy process. Methods: Patients kept an electronic diary for 13 weeks. Pain intensity, depression, sleep quality, anxiety and self-efficacy were assessed via visual analogue scales. The resulting multivariate time series were aggregated over individuals, and partial spectral coherences between each pair of the variables were calculated. From the partial coherences, interaction graphs were plotted. Results: Within the resulting graphical model, self-efficacy was strongly related to pain intensity, depression and sleep quality. All other relations were substantially weaker. There was no direct relationship between pain intensity and sleep quality. Conclusions: The relations between two variables within the therapy process are mainly induced by self-efficacy. Interaction graphs can be used to pool time series data of several patients and thus to assess the common underlying dependency structure of a group of patients. The graphical representation is easily comprehensible and allows to distinguish between direct and indirect relationships.
80D10329	Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.
7D843341	A Fast Discrete Cosine Transform algorithm has been developed which provides a factor of six improvement in computational complexity when compared to conventional Discrete Cosine Transform algorithms using the Fast Fourier Transform. The algorithm is derived in the form of matrices and illustrated by a signal-flow graph, which may be readily translated to hardware or software implementations.
7963E765	The forensic investigation of the origin and cause of a fire incident is a particularly demanding area of expertise. As the available evidence is often incomplete or vague, uncertainty is a key element. The present study is an attempt to approach this through the use of Bayesian networks, which have been found useful in assisting human reasoning in a variety of disciplines in which uncertainty plays a central role. The present paper describes the construction of a Bayesian network (BN) and its use for drawing inferences about propositions of interest, based upon a single, possibly non replicable item of evidence: detected residual quantities of a flammable liquid in fire debris.
7A1904AD	A new recursive algorithm and two types of circuit architectures are presented for the computation of the two-dimensional discrete cosine transform (2D DCT). The new algorithm permits to compute the 2D DCT by a simple procedure of the 1D recursive calculations involving only cosine coefficients. The recursive kernel for the proposed algorithm contains a small number of operations. Also, it requires a smaller number of pre-computed data compared with many of existing algorithms in the same category. The kernel can be easily implemented in a simple circuit block with a short critical delay path. In order to evaluate the performance improvement resulting from the new algorithm, an architecture for the 2D DCT designed by direct mapping from the computation structure of the proposed algorithm has been implemented in an FPGA board. The results show that the reduction of the hardware consumption can easily reach 25 and the clock frequency can increase 17 compared with a system implementing a recently reported 2D DCT recursive algorithm. For a further reduction of the hardware, another architecture has been proposed for the same 2D DCT computation. Using one recursive computation block to perform different functions, this architecture needs only approximately one-half of the hardware that is required in the first architecture, which has been confirmed by an FPGA implementation.
7B64D19B	The discrete cosine transform of type IV (DCT-IV) and corresponding discrete sine transform of type IV (DST-IV) have played key role in the efficient implementation of orthogonal lapped transforms and perfect reconstruction cosine-modulated filter banks such as the oddly stacked modified discrete cosine transform (MDCT) or equivalently, the modulated lapped transform (MLT). However, the DCT-IV and DST-IV of double sizes are related to two variants of filter banks defined by Dolby Labs AC-3 digital audio compression algorithm. Since these two variants of filter banks are efficiently computed by recently proposed new fast algorithm for the oddly stacked MDCT (Signal Processing 82 (2002) 433), it is shown that the efficient DCT-IV and DST-IV computation can be realized via the MDCT of double size. The careful analysis of regular structure of the new fast MDCT algorithm allows to extract a new DCT-IV/DST-IV computational structure and to suggest a new sparse matrix factorization of the DCT-IV matrix. Finally, the new DCT-IV/DST-IV computational structure provides an alternative efficient implementation of the forward and inverse MDCT in layer III of MPEG (MP3) audio coding.
7A8EB39B	Recommender system provides users with personalized suggestions of product or information. Typically, recommender systems rely on a bipartite graph model to capture user interest. As an extension, some boosted methods analyze content information to further improve the quality of personalized recommendation. However, due to the prevalence of short and sparse messages in online social media, traditional content-boosted methods do not guarantee to capture user preference accurately especially for web contents. In this paper, we propose a novel graphical model to extract hidden topics from web contents, cluster web contents, and detect users' interests on each cluster. In addition, we introduce two reranking models which utilize the detected user interest to further boost the quality of personalized recommendation. Experiment results on a public dataset demonstrated the limitation of a traditional content-boosted approach, and also showed the validity of our proposed techniques.
77988EDA	Two types of efficient algorithms for fast implementation of the 2-D discrete cosine transform (2-D DCT) are developed. One involves recursive structure which implies that the algorithm for (M/2 X N/2) block be extended to (M X N/2) (M/2 X M) and (M X N) blocks (M and N are integer powers of two). The second algorithm is nonrecursive and therefore it has to be tailored for each block size. Both algorithms involve real arithmetic and they reduce the number of multiplications significantly compared to the fast algorithm developed by Chen et al. [8], while the number of additions remain unchanged.
82078F27	We introduce the structural interface algorithm for exact probabilistic inference in dynamic Bayesian networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.
7E271971	We propose a Bayesian Network (BN) model to integrate multiple contextual information and the image measurements for image segmentation. The BN model systematically encodes the contextual relationships between regions, edges and vertices, as well as their image measurements with uncertainties. It allows a principled probabilistic inference to be performed so that image segmentation can be achieved through a most probable explanation (MPE) inference in the BN model. We have achieved encouraging results on the horse images from the Weizmann dataset. We have also demonstrated the possible ways to extend the BN model so as to incorporate other contextual information such as the global object shape and human intervention for improving image segmentation. Human intervention is encoded as new evidence in the BN model. Its impact is propagated through belief propagation to update the states of the whole model. From the updated BN model, new image segmentation is produced.
7D05A275	As the circuit complexity is increasing in demand for the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera is one of them where realtime image capturing, compression and storage of compressed image data is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image.
7700F7BC	Context aware recommender systems go beyond the traditional personalized recommendation models by incorporating a form of situational awareness. They provide recommendations that not only correspond to a users preference profile, but that are also tailored to a given situation or context. We consider the setting in which contextual information is represented as a subset of an item feature space describing short-term interests or needs of a user in a given situation. This contextual information can be provided by the user in the form of an explicit query, or derived implicitly. We propose a unified probabilistic model that integrates user profiles, item representations, and contextual information. The resulting recommendation framework computes the conditional probability of each item given the user profile and the additional context. These probabilities are used as recommendation scores for ranking items. Our model is an extension of the Latent Dirichlet Allocation (LDA) model that provides the capability for joint modeling of users, items, and the meta-data associated with contexts. Each user profile is modeled as a mixture of the latent topics. The discovered latent topics enable our system to handle missing data in item features. We demonstrate the applica- tion of our framework for article and music recommendation. In the latter case, the set of popular tags from social tagging Web sites are used for context descriptions. Our evaluation results show that considering context can help improve the quality of recommendations.
5E453122	In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al.
7F4AA6EA	Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.
809ACCE2	Due to the exponential growth of information on the Web, Recommender Systems have been developed to generate suggestions to help users overcome information overload and sift through huge amounts of information efficiently. Many existing approaches to recommender systems can neither handle very large datasets nor easily deal with users who have made very few ratings. Moreover, traditional recommender systems consider only the rating information, resulting in the loss of flexibility. Tagging has recently emerged as a popular way for users to annotate, organize and share resources on the Web. Several research tasks have shown that tags can represent users judgments about Web contents quite accurately. In the light of the facts that both the rating activity and tagging activity can reflect users opinions, this paper proposes a factor analysis approach called TagRec based on a unified probabilistic matrix factorization by utilizing both users tagging information and rating information. The complexity analysis indicates that our approach can be applied to very large datasets. Furthermore, experimental results on MovieLens data set show that our method performs better than the state-of-the-art approaches.
7B368C93	Bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for Gaussian models. For more robust inferences, it is natural to consider extensions to t-distribution models. We argue that the classical multivariate t-distribution, defined using a single latent Gamma random variable to rescale a Gaussian random vector, is of little use in more highly multivariate settings, and propose other, more flexible t-distributions. Using an independent Gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. The associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. However, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. To this end we propose the use of Dirichlet processes for adaptive clustering of the latent Gamma-scalars, each of which may then divide a group of latent Gaussian variables. The resulting Dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties. 
81262D6D	A Complex Adaptive System (CAS) is a network of self-organizing, intelligent agents that share knowledge and adapt their operations in order to achieve overall system goals. Three things are needed to understand, design, and evaluate CAS. First, a mathematical model or way-of-thinking about CAS, called Context-Sensitive Systems (CSS) theory, is required to provide a solid foundation upon which to represent and describe the kinds of interactions that occur among the CAS agents during system operation. Second, a graphical modeling language is required that implements CSS theory in a way that enhances visualization and understanding of CAS. Third, a systems design and evaluation tool is required that makes it easy to apply CSS theory, expressed using a graphical modeling language, to understand, design, and evaluate CAS. As an example, an OpEMCSS model of two intelligent agents is discussed that learn rules and maximize their average reward in the prisoner's dilemma game.
80D1B10B	We present a dynamic graphical model (DGM) for automated multi-instrument musical transcription. By multi-instrument transcription, we mean a system capable of listening to a recording in which two or more instruments are playing, and identifying both the notes that were played and the instruments that played them. Our transcription system models two musical instruments, each capable of playing at most one note at a time. We present results for two-instrument transcription on piano and violin sounds.
8613899F	Point of interest (POI) recommendation is a popular topic on location-based social networks (LBSNs). Geographical proximity, known as a unique feature of LBSNs, significantly affects user check-in behavior. However, most of prior studies characterize the geographical influence based on a universal or personalized distribution of geographic distance, leading to unsatisfactory recommendation results. In this paper, the personalized geographical influence in a two-dimensional geographical space is modeled using the data field method, and we propose a semi-supervised probabilistic model based on a factor graph model to integrate different factors such as the geographical influence. Moreover, a distributed learning algorithm is used to scale up our method to large-scale data sets. Experimental results based on the data sets from Foursquare and Gowalla show that our method outperforms other competing POI recommendation techniques.
75B26557	We present two new models that take into account the information available in user-created favorites lists for enhancing the quality of item recommendation. The first model uses the popularity and ratings of items in the lists to predict ratings for new items to users that have rated some items on the lists. The second model is a matrix factorization model that incorporates lists as implicit feedback in ratings prediction. We compare our two approaches against another work for utilizing favorites lists, as well as the popular Singular Value Decomposition (SVD) on two large Amazon datasets and show that utilizing favorites lists gives significant improvements, especially in cold-start cases.
761D5749	We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance.
7B59C5C0	A good shopping recommender system can boost sales in a retailer store. To provide accurate recommendation, the recommender needs to accurately predict a customers preference, an ability difficult to acquire. Conventional data mining techniques, such as association rule mining and collaborative filtering, can generally be applied to this problem, but rarely produce satisfying results due to the skewness and sparsity of transaction data. In this paper, we report the lessons that we learned in two real-world data mining applications for personalized shopping recommendation. We learned that extending a collaborative filtering method based on ratings (e.g., GroupLens) to perform personalized shopping recommendation is not trivial and that it is not appropriate to apply association-rule based methods (e.g., the IBM SmartPad system) for large scale prediction of customers shopping preferences. Instead, a probabilistic graphical model can be more effective in handling skewed and sparse data. By casting collaborative filtering algorithms in a probabilistic framework, we derived HyPAM (Hybrid Poisson Aspect Modelling), a novel probabilistic graphical model for personalized shopping recommendation. Experimental results show that HyPAM outperforms GroupLens and the IBM method by generating much more accurate predictions of what items a customer will actually purchase in the unseen test data.
7576CA59	A new VLSI algorithm and its associated systolic array architecture for a prime length type IV discrete cosine transform is presented. They represent the basis of an efficient design approach for deriving a linear systolic array architecture for type IV DCT. The proposed algorithm uses a regular computational structure called pseudoband correlation structure that is appropriate for a VLSI implementation. The proposed algorithm is then mapped onto a linear systolic array with a small number of I/O channels and low I/O bandwidth. The proposed architecture can be unified with that obtained for type IV DST due to a similar kernel. A highly efficient VLSI chip can be thus obtained with good performance in the architectural topology, computing parallelism, processing speed, hardware complexity and I/O costs similar to those obtained for circular correlation and cyclic convolution computational structures.
7D3C2818	AnN-point discrete Fourier transform (DFT) algorithm can be used to evaluate a discrete cosine transform by a simple rearrangement of the input data. This method is about two times faster compared to the conventional method which uses a2N-point DFT.