03F42860	In the Clinical Decision Support System (CDSS), over-fitting phenomenon may appear when decision tree algorithm was used. For this problem, this paper will make use of the Rough Set theory to the training set for attribute reduction, the decision tree built by using the decision tree algorithm was used to predict the test data. In this paper, 46 copies of coronary heart disease clinical data were used to test the improved algorithm. Comparing the accuracy of the algorithm and the improved algorithm, we can know that, the improved algorithm has a better recognition rate for the diagnosis of coronary heart disease, and effectively solves the over-fitting phenomenon in the Decision Tree Algorithm.
77531D91	Identify individual and environmental variables associated with caregiver stability and instability for children in diverse permanent placement types (i.e., reunification, adoption, and long-term foster care/guardianship with relatives or non-relatives), following 5 or more months in out-of-home care prior to age 4 due to substantiated maltreatment.Participants were 285 children from the Southwestern site of Longitudinal Studies of Child Abuse and Neglect (LONGSCAN). Caregiver instability was defined as a change in primary caregiver between ages 6 and 8 years. Classification and regression tree (CART) analysis was used to identify the strongest predictors of instability from multiple variables assessed at age 6 with caregiver and child reports within the domains of neighborhood/community characteristics, caregiving environment, caregiver characteristics, and child characteristics.One out of 7, or 14% of the 285 children experienced caregiver instability in their permanent placement between ages 6 and 8. The strongest predictor of stability was whether the child had been placed in adoptive care. However, for children who were not adopted, a number of contextual factors (e.g., father involvement, expressiveness within the family) and child characteristics (e.g., intellectual functioning, externalizing problem behaviors) predicted stability and instability of permanent placements.Current findings suggest that a number of factors should be considered, in addition to placement type, if we are to understand what predicts caregiver stability and find stable permanent placements for children who have entered foster care. These factors include involvement of a father figure, family functioning, and child functioning.Adoption was supported as a desired permanent placement in terms of stability, but results suggest that other placement types can also lead to stability.In fact, with attention to providing biological parents, relative, and non-relative caregivers with support and resources the likelihood that a child will have a stable caregiver may be increased.
80DFDDF3	A common approach to split selection in classification trees is to search through all possible splits generated by predictor variables. A splitting criterion is then used to evaluate those splits and the one with the largest criterion value is usually chosen to actually channel samples into corresponding subnodes. However, this greedy method is biased in variable selection when the numbers of the available split points for each variable are different. Such result may thus hamper the intuitively appealing nature of classification trees. The problem of the split selection bias for two-class tasks with numerical predictors is examined. The statistical explanation of its existence is given and a solution based on the P-values is provided, when the Pearson chi-square statistic is used as the splitting criterion.
7E8F41FE	A new model for supervised classification based on probabilistic decision graphs is introduced. A probabilistic decision graph (PDG) is a graphical model that efficiently captures certain context specific independencies that are not easily represented by other graphical models traditionally used for classification, such as the Naïve Bayes (NB) or Classification Trees (CT). This means that the PDG model can capture some distributions using fewer parameters than classical models. Two approaches for constructing a PDG for classification are proposed. The first is to directly construct the model from a dataset of labelled data, while the second is to transform a previously obtained Bayesian classifier into a PDG model that can then be refined. These two approaches are compared with a wide range of classical approaches to the supervised classification problem on a number of both real world databases and artificially generated data.
7DD171A0	Classification trees are a popular tool in applied statistics because their heuristic search approach based on impurity reduction is easy to understand and the interpretation of the output is straightforward. However, all standard algorithms suffer from a major problem: variable selection based on standard impurity measures as the Gini Index is biased. The bias is such that, e.g., splitting variables with a high amount of missing values—even if missing completely at random (MCAR)—are artificially preferred. A new split selection criterion that avoids variable selection bias is introduced. The exact distribution of the maximally selected Gini gain is derived by means of a combinatorial approach and the resulting -value is suggested as an unbiased split selection criterion in recursive partitioning algorithms. The efficiency of the method is demonstrated in simulation studies and a real data study from veterinary gynecology in the context of binary classification and continuous predictor variables with different numbers of missing values. The proposed method is extendible to categorical and ordinal predictor variables and to other split selection criteria such as the cross-entropy.
79406A62	In many medical applications, longitudinal data sets are available. Longitudinal data, as well as observations from paired organs, show a dependency structure which should be respected in the evaluation. Adler et al. (Comput Stat Data Anal 53(3):718–729, 2009) proposed various bootstrapping strategies for ensemble methods based on classification trees for two measurements of paired organs. These strategies have shown to improve the classification performance compared to the traditional approach, where only one observation per subject is used. We extend the methodology to the situation, where an arbitrary number of observations per individual are available and investigate the performance of the proposed methods with bagged classification trees (bagging) and random forests in the situation of longitudinal data. Moreover, we adapt the estimation of classification performance criteria to repeated measurements data. The clinical data set consists of morphological examinations of both eyes of glaucoma patients and healthy controls over a time period of up to 7 years. The performance of our modified classifiers is evaluated by a subject-based leave-one-out bootstrap ROC analysis. Simulation results and results for the glaucoma data set demonstrate that our proposal is an improvement of adhoc strategies and of the use all measurements of each subject or block strategy.
589C9711	This paper describes the evaluation of a WSD method withinSENSEVAL. This method is based on Semantic Classification Trees (SCTs)and short context dependencies between nouns and verbs. The trainingprocedure creates a binary tree for each word to be disambiguated. SCTsare easy to implement and yield some promising results. The integrationof linguistic knowledge could lead to substantial improvement.
6A1B7A68	Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.
7AF08408	In recent years, classification learning for data streams has become an important and active research topic. A major challenge posed by data streams is that their underlying concepts can change over time, which requires current classifiers to be revised accordingly and timely. To detect concept change, a common methodology is to observe the online classification accuracy. If accuracy drops below some threshold value, a concept change is deemed to have taken place. An implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as a symptom of concept change. Unfortunately however, this assumption is often violated in the real world where data streams carry noise that can also introduce a significant reduction in classification accuracy. To compound this problem, traditional noise cleansing methods are incompetent for data streams. Those methods normally need to scan data multiple times whereas learning for data streams can only afford one-pass scan because of data’s high speed and huge volume. Another open problem in data stream classification is how to deal with missing values. When new instances containing missing values arrive, how a learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored. To solve these problems, this paper proposes a novel classification algorithm, flexible decision tree (FlexDT), which extends fuzzy logic to data stream classification. The advantages are three-fold. First, FlexDT offers a flexible structure to effectively and efficiently handle concept change. Second, FlexDT is robust to noise. Hence it can prevent noise from interfering with classification accuracy, and accuracy drop can be safely attributed to concept change. Third, it deals with missing values in an elegant way. Extensive evaluations are conducted to compare FlexDT with representative existing data stream classification algorithms using a large suite of data streams and various statistical tests. Experimental results suggest that FlexDT offers a significant benefit to data stream classification in real-world scenarios where concept change, noise and missing values coexist.
560B2421	Knowledge of the composition and areal distribution of aquatic vegetation types, as well as their seasonal and interannual variations, is crucial for managing and maintaining the balance of lake ecosystems. In this study, a series of remotely sensed images with a resolution of 30 m (HJ-CCD and Landsat TM) were collected and used to map the distribution of aquatic vegetation types in Taihu Lake, China. Seasonal and interannual dynamics of aquatic vegetation types were explored and analyzed. The distribution areas of Type I (emergent, floating-leaved and floating vegetation) and Type II (submerged vegetation) were used to model their growing season phenology by double logistic functions. The resulting double logistic models showed, the area of Type I reached its peak in mid-August, and the maximum area for Type II occurred in mid-September. From 1984 to 2013, Type I area increased continuously from 59.75 km2 to 148.00 km2 (R2 = 0.84), whereas the area covered by Type II first increased and then decreased, with a trend conforming to a significant quadratic curve (R2 = 0.83). The eutrophication and stable state of Taihu Lake was assessed using a simple indicator which was expressed as a ratio of Type II area to Type I area. The results showed that the eutrophication in the lake might have been increasing in the area studied since 2000. Additionally, the results showed that air temperature had likely a direct effect on the growth of Type I (R2 = 0.66) and a significant, but delayed, effect on the growth of Type II.
76CDF761	Economic evaluation of a new oil well is important for decision-making in the petroleum industry, and this evaluation is based on a good prediction on a well's production. However, it is difficult to accurately predict a well's production due to the complex subsurface conditions of reservoirs. The industrial standard approach is to use either curve-fitting methods or complex and time-consuming reservoir simulations. In this paper, an enhanced decision tree learning approach called neural-based decision tree (NDT) model is applied in an attempt to investigate its performance in predicting petroleum production. The primary strength of this model is that it can capture dependencies among attributes, and therefore, it is likely to provide an improved or more accurate prediction (Lee and Yen, 2002).This paper presents an application of the NDT model for petroleum prediction. Our models were developed based on the five most significant parameters that affect oil production: permeability, porosity, first shut-in pressure, residual oil and saturation of water. The five parameters were used as input variables, and oil production is the output variable for modeling. Four different models were generated in the modeling process, and each involves a different combination of parameters. First, an overall oil production model is developed using the three geoscience parameters of permeability, porosity and first shut-in pressure. Secondly, two different models, with different input parameters, were developed to predict production in the post-water flooding stage only. The results of the above models indicate that data-driven models may not be effective for classifying the data set. Hence, a trend model was developed in an attempt to improve the effectiveness and accuracy of the predictive model. The result shows that the trend model can provide an improved performance, and its performance is comparable to that of the artificial neural network.
74267BE0	This paper presents a novel decision-tree induction for a multi-objective data set, i.e. a data set with a multi-dimensional class. Inductive decision-tree learning is one of the frequently-used methods for a single-objective data set, i.e. a data set with a single-dimensional class. However, in a real data analysis, we usually have multiple objectives, and a classifier which explains them simultaneously would be useful and would exhibit higher readability. A conventional decision-tree inducer requires transformation of a multi-dimensional class into a single-dimensional class, but such a transformation can considerably worsen both accuracy and readability. In order to circumvent this problem we propose a bloomy decision tree which deals with a multi-dimensional class without such transformations. A bloomy decision tree has a set of split nodes each of which splits examples according to their attribute values, and a set of flower nodes each of which predicts a class dimension of examples. A flower node appears not only at the fringe of a tree but also inside a tree. Our pruning is executed during tree construction, and evaluates each class dimension based on Cramér’s V. The proposed method has been implemented as D3-B (Decision tree in Bloom), and tested with eleven data sets. The experiments showed that D3-B has higher accuracies in nine data sets than C4.5 and tied with it in the other two data sets. In terms of readability, D3-B has a smaller number of split nodes in all data sets, and thus outperforms C4.5.
58772DB1	Numerical data poses a problem to symbolic learning methods, since numerical value ranges inherently need to be partitioned into intervals for representation and handling. An evaluation function is used to approximate the goodness of different partition candidates. Most existing methods for multisplitting on numerical attributes are based on heuristics, because of the apparent efficiency advantages. We characterize a class of well-behaved cumulative evaluation functions for which efficient discovery of the optimal multisplit is possible by dynamic programming. A single pass through the data suffices to evaluate multisplits of all arities. This class contains many important attribute evaluation functions familiar from symbolic machine learning research. Our empirical experiments convey that there is no significant differences in efficiency between the method that produces optimal partitions and those that are based on heuristics. Moreover, we demonstrate that optimal multisplitting can be beneficial in decision tree learning in contrast to using the much applied binarization of numerical attributes or heuristical multisplitting.
7DA028E8	The available e-data throughout the Web are growing at such a high rate that data mining on the web is considered the biggest challenge of information technology. As a result it is crucial to find new and innovative ways for classifying and mining those huge amounts of data. In this paper we present an implementation of a state-of-the-art data mining algorithm on a modern FPGA. This is one of the first approaches utilizing the resources of an FPGA to accelerate certain very CPU intensive data-mining/data classification schemes and our real-world results from actual runs on hardware demonstrate that it is a highly promising one. In particular, our FPGA-based system achieves, depending on the data classified, a speedup from 4x and up to 50x (on average 25x) when compared with a state-of-the art multi-core CPU, including I/O overhead.
757153C9	The basic concepts of a multi-stage classification strategy, the decision tree classifier, are presented. The two main methods to design decision trees are presented and discussed along with some experimental results. An attempt is made to describe an Applicable Logic for the design of decision trees. Advantages and disadvantages of the both design approaches are discussed.
75881784	Given learning samples from a raster data set, spatial decision tree learning aims to find a decision tree classifier that minimizes classification errors as well as salt-and-pepper noise. The problem has important societal applications such as land cover classification for natural resource management. However, the problem is challenging due to the fact that learning samples show spatial autocorrelation in class labels, instead of being independently identically distributed. Related work relies on local tests (i.e., testing feature information of a location) and cannot adequately model the spatial autocorrelation effect, resulting in salt-and-pepper noise. In contrast, we recently proposed a focal-test-based spatial decision tree (FTSDT), in which the tree traversal direction of a sample is based on both local and focal (neighborhood) information. Preliminary results showed that FTSDT reduces classification errors and salt-and-pepper noise. This paper extends our recent work by introducing a new focal test approach with adaptive neighborhoods that avoids over-smoothing in wedge-shaped areas. We also conduct computational refinement on the FTSDT training algorithm by reusing focal values across candidate thresholds. Theoretical analysis shows that the refined training algorithm is correct and more scalable. Experiment results on real world data sets show that new FTSDT with adaptive neighborhoods improves classification accuracy, and that our computational refinement significantly reduces training time.
7E7B8D13	Support vector machines (SVMs) have shown strong generalization ability in a number of application areas, including protein structure prediction. However, the poor comprehensibility hinders the success of the SVM for protein structure prediction. The explanation of how a decision made is important for accepting the machine learning technology, especially for applications such as bioinformatics. The reasonable interpretation is not only useful to guide the "wet experiments," but also the extracted rules are helpful to integrate computational intelligence with symbolic AI systems for advanced deduction. On the other hand, a decision tree has good comprehensibility. In this paper, a novel approach to rule generation for protein secondary structure prediction by integrating merits of both the SVM and decision tree is presented. This approach combines the SVM with decision tree into a new algorithm called SVM_DT, which proceeds in three steps. This algorithm first trains an SVM. Then, a new training set is generated through careful selection from the output of the SVM. Finally, the obtained training set is used to train a decision tree learning system and to extract the corresponding rule sets. The results of the experiments of protein secondary structure prediction on RS126 data set show that the comprehensibility of SVM_DT is much better than that of the SVM. Moreover, the generalization ability of SVM_DT is better than that of C4.5 decision trees and is similar to that of the SVM. Hence, SVM_DT can be used not only for prediction, but also for guiding biological experiments
7FB04BF9	The ideal use of small multilayer nets at the decision nodes of a binary classification tree to extract nonlinear features is proposed. The nets are trained and the tree is grown using a gradient-type learning algorithm in the multiclass case. The method improves on standard classification tree design methods in that it generally produces trees with lower error rates and fewer nodes. It also reduces the problems associated with training large unstructured nets and transfers the problem of selecting the size of the net to the simpler problem of finding a tree of the right size. An efficient tree pruning algorithm is proposed for this purpose. Trees constructed with the method and the CART method are compared on a waveform recognition problem and a handwritten character recognition problem. The approach demonstrates significant decrease in error rate and tree size. It also yields comparable error rates and shorter training times than a large multilayer net trained with backpropagation on the same problems.
7887D583	The classification of large dimensional data sets arising from the merging of remote sensing data with more traditional forms of ancillary data causes a significant computational problem. Decision tree classification is a popular approach to the problem. This type of classifier is characterized by the property that samples are subjected to a sequence of decision rules before they are assigned to a unique class. If a decision tree classifier is well designed, the result in many cases is a classification scheme which is accurate, flexible, and computationally efficient. This correspondence provides an automated technique for effective decision tree design which relies only on a priori statistics. This procedure utilizes canonical transforms and Bayes table look-up decision rules. An optimal design at each node is derived based on the associated decision table. A procedure for computing the global probability of correct classification is also provided. An example is given in which class statistics obtained from an actual Landsat scene are used as input to the program. The resulting decision tree design has an associated probability of correct classification of 0.75 compared to the theoretically optimum 0.79 probability of correct classification associated with a full dimensional Bayes classifier. Recommendations for future research are included.
7C66892E	A critical issue in classification tree design-obtaining right-sized trees, i.e. trees which neither underfit nor overfit the data-is addressed. Instead of stopping rules to halt partitioning, the approach of growing a large tree with pure terminal nodes and selectively pruning it back is used. A new efficient iterative method is proposed to grow and prune classification trees. This method divides the data sample into two subsets and iteratively grows a tree with one subset and prunes it with the other subset, successively interchanging the roles of the two subsets. The convergence and other properties of the algorithm are established. Theoretical and practical considerations suggest that the iterative free growing and pruning algorithm should perform better and require less computation than other widely used tree growing and pruning algorithms. Numerical results on a waveform recognition problem are presented to support this view.
7EAD4110	A survey is presented of current methods for decision tree classifier (DTC) designs and the various existing issues. After considering potential advantages of DTCs over single-state classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. The relation between decision trees and neutral networks (NN) is also discussed.
5FE9C8DA	The design of algorithms that explore multiple representation languages and explore different search spaces has an intuitive appeal. In the context of classification problems, algorithms that generate multivariate trees are able to explore multiple representation languages by using decision tests based on a combination of attributes. The same applies to model-tree algorithms in regression domains, but using linear models at leaf nodes. In this paper, we study where to use combinations of attributes in decision tree learning. We present an algorithm for multivariate tree learning that combines a univariate decision tree with a discriminant function by means of constructive induction. This algorithm is able to use decision nodes with multivariate tests, and leaf nodes that predict a class using a discriminant function. Multivariate decision nodes are built when growing the tree, while functional leaves are built when pruning the tree. Functional trees can be seen as a generalization of multivariate trees. Our algorithm was compared against to its components and two simplified versions using 30 benchmark data sets. The experimental evaluation shows that our algorithm has clear advantages with respect to the generalization ability and model sizes at statistically significant confidence levels.
7E4C6FAC	Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.
79BCE1C3	Supervised classification is one of the important tasks in remote sensing image interpretation, in which the image pixels are classified to various predefined land use/land cover classes based on the spectral reflectance values in different bands. In reality some classes may have very close spectral reflectance values that overlap in feature space. This produces spectral confusion among the classes and results in inaccurate classified images. To remove such spectral confusion one requires extra spectral and spatial knowledge. This report presents a decision tree classifier approach to extract knowledge from spatial data in form of classification rules using Gini Index and Shannon Entropy (Shannon and Weaver, 1949) to evaluate splits. This report also features calculation of optimal dataset size required for rule generation, in order to avoid redundant Input/output and processing.
8020CD31	This paper describes research to obtain continental and global scale maps of urban land cover from remotely sensed imagery, specifically utilizing newly available one kilometer data from the MODIS sensor. Defining the extent of urban land is crucial, since knowledge of the size and spatial distribution of cities is important on a number of fronts, from resource management to economic development planning to regional and global climate modeling. The algorithm used for this work is a supervised decision tree classifier, and the technique of boosting is exploited to improve classification accuracy and to provide a means to correct major sources of error using available prior information. First results for North America indicate that the incorporation of. ancillary information successfully improves urban classification results, resolving confusion between the urban and barren classes that normally occurs when only MODIS data is used.
80B879CA	This paper introduces a hybrid learning methodology that integrates genetic algorithms (GAs) and decision tree learning (ID3) in order to evolve optimal subsets of discriminatory features for robust pattern classification. A GA is used to search the space of all possible subsets of a large set of candidate discrimination features. For a given feature subset, ID3 is invoked to produce a decision tree. The classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set, which, in turn, is used by the GA to evolve better feature sets. This GA-ID3 process iterates until a feature subset is found with satisfactory classification performance. Experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data. The results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection.
79D84E5F	An algorithm to map burnt areas has been developed for SPOT VEGETATION (VGT) data in Australian woodland savannas. A time series of daily VGT images (15 May to 15 July 1999) was composited into 10-day periods by applying a minimum value criterion to the near-infrared band (0.78–0.89 mm). The algorithm was developed using a classification tree methodology that was confirmed as a powerful means of image classification. This methodology allowed the identification of three classes of burnt surfaces that appear to be differentiated by the proportion of the pixel that is burnt, the intensity of the fire and the density of the tree layer. The performance of the algorithm was assessed by classification of one VGT composite image (31 May–9 June) using, as representative of the ground truth, burnt areas extracted from two Landsat TM scenes (9 June). We randomly extracted 30 windows (each of ~14 km by 14 km) for which we compared the percentage of area burnt as derived from TM and VGT. The estimated mean absolute deviation in the percentage of the area burnt in each window is ±6.3%. In the area common to the two datasets a total amount of 6473 km2 was estimated to be burnt in the VGT classification against 7536 km2 that was burnt according to TM images. The accuracy of the classification was found to vary with the vegetation type being the most accurate estimate in low woodland with an underestimation error of 8.6%. These results show that VGT could be a very useful sensor for burnt area mapping over large woodland areas, although the low spatial resolution and the lack of a thermal band can be a limitation in certain conditions (e.g. understorey burns). The same methodology will be applied to map burnt areas for the entire Australian continent.
7ABDCCD6	The purposes of this study are to identify the strongest clinical parameters in relation to in-hospital mortality, which are available in the earliest phase of the hospitalization of patients, and to create an easy tool for the early identification of patients at risk.The classification and regression tree analysis was applied to data from the Acute Heart Failure Database-Main registry comprising patients admitted to specialized cardiology centers with all syndromes of acute heart failure. The classification model was built on derivation cohort (n = 2543) and evaluated on validation cohort (n = 1387).The classification tree stratifies patients according to the presence of cardiogenic shock (CS), the level of creatinine, and the systolic blood pressure (SBP) at admission into the 5 risk groups with in-hospital mortality ranging from 2.8% to 66.2%. Patients without CS and creatinine level of 155 μmol/L or less were classified into very-low-risk group; patients without CS, creatinine level greater than 155 μmol/L, and SBP greater than 103 mm Hg, into low-risk group, whereas patients without CS, creatinine level greater than 155 μmol/L, and SBP of 103 mm Hg or lower, into intermediate-risk group. The high-risk group patients had CS and creatinine of 140 μmol/L or less; patients with CS and creatinine level greater than 140 μmol/L belong to very-high-risk group. The area under receiver operating characteristic curve was 0.823 and 0.832, and the value of Brier's score was estimated on level 0.091 and 0.084, for the derivation and the validation cohort, respectively.The presented classification model effectively stratified patients with all syndromes of acute heart failure into in-hospital mortality risk groups and might be of advantage for clinical practice.
79931D06	Classification trees (CT) have been used successfully in the past to classify aquatic vegetation from spectral indices (SI) obtained from remotely-sensed images. However, applying CT models developed for certain image dates to other time periods within the same year or among different years can reduce the classification accuracy. In this study, we developed CT models with modified thresholds using extreme SI values (CTm) to improve the stability of the models when applying them to different time periods. A total of 903 ground-truth samples were obtained in September of 2009 and 2010 and classified as emergent, floating-leaf, or submerged vegetation or other cover types. Classification trees were developed for 2009 (Model-09) and 2010 (Model-10) using field samples and a combination of two images from winter and summer. Overall accuracies of these models were 92.8% and 94.9%, respectively, which confirmed the ability of CT analysis to map aquatic vegetation in Taihu Lake. However, Model-10 had only 58.9–71.6% classification accuracy and 31.1–58.3% agreement (i.e., pixels classified the same in the two maps) for aquatic vegetation when it was applied to image pairs from both a different time period in 2010 and a similar time period in 2009. We developed a method to estimate the effects of extrinsic (EF) and intrinsic (IF) factors on model uncertainty using Modis images. Results indicated that 71.1% of the instability in classification between time periods was due to EF, which might include changes in atmospheric conditions, sun-view angle and water quality. The remainder was due to IF, such as phenological and growth status differences between time periods. The modified version of Model-10 (i.e. CTm) performed better than traditional CT with different image dates. When applied to 2009 images, the CTm version of Model-10 had very similar thresholds and performance as Model-09, with overall accuracies of 92.8% and 90.5% for Model-09 and the CTm version of Model-10, respectively. CTm decreased the variability related to EF and IF and thereby improved the applicability of the models to different time periods. In both practice and theory, our results suggested that CTm was more stable than traditional CT models and could be used to map aquatic vegetation in time periods other than the one for which the model was developed.
75542B44	Using 1998 and 1999 singleton birth data of the State of Florida, we study the stability of classification trees. Tree stability depends on both the learning algorithm and the specific data set. In this study, test samples are used in statistical learning to evaluate both stability and predictive performance. We also use the resampling technique bootstrap, which can be regarded as data self-perturbation, to evaluate the sensitivity of the modeling algorithm with respect to the specific data set. We demonstrate that the selection of the cost function plays an important role in stability. In particular, classifiers with equal misclassification costs and equal priors are less stable compared to those with unequal misclassification costs and equal priors.
7767905F	The instability problem of decision tree classification algorithms is that small changes in input training samples may cause dramatically large changes in output classification rules. Different rules generated from almost the same training samples are against human intuition and complicate the process of decision making. In this paper, we present fundamental theorems for the instability problem of decision tree classifiers. The first theorem gives the relationship between a data change and the resulting tree structure change (i.e. split change). The second theorem, Instability Theorem, provides the cause of the instability problem. Based on the two theorems, algorithmic improvements can be made to lessen the instability problem. Empirical results illustrate the theorem statements. The trees constructed by the proposed algorithm are more stable, noise-tolerant, informative, expressive, and concise. Our proposed sensitivity measure can be used as a metric to evaluate the stability of splitting predicates. The tree sensitivity is an indicator of the confidence level in rules and the effective lifetime of rules.
7598242E	When managers and researchers encounter a data set, they typically ask two key questions: (1) Which model (from a candidate set) should I use? And (2) if I use a particular model, when is it going to likely work well for my business goal? This research addresses those two questions and provides a rule, i.e., a decision tree, for data analysts to portend the “winning model” before having to fit any of them for longitudinal incidence data. We characterize data sets based on managerially relevant (and easy-to-compute) summary statistics, and we use classification techniques from machine learning to provide a decision tree that recommends when to use which model. By doing the “legwork” of obtaining this decision tree for model selection, we provide a time-saving tool to analysts. We illustrate this method for a common marketing problem (i.e., forecasting repeat purchasing incidence for a cohort of new customers) and demonstrate the method's ability to discriminate among an integrated family of a hidden Markov model (HMM) and its constrained variants. We observe a strong ability for data set characteristics to guide the choice of the most appropriate model, and we observe that some model features (e.g., the “back-and-forth” migration between latent states) are more important to accommodate than are others (e.g., the inclusion of an “off” state with no activity). We also demonstrate the method's broad potential by providing a general “recipe” for researchers to replicate this kind of model classification task in other managerial contexts (outside of repeat purchasing incidence data and the HMM framework).
5D25C398	Decision tree learning has become a popular and practical method in data mining because of its high predictive accuracy and ease of use. However, a set of if-then rules generated from large trees may be preferred in many cases because of at least three reasons: (i) large decision trees are difficult to understand as we may not see their hierarchical structure or get lost in navigating them, (ii) the tree structure may cause individual subconcepts to be fragmented (this is sometimes known as the “replicated subtree” problem), (iii) it is easier to combine new discovered rules with existing knowledge in a given domain. To fulfill that need, the popular decision tree learning system C4.5 applies a rule post-pruning algorithm to transform a decision tree into a rule set. However, by using a global optimization strategy, C4.5rules functions extremely slow on large datasets. On the other hand, rule post-pruning algorithms that learn a set of rules by the separate-and-conquer strategy such as CN2, IREP, or RIPPER can be scalable to large datasets, but they suffer from the crucial problem of overpruning, and do not often achieve a high accuracy as C4.5. This paper proposes a scalable algorithm for rule post-pruning of large decision trees that employs incremental pruning with improvements in order to overcome the overpruning problem. Experiments show that the new algorithm can produce rule sets that are as accurate as those generated by C4.5 and is scalable for large datasets.
78504EB4	We present a framework for flexible nonlinear contextual image classification. The framework integrates classical and recent models for image classification, ranging from a multivariate Gaussian classifier, to MLP neural nets, classification trees and recent regression models based on general additive models, and combines them with a Markov random field for spatial context. The effect of using the different nonlinear discriminant functions is compared with the effect of using an MRF model for spatial context. In general, the use of an MRF model results in larger improvements in classification accuracy than using different nonlinear discriminant functions, but the combination of them can give large improvements.
7E53AC45	Decision tree classifiers have received much recent attention, particularly with regards to land cover classifications at continental to global scales. Despite their many benefits and general flexibility, the use of decision trees with high spatial resolution data has not yet been fully explored. In support of the National Park Service (NPS) Vegetation Mapping Program (VMP), we have examined the feasibility of using a commercially available decision tree classifier with multitemporal satellite data from the Enhanced Thematic Mapper-Plus (ETM+) instrument to map 11 land cover types at the Delaware Water Gap National Recreation Area near Milford, PA. Ensemble techniques such as boosting and consensus filtering of the training data were used to improve both the quality of the input training data as well as the final products.Using land cover classes as specified by the National Vegetation Classification Standard at the Formation level, the final land cover map has an overall accuracy of 82% (κ=0.80) when tested against a validation data set acquired on the ground (n=195). This same accuracy is 99.5% when considering only forest vs. nonforest classes. Usage of ETM+ scenes acquired at multiple dates improves the accuracy over the use of a single date, particularly for the different forest types. These results demonstrate the potential applicability and usability of such an approach to the entire National Park system, and to high spatial resolution land cover and forest mapping applications in general.
7683DF85	Multiseason reflectance data from radiometrically and geometrically corrected multispectral SPOT-5 images of 10-m resolution were combined with thorough field campaigns and land cover digitizing using a binary classification tree algorithm to estimate the area of marshes covered with common reeds (Phragmites australis) and submerged macrophytes (Potamogeton pectinatus, P. pusillus, Myriophyllum spicatum, Ruppia maritima, Chara sp.) over an area of 145,000 ha. Accuracy of these models was estimated by cross-validation and by calculating the percentage of correctly classified pixels on the resulting maps. Robustness of this approach was assessed by applying these models to an independent set of images using independent field data for validation. Biophysical parameters of both habitat types were used to interpret the misclassifications. The resulting trees provided a cross-validation accuracy of 98.7% for common reed and 97.4% for submerged macrophytes. Variables discriminating reed marshes from other land covers were the difference in the near-infrared band between March and June, the Optimized Soil Adjusted Vegetation Index of December, and the Normalized Difference Water Index (NDWI) of September. Submerged macrophyte beds were discriminated with the shortwave-infrared band of December, the NDWI of September, the red band of September and the Simple Ratio index of March. Mapping validations provided accuracies of 98.6% (2005) and 98.1% (2006) for common reed, and 86.7% (2005) and 85.9% (2006) for submerged macrophytes. The combination of multispectral and multiseasonal satellite data thus discriminated these wetland vegetation types efficiently. Misclassifications were partly explained by digitizing inaccuracies, and were not related to biophysical parameters for reedbeds. The classification accuracy of submerged macrophytes was influenced by the proportion of plants showing on the water surface, percent cover of submerged species, water turbidity, and salinity. Classification trees applied to time series of SPOT-5 images appear as a powerful and reliable tool for monitoring wetland vegetation experiencing different hydrological regimes even with a small training sample (N = 25) when initially combined with thorough field measurements.
7C8CCCB4	Aquatic vegetation plays an important role in maintaining the balance of lake ecosystems. Thus, classifying and mapping aquatic vegetation is a priority for lake management. Classification tree (CT) approaches have been used successfully to map aquatic vegetation from spectral indices obtained from remotely sensed images. However, due to the effects of extrinsic and intrinsic factors, applying a CT model developed for imagery from one date to imagery from another date or a different dataset likely would reduce the classification accuracy. In this study, three spectral features (SFs) were selected to develop a CT model for identifying aquatic vegetation in Taihu Lake. Three traditional CT models with three SFs were developed using CT analysis based on satellite images acquired on 11 July, 16 August and 26 September 2013, and corresponding ground-truth samples, from the Huangjing-1A/B Charge-Coupled Device (HJ-CCD) images, environment and disaster reduction small satellites that were launched by China Center for Resources Satellite Data and Application (CRESDA). The overall accuracies of traditional CT models were 82%, 80% and 84%. We then tested two methods to modify CT model thresholds to adjust the traditional CT models based on image date to determine if the results would enable us to map and classify aquatic vegetation for periods when no ground-based data were available. We assessed the results with ground-truth samples and area agreement with traditional CT models. Results showed that CT models modified from a linear adjustment based on the relationship between ranked values of SFs between two image dates produced map accuracies comparable with those obtained from the traditional CT models and suggest that the method we proposed is feasible for mapping aquatic vegetation types in lakes when ground data are not available.
76EB6675	Decision tree algorithm is a kind of data mining model to make induction learning algorithm based on examples. It is easy to extract display rule, has smaller computation amount, and could display important decision property and own higher classification precision. For the study of data mining algorithm based on decision tree, this article put forward specific solution for the problems of property value vacancy, multiple-valued property selection, property selection criteria, propose to introduce weighted and simplified entropy into decision tree algorithm so as to achieve the improvement of ID3 algorithm. The experimental results show that the improved algorithm is better than widely used ID3 algorithm at present on overall performance.
75087187	In previous attempts to identify aquatic vegetation from remotely-sensed images using classification trees (CT), the images used to apply CT models to different times or locations necessarily originated from the same satellite sensor as that from which the original images used in model development came, greatly limiting the application of CT. We have developed an effective normalization method to improve the robustness of CT models when applied to images originating from different sensors and dates. A total of 965 ground-truth samples of aquatic vegetation types were obtained in 2009 and 2010 in Taihu Lake, China. Using relevant spectral indices (SI) as classifiers, we manually developed a stable CT model structure and then applied a standard CT algorithm to obtain quantitative (optimal) thresholds from 2009 ground-truth data and images from Landsat7-ETM+, HJ-1B-CCD, Landsat5-TM and ALOS-AVNIR-2 sensors. Optimal CT thresholds produced average classification accuracies of 78.1%, 84.7% and 74.0% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. However, the optimal CT thresholds for different sensor images differed from each other, with an average relative variation (RV) of 6.40%. We developed and evaluated three new approaches to normalizing the images. The best-performing method (Method of 0.1% index scaling) normalized the SI images using tailored percentages of extreme pixel values. Using the images normalized by Method of 0.1% index scaling, CT models for a particular sensor in which thresholds were replaced by those from the models developed for images originating from other sensors provided average classification accuracies of 76.0%, 82.8% and 68.9% for emergent vegetation, floating-leaf vegetation and submerged vegetation, respectively. Applying the CT models developed for normalized 2009 images to 2010 images resulted in high classification (78.0%–93.3%) and overall (92.0%–93.1%) accuracies. Our results suggest that Method of 0.1% index scaling provides a feasible way to apply CT models directly to images from sensors or time periods that differ from those of the images used to develop the original models.
80DCAAC2	This research used classification tree analysis and logistic regression models to identify risk factors related to short- and long-term abstinence. Baseline and cessation outcome data from two smoking cessation trials, conducted from 2001 to 2002 in two Midwestern urban areas, were analyzed. There were 928 participants (53.1% women, 81.8% White) with complete data. Both analyses suggest that relapse risk is produced by interactions of risk factors and that early and late cessation outcomes reflect different vulnerability factors. The results illustrate the dynamic nature of relapse risk and suggest the importance of efficient modeling of interactions in relapse prediction.
78F9553F	Besides serving as prediction models, classification trees are useful for finding important predictor variables and identifying interesting subgroups in the data. These functions can be compromised by weak split selection algorithms that have variable selection biases or that fail to search beyond local main effects at each node of the tree. The resulting models may include many irrelevant variables or select too few of the important ones. Either eventuality can lead to erroneous conclusions. Four techniques to improve the precision of the models are proposed and their effectiveness compared with that of other algorithms, including tree ensembles, on real and simulated data sets. 
7A88004A	The literature on microbial source tracking (MST) suggests that DNA analysis of fecal samples leads to more reliable determinations of bacterial sources of surface water contamination than antibiotic resistance analysis (ARA). Our goal is to determine whether the increased reliability, if any, in library-based MST developed with DNA data is sufficient to justify its higher cost, where the bacteria source predictions are used in TMDL surface water management programs. We describe an application of classification trees for MST applied to ARA and DNA data from samples collected in the Potomac River Watershed in Maryland. Conclusions concerning the comparison of ARA and DNA data, although preliminary at the current time, suggest that the added cost of obtaining DNA data in comparison to the cost of ARA data may not be justified, where MST is applied in TMDL surface water management programs.