7FE594DD	Over the past two decades, national reforestation efforts and anthropogenic pressures have altered the landscape of northeastern China. In an effort to develop an operational forest monitoring system for the expansive northeastern China region, the accuracy of satellite-derived forest metric information has been evaluated. An area within Heilongjiang province served as the study site. This region, which spans portions of both the Tahe and Mohe Bureaus of Forestry, lost over 1 million hectares of forest during a large forest fire in May/June of 1987. One year after the conflagration, a ten-year reforestation effort commenced. Using Landsat data spanning 1986 to 2002, the photosynthetic recovery of the forest vegetation has been documented. Subsequently, Landsat 7 ETM+ data have been used to establish forest metric correlations. To quantify forest metrics, correlations between Landsat spectral information and in situ measurements (species composition, age, average diameter at breast height (dbh), crown closure, height, and timber volume), as surveyed by the local Forest Management Bureau in 1999/2000, were developed. Preliminary results indicate that age, dbh, and average stand height are well correlated to spectral data and can be used to develop predictive models, while timber volume shows little to no relationship. Hereafter, these methodologies and correlations will be extended both spatially and temporally to provide a regional assessment of carbon sequestration.
80167C1F	Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers.
7F14266A	We present an algorithm for jointly learning a consistent bidirectional generative-recognition model that combines top-down and bottom-up processing for monocular 3d human motion reconstruction. Learning progresses in alternative stages of self-training that optimize the probability of the image evidence: the recognition model is tunned using samples from the generative model and the generative model is optimized to produce inferences close to the ones predicted by the current recognition model. At equilibrium, the two models are consistent. During on-line inference, we scan the image at multiple locations and predict 3d human poses using the recognition model. But this implicitly includes one-shot generative consistency feedback. The framework provides a uniform treatment of human detection, 3d initialization and 3d recovery from transient failure. Our experimental results show that this procedure is promising for the automatic reconstruction of human motion in more natural scene settings with background clutter and occlusion.
78A62349	The classification techniques of discharging pulses in EDM have been proved critical in improving productivity, precision and lowering the cost of products, etc. In this paper, an easily implemented method is developed to describe the variations of EDM process, represented by gap states. On the basis of a time series of gap states from a machining process, the paper first studied a general descriptive model for EDM process, and then equivalently simplified the model for application; after spectral analysis, preprocessing of data, parameters selection and model validation proposed a well-defined model. Finally, by using this model structure and size, an online time-varied predictive model was developed. Experimental verifications showed that this predictive model can quickly and accurately provide one step ahead predictions with mean error less than 2%. This model makes clear that variations of EDM process represented by gap states can be predicted online with a high precision.
7E791BA1	This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than "traditional" code metrics, which can only be collected at a later phase of the software development processes.
7AF8CAAA	Stream flow prediction is crucial for water resource planning and management at the watershed scale. This study investigates various modelling approaches, namely, autoregressive integrated moving average, deseasonalized autoregressive moving average, artificial neural network (ANN), simulator for water resources in rural basins, and identification of hydrographs and components from rainfall, evaporation, and stream (IHACRES) models, to simulate and predict stream flow in Kasilian watershed in northern Iran. This research represents a case study on forest watershed modelling with the lack of enough hydro-meteorological data. The comparison of the prediction performance of the models was done based on some error estimation criteria. The results indicate that the ANN and IHACRES models perform better than the two other modelling approaches. The advantages and disadvantages of different hydrological models are discussed.
7AA73EFC	The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module’s fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA’s Metrics Data Program data repository.
775312C0	A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.
7BE3E94B	Different software reliability models can produce very different answers when called on to predict future reliability in a reliability growth context. Users need to know which, if any, of the competing predictions are trustworthy. Some techniques are presented which form the basis of a partial solution to this problem. Rather than attempting to decide which model is generally best, the approach adopted allows a user to decide on the most appropriate model for each application.
79F1D35C	It is shown that neural network reliability growth models have a significant advantage over analytic models in that they require only failure history as input and not assumptions about either the development environment or external parameters. Using the failure history, the neural-network model automatically develops its own internal model of the failure process and predicts future failures. Because it adjusts model complexity to match the complexity of the failure history, it can be more accurate than some commonly used analytic models. Results with actual testing and debugging data which suggest that neural-network models are better at endpoint predictions than analytic models are presented.
7D71156A	Model-based control for two rapid thermal processing systems has been performed. The condition number of the process influences the control strategies selected and the quality of control that can be achieved. For the Texas Instruments RTP system, a successively linearized quadratic dynamic matrix control (QDMC) strategy using a reduced set of outputs has been developed. Experimental results of the control strategy are presented and compared with internal model control with gain scheduling. The nonlinear controller discussed shows superior performance for the test case studied. The second RTP system designed by SEMATECH exhibits improved controllability. This system has the potential for tighter control of wafer temperature using gain scheduling.
766E4289	In the existing studies on software prediction, the most proposed methods are usually assessed over the public datasets like NASA metrics data repository, which include a combination of code metrics merely. Obviously, the process metric is also one of the key factors that affect the defect-proneness of software modules. In this paper, life-cycle based management process metrics set and history change process metrics set have been proposed based on the characteristics of development process. In order to analyze the importance of these different metrics for predicting defects in aerospace tracking telemetry and control (TT&C) software, an improved PSO optimized support vector machine algorithm (PSO-SVM) has been presented and took into application. The experiment results over the actual TT&C projects suggest that the prediction performance can be significance improved if the 2 kinds of process metrics are included in the model.
7DF540E4	The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting support vector machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a medical imaging system software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models
7F455829	Contemporary evidence suggests that most field faults in software applications are found in a small percentage of the software's components. This means that if these faulty software components can be detected early in the development project's life cycle, mitigating actions can be taken, such as a redesign. For object-oriented applications, prediction models using design metrics can be used to identify faulty classes early on. In this paper we report on a study that used object-oriented design metrics to construct such prediction models. The study used data collected from one version of a commercial Java application for constructing a prediction model. The model was then validated on a subsequent release of the same application. Our results indicate that the prediction model has a high accuracy. Furthermore, we found that an export coupling (EC) metric had the strongest association with fault-proneness, indicating a structural feature that may be symptomatic of a class with a high probability of latent faults.
80017093	One of the challenges that technology computer-aided design must meet currently is the analysis of the performance of groups of components, interconnects, and, generally speaking, large parts of the IC. This enables predictions that the simulation of single components cannot achieve. In this paper, we focus on the simulation of backend processes, interconnect capacitances, and time delays. The simulation flows start from the blank wafer surface and result in device information for the circuit designer usable from within SPICE. In order to join topography and backend simulations, deposition, etching, and chemical mechanical planarization processes in the various metal lines are used to build up the backend stack, starting from the flat wafer surface. Depending on metal combination, line-to-line space, and line width, thousands of simulations are required whose results are stored in a database. Finally, we present simulation results for the backend of a 100-nm process, where the influence of void formation between metal lines profoundly impacts the performance of the whole interconnect stack, consisting of aluminum metal lines, and titanium nitride local interconnects. Scanning electron microscope images of test structures are compared to topography simulations, and very good agreement is found. Moreover, charge-based capacitance measurements were carried out to validate the capacitance extraction, and it was found that the error is smaller than four percent. These simulations assist the consistent fabrication of voids, which is economically advantageous compared to low-/spl kappa/ materials, which suffer from integration problems.
7E183060	Recent studies have shown that trafic in telecommunication networks exhibits long-range dependence (LRD). Accurate modelling and analysis of teletrafic incorporating LRD is therefore required in network engineering. Prediction of trafic levels can play an important role in teletrafic analysis for dynamic resource allocation and traffic management. This paper presents the formulation of a model based recursive , linear minimum mean-square error predic- tor for LRD processes. A Kalman predictor is pro- posed for LAD processes modelled by fractional autoregressive integrated moving average CfARIMA) models. The family of fARIMA models can account for long range, as well as short range and qwi-peri- odic dependencies typical of teletraffic data.
75D8B052	Software has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome.In this research, we identify aspects which could influence project success, build prediction models based on the aspects using data collected from multiple companies, and then test their performance on data from a single organization.A survey-based empirical investigation was used to examine variables and factors that contribute to project outcome. Variables that were highly correlated to project success were selected and the set of variables was reduced to three factors by using principal components analysis. A logistic regression model was built for both the set of variables and the set of factors, using heterogeneous data collected from two different countries and a variety of organizations. We tested these models by using a homogeneous hold-out dataset from one organization. We used the receiver operating characteristic (ROC) analysis to compare the performance of the variable and factor-based models when applied to the homogeneous dataset.We found that using raw variables or factors in the logistic regression models did not make any significant difference in predictive capability. The prediction accuracy of these models is more balanced when the cut-off is set to the ratio of success to failures in the datasets used to build the models. We found that the raw variable and factor-based models predict significantly better than random chance.We conclude that an organization wishing to estimate whether a project will succeed or fail may use a model created from heterogeneous data derived from multiple organizations.
75B4B94E	Quantitative structure–property relationship (QSPR) models are widely used for prediction of properties, activities and/or toxicities of new chemicals. Validation strategies check the reliability of predictions of QSPR models. The classical metrics like Q2 and R2pred (Q2ext) are commonly used, besides other techniques, for internal validation (mostly leave-one-out) and external validation (test set validation) respectively. Recently, we have proposed a set of novel rm2 metrics which has been extensively used by us and other research groups for validation of QSPR models. In the present attempt, some additional variants of rm2 metrics have been proposed and their applications in judging the quality of predictions of QSPR models have been shown by analyzing results of the QSPR models obtained from three different data sets (n = 119, 90, and 384). In each case, 50 combinations of training and test sets have been generated, and models have been developed based on the training set compounds and subsequently applied for prediction of responses of the test set compounds. Finally, models for a particular data set have been ranked according to the quality of predictions. The role of different validation metrics (including classical metrics and different variants of rm2 metrics) in differentiating the “good” (predictive) models from the “bad” (low predictive) models has been studied. Finally, a set of guidelines has been proposed for checking the predictive quality of QSPR models.
7F2DDA1D	Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making process are evaluated.
7D9B3629	A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.
76232CEB	There is no universally applicable software reliability growth model which can be trusted to give accurate predictions of reliability in all circumstances. A technique of analyzing predictive accuracy called the u-plot allows a user to estimate the relationship between the predicted reliability and the true reliability. It is shown how this can be used to improve reliability predictions in a very general way by a process of recalibration. Simulation results show that the technique gives improved reliability predictions in a large proportion of cases. However, a user does not need to trust the efficacy of recalibration, since the new reliability estimates produced by the technique are truly predictive and their accuracy in a particular application can be judged using the earlier methods. The generality of this approach suggests its use whenever a software reliability model is used. Indeed, although this work arose from the need to address the poor performance of software reliability models, it is likely to have applicability in other areas such as reliability growth modeling for hardware.
7D45D473	Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the "quality" of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the "Goldilock's Conjecture," that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian Belief Networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of "software decomposition" in order to test hypotheses about defect introduction and help construct a better science of software engineering.
80212C28	The authors present an automated modeling technique which can be used as an alternative to regression techniques to improve the quality of the software development process. The modeling process will allow for the reliable detection of potential problem areas and for the interpretation of the cause of the problem so that the most appropriate remedial action can be taken. It is shown that it can be used to facilitate the identification and aid the interpretation of the significant trends which characterize high risk components in several Ada systems. The effectiveness of the technique is evaluated based on a comparison with logistic regression based models.
7F34FB79	In this paper we characterize and model the cost of rework in a Component Factory (CF) organization. A CF is responsible for developing and packaging reusable software components. Data was collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC. We then constructed a predictive model of the cost of rework using the C4.5 system for generating a logical classification model. The predictor variables for the model are measures of internal software product attributes. The model demonstrates good prediction accuracy, and can be used by managers to allocate resources for corrective maintenance activities. Furthermore, we used the model to generate proscriptive coding guidelines to improve programming practices so that the cost of rework can be reduced in the future. The general approach we have used is applicable to other environments.
803FCA57	This paper presents a viscous and coulomb friction measurement procedure for mechanical systems equipped with high-precision velocity sensors. The proposed method exploits the structure of the velocity response predicted by several standard friction models when a force or torque is applied in a ramp fashion. Experimental results show the high accuracy of the proposed measurement procedure.
7BAB0624	Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.
7E8EABA8	Empirical studies on software prediction models do not converge with respect to the question "which prediction model is best?" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.
761A7BE9	This paper describes an empirical comparison of several modeling techniques for predicting the quality of software components early in the software life cycle. Using software product measures, we built models that classify components as high-risk, i.e., likely to contain faults, or low-risk, i.e., likely to be free of faults. The modeling techniques evaluated in this study include principal component analysis, discriminant analysis, logistic regression, logical classification models, layered neural networks, and holographic networks. These techniques provide a good coverage of the main problem-solving paradigms: statistical analysis, machine learning, and neural networks. Using the results of independent testing, we determined the absolute worth of the predictive models and compare their performance in terms of misclassification errors, achieved quality, and verification cost. Data came from 27 software systems, developed and tested during three years of project-intensive academic courses. A surprising result is that no model was able to effectively discriminate between components with faults and components without faults.
5B09215F	The information about which modules of a future version of a software system are defect-prone is a valuable planning aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. However, constructing effective defect prediction models in an industrial setting involves a number of key questions. In this paper we discuss ten key questions identified in context of establishing defect prediction in a large software development project. Seven consecutive versions of the software system have been used to construct and validate defect prediction models for system test planning. Furthermore, the paper presents initial empirical results from the studied project and, by this means, contributes answers to the identified questions.
75E31632	In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.
7BB98473	The identification of high-risk components early in the life cycle is addressed. A solution that casts this as a classification problem is examined. The proposed approach derives models of problematic components, based on their measurable attributes and those of their development processes. The models provide a basis for forecasting which components are likely to share the same high-risk properties, such as being error-prone or having a high development cost. Developers can use these classification techniques to localize the troublesome 20% of the system. The method for generating the models, called automatic generation of metric-based classification trees, uses metrics from previous releases or projects to identify components that are historically high-risk.
7FC6F995	The information about which modules in a software system's future version are potentially defective is a valuable aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. Constructing effective defect prediction models in an industrial setting involves the decision from what data source the defect predictors should be derived. In this paper we compare defect prediction results based on three different data sources of a large industrial software system to answer the question what repositories to mine. In addition, we investigate whether a combination of different data sources improves the prediction results. The findings indicate that predictors derived from static code and design analysis provide slightly yet still significant better results than predictors derived from version control, while a combination of all data sources showed no further improvement.
774787F7	We propose the use of regression analysis to generate accurate predictive models for physical metrics using design metrics as input. We validate our approach with 40+ implementations of three systems in two development scenarios: system evolution and first design. Results show maximum prediction errors of 1.66% during system evolution. In a first design scenario, the average error is 15% with the maximum error still below 20% for all physical metrics. This approach provides a fast and accurate strategy to boost embedded software productivity and quality, by estimating Non-Functional Requirements (NFRs) during the first design stages.
7C5F7EE4	It is hard for security practitioners and decision-makers to know what level of protection they are getting from their investments in security, especially when they have invested in a number of technologies and processes which interact and combine together. It is even harder to estimate how well these investments can be expected to protect their organizations in the future as security policies, regulations and the threat environment are constantly changing. In this paper we propose that for measuring the effectiveness of security processes in large organizations, a greater emphasis needs to be put on process-based metrics, in contrast to the more commonly used symptomatic lagging indicators. We show, by means of two case studies, how these process-based metrics can be combined with executable, predictive models, based on a sound mathematical foundation, to both assess organizations' security processes under current conditions and predict how well they are likely to perform in potential future scenarios, which may include changes in working practices, policies or threat levels, or new investments in security. We present two case studies, in the areas of vulnerability threat management, and identity and access management, as significant examples to illustrate how this modeling and simulation-based approach can be used to provide a rich picture of how well existing security processes are protecting the organization and to answer "what-if" questions, such as exploring the effects of a change in security policy or an investment in new security technology. Our approach enables the organization to apply the metrics that are most relevant to its business, and provide a comprehensive view that shows the benefits and losses to the different stakeholders.
7C4BF42C	The software development process is usually affected by many risk factors that may cause the loss of control and failure, thus which need to be identified and mitigated by project managers. Software development companies are currently improving their process by adopting internationally accepted practices, with the aim of avoiding risks and demonstrating the quality of their work.This paper aims to develop a method to identify which risk factors are more influential in determining project outcome. This method must also propose a cost effective investment of project resources to improve the probability of project success.To achieve these aims, we use the probability of success relative to cost to calculate the efficiency of the probable project outcome. The definition of efficiency used in this paper was proposed by researchers in the field of education. We then use this efficiency as the fitness function in an optimization technique based on genetic algorithms. This method maximizes the success probability output of a prediction model relative to cost.The optimization method was tested with several software risk prediction models that have been developed based on the literature and using data from a survey which collected information from in-house and outsourced software development projects in the Chilean software industry. These models predict the probability of success of a project based on the activities undertaken by the project manager and development team. The results show that the proposed method is very useful to identify those activities needing greater allocation of resources, and which of these will have a higher impact on the projects success probability.Therefore using the measure of efficiency has allowed a modular approach to identify those activities in software development on which to focus the project's limited resources to improve its probability of success. The genetic algorithm and the measure of efficiency presented in this paper permit model independence, in both prediction of success and cost evaluation.
7FCFF9BD	Models for projecting software defects from analyses of Ada designs are described. The research is motivated by the need for technology to analyze designs for their likely effect on software quality. The models predict defect density based on product and process characteristics. Product characteristics are extracted from a static analysis of Ada subsystems, focusing on context coupling, visibility, and the import-export of declarations. Process characteristics provide for effects of reuse level and extent of changes. Multivariate regression analyses were conducted with empirical data from industry/government-developed projects: 16 Ada subsystems totaling 149000 source lines of code. The resulting models explain 63-74% of the variation in defect density of the subsystems. Context coupling emerged as a consistently significant variable in the models.
7FD35685	The usefulness of connectionist models for software reliability growth prediction is illustrated. The applicability of the connectionist approach is explored using various network models, training regimes, and data representation methods. An empirical comparison is made between this approach and five well-known software reliability growth models using actual data sets from several different software projects. The results presented suggest that connectionist models may adapt well across different data sets and exhibit a better predictive accuracy. The analysis shows that the connectionist approach is capable of developing models of varying complexity.
7A65CAD0	It would be valuable to use metrics to identify the fault-proneness of software modules. It is important to select the most appropriate particular metric subset for fault-proneness prediction. We proposed an approach of metrics selection, which firstly utilized the correlation analysis to eliminate the high the correlation metrics and then ranked the remaining metrics based on the gray relational analysis. Three classifiers, that were logistic regression model, NaiveBayes, and J48, were utilized to empirically investigate the usefulness of selected metrics. Our results, based on a public domain NASA data set, indicate that 1) proposed method for metrics selection is effective, and 2) using 3–4 metrics gets the balanced performance for fault-proneness prediction of software modules.
7941323C	We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group. 
7957374C	We present the results of an empirical study in which we have investigated machine learning (ML) algorithms with regard to their capabilities to accurately assess the correctability of faulty software components. Three different families of algorithms have been analyzed. We have used (1) fault data collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC and (2) product measures extracted directly from the faulty components of this library.
7FEB3BFA	This paper presents the application of our recently developed theory on model validation for control and controller validation in a prediction error framework to a realistic industrial case study. The industrial application concerns the control of the silicon concentration in a ferrosilicon production process. Our case study produces findings about the design of the validation experiment (validation in open or closed loop). It also illustrates the respective merits of the tools developed, respectively, for control oriented model validation and for the validation of a particular controller.
7F82664B	The goal of software process model reuse should be to provide a comprehensive process model that guides and coordinates all roles within a project. This means that roles do not use their own process model, but all roles use a single, integrated one that represents all needed information. Views are derived to present information necessary for performing a role's tasks. Because every project is unique, reuse of software process models requires tailoring. Reusing generic, large software process models seems to be impractical, because of many interdependencies between generic parts of the model. Process engineering will only be successful when techniques are developed to formulate units of software process modules (staying relatively stable from project to project), to tailor them according to new contexts, and to integrate them into a comprehensive software process model. The hypothesis of this paper is that views are the process modules to be reused. Views are less complex and easier to maintain than a comprehensive process model.
7C1E8237	The shelf-life of ready-to-eat vegetable salads established by manufacturer is usually 7–14 days depending on the type of vegetable, and is determined by loss in organoleptic qualities. A more objective method to predict shelf-life and spoilage would be desirable. The present study monitored the evolution of spoilage organisms in a mixed salad of red cabbage, lettuce and carrot stored at 4°C, 10°C and 15°C. Changes in carbon dioxide and oxygen concentrations and pH were also monitored. Predictive modelling was used to establish a theoretical shelf-life time as a function of temperature. Lactic acid bacteria at levels of 106 cfu/g appeared to be related to both spoilage and theoretically-predicted shelf-life values.
80BB20A1	The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more "messy" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the "best" prediction system.
7DEC9E18	 Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.
7E2B5A1F	Web page metrics is one of the key elements in measuring various attributes of web site. Metrics gives the concrete values to the attributes of web sites which may be used to compare different web pages. The web pages can be compared based on the page size, information quality ,screen coverage, content coverage etc. Internet and website are emerging media and service avenue requiring improvements in their quality for better customer services for wider user base and for the betterment of human kind. E-business is emerging and websites are not just medium for communication, but they are also the products for providing services. Measurement is the key issue for survival of any organization Therefore to measure and evaluate the websites for quality and for better understanding, the key issues related to website engineering is very important. In this paper we collect data from webby awards data (2007-2010) and classify the websites into good sites and bad sites on the basics of the assessed metrics. To achieve this aim we investigate 15 metrics proposed by various researchers. We present the findings of quantitative analysis of web page attributes and how these attributes are calculated. The result of this paper can be used in quantitative studies in web site designing. The metrics captured in the predicted model can be used to predict the goodness of website design.
7EC23763	A new empirical algorithm-CWAVE [1] was developed at the German aerospace center (DLR). A global dataset of two years (September 1998 to December 2000) of ERS SAR data was reprocessed to more than one million SAR imagettes. Met ocean parameters like significant ocean wave height (Hs), wind speed (U10) and mean wave period (Tm-10) are derived from the SAR images using the CWAVE algorithm [1]. The results are compared to collocated ERS altimeter data and in situ measurements from NOAA buoys and observations taken onboard the vessel Polarstern. It is shown that the SAR derived Hs is comparable in quality to altimeter measurements and can thus be used for real time assimilation.
7F452676	Releasing a software at the right time is crucial to any software house. A set of critical questions that any software project manager has to deal with: “Is the software under development ready to be released now?”, “How many faults are remaining in the software?”, “How many changes are required to correct the errors?”, “How much time is needed to change the programs?” and “How much is the maintenance cost after the software is placed into production?”
801ED71F	This paper focuses on the development of computationally-efficient predictive control algorithms for nonlinear parabolic PDEs with state and control constraints arising in the context of diffusion-reaction processes. Specifically, we consider a diffusion-reaction process described by a nonlinear parabolic PDE and address the problem of stabilization of an unstable steady-state subject to input and state constraints. Galerkin's method is used to derive finite-dimensional systems that capture the dominant dynamics of the parabolic PDF, which are subsequently used for controller design. Various MPC formulations are constructed on the basis of the finite dimensional approximations that differ in the way the evolution of the fast eigenmodes is accounted for in the performance objective and state constraints. The impact of these differences on the ability of the predictive controller to enforce state constraints satisfaction in the infinite-dimensional system is analyzed. Finally, the MPC formulations arc applied, through simulation, to the problem of stabilizing an unstable steady-state of a nonlinear model of a diffusion-reaction process subject to state and control constraints.
7C395478	Sentinel lymph node (SN) biopsy offers the possibility of selective axillary treatment for breast cancer patients, but there are only limited means for the selective treatment of SN-positive patients. Eight predictive models assessing the risk of non-SN involvement in patients with SN metastasis were tested in a multi-institutional setting. Data of 200 consecutive patients with metastatic SNs and axillary lymph node dissection from each of the 5 participating centres were entered into the selected non-SN metastasis predictive tools. There were significant differences between centres in the distribution of most parameters used in the predictive models, including tumour size, type, grade, oestrogen receptor positivity, rate of lymphovascular invasion, proportion of micrometastatic cases and the presence of extracapsular extension of SN metastasis. There were also significant differences in the proportion of cases classified as having low risk of non-SN metastasis. Despite these differences, there were practically no such differences in the sensitivities, specificities and false reassurance rates of the predictive tools. Each predictive tool used in clinical practice for patient and physician decision on further axillary treatment of SN-positive patients may require individual institutional validation; such validation may reveal different predictive tools to be the best in different institutions.
6F592096	A discussion is presented of a software process modeling case study. The four primary objectives of software process modeling are summarized. Thirteen substantive requirements for a software modeling approach are identified. The author has developed a model of this process utilizing a commercially available automated system called STATEMATE. Although this system was originally developed to aid in specifying and designing real-time reactive systems software, successful experiences indicate that it is well suited for modeling software processes. STATEMATE offers a representation formalism that is highly visual, yet formally defined.
75B8BB3E	This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases – both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.
7D839AFC	Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.
80C733FD	Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.