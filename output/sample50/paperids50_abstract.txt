7A4B19C2	Fisher discriminant analysis (FDA) is a popular method for supervised dimensionality reduction. FDA seeks for an embedding transformation such that the ratio of the between-class scatter to the within-class scatter is maximized. Labeled data, however, often consume much time and are expensive to obtain, as they require the efforts of human annotators. In order to cope with the problem of effectively combining unlabeled data with labeled data to find the embedding transformation, we propose a novel method, called subspace semi-supervised Fisher discriminant analysis (SSFDA), for semi-supervised dimensionality reduction. SSFDA aims to find an embedding transformation that respects the discriminant structure inferred from the labeled data and the intrinsic geometrical structure inferred from both the labeled and unlabeled data. We also show that SSFDA can be extended to nonlinear dimensionality reduction scenarios by applying the kernel trick. The experimental results on face recognition demonstrate the effectiveness of our proposed algorithm
6FCF090C	Many Data Analysis tasks deal with data which are presented in high-dimensional spaces, and the ‘curse of dimensionality’ phenomena is often an obstacle to the use of many methods, including Neural Network methods, for solving these tasks. To avoid these phenomena, various Representation learning algorithms are used, as a first key step in solutions of these tasks, to transform the original high-dimensional data into their lower-dimensional representations so that as much information as possible is preserved about the original data required for the considered task. The above Representation learning problems are formulated as various Dimensionality Reduction problems (Sample Embedding, Data Manifold embedding, Data Manifold reconstruction and newly proposed Tangent Bundle Manifold Learning) motivated by various Data Analysis tasks. A new geometrically motivated algorithm that solves all the considered Dimensionality Reduction problems is presented.
77021FAC	We introduce Schroedinger Eigenmaps (SE), a new semi-supervised manifold learning and recovery technique. This method is based on an implementation of graph Schroedinger operators with appropriately constructed barrier potentials as carriers of labeled information. We use our approach for the analysis of standard biomedical datasets and new multispectral retinal images.
804ACF9F	We aim to infer 3D body pose directly from human silhouettes. Given a visual input (silhouette), the objective is to recover the intrinsic body configuration, recover the viewpoint, reconstruct the input and detect any spatial or temporal outliers. In order to recover intrinsic body configuration (pose) from the visual input (silhouette), we explicitly learn view-based representations of activity manifolds as well as learn mapping functions between such central representations and both the visual input space and the 3D body pose space. The body pose can be recovered in a closed form in two steps by projecting the visual input to the learned representations of the activity manifold, i.e., finding the point on the learned manifold representation corresponding to the visual input, followed by interpolating 3D pose.
7E638D24	This paper presents a novel discriminative learning method, called manifold discriminant analysis (MDA), to solve the problem of image set classification. By modeling each image set as a manifold, we formulate the problem as classification-oriented multi-manifolds learning. Aiming at maximizing “manifold margin”, MDA seeks to learn an embedding space, where manifolds with different class labels are better separated, and local data compactness within each manifold is enhanced. As a result, new testing manifold can be more reliably classified in the learned embedding space. The proposed method is evaluated on the tasks of object recognition with image sets, including face recognition and object categorization. Comprehensive comparisons and extensive experiments demonstrate the effectiveness of our method
75EB5739	We consider the problem of image representation for visual analysis. When representing images as vectors, the feature space is of very high dimensionality, which makes it difficult for applying statistical techniques for visual analysis. To tackle this problem, matrix factorization techniques, such as Singular Vector Decomposition (SVD) and Non-negative Matrix Factorization (NMF), received an increasing amount of interest in recent years. Matrix factorization is an unsupervised learning technique, which finds a basis set capturing high-level semantics in the data and learns coordinates in terms of the basis set. However, the representations obtained by them are highly dense and can not capture the intrinsic geometric structure in the data. In this paper, we propose a novel method, called Sparse Concept Coding (SCC), for image representation and analysis. Inspired from the recent developments on manifold learning and sparse coding, SCC provides a sparse representation which can capture the intrinsic geometric structure of the image space. Extensive experimental results on image clustering have shown that the proposed approach provides a better representation with respect to the semantic structure.
15308459	Strong auto-correlation and cross-correlation in chemical processes data can be dealt well by canonical variate analysis (CVA) algorithm, but this algorithm can't solve nonlinear problem of chemical process data. So a fault diagnosis CVA algorithm of chemical process based on isometric feature mapping (ISOMAP) is proposed in this paper. At first, this algorithm uses ISOMAP algorithm of manifold learning to achieve realize nonlinear dimensionality reduction for initial data and maintain internal geometry structure of data. Then CVA is used to the extracted low dimensional data to obtain process state space description and SPE statistics. Fault detection simulation results of TE process show that the proposed algorithm is more effective to detect faults of chemical process than CVA algorithm.
7E6E38EF	This paper describes how face recognition can be effected using 3D shape information extracted from single 2D image views. We characterise the shape of the field of facial normals using a statistical model based on principal geodesic analysis. The model can be fitted to 2D brightness images of faces to recover a vector of shape parameters. Since it captures variations in a field of surface normals, the dimensionality of the shape vector is twice the number of image pixels. We investigate how to perform face recognition using the output of PGA by applying a number of dimensionality reduction techniques including principal components analysis, locally linear embedding, locality preserving projection and Isomap.
7E2F6F8B	Recently, two-dimensional canonical correlation analysis (2DCCA) has been proposed for image analysis. 2DCCA seeks linear correlation based on images directly. It fails to identify nonlinear correlation between two sets of images. In this letter, we present a new manifold learning method called local 2DCCA (L2DCCA) to identify the local correlation. Different from 2DCCA in which images are globally equally treated, L2DCCA weights images differently according to their closeness. That is, the correlation is measured locally, which makes L2DCCA more accurate in finding correlative information. Computationally, L2DCCA is formulated as solving generalized eigenvalue equations tuned by Laplacian matrices. Like 2DCCA, the implementation of L2DCCA is straightforward. Experiments on FERET and UMIST face databases show the effectiveness of the proposed method.
8081F9E6	The objective of this work is to automatically detect the use of game bots in online games based on the trajectories of account users. Online gaming has become one of the most popular Internet activities in recent years, but cheating activity, such as the use of game bots, has increased as a consequence. Generally, the gaming community disapproves of the use of bots, as users may obtain unreasonable rewards without making corresponding efforts. However, game bots are hard to detect because they are designed to simulate human game playing behavior and they follow game rules exactly. Existing methods cannot solve the problem as the differences between bot and human trajectories are generally hard to describe. In this paper, we propose a method for detecting game bots based on some dissimilarity measurements between the trajectories of either bots or human users. The measurements are combined with manifold learning and classification techniques for detection; and the approach is generalizable to any game in which avatars' movements are controlled by the players directly. Through real-life data traces, we observe that the trajectories of bots and humans are very different. Since certain human behavior patterns are difficult to mimic, the characteristic can be used as a signature for bot detection. To evaluate the proposed scheme's performance, we conduct a case study of a popular online game called Quake 2. The results show that the scheme can achieve a high detection rate or classification accuracy on a short trace of several hundred seconds.
786EBF99	A lot of nonlinear embedding techniques have been developed to recover the intrinsic low-dimensional manifolds embedded in the high-dimensional space. However, the quantitative evaluation criteria are less studied in literature. The embedding quality is usually evaluated by visualization which is subjective and qualitative. The few existing evaluation methods to estimate the embedding quality, neighboring preservation rate for example, are not widely applicable. In this paper, we propose several novel criteria for quantitative evaluation, by considering the global smoothness and co-directional consistence of the nonlinear embedding algorithms. The proposed criteria are geometrically intuitive, simple, and easy to implement with a low computational cost. Experiments show that our criteria capture some new geometrical properties of the nonlinear embedding algorithms, and can be used as a guidance to deal with the embedding of the out-of-samples.
7FBD2B42	In (Yang et al., 2007), UDP is proposed to address the limitation of LPP for the clustering and classification tasks. In this communication, we show that the basic ideas of UDP and LPP are identical. In particular, UDP is just a simplified version of LPP on the assumption that the local density is uniform.
7C9A92D9	Conventional appearance-based face recognition methods usually assume that there are multiple samples per person (MSPP) available for discriminative feature extraction during the training phase. In many practical face recognition applications such as law enhancement, e-passport, and ID card identification, this assumption, however, may not hold as there is only a single sample per person (SSPP) enrolled or recorded in these systems. Many popular face recognition methods fail to work well in this scenario because there are not enough samples for discriminant learning. To address this problem, we propose in this paper a novel discriminative multimanifold analysis (DMMA) method by learning discriminative features from image patches. First, we partition each enrolled face image into several nonoverlapping patches to form an image set for each sample per person. Then, we formulate the SSPP face recognition as a manifold-manifold matching problem and learn multiple DMMA feature spaces to maximize the manifold margins of different persons. Finally, we present a reconstruction-based manifold-manifold distance to identify the unlabeled subjects. Experimental results on three widely used face databases are presented to demonstrate the efficacy of the proposed approach.
7EA8FC35	Recently, manifold learning has been widely exploited in pattern recognition, data analysis, and machine learning. This paper presents a novel framework, called Riemannian manifold learning (RML), based on the assumption that the input high-dimensional data lie on an intrinsically low-dimensional Riemannian manifold. The main idea is to formulate the dimensionality reduction problem as a classical problem in Riemannian geometry, that is, how to construct coordinate charts for a given Riemannian manifold? We implement the Riemannian normal coordinate chart, which has been the most widely used in Riemannian geometry, for a set of unorganized data points. First, two input parameters (the neighborhood size k and the intrinsic dimension d) are estimated based on an efficient simplicial reconstruction of the underlying manifold. Then, the normal coordinates are computed to map the input high-dimensional data into a low- dimensional space. Experiments on synthetic data, as well as real-world images, demonstrate that our algorithm can learn intrinsic geometric structures of the data, preserve radial geodesic distances, and yield regular embeddings.
7EBF04F2	We introduce Schroedinger Eigenmaps (SE), a new semi-supervised manifold learning and recovery technique. This method is based on an implementation of graph Schroedinger operators with appropriately constructed barrier potentials as carriers of labeled information. We use our approach for the analysis of standard biomedical datasets and new multispectral retinal images.
7E8C1A6C	Orthogonal neighborhood-preserving projection (ONPP) is a recently developed orthogonal linear algorithm for overcoming the out-of-sample problem existing in the well-known manifold learning algorithm, i.e., locally linear embedding. It has been shown that ONPP is a strong analyzer of high-dimensional data. However, when applied to classification problems in a supervised setting, ONPP only focuses on the intraclass geometrical information while ignores the interaction of samples from different classes. To enhance the performance of ONPP in classification, a new algorithm termed discriminative ONPP (DONPP) is proposed in this paper. DONPP 1) takes into account both intraclass and interclass geometries; 2) considers the neighborhood information of interclass relationships; and 3) follows the orthogonality property of ONPP. Furthermore, DONPP is extended to the semisupervised case, i.e., semisupervised DONPP (SDONPP). This uses unlabeled samples to improve the classification accuracy of the original DONPP. Empirical studies demonstrate the effectiveness of both DONPP and SDONPP.
788099C9	We extend the Nonparametric Discriminant Analysis (NDA) algorithm to a semi-supervised dimensionality reduction technique, called Semi-supervised Nonparametric Discriminant Analysis (SNDA). SNDA preserves the inherent advantages of NDA, that is, relaxing the Gaussian assumption required for the traditional LDA-based methods. SNDA takes advantage of both the discriminating power provided by the NDA method and the locality-preserving power provided by the manifold learning. Specifically, the labeled data points are used to maximize the separability between different classes and both the labeled and unlabeled data points are used to build a graph incorporating neighborhood information of the data set. Experiments on synthetic as well as real datasets demonstrate the effectiveness of the proposed approach. 
77599537	Texture classification is one of the most important tasks in computer vision field and it has been extensively investigated in the last several decades. Previous texture classification methods mainly used the template matching based methods such as Support Vector Machine and k-Nearest-Neighbour for classification. Given enough training images the state-of-the-art texture classification methods could achieve very high classification accuracies on some benchmark databases. However, when the number of training images is limited, which usually happens in real-world applications because of the high cost of obtaining labelled data, the classification accuracies of those state-of-the-art methods would deteriorate due to the overfitting effect. In this paper we aim to develop a novel framework that could correctly classify textural images with only a small number of training images. By taking into account the repetition and sparsity property of textures we propose a sparse representation based multi-manifold analysis framework for texture classification from few training images. A set of new training samples are generated from each training image by a scale and spatial pyramid, and then the training samples belonging to each class are modelled by a manifold based on sparse representation. We learn a dictionary of sparse representation and a projection matrix for each class and classify the test images based on the projected reconstruction errors. The framework provides a more compact model than the template matching based texture classification methods, and mitigates the overfitting effect. Experimental results show that the proposed method could achieve reasonably high generalization capability even with as few as 3 training images, and significantly outperforms the state-of-the-art texture classification approaches on three benchmark datasets.
5A9CAA99	Many of our activities on computer need a verification step for authorized access. The goal of verification is to tell apart the true account owner from intruders. We propose a general approach for user verification based on user trajectory inputs. The approach is labor-free for users and is likely to avoid the possible copy or simulation from other non-authorized users or even automatic programs like bots. Our study focuses on finding the hidden patterns embedded in the trajectories produced by account users. We employ a Markov chain model with Gaussian distribution in its transitions to describe the behavior in the trajectory. To distinguish between two trajectories, we propose a novel dissimilarity measure combined with a manifold learnt tuning for catching the pairwise relationship. Based on the pairwise relationship, we plug-in any effective classification or clustering methods for the detection of unauthorized access. The method can also be applied for the task of recognition, predicting the trajectory type without pre-defined identity. Given a trajectory input, the results show that the proposed method can accurately verify the user identity, or suggest whom owns the trajectory if the input identity is not provided.
80EC51B1	Dimensionality reduction plays a fundamental role in data processing, for which principal component analysis (PCA) is widely used. In this paper, we develop the Laplacian PCA (LPCA) algorithm which is the extension of PCA to a more general form by locally optimizing the weighted scatter. In addition to the simplicity of PCA, the benefits brought by LPCA are twofold: the strong robustness against noise and the weak metric-dependence on sample spaces. The LPCA algorithm is based on the global alignment of locally Gaussian or linear subspaces via an alignment technique borrowed from manifold learning. Based on the coding length of local samples, the weights can be determined to capture the local principal structure of data. We also give the exemplary application of LPCA to manifold learning. Manifold unfolding (non-linear dimensionality reduction) can be performed by the alignment of tangential maps which are linear transformations of tangent coordinates approximated by LPCA. The superiority of LPCA to PCA and kernel PCA is verified by the experiments on face recognition (FRGC version 2 face database) and manifold (Scherk surface) unfolding.
795AC0B1	Many unsupervised algorithms for nonlinear dimensionality reduction, such as locally linear embedding (LLE) and Laplacian eigenmaps, are derived from the spectral decompositions of sparse matrices. While these algorithms aim to preserve certain proximity relations on average, their embeddings are not explicitly designed to preserve local features such as distances or angles. In this paper, we show how to construct a low dimensional embedding that maximally preserves angles between nearby data points. The embedding is derived from the bottom eigenvectors of LLE and/or Laplacian eigenmaps by solving an additional (but small) problem in semidefinite programming, whose size is independent of the number of data points. The solution obtained by semidefinite programming also yields an estimate of the data's intrinsic dimensionality. Experimental results on several data sets demonstrate the merits of our approach.
7A71B1B2	Recently, several manifold learning algorithms have been proposed, such as ISOMAP (Tenenbaum et al., 2000), Locally Linear Embedding (Roweis & Saul, 2000), Laplacian Eigenmap (Belkin & Niyogi, 2001), Locality Preserving Projection (LPP) (He & Niyogi, 2003), etc. All of them aim at discovering the meaningful low dimensional structure of the data space. In this paper, we present a statistical analysis of the LPP algorithm. Different from Principal Component Analysis (PCA) which obtains a subspace spanned by the largest eigenvectors of the global covariance matrix, we show that LPP obtains a subspace spanned by the smallest eigenvectors of the local covariance matrix. We applied PCA and LPP to real world document clustering task. Experimental results show that the performance can be significantly improved in the subspace, and especially LPP works much better than PCA.
6C921C82	We present a method to identify the trajectories of moving vehicles from various viewpoints using manifold learning to be implemented on an embedded platform for traffic surveillance. We use a robust kernel Isomap to estimate the intrinsic low-dimensional manifold of input space. During training, the extracted features of the training data are projected on to a 2D manifold and features corresponding to each trajectory are clustered in to k clusters, each represented as a Gaussian model. During identification, features of test data are projected on to the 2D manifold constructed during training and the Mahalanobis distance between test data and Gaussian models of each trajectory is evaluated to identify the trajectory. Experimental results demonstrate the effectiveness of the proposed method in estimating the trajectories of the moving vehicles, even though shapes and sizes of vehicles change rapidly.
0418F3CE	The spectra of color can represent a color in the most accurate way, but the dimension of the spectral data is too high to process. This paper aims to analyze the spectral reflectance curves of 1269 Munsell standard color samples with some influential algorithms in manifold learning. Experimental results reveal that the intrinsic dimension of the embedded manifold in the spectral Munsell color space is 3 and the 3-dimensional structure of this manifold looks like a cone, consistent with the development and structure of the Munsell color system.
78F132A8	We present a method that automatically detects chewing events in surveillance video of a subject. Firstly, an Active Appearance Model (AAM) is used to track a subject’s face across the video sequence. It is observed that the variations in the AAM parameters across chewing events demonstrate a distinct periodicity. We utilize this property to discriminate between chewing and non-chewing facial actions such as talking. A feature representation is constructed by applying spectral analysis to a temporal window of model parameter values. The estimated power spectra subsequently undergo non-linear dimensionality reduction via spectral regression. The low-dimensional representations of the power spectra are employed to train a Support Vector Machine (SVM) binary classifier to detect chewing events. Experimental results yielded a cross validated percentage agreement of 93.4%, indicating that the proposed system provides an efficient approach to automated chewing detection.
7D6B07D0	The main objective of the work presented here is to introduce a supervised, nonlinear dimensionality reduction technique which performs well-known linear discriminant analysis in a local way and which is able to provide a powerful mapping with less computational effort than other nonlinear reduction methods. Additionally, because of the close connection of the new approach to Fisher's LDA, it is more clear that it acts discriminatively, which is not immediately apparent from previous formulations. The method makes use of the optimal scoring framework advocated by Hastie et al. and it is coined local discriminant analysis (lDA)
7C8568F0	Tropical cyclones (TCs) are one of the worst nature disasters. The analysis of tropical cyclones' similarity is important in the TCs forecasting. In this paper, the similarities of TCs are studied in a fixed low dimension space. Both TC path and image are analyzed. Isometric Mapping (ISOMAP) is used to do dimensionality reduction. Several distribution characters, such as mean value, coefficient of skewness and kurtosis, are used as the features of clustering. The similarity of TCs in the low dimension space is homologous as in the high dimension space. Experiments also show that the similarity of TC paths is consistent with the similarity of TC images.
7E7AC384	Kernel discriminant analysis (KDA) is an effective approach for supervised nonlinear dimensionality reduction. Probabilistic models can be used with KDA to improve its robustness. However, the state of the art of such models could only handle bi- nary class problems, which confines their applica- tion in many real world problems. To overcome this limitation, we propose a novel nonparametric probabilistic model based on Gaussian Process for KDA to handle multiclass problems. The model provides a novel Bayesian interpretation for KDA, which allows its parameters to be automatically tuned through the optimization of the marginal log- likelihood of the data. Empirical study demon- strates the efficacy of the proposed model. 
59C1D04F	We address dimensionality estimation and nonlinear manifold inference starting from point inputs in high dimensional spaces using tensor voting. The proposed method operates locally in neighborhoods and does not involve any global computations. It is based on information propagation among neighboring points implemented as a voting process. Unlike other local approaches for manifold learning, the quantity propagated from one point to another is not a scalar, but is in the form of a tensor that provides considerably richer information. The accumulation of votes at each point provides a reliable estimate of local dimensionality, as well as of the orientation of a potential manifold going through the point. Reliable dimensionality estimation at the point level is a major advantage over competing methods. Moreover, the absence of global operations allows us to process significantly larger datasets. We demonstrate the effectiveness of our method on a variety of challenging datasets
76C2E023	The significant economic contributions of the tourism industry in recent years impose an unprecedented force for data mining and machine learning methods to analyze tourism data. The intrinsic problems of raw data in tourism are largely related to the complexity, noise and nonlinearity in the data that may introduce many challenges for the existing data mining techniques such as rough sets and neural networks. In this paper, a novel method using SVM-based classification with two nonlinear feature projection techniques is proposed for tourism data analysis. The first feature projection method is based on ISOMAP (Isometric Feature Mapping), which is a class of manifold learning approaches for dimension reduction. By making use of ISOMAP, part of the noisy data can be identified and the classification accuracy of SVMs can be improved by appropriately discarding the noisy training data. The second feature projection method is a probabilistic space mapping technique for scale transformation. Experimental results on expenditure data of business travelers show that the proposed method can improve prediction performance both in terms of testing accuracy and statistical coincidence. In addition, both of the feature projection methods are helpful to reduce the training time of SVMs.
79A05253	A framework for the regularized and robust estimation of non-uniform dimensionality and density in high dimensional noisy data is introduced in this work. This leads to learning stratifications, that is, mixture of manifolds representing different characteristics and complexities in the data set. The basic idea relies on modeling the high dimensional sample points as a process of translated Poisson mixtures, with regularizing restrictions, leading to a model which includes the presence of noise. The translated Poisson distribution is useful to model a noisy counting process, and it is derived from the noise-induced translation of a regular Poisson distribution. By maximizing the log-likelihood of the process counting the points falling into a local ball, we estimate the local dimension and density. We show that the sequence of all possible local countings in a point cloud formed by samples of a stratification can be modeled by a mixture of different translated Poisson distributions, thus allowing the presence of mixed dimensionality and densities in the same data set. With this statistical model, the parameters which best describe the data, estimated via expectation maximization, divide the points in different classes according to both dimensionality and density, together with an estimation of these quantities for each class. Theoretical asymptotic results for the model are presented as well. The presentation of the theoretical framework is complemented with artificial and real examples showing the importance of regularized stratification learning in high dimensional data analysis in general and computer vision and image analysis in particular.
7F76954A	Can we detect low dimensional structure in high dimensional data sets of images and video? The problem of dimensionality reduction arises often in computer vision and pattern recognition. In this paper, we propose a new solution to this problem based on semidefinite programming. Our algorithm can be used to analyze high dimensional data that lies on or near a low dimensional manifold. It overcomes certain limitations of previous work in manifold learning, such as Isomap and locally linear embedding. We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on actual images of faces, handwritten digits, and solid objects.
7EA1A872	This paper presents a new framework for manifold learning based on a sequence of principal polynomials that capture the possibly nonlinear nature of the data. The proposed Principal Polynomial Analysis (PPA) generalizes PCA by modeling the directions of maximal variance by means of curves, instead of straight lines. Contrarily to previous approaches, PPA reduces to performing simple univariate regressions, which makes it computationally feasible and robust. Moreover, PPA shows a number of interesting analytical properties. First, PPA is a volume-preserving map, which in turn guarantees the existence of the inverse. Second, such an inverse can be obtained in closed form. Invertibility is an important advantage over other learning methods, because it permits to understand the identified features in the input domain where the data has physical meaning. Moreover, it allows to evaluate the performance of dimensionality reduction in sensible (input-domain) units. Volume preservation also allows an easy computation of information theoretic quantities, such as the reduction in multi-information after the transform. Third, the analytical nature of PPA leads to a clear geometrical interpretation of the manifold: it allows the computation of Frenet–Serret frames (local features) and of generalized curvatures at any point of the space. And fourth, the analytical Jacobian allows the computation of the metric induced by the data, thus generalizing the Mahalanobis distance. These properties are demonstrated theoretically and illustrated experimentally. The performance of PPA is evaluated in dimensionality and redundancy reduction, in both synthetic and real datasets from the UCI repository.
7AF023AD	Manifold learning has been demonstrated as an effective way to represent intrinsic geometrical structure of samples. In this paper, a new manifold learning approach, named Local Coordinates Alignment (LCA), is developed based on the alignment technique. LCA first obtains local coordinates as representations of local neighborhood by preserving proximity relations on a patch, which is Euclidean. Then, these extracted local coordinates are aligned to yield the global embeddings. To solve the out of sample problem, linearization of LCA (LLCA) is proposed. In addition, in order to solve the non-Euclidean problem in real world data when building the locality, kernel techniques are utilized to represent similarity of the pairwise points on a local patch. Empirical studies on both synthetic data and face image sets show effectiveness of the developed approaches.
7DB89D55	In this paper, we propose a framework for unsupervised analysis of human behavior based on manifold learning. First, a pairwise human posture distance matrix is calculated from a training action sequence. Then, the isometric feature mapping (Isomap) algorithm is applied to construct a low-dimensional structure from the distance matrix. The data points in the Isomap space are consequently represented as a time-series of low-dimensional points. A temporal segmentation technique is then applied to segment the time series into subseries corresponding to atomic actions. Next, a dynamic time warping (DTW) approach is applied for clustering atomic action sequences. Finally, we use the clustering results to learn and classify atomic actions using the nearest neighbor rule. Experiments conducted on real data demonstrate the efficacy of the proposed method.
80DF96F7	Past decades, numerous spectral analysis based algorithms have been proposed for dimensionality reduction, which plays an important role in machine learning and artificial intelligence. However, most of these existing algorithms are developed intuitively and pragmatically, i.e., on the base of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide some a systematic framework for understanding the common properties and intrinsic differences in the algorithms. In this paper, we propose such a framework, i.e., “patch alignment”, which consists of two stages: part optimization and whole alignment. With the proposed framework, various algorithms including the conventional linear algorithms and the manifold learning algorithms are reformulated into a unified form, which gives us some new understandings on these algorithms.
5F7D8B94	Activity recognition in complex scenes can be very challenging because human actions are unconstrained and may be observed from multiple views. While progress has been made in recognizing activities from fixed views, more research is needed in developing view invariant recognition methods. Furthermore, the recognition and classification of activities involves processing data in the space and time domains, which involves large amounts of data and can be computationally expensive to process. To accommodate for view invariance and high dimensional data we propose the use of Manifold Learning using Locality Preserving Projections (LPP). We develop an efficient set of features based on radial distance and present a Manifold Learning framework for learning low dimensional representations of action primitives that can be used to recognize activities at multiple views. Using our approach we present high recognition rates on the Inria IXMAS dataset.
78B407CA	Many natural image sets are samples of a low-dimensional manifold in the space of all possible images. Understanding this manifold is a key first step in understanding many sets of images, and manifold learning approaches have recently been used within many application domains, including face recognition, medical image segmentation, gait recognition and hand-written character recognition. This paper attempts to characterize the special features of manifold learning on image data sets, and to highlight the value and limitations of these approaches. 
6D213A1C	A metric graph is a 1-dimensional stratified metric space consisting of vertices and edges or loops glued together. Metric graphs can be naturally used to represent and model data that take the form of noisy filamentary structures, such as street maps, neurons, networks of rivers and galaxies. We consider the statistical problem of reconstructing the topology of a metric graph embedded in R^D from a random sample. We derive lower and upper bounds on the minimax risk for the noiseless case and tubular noise case. The upper bound is based on the reconstruction algorithm given in Aanjaneya 
803972A0	Many of our activities on computer need a verification step for authorized access. The goal of verification is to tell apart the true account owner from intruders. We propose a general approach for user verification based on user trajectory inputs. The approach is labor-free for users and is likely to avoid the possible copy or simulation from other non-authorized users or even automatic programs like bots. Our study focuses on finding the hidden patterns embedded in the trajectories produced by account users. We employ a Markov chain model with Gaussian distribution in its transitions to describe the behavior in the trajectory. To distinguish between two trajectories, we propose a novel dissimilarity measure combined with a manifold learnt tuning for catching the pairwise relationship. Based on the pairwise relationship, we plug-in any effective classification or clustering methods for the detection of unauthorized access. The method can also be applied for the task of recognition, predicting the trajectory type without pre-defined identity. Given a trajectory input, the results show that the proposed method can accurately verify the user identity, or suggest whom owns the trajectory if the input identity is not provided.
7745A8F7	Defect detection has been considered an efficient way to increase the yield rate of panels in thin film transistor liquid crystal display (TFT-LCD) manufacturing. In this study we focus on the array process since it is the first and key process in TFT-LCD manufacturing. Various defects occur in the array process, and some of them could cause great damage to the LCD panels. Thus, how to design a method that can robustly detect defects from the images captured from the surface of LCD panels has become crucial. Previously, support vector data description (SVDD) has been successfully applied to LCD defect detection. However, its generalization performance is limited. In this paper, we propose a novel one-class machine learning method, called quasiconformal kernel SVDD (QK-SVDD) to address this issue. The QK-SVDD can significantly improve generalization performance of the traditional SVDD by introducing the quasiconformal transformation into a predefined kernel. Experimental results, carried out on real LCD images provided by an LCD manufacturer in Taiwan, indicate that the proposed QK-SVDD not only obtains a high defect detection rate of 96%, but also greatly improves generalization performance of SVDD. The improvement has shown to be over 30%. In addition, results also show that the QK-SVDD defect detector is able to accomplish the task of defect detection on an LCD image within 60 ms.
5D54CAB5	We present a high-throughput computer-aided system for the segmentation and classification of glands in high resolution digitized images of needle core biopsy samples of the prostate. It will allow for rapid and accurate identification of suspicious regions on these samples. The system includes the following three modules: 1) a hierarchical frequency weighted mean shift normalized cut (HNCut) for initial detection of glands; 2) a geodesic active contour (GAC) model for gland segmentation; and 3) a diffeomorphic based similarity (DBS) feature extraction for classification of glands as benign or cancerous. HNCut is a minimally supervised color based detection scheme that combines the frequency weighted mean shift and normalized cuts algorithms to detect the lumen region of candidate glands. A GAC model, initialized using the results of HNCut, uses a color gradient based edge detection function for accurate gland segmentation. Lastly, DBS features are a set of morphometric features derived from the nonlinear dimensionality reduction of a dissimilarity metric between shape models. The system integrates these modules to enable the rapid detection, segmentation, and classification of glands on prostate biopsy images. Across 23 H & E stained prostate studies of whole-slides, 105 regions of interests (ROIs) were selected for the evaluation of segmentation and classification. The segmentation results were evaluated on 10 ROIs and compared to manual segmentation in terms of mean distance (2.6 ±0.2 pixels), overlap (62±0.07%), sensitivity (85±0.01%), specificity (94±0.003%) and positive predictive value (68±0.08%). Over 105 ROIs, the classification accuracy for glands automatically segmented was (82.5 ±9.10%) while the accuracy for glands manually segmented was (82.89 ±3.97%); no statistically significant differences were identified between the classification results.
7CBFD502	Magnetic resonance spectroscopy (MRS) has been shown to have great clinical potential as a supplement to magnetic resonance imaging in the detection of prostate cancer (CaP). MRS provides functional information in the form of changes in the relative concentration of specific metabolites including choline, creatine, and citrate which can be used to identify potential areas of CaP. With a view to assisting radiologists in interpretation and analysis of MRS data, some researchers have begun to develop computer-aided detection (CAD) schemes for CaP identification from spectroscopy. Most of these schemes have been centered on identifying and integrating the area under metabolite peaks which is then used to compute relative metabolite ratios. However, manual identification of metabolite peaks on the MR spectra, and especially via CAD, is a challenging problem due to low signal-to-noise ratio, baseline irregularity, peak overlap, and peak distortion. In this article the authors present a novel CAD scheme that integrates nonlinear dimensionality reduction (NLDR) with an unsupervised hierarchical clustering algorithm to automatically identify suspicious regions on the prostate using MRS and hence avoids the need to explicitly identify metabolite peaks. The methodology comprises two stages. In stage 1, a hierarchical spectral clustering algorithm is used to distinguish between extracapsular and prostatic spectra in order to localize the region of interest (ROI) corresponding to the prostate. Once the prostate ROI is localized, in stage 2, a NLDR scheme, in conjunction with a replicated clustering algorithm, is used to automatically discriminate between three classes of spectra (normal appearing, suspicious appearing, and indeterminate). The methodology was quantitatively and qualitatively evaluated on a total of 18 1.5 T in vivo prostate T2-weighted (w) and MRS studies obtained from the multisite, multi-institutional American College of Radiology (ACRIN) trial. In the absence of the precise ground truth for CaP extent on the MR imaging for most of the ACRIN studies, probabilistic quantitative metrics were defined based on partial knowledge on the quadrant location and size of the tumor. The scheme, when evaluated against this partial ground truth, was found to have a CaP detection sensitivity of 89.33% and specificity of 79.79%. The results obtained from randomized threefold and fivefold cross validation suggest that the NLDR based clustering scheme has a higher CaP detection accuracy compared to such commonly used MRS analysis schemes as z score and PCA. In addition, the scheme was found to be robust to changes in system parameters. For 6 of the 18 studies an expert radiologist laboriously labeled each of the individual spectra according to a five point scale, with 1/2 representing spectra that the expert considered normal and 3/4/5 being spectra the expert deemed suspicious. When evaluated on these expert annotated datasets, the CAD system yielded an average sensitivity (cluster corresponding to suspicious spectra being identified as the CaP class) and specificity of 81.39% and 64.71%, respectively.
7E562A5B	In recent years, online gaming has become one of the most popular Internet activities, but cheating activity, such as the use of game bots, has increased as a consequence. Generally, the gaming community disapproves of the use of game bots, as bot users obtain unreasonable rewards without corresponding efforts. However, bots are hard to detect because they are designed to simulate human game playing behavior and they follow game rules exactly. Existing detection approaches either disrupt players' gaming experiences, or they assume game bots are run as standalone clients or assigned a specific goal, such as aim bots in FPS games.In this paper, we propose a manifold learning approach for detecting game bots. It is a general technique that can be applied to any game in which avatars' movement is controlled by the players directly. Through real-life data traces, we show that the trajectories of human players and those of game bots are very different. In addition, although game bots may endeavor to simulate players' decisions, certain human behavior patterns are difficult to mimic because they are AI-hard. Taking Quake 2 as a case study, we evaluate our scheme's performance based on real-life traces. The results show that the scheme can achieve a detection accuracy of 98% or higher on a trace of 700 seconds.
7A72BAF4	Frequent occurrence of ocular artifacts leads to serious problems in interpreting and analyzing the electroencephalogram (EEG). In the present paper, a novel and robust technique is proposed to eliminate ocular artifacts from EEG signals in real time. Independent Component Analysis (ICA) is used to decompose EEG signals. The features of topography and power spectral density of those components are extracted. Moreover, we introduce manifold learning algorithm, a recently popular dimensionality reduction technique, to reduce the dimensionality of initial features, and then those new features are fed to a classifier to identify ocular artifacts components. A k-nearest neighbor classifier is adopted to classify components because classification results show that manifold learning with the nearest neighbor algorithm works best. Finally, the artifact removal method proposed here is evaluated by the comparisons of EEG data before and after artifact removal. The results indicate that the method proposed could remove ocular artifacts effectively from EEG signals with little distortion of the underlying brain signals and be satisfied the real-time application.
7B8AC148	In this paper, we propose a novel orthogonal manifold learning algorithm for semisupervised dimension reduction, referred to as Flexible Orthogonal Semisupervised Dimension Reduction (FODR). Our algorithm is based on the recently-developed algorithm, called Trace Ratio Based Flexible Semisupervised Discriminant Analysis (TR-FSDA). TR-FSDA introduces an orthogonality constraint and a flexible regularizer to relax such a hard linear constraint in Semisupervised Discriminant Analysis (SDA) that the low-dimensional representation is constrained to lie within the linear subspace spanned by the data, whose solution follows from solving a trace ratio problem iteratively. However, it is not guaranteed that TR-FSDA always converges. Instead of finding the orthogonal projection vectors once, our algorithm produces the orthogonal projection vectors, step by step. In each time of iterations, an orthogonal projection vector and a one-dimensional data representation are produced by solving a standard Rayleigh Quotient problem, and more importantly, the determination of a new orthogonal projection vector does not involve the knowledge of the specific statistical property for the previously-obtained orthogonal projection vectors. Therefore, it is not necessary for our FODR algorithm to guarantee the convergence. The experiments are tried out on COIL20, UMIST, ORL, YALE, MPEG-7, FERET, and Handwritten DIGIT databases, and show the effectiveness of the proposed algorithm.
79B32AC5	In this paper, linear local tangent space alignment (LLTSA), as a novel linear dimensionality reduction algorithm, is proposed. It uses the tangent space in the neighborhood of a data point to represent the local geometry, and then aligns those local tangent spaces in the low-dimensional space which is linearly mapped from the raw high-dimensional space. Since images of faces often belong to a manifold of intrinsically low dimension, we develop LLTSA algorithm for effective face manifold learning and recognition. Comprehensive comparisons and extensive experiments show that LLTSA achieves much higher recognition rates than a few competing methods.
76FD2C4F	Most of the existing methods for pedestrian detection work well, only when the following assumption is satisfied: the features extracted from the training dataset and the testing dataset have very similar distributions in the feature space. However, in practice, this assumption does not hold because of the scene complexity and variation. In this paper, a new method is proposed for detecting pedestrians in various scenes based on the transfer learning technique. Our proposed method employs the following two strategies for improving the pedestrian detection performance. First, a new sample screening method based on manifold learning is proposed. The basic idea is to choose samples from the training set, which may be similar to the samples from the unseen scene, and then merge the selected samples into the unseen set. Second, a new classification model based on transfer learning is proposed. The advantage of the classification model is that only a small number of samples need to be used from the unseen scenes. Most of the training samples are still obtained from the training scene, which take up to 90% of the entire training samples. Compared to the traditional pedestrian detection methods, the proposed algorithm can adapt to different scenes for detecting pedestrians. Experiments on two pedestrian detection benchmark datasets, DC and NICTA, showed that the method can obtain better performance as compared to other previous methods.
80AAA000	Manifold learning have attracted considerable attention over the last decade. The most frequently used functional is the l2-norm of the gradient of the function. In this paper, we consider the linear manifold learning problem by minimizing this functional with appropriate constraint. We provide theoretical analysis on both the functional and the constraint, which shows the affine hulls of the manifold and the connected components are essential to linear manifold learning problem. Based on the theoretical analysis, we introduce a novel linear manifold learning algorithm called approximately harmonic projection (AHP). Unlike canonical linear methods such as principal component analysis, our method is sensitive to the connected components. This makes our method especially applicable to data clustering. We conduct several experimental results on three real data sets to demonstrate the effectiveness of our proposed method.
7C46A294	This paper introduces Multiple Manifold Locally Linear Embedding (MM-LLE) learning. This method learns multiple manifolds corresponding to multiple classes in a data set. The proposed approach to manifold learning includes a supervised form of neighborhood selection in learning individual manifolds that correspond to each class of data. Furthermore, MM-LLE uses manifold–manifold distance (MMD) as a measure to find the optimum low-dimensional space needed to achieve high classification accuracy. When classifying new data samples, in addition to the conventional classification techniques used in the past literature to classify new data in the manifold space, we introduce a point-to-manifold distance (PMD) metric used to measure the distance between points and manifolds. Experimental results reported in this paper compare the recognition rates for a number of different manifold learning methods. The proposed MM-LLE technique has various applications in classification and object recognition.
7EBC281D	Nowadays, product development in the car industry heavily relies on numerical simulations. For example, it is used to explore the influence of design parameters on the weight, costs or functional properties of new car models. Car engineers spend a considerable amount of their time analyzing these influences by inspecting the arising simulations one at a time. Here, we propose using methods from machine learning to semi-automatically analyze the arising finite element data and thereby significantly assist in the overall engineering process.We combine clustering and nonlinear dimensionality reduction to show that the method is able to automatically detect parameter dependent structure instabilities or reveal bifurcations in the time-dependent behavior of beams. In particular we study recent nonlinear and sparse grid approaches, respectively. Our examples demonstrate the strong potential of our approach for reducing the data analysis effort in the engineering process, and emphasize the need for nonlinear methods for such tasks.
7C300835	Dimensionality reduction algorithms, which aim to obtain low-dimensional feature representation with enhanced discrimination power, have attracted great attention for face recognition. Local Fisher discriminant analysis (LFDA) is a recently developed linear dimensionality reduction algorithm. It has been shown that LFDA is a strong analyzer of high-dimensional data. However, LFDA is a linear method, and this makes it difficult to describe the complex nonlinearity of face images. In addition, LFDA only focuses on using a single data descriptor to depict the whole face image dataset, while lacks a systematic way of integrating multiple image features for dimensionality reduction. To enhance the performance of LFDA in face recognition, a new algorithm termed multiple kernel local Fisher discriminant analysis (MKLFDA) is proposed in this paper. MKLFDA produces nonlinear discriminant features via kernel theory, and considers multiple image features with multiple base kernels. Experimental results on three face databases demonstrate the effectiveness of the proposed algorithm.