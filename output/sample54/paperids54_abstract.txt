5BB733DA	Graph cuts based interactive segmentation has become very popular over the last decade. In standard graph cuts, the extraction of foreground object in a complex background often leads to many segmentation errors and the parameter λ in the energy function is hard to select. In this paper, we propose an iterated graph cuts algorithm, which starts from the sub-graph that comprises the user labeled foreground/background regions and works iteratively to label the surrounding un-segmented regions. In each iteration, only the local neighboring regions to the labeled regions are involved in the optimization so that much interference from the far unknown regions can be significantly reduced. To improve the segmentation efficiency and robustness, we use the mean shift method to partition the image into homogenous regions, and then implement the proposed iterated graph cuts algorithm by taking each region, instead of each pixel, as the graph node for segmentation. Extensive experiments on benchmark datasets demonstrated that our method gives much better segmentation results than the standard graph cuts and the GrabCut methods in both qualitative and quantitative evaluation. Another important advantage is that it is insensitive to the parameter λ in optimization.
7FAF78B0	A new robust watermarking method, named QIM-NSGA-II, is proposed based on the non-dominated sorting genetic algorithm II (NSGA-II) and quantization index modulation (QIM). The NSGA-II algorithm is utilized to find out the optimal embedding position and adaptive quantization step for embedding watermark into a carrier image in the framework of QIM. In the process of searching an optimal solution, the trade-off between robustness and image fidelity of the watermarked image is represented by the Pareto-Front discovered by NSGA-II. Experiment results show that the proposed scheme has a good robustness against common attacks, such as amplitude scaling, noise, filtering, cropping, JPEG compression.
7E397560	This paper presents the effects of multiscale windowed denoising of spectral signatures before segmentation of hyperspectral images. In the proposed denoising approach it is intended to exploit both spectral and spatial information of the hyperspectral images by using wavelets and principal component analysis. The windowed structure incorporated for this method exploits spatial information by making use of possibly highly correlated pixels. In addition to the proposed method, the segmented PCA is also investigated and compared in the experimental results with a proper modification. In the segmentation process, the K-means and fuzzy-ART algorithms are used. Especially fuzzy-ART is a fast learning network and can be used in high dimensional and high volume data such as hyperspectral images. In the experiments it has been shown that multiscale windowed principal component denoising has positive effects on the segmentation/clustering level.
7D6A2FF8	This paper addresses the problem of object detection and recognition in complex scenes, where objects are partially occluded. The approach presented herein is based on the hypothesis that a careful analysis of visible object details at various scales is critical for recognition in such settings. In general, however, computational complexity becomes prohibitive when trying to analyze multiple sub-parts of multiple objects in an image. To alleviate this problem, we propose a generative-model framework—namely, dynamic tree-structure belief networks (DTSBNs). This framework formulates object detection and recognition as inference of DTSBN structure and image-class conditional distributions, given an image. The causal (Markovian) dependencies in DTSBNs allow for design of computationally efficient inference, as well as for interpretation of the estimated structure as follows: each root represents a whole distinct object, while children nodes down the sub-tree represent parts of that object at various scales. Therefore, within the DTSBN framework, the treatment and recognition of object parts requires no additional training, but merely a particular interpretation of the tree/subtree structure. This property leads to a strategy for recognition of objects as a whole through recognition of their visible parts. Our experimental results demonstrate that this approach remarkably outperforms strategies without explicit analysis of object parts.
7C680F0E	Topology is an important prior in many image segmentation tasks. In this paper, we design and implement a novel graph-based min-cut/max-flow algorithm that incorporates topology priors as global constraints. We show that the optimization of the energy function we consider here is NP-hard. However, our algorithm is guaranteed to find an approximate solution that conforms to the initialization, which is a desirable property in many applications since the globally optimum solution does not consider any initialization information. The key innovation of our algorithm is the organization of the search for maximum flow in a way that allows consideration of topology constraints. In order to achieve this, we introduce a label attribute for each node to explicitly handle the topology constraints, and we use a distance map to keep track of those nodes that are closest to the boundary. We employ the bucket priority queue data structure that records nodes of equal distance and we efficiently extract the node with minimal distance value. Our methodology of embedding distance functions in a graph-based algorithm is general and can also account for other geometric priors. Experimental results show that our algorithm can efficiently handle segmentation cases that are challenging for graph-cut algorithms. Furthermore, our algorithm is a natural choice for problems with rich topology priors such as object tracking.
8064C0D7	This paper proposes a new object representation, called Connected Segmentation Tree (CST), which captures canonical characteristics of the object in terms of the photometric, geometric, and spatial adjacency and containment properties of its constituent image regions. CST is obtained by augmenting the object’s segmentation tree (ST) with inter-region neighbor links, in addition to their recursive embedding structure already present in ST. This makes CST a hierarchy of region adjacency graphs. A region’s neighbors are computed using an extension to regions of the Voronoi diagram for point patterns. Unsupervised learning of the CST model of a category is formulated as matching the CST graph representations of unlabeled training images, and fusing their maximally matching subgraphs. A new learning algorithm is proposed that optimizes the model structure by simultaneously searching for both the most salient nodes (regions) and the most salient edges (containment and neighbor relationships of regions) across the image graphs. Matching of the category model to the CST of a new image results in simultaneous detection, segmentation and recognition of all occurrences of the category, and a semantic explanation of these results.
7E204DC5	We propose a generic grouping algorithm that constructs a hierarchy of regions from the output of any contour detector. Our method consists of two steps, an oriented watershed transform (OWT) to form initial regions from contours, followed by construction of an ultra-metric contour map (UCM) defining a hierarchical segmentation. We provide extensive experimental evaluation to demonstrate that, when coupled to a high-performance contour detector, the OWT-UCM algorithm produces state-of-the-art image segmentations. These hierarchical segmentations can optionally be further refined by user-specified annotations.
8006580F	Graph cut is a popular technique for interactive image segmentation. However, it has certain shortcomings. In particular, graph cut has problems with segmenting thin elongated objects due to the “shrinking bias”. To overcome this problem, we propose to impose an additional connectivity prior, which is a very natural assumption about objects. We formulate several versions of the connectivity constraint and show that the corresponding optimization problems are all NP-hard. For some of these versions we propose two optimization algorithms: (i) a practical heuristic technique which we call DijkstraGC, and (ii) a slow method based on problem decomposition which provides a lower bound on the problem. We use the second technique to verify that for some practical examples DijkstraGC is able to find the global minimum.
7D66ECD1	This paper presents a unified framework for object detection, segmentation, and classification using regions. Region features are appealing in this context because: (1) they encode shape and scale information of objects naturally; (2) they are only mildly affected by background clutter. Regions have not been popular as features due to their sensitivity to segmentation errors. In this paper, we start by producing a robust bag of overlaid regions for each image using Arbeldez et al., CVPR 2009. Each region is represented by a rich set of image cues (shape, color and texture). We then learn region weights using a max-margin framework. In detection and segmentation, we apply a generalized Hough voting scheme to generate hypotheses of object locations, scales and support, followed by a verification classifier and a constrained segmenter on each hypothesis. The proposed approach significantly outperforms the state of the art on the ETHZ shape database(87.1% average detection rate compared to Ferrari et al. 's 67.2%), and achieves competitive performance on the Caltech 101 database.
7D53354D	Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
7E12A1BA	We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.
72286762	Depth estimation and semantic segmentation are two fundamental problems in image understanding. While the two tasks are strongly correlated and mutually beneficial, they are usually solved separately or sequentially. Motivated by the complementary properties of the two tasks, we propose a unified framework for joint depth and semantic prediction. Given an image, we first use a trained Convolutional Neural Network (CNN) to jointly predict a global layout composed of pixel-wise depth values and semantic labels. By allowing for interactions between the depth and semantic information, the joint network provides more accurate depth prediction than a state-of-the-art CNN trained solely for depth prediction [6]. To further obtain fine-level details, the image is decomposed into local segments for region-level depth and semantic prediction under the guidance of global layout. Utilizing the pixel-wise global prediction and region-wise local prediction, we formulate the inference problem in a two-layer Hierarchical Conditional Random Field (HCRF) to produce the final depth and semantic map. As demonstrated in the experiments, our approach effectively leverages the advantages of both tasks and provides the state-of-the-art results.
7046E954	This paper presents a novel variational method for im age segmentation that unifies boundary and region-based information sources under the Geodesic Active Region framework. A statistical analysis based on the Minimum Description Length criterion and the Maximum Likelihood Principle for the observed density function (image histogram) using a mixture of Gaussian elements, indicates the number of the different regions and their intensity properties. Then, the boundary information is determined using a probabilistic edge detector, while the region information is estimated using the Gaussian components of the mixture model. The defined objective function is mini mized using a gradientdescent method where a level set approach is used to implement the resulting PDE system. According to the motion equations, the set of initial curves is propagated toward the segmentation result under the influence of boundary and region-based segmentation forces, and being constrained by a regularity force. The changes of topology are naturally handled thanks to the level set implementation, while a coupled multi-phase propagation is adopted that increases the robustness and the convergence rate by imposing the idea of mutually exclusive propagating curves. Finally, to reduce the required computational cost and the risk of convergence to local minima, a multi-scale approach is also considered. The performance of our method is demonstrated on a variety of real images.
5A16827E	We propose a variational framework for the integration of multiple competing shape priors into level set based segmentation schemes. By optimizing an appropriate cost functional with respect to both a level set function and a (vector-valued) labeling function, we jointly generate a segmentation (by the level set function) and a recognition-driven partition of the image domain (by the labeling function) which indicates where to enforce certain shape priors. Our framework fundamentally extends previous work on shape priors in level set segmentation by directly addressing the central question of where to apply which prior. It allows for the seamless integration of numerous shape priors such that—while segmenting both multiple known and unknown objects—the level set process may selectively use specific shape knowledge for simultaneously enhancing segmentation and recognizing shape.
70B302A3	In recent years, segmentation with graph cuts is increasingly used for a variety of applications, such as photo/video editing, medical image processing, etc. One of the most common applications of graph cut segmentation is extracting an object of interest from its background. If there is any knowledge about the object shape (i.e. a shape prior), incorporating this knowledge helps to achieve a more robust segmentation. In this paper, we show how to implement a star shape prior into graph cut segmentation. This is a generic shape prior, i.e. it is not specific to any particular object, but rather applies to a wide class of objects, in particular to convex objects. Our major assumption is that the center of the star shape is known, for example, it can be provided by the user. The star shape prior has an additional important benefit - it allows an inclusion of a term in the objective function which encourages a longer object boundary. This helps to alleviate the bias of a graph cut towards shorter segmentation boundaries. In fact, we show that in many cases, with this new term we can achieve an accurate object segmentation with only a single pixel, the center of the object, provided by the user, which is rarely possible with standard graph cut interactive segmentation.
7E663631	The objective of image segmentation is to extract meaningful objects. A meaningful segmentation selects the proper threshold values to optimize a criterion using entropy. The conventional multilevel thresholding methods are efficient for bi-level thresholding. However, they are computationally expensive when extended to multilevel thresholding since they exhaustively search the optimal thresholds to optimize the objective functions. To overcome this problem, two successful swarm-intelligence-based global optimization algorithms, cuckoo search (CS) algorithm and wind driven optimization (WDO) for multilevel thresholding using Kapur’s entropy has been employed. For this purpose, best solution as fitness function is achieved through CS and WDO algorithm using Kapur’s entropy for optimal multilevel thresholding. A new approach of CS and WDO algorithm is used for selection of optimal threshold value. This algorithm is used to obtain the best solution or best fitness value from the initial random threshold values, and to evaluate the quality of a solution, correlation function is used. Experimental results have been examined on standard set of satellite images using various numbers of thresholds. The results based on Kapur’s entropy reveal that CS, ELR-CS and WDO method can be accurately and efficiently used in multilevel thresholding problem.
78AF2E9C	In this paper, a modified artificial bee colony (MABC) algorithm based satellite image segmentation using different objective function has been presented to find the optimal multilevel thresholds. Three different methods are compared with this proposed method such as ABC, particle swarm optimization (PSO) and genetic algorithm (GA) using Kapur’s, Otsu and Tsallis objective function for optimal multilevel thresholding. The experimental results demonstrate that the proposed MABC algorithm based segmentation can efficiently and accurately search multilevel thresholds, which are very close to optimal ones examined by the exhaustive search method. In MABC algorithm, an improved solution search equation is used which is based on the bee’s search only around the best solution of previous iteration to improve exploitation. In addition, to improve global convergence when generating initial population, both chaotic system and opposition-based learning method are employed. Compared to other thresholding methods, segmentation results of the proposed MABC algorithm is most promising, and the computational time is also minimized.
79E0D29E	Picture segmentation is expressed as a sequence of decision problems within the framework of a split-and-merge algorithm. First regions of an arbitrary initial segmentation are tested for uniformity and if not uniform they are subdivided into smaller regions, or set aside if their size is below a given threshold. Next regions classified as uniform are subject to a cluster analysis to identify similar types which are merged. At this point there exist reliable estimates of the parameters of the random field of each type of region and they are used to classify some of the remaining small regions. Any regions remaining after this step are considered part of a boundary ambiguity zone. The location of the boundary is estimated then by interpolation between the existing uniform regions. Experimental results on artificial picutres are also included.
7E5B25FF	A method of image segmentation was recently introduced based on defining links between pixels at adjacent levels of a “pyramid” of reduced-resolution versions of the image. This paper studies some of the problems that arise with linked-pyramid segmentation, and proposes a two-stage segmentation process that overcomes these problems.
7A19A32A	In this paper, a prototype delta-sigma ADC is implemented in a 0.18μm 2P5M CMOS process. The input signal sampling capacitors are shared with the front-end DAC capacitors. The sampling frequency is 50MHz and oversampling ratio is 24. The out-of-band peaking is deliberately set to help the stability and to allow larger input signals to be processed by the loop. This modulator achieves 78.2dB peak SNDR and 79.3dB peak SNR while consuming 1.35mW analog and 1.55mW digital power from 1.5V supplies. The major portion of the digital power (1.3mW) is consumed by an overdesigned generic clock generator to provide flexibility in testing with various sampling frequencies. The rest of the digital power (0.25mW) includes the DLL, digital counter, DWA and the comparator. The achieved minimal analog power is the direct result of the extra order of noise shaping, and the elimination of the flash ADC and the typical large capacitive loading that comes with it. The FoM is 210fJ/conversion-step and it can easily be reduced further with redesign (i.e., eliminating the wasted clock generator power).
7532C61B	This paper addresses the problem of semantic segmentation, where the possible class labels are from a predefined set. We exploit top-down guidance, i.e., the coarse localization of the objects and their class labels provided by object detectors. For each detected bounding box, figure-ground segmentation is performed and the final result is achieved by merging the figure-ground segmentations. The main idea of the proposed approach, which is presented in our preliminary work, is to reformulate the figure-ground segmentation problem as sparse reconstruction pursuing the object mask in a nonparametric manner. The latent segmentation mask should be coherent subject to sparse error caused by intra-category diversity; thus, the object mask is inferred by making use of sparse representations over the training set. To handle local spatial deformations, local patch-level masks are also considered and inferred by sparse representations over the spatially nearby patches. The sparse reconstruction coefficients and the latent mask are alternately optimized by applying the Lasso algorithm and the accelerated proximal gradient method. The proposed formulation results in a convex optimization problem; thus, the global optimal solution is achieved. In this paper, we provide theoretical analysis of the convergence and optimality. We also give an extended numerical analysis of the proposed algorithm and a comprehensive comparison with the related semantic segmentation methods on the challenging PASCAL visual object class object segmentation datasets and the Weizmann horse dataset. The experimental results demonstrate that the proposed algorithm achieves a competitive performance when compared with the state of the arts.
754B3DE9	In this paper, we present a single-path multi-bit delta-sigma analog-to-digital converter (ΔΣ ADC) architecture that uses time as a reference for performing multi-bit digital-to-analog conversion in the feedback path. The architecture uses a voltage-controlled oscillator as a multi-bit quantizer. In the feedback path of the ΔΣ ADC, the errors due to component mismatch are avoided by using a single-path for all the levels in a multi-bit digital-to-analog converter (DAC). The technique eliminates the need for feedback DAC architectures with static and dynamic component matching
7674D351	For a block fading n_t x n_r,n_t > n_r MIMO channel, we propose a precoding scheme that achieves both n_tn_rth order diversity as well as rate of n_t symbols per channel use, feeding back B(n_t-1) bits as partial channel state information to the transmitter (CSIT), where 2^B is the number of quantization states available. We establish the optimality of the uniform quantizer which achieves minimum loss in coding gain due to quantization of feedback values of our precoding scheme in comparison to the non-uniform quantization. We also lay down the guidelines for constellation sets with which our precoding scheme can achieve full diversity. We also derive the order of complexity involved in computing the precoder matrix and show that it is independent of the size of constellation sets. We compare our BER results with that of precoding schemes in the literature utilizing full CSIT as well as that with partial CSIT. We also investigate the loss in error rate performance due to imperfect channel knowledge at the receiver and present simulation results to verify our claims.
7E8AFD39	An unsupervised segmentation approach to classification of multispectral image is suggested here in Markov random field (MRF) frame work. This work generalizes the work of Sarkar et al. (2000) on gray value images for multispectral images and is extended for landuse classification. The essence of this approach is based on capturing intrinsic characters of tonal and textural regions of any multispectral image. The approach takes an initially oversegmented image and the original. multispectral image as the input and defines a MRF over region adjacency graph (RAG) of the initially segmented regions. Energy function minimization associated with the MRF is carried out by applying a multivariate statistical test. A cluster validation scheme is outlined after obtaining optimal segmentation. Quantitative evaluation of classification accuracy of test data for three illustrations are shown and compared with conventional maximum likelihood procedure. Comparison of the proposed methodology with a recent work of texture segmentation in the literature has also been provided. The findings of the proposed method are found to be encouraging.
81482B20	A simple technique has been suggested to obtain optimal segmentation based on tonal and textural characteristics of an image using the Markov random field (MRF) model. The technique takes an initially over segmented image as well as the original image as its inputs and defines an MRF over the region adjacency graph (RAG) of the initially segmented regions. A tonal-region based segmentation technique due to Kartikeyan and Sarkar (1989) has been used for initial segmentation. The energy function has been defined over the first order cliques of the MRF. The essence of this approach is primarily based on quantitative values of the second order statistics, on region characteristics and consequently deciding upon the action of merging neighboring regions using the F-statistic. The effectiveness of our approach is demonstrated with wide variety of real life examples viz., indoor, outdoor and satellite and a comparison of its output with that of a previous work in the literature has been provided.
7D9F3E81	We present a new image segmentation algorithm based on a tree-structured binary MRF model. The image is recursively segmented in smaller and smaller regions until a stopping condition, local to each region, is met. Each elementary binary segmentation is obtained as the solution of a MAP estimation problem, with the region prior modeled as an MRF. Since only binary fields are used, and thanks to the tree structure, the algorithm is quite fast, and allows one to address the cluster validation problem in a seamless way. In addition, all field parameters are estimated locally, allowing for some spatial adaptivity. To improve segmentation accuracy, a split-and-merge procedure is also developed and a spatially adaptive MRF model is used. Numerical experiments on multispectral images show that the proposed algorithm is much faster than a similar reference algorithm based on "flat" MRF models, and its performance, in terms of segmentation accuracy and map smoothness, is comparable or even superior.
7F47189C	Automatic three-dimensional (3-D) segmentation of the brain from magnetic resonance (MR) scans is a challenging problem that has received an enormous amount of attention lately. Of the techniques reported in the literature, very few are fully automatic. In this paper, we present an efficient and accurate, fully automatic 3-D segmentation procedure for brain MR scans. It has several salient features; namely, the following. 1) Instead of a single multiplicative bias field that affects all tissue intensities, separate parametric smooth models are used for the intensity of each class. 2) A brain atlas is used in conjunction with a robust registration procedure to find a nonrigid transformation that maps the standard brain to the specimen to be segmented. This transformation is then used to: segment the brain from nonbrain tissue; compute prior probabilities for each class at each voxel location and find an appropriate automatic initialization. 3) Finally, a novel algorithm is presented which is a variant of the expectation-maximization procedure, that incorporates a fast and accurate way to find optimal segmentations, given the intensity models along with the spatial coherence assumption. Experimental results with both synthetic and real data are included, as well as comparisons of the performance of our algorithm with that of other published methods.
7D92ECD1	Accurately segmenting and quantifying structures is a key issue in biomedical image analysis. The two conventional methods of image segmentation, region-based segmentation, and boundary finding, often suffer from a variety of limitations. Here the authors propose a method which endeavors to integrate the two approaches in an effort to form a unified approach that is robust to noise and poor initialization. The authors' approach uses Green's theorem to derive the boundary of a homogeneous region-classified area in the image and integrates this with a gray level gradient-based boundary finder. This combines the perceptual notions of edge/shape information with gray level homogeneity. A number of experiments were performed both on synthetic and real medical images of the brain and heart to evaluate the new approach, and it is shown that the integrated method typically performs better when compared to conventional gradient-based deformable boundary finding. Further, this method yields these improvements with little increase in computational overhead, an advantage derived from the application of the Green's theorem.
7DF6E3D8	In recent years, many image segmentation approaches have been based on Markov random fields (MRFs). The main assumption of the MRF approaches is that the class parameters are known or can be obtained from training data. In this paper the authors propose a novel method that relaxes this assumption and allows for simultaneous parameter estimation and vector image segmentation. The method is based on a tree structure (TS) algorithm which is combined with Besag's iterated conditional modes (ICM) procedure. The TS algorithm provides a mechanism for choosing initial cluster centers needed for initialization of the ICM. The authors' method has been tested on various one-dimensional (1-D) and multidimensional medical images and shows excellent performance. In this paper the authors also address the problem of cluster validation. They propose a new maximum a posteriori (MAP) criterion for determination of the number of classes and compare its performance to other approaches by computer simulations.
7CF618D4	A class of constraint-satisfaction neural networks (CSNNs) is proposed for solving the problem of medical image segmentation, which can be formulated as a constraint-satisfaction problem (CSP). A CSNN consists of a set of objects, a set of labels for each object, a collection of constraint relations linking the labels of neighboring objects, and a topological constraint describing the neighborhood relationship among various objects. Each label for a particular object indicates one possible interpretation for that object. The CSNN can be viewed as a collection of neurons that interconnect with each other. The connections and the topology of a CSNN are used to represent the constraints in a CSP. The mechanism of the neural network is to find a solution that satisfies all the constraints in order to achieve a global consistency. The final solution outlines segmented areas and simultaneously satisfies all the constraints. This technique has been applied to medical images, and the results show that the, method is a very promising approach to image segmentation,.
7DC6766A	This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.
7EB30A18	We present a probabilistic framework namely, multiscale generative models known as dynamic trees (DT), for unsupervised image segmentation and subsequent matching of segmented regions in a given set of images. Beyond these novel applications of DTs, we propose important additions for this modeling paradigm. First, we introduce a novel DT architecture, where multilayered observable data are incorporated at all scales of the model. Second, we derive a novel probabilistic inference algorithm for DTs, structured variational approximation (SVA), which explicitly accounts for the statistical dependence of node positions and model structure in the approximate posterior distribution, thereby relaxing poorly justified independence assumptions in previous work. Finally, we propose a similarity measure for matching dynamic-tree models, representing segmented image regions, across images. Our results for several data sets show that DTs are capable of capturing important component-subcomponent relationships among objects and their parts, and that DTs perform well in segmenting images into plausible pixel clusters. We demonstrate the significantly improved properties of the SVA algorithm, both in terms of substantially faster convergence rates and larger approximate posteriors for the inferred models, when compared with competing inference algorithms. Furthermore, results on unsupervised object recognition demonstrate the viability of the proposed similarity measure for matching dynamic-structure statistical models.
7E4F7C77	Semantic segmentation and object detection are nowadays dominated by methods operating on regions obtained as a result of a bottom-up grouping process (segmentation) but use feature extractors developed for recognition on fixed-form (e.g. rectangular) patches, with full images as a special case. This is most likely suboptimal. In this paper we focus on feature extraction and description over free-form regions and study the relationship with their fixed-form counterparts. Our main contributions are novel pooling techniques that capture the second-order statistics of local descriptors inside such free-form regions. We introduce second-order generalizations of average and max-pooling that together with appropriate non-linearities, derived from the mathematical structure of their embedding space, lead to state-of-the-art recognition performance in semantic segmentation experiments without any type of local feature coding. In contrast, we show that codebook-based local feature coding is more important when feature extraction is constrained to operate over regions that include both foreground and large portions of the background, as typical in image classification settings, whereas for high-accuracy localization setups, second-order pooling over free-form regions produces results superior to those of the winning systems in the contemporary semantic segmentation challenges, with models that are much faster in both training and testing.
7D2A29EF	Parametric image segmentation consists of finding a label field that defines a partition of an image into a set of nonoverlapping regions and the parameters of the models that describe the variation of some property within each region. A new Bayesian formulation for the solution of this problem is presented, based on the key idea of using a doubly stochastic prior model for the label field, which allows one to find exact optimal estimators for both this field and the model parameters by the minimization of a differentiable function. An efficient minimization algorithm and comparisons with existing methods on synthetic images are presented, as well as examples of realistic applications to the segmentation of Magnetic Resonance volumes and to motion segmentation.
8220B77E	Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.
7D1D8D7A	The solution of the segmentation problem requires a mechanism for partitioning the image array into low-level entities based on a model of the underlying image structure. A piecewise-smooth surface model for image data that possesses surface coherence properties is used to develop an algorithm that simultaneously segments a large class of images into regions of arbitrary shape and approximates image data with bivariate functions so that it is possible to compute a complete, noiseless image reconstruction based on the extracted functions and regions. Surface curvature sign labeling provides an initial coarse image segmentation, which is refined by an iterative region-growing method based on variable-order surface fitting. Experimental results show the algorithm's performance on six range images and three intensity images.
80B5F3F2	We present an algorithm that integrates multiple region segmentation maps and edge maps. It operates independently of image sources and specific region-segmentation or edge-detection techniques. User-specified weights and the arbitrary mixing of region/edge maps are allowed. The integration algorithm enables multiple edge detection/region segmentation modules to work in parallel as front ends. The solution procedure consists of three steps. A maximum likelihood estimator provides initial solutions to the positions of edge pixels from various inputs. An iterative procedure using only local information (without edge tracing) then minimizes the contour curvature. Finally, regions are merged to guarantee that each region is large and compact. The channel-resolution width controls the spatial scope of the initial estimation and contour smoothing to facilitate multiscale processing. Experimental results are demonstrated using data from different types of sensors and processing techniques. The results show an improvement over individual inputs and a strong resemblance to human-generated segmentation.
7FB1ABBD	Suppose a set of arbitrary (unlabeled) images contains frequent occurrences of 2D objects from an unknown category. This paper is aimed at simultaneously solving the following related problems: 1) unsupervised identification of photometric, geometric, and topological properties of multiscale regions comprising instances of the 2D category, 2) learning a region-based structural model of the category in terms of these properties, and 3) detection, recognition, and segmentation of objects from the category in new images. To this end, each image is represented by a tree that captures a multiscale image segmentation. The trees are matched to extract the maximally matching subtrees across the set, which are taken as instances of the target category. The extracted subtrees are then fused into a tree union that represents the canonical category model. Detection, recognition, and segmentation of objects from the learned category are achieved simultaneously by finding matches of the category model with the segmentation tree of a new image. Experimental validation on benchmark data sets demonstrates the robustness and high accuracy of the learned category models when only a few training examples are used for learning without any human supervision.
7FD6645C	Among the existing texture segmentation methods, those relying on Markov random fields have retained substantial interest and have proved to be very efficient in supervised mode. The use of Markov random fields in unsupervised mode is, however, hampered by the parameter estimation problem. The recent solutions proposed to overcome this difficulty rely on assumptions about the shapes of the textured regions or about the number of textures in the input image that may not be satisfied in practice. In this paper, an evolutionary approach, selectionist relaxation, is proposed as a solution to the problem of segmenting Markov random field modeled textures in unsupervised mode. In selectionist relaxation, the computation is distributed among a population of units that iteratively evolves according to simple and local evolutionary rules. A unit is an association between a label and a texture parameter vector. The units whose likelihood is high are allowed to spread over the image and to replace the units that receive lower support from the data. Consequently, some labels are growing while others are eliminated. Starting with an initial random population, this evolutionary process eventually results in a stable labelization of the image, which is taken as the segmentation. In this work, the generalized Ising model is used to represent textured data. Because of the awkward nature of the partition function in this model, a high-temperature approximation is introduced to allow the evaluation of unit likelihoods. Experimental results on images containing various synthetic and natural textures are reported.
795EDA89	The task of segmenting an image and that of estimating properties of image regions may be highly interdependent. The goal of segmentation is to partition the image into regions with more or less homogeneous properties; but the processes which estimate these properties should be confined within individual regions. A cooperative, iterative approach to segmentation and property estimation is defined; the results of each process at a given iteration are used to adjust the other process at the next iteration. A linked pyramid structure provides a framework for this process iteration. This hierarchical structure ensures rapid convergence even with strictly local communication between pyramid nodes.
7B2EC90E	The ISODATA clustering algorithm is investigated for analysis of a one-dimensional feature space. The algorithm is an iterative thresholding scheme for numbers of classes restricted to two, so that it always terminates in one-dimensional two-class case. For a number of classes larger than two, ISODATA can be applied to requantize images into a specified number of gray levels; in this case, reasonable data compression can be achieved without significant image distortion.
77D078A3	Image segmentation is a subfield of image analysis whose potential for applications has stimulated both practical and theoretical research, particularly in the last decade. A selection of papers is reviewed to give an idea of the main lines of attack that are being pursued at present.
7E7DADAE	A statistical model to segment clinical magnetic resonance (MR) images in the presence of noise and intensity inhomogeneities is proposed. Inhomogeneities are considered to be multiplicative low-frequency variations of intensities that are due to the anomalies of the magnetic fields of the scanners. The measurements are modeled as a Gaussian mixture where inhomogeneities present a bias field in the distributions. The piecewise contiguous nature of the segmentation is modeled by a Markov random field (MRF). A greedy algorithm based on the iterative conditional modes (ICM) algorithm is used to find an optimal segmentation while estimating the model parameters. Results with simulated and hand-segmented images are presented to compare performance of the algorithm with other statistical methods. Segmentation results with MR head scans acquired from four different clinical scanners are presented.
80D36529	Image contour detection is fundamental to many image analysis applications, including image segmentation, object recognition and classification. However, highly accurate image contour detection algorithms are also very computationally intensive, which limits their applicability, even for offline batch processing. In this work, we examine efficient parallel algorithms for performing image contour detection, with particular attention paid to local image analysis as well as the generalized eigensolver used in Normalized Cuts. Combining these algorithms into a contour detector, along with careful implementation on highly parallel, commodity processors from Nvidia, our contour detector provides uncompromised contour accuracy, with an F-metric of 0.70 on the Berkeley Segmentation Dataset. Runtime is reduced from 4 minutes to 1.8 seconds. The efficiency gains we realize enable high-quality image contour detection on much larger images than previously practical, and the algorithms we propose are applicable to several image segmentation approaches. Efficient, scalable, yet highly accurate image contour detection will facilitate increased performance in many computer vision applications.
783112DD	The paper presents a novel variational method for supervised texture segmentation. The textured feature space is generated by filtering the given textured images using isotropic and anisotropic filters, and analyzing their responses as multi-component conditional probability density functions. The texture segmentation is obtained by unifying region and boundary based information as an improved Geodesic Active Contour Model. The defined objective function is minimized using a gradient-descent method where a level set approach is used to implement the obtained PDE. According to this PDE, the curve propagation towards the final solution is guided by boundary and region based segmentation forces, and is constrained by a regularity force. The level set implementation is performed using a fast front propagation algorithm where topological changes are naturally handled. The performance of our method is demonstrated on a variety of synthetic and real textured frames.
716C42F4	This paper applies shuffling to digital watermarking and data hiding. The data embedding capacity in the multimedia source generally varies significantly from one part of the source to another. Sequential embedding is very sensitive to noise which may cause synchronization problem; the common but conservative solution via partitioning an image into large segments and embedding only one bit per segment is wasteful of the data embedding capacity. This paper shows how random shuffling can be used to equalize the uneven distribution of embedding capacity. The effectiveness of random shuffling is demonstrated by analysis and experiments.
808E9FB4	In this paper, the development and application of a fast algorithm for segmentation of textured images is discussed. It is based on Markov random fields as a method of feature extraction. We present a post-processing algorithm which increases the classification accuracy of an initial pixel-by-pixel scheme. The algorithm employs a majority decision concept to counteract the misclassification caused by multiple textures in a computational window. The method is then extended to yield a high speed algorithm which combines pixel and region classification, affording large computational savings. Experiments for both synthetic and real images, yielding accurate results, are reported.
7CA2BEFD	The motion compensation is one of the most important mechanisms in the context of video coding, allowing the efficient coding of temporal information. Video coding standards have been using entropy coding for motion information which is more prone to transmission errors than fixed-length coding. In this paper, we propose a joint source-channel decoding (JSCD) scheme for robustly decoding motion vectors, enabling the traditional usage of motion compensation in noisy channel scenarios while still maintaining a good coding efficiency. At the encoder, motion vectors are simply coded using scalar quantization enabling scalable video coding setups. The decoder then tries to recover corrupted motion vectors by means of a maximum-a-posteriori (MAP) estimation. This scheme works in open-loop mode, not requiring feedback from the decoder, which is equally important for noisy channel environments. Because some of the coding complexity is brought from the encoder to the decoder, this scheme can be employed in nontraditional applications like video streaming from low power mobile peers.
7588D026	Arbitrary domains represent one of the most difficult areas for image classification algorithms to categorize effectively. Inconsistent features require a computationally expensive multipartite approach to search for possible underlying structures within datasets. This paper proposes a new approach to the problem by applying a self-developed, non-linear, multi-scale image segmentation method to identify and extract prominent regions among several visual features expressing color, texture and layout properties. Integrating this method with the Layered Self-Organizing Map has achieved a simple yet powerful multifaceted Artificial Neural Network classifier for mixed domains which has improved abstract classification precision when compared against unsegmented classification methods.
7E3786A0	We developed a binarization approach to handle a large variety of images, from scanned flatbed images to images acquired by mobile phone cameras. The binarization is targeted at creating layers of binary images for processing by OCR engines. The layers are classified spatially and by intensity and color. First textual pixels are classified by a text operator. The text kernel is then segmented by intensity/color levels and layout analysis techniques to create regions of similar text. Finally, adaptive binarization is applied to each region to obtain superior binary images. Our experimental results show the advantages of our method over local binarization methods.
5E07E2EE	Semantic understanding of environments is an important problem in robotics in general and intelligent autonomous systems in particular. In this paper, we propose a semantic segmentation algorithm which effectively fuses information from images and 3D point clouds. The proposed method incorporates information from multiple scales in an intuitive and effective manner. A late-fusion architecture is proposed to maximally leverage the training data in each modality. Finally, a pairwise Conditional Random Field (CRF) is used as a post-processing step to enforce spatial consistency in the structured prediction. The proposed algorithm is evaluated on the publicly available KITTI dataset [1] [2], augmented with additional pixel and point-wise semantic labels for building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence regions. A per-pixel accuracy of 89.3% and average class accuracy of 65.4% is achieved, well above current state-of-the-art
7E040242	In this paper, heterogeneous clutter models are used to describe polarimetric synthetic aperture radar (PolSAR) data. The KummerU distribution is introduced to model the PolSAR clutter. Then, a detailed analysis is carried out to evaluate the potential of this new multivariate distribution. It is implemented in a hierarchical maximum likelihood segmentation algorithm. The segmentation results are shown on both synthetic and high-resolution PolSAR data at the X- and L-bands. Finally, some methods are examined to determine automatically the “optimal” number of segments in the final partition.
7C1006EF	We propose a variational framework for the integration of multiple competing shape priors into level set based segmentation schemes. By optimizing an appropriate cost functional with respect to both a level set function and a (vector-valued) labeling function, we jointly generate a segmentation (by the level set function) and a recognition-driven partition of the image domain (by the labeling function) which indicates where to enforce certain shape priors. Our framework fundamentally extends previous work on shape priors in level set segmentation by directly addressing the central question of where to apply which prior. It allows for the seamless integration of numerous shape priors such that—while segmenting both multiple known and unknown objects—the level set process may selectively use specific shape knowledge for simultaneously enhancing segmentation and recognizing shape.
81167D77	We propose a new multiphase level set framework for image segmentation using the Mumford and Shah model, for piecewise constant and piecewise smooth optimal approximations. The proposed method is also a generalization of an active contour model without edges based 2-phase segmentation, developed by the authors earlier in T. Chan and L. Vese (1999. In Scale-Space'99, M. Nilsen et al. (Eds.), LNCS, vol. 1682, pp. 141–151) and T. Chan and L. Vese (2001. IEEE-IP, 10(2):266–277). The multiphase level set formulation is new and of interest on its own: by construction, it automatically avoids the problems of vacuum and overlap; it needs only log n level set functions for n phases in the piecewise constant case; it can represent boundaries with complex topologies, including triple junctions; in the piecewise smooth case, only two level set functions formally suffice to represent any partition, based on The Four-Color Theorem. Finally, we validate the proposed models by numerical results for signal and image denoising and segmentation, implemented using the Osher and Sethian level set method.
7E51F583	Combinatorial graph cut algorithms have been successfully applied to a wide range of problems in vision and graphics. This paper focusses on possibly the simplest application of graph-cuts: segmentation of objects in image data. Despite its simplicity, this application epitomizes the best features of combinatorial graph cuts methods in vision: global optima, practical efficiency, numerical robustness, ability to fuse a wide range of visual cues and constraints, unrestricted topological properties of segments, and applicability to N-D problems. Graph cuts based approaches to object extraction have also been shown to have interesting connections with earlier segmentation methods such as snakes, geodesic active contours, and level-sets. The segmentation energies optimized by graph cuts combine boundary regularization with region-based properties in the same fashion as Mumford-Shah style functionals. We present motivation and detailed technical description of the basic combinatorial optimization framework for image segmentation via s/t graph cuts. After the general concept of using binary graph cut algorithms for object segmentation was first proposed and tested in Boykov and Jolly (2001), this idea was widely studied in computer vision and graphics communities. We provide links to a large number of known extensions based on iterative parameter re-estimation and learning, multi-scale or hierarchical approaches, narrow bands, and other techniques for demanding photo, video, and medical applications.
76BE721F	In this paper, we make two contributions to the field of level set based image segmentation. Firstly, we propose shape dissimilarity measures on the space of level set functions which are analytically invariant under the action of certain transformation groups. The invariance is obtained by an intrinsic registration of the evolving level set function. In contrast to existing approaches to invariance in the level set framework, this closed-form solution removes the need to iteratively optimize explicit pose parameters. The resulting shape gradient is more accurate in that it takes into account the effect of boundary variation on the object’s pose.Secondly, based on these invariant shape dissimilarity measures, we propose a statistical shape prior which allows to accurately encode multiple fairly distinct training shapes. This prior constitutes an extension of kernel density estimators to the level set domain. In contrast to the commonly employed Gaussian distribution, such nonparametric density estimators are suited to model aribtrary distributions.We demonstrate the advantages of this multi-modal shape prior applied to the segmentation and tracking of a partially occluded walking person in a video sequence, and on the segmentation of the left ventricle in cardiac ultrasound images. We give quantitative results on segmentation accuracy and on the dependency of segmentation results on the number of training shapes.
7617B343	Building on recent progress in modeling filter response statistics of natural images we integrate a statistical model into a variational framework for image segmentation. Incorporated in a sound probabilistic distance measure the model drives level sets toward meaningful segmentations of complex textures and natural scenes. Since each region comprises two model parameters only the approach is computationally efficient and enables the application of variational segmentation to a considerably larger class of real-world images. We validate the statistical basis of our approach on thousands of natural images and demonstrate that our model outperforms recent variational segmentation methods based on second-order statistics.
7F0CAE4C	In this paper, a prototype delta-sigma ADC is implemented in a 0.18μm 2P5M CMOS process. The input signal sampling capacitors are shared with the front-end DAC capacitors. The sampling frequency is 50MHz and oversampling ratio is 24. The out-of-band peaking is deliberately set to help the stability and to allow larger input signals to be processed by the loop. This modulator achieves 78.2dB peak SNDR and 79.3dB peak SNR while consuming 1.35mW analog and 1.55mW digital power from 1.5V supplies. The major portion of the digital power (1.3mW) is consumed by an overdesigned generic clock generator to provide flexibility in testing with various sampling frequencies. The rest of the digital power (0.25mW) includes the DLL, digital counter, DWA and the comparator. The achieved minimal analog power is the direct result of the extra order of noise shaping, and the elimination of the flash ADC and the typical large capacitive loading that comes with it. The FoM is 210fJ/conversion-step and it can easily be reduced further with redesign (i.e., eliminating the wasted clock generator power).
5A8BF9CF	Computer aided diagnosis of breast cancers often relies on automatic image analysis of histopathology images. The automatic region segmentation in breast cancer is challenging due to: i) large regional variations, and ii) high computational costs of pixel-wise segmentation. Deep convolutional neural network (CNN) is proven to be an effective method for image recognition and classification. However, it is often computationally expensive. In this paper, we propose to apply a fast scanning deep convolutional neural network (fCNN) to pixel-wise region segmentation. The fCNN removes the redundant computations in the original CNN without sacrificing its performance. In our experiment it takes only 2.3 seconds to segment an image with size 1000 × 1000. The comparison experiments show that the proposed system outperforms both the LBP feature-based and texton-based pixel-wise methods.
684788E1	This paper presents a knowledge-based system to interpret laser radar (ladar) images. The objective of this research is to detect and recognize man-made objects in outdoor scenes. Our system applies themultisensor fusion approach to multiple ladar modalities to improve both segmentation and interpretation. The segmentation modules are written in C. The knowledge-based interpretation system is constructed usingKEE and Lisp. Low-level attributes of image segments (regions) are computed by the segmentation modules and then converted to theKEE format. The interpretation system applies forward chaining in a bottom-up fashion to derive object-level interpretation from input generated by low-level processing and segmentation modules. The interpretation modules detect man-made objects from the background using low-level attributes. Segments are grouped into objects, which are then classified into predefined categories (vehicles, ground, etc.). The efficiency of the interpretation system is enhanced by transferring nonsymbolic processing tasks to a concurrent service manager (program). Experimental results using ladar data are presented.
58B34D41	 A ΔΣ ADC using an LSB-first quantizer (LSBFQ) is proposed. LSBFQ are energy-efficient ADCs for processing signals with low activity, and have been proposed as standalone quantizers for sensor and biomedical applications. Since the quantizers in highly oversampled multibit ΔΣ ADC process signals with low average activity, the LSBFQ is an ideal quantizer solution. In order to avoid clocking the LSBFQ at a rate much faster than the rest of the ΔΣ ADC, it is proposed that the quantizer be provided a fixed number of comparison cycles, then be interrupted regardless of whether the conversion has fully completed. This is acceptable because for high oversampling ratios (OSR), the average code change is small and an N-bit conversion can usually be completed in fewer than N comparison cycles. In the rare cases that the quantizer is interrupted early, it injects slightly more quantization noise into the loop filter, which is filtered and shaped with little impact on signal-to-noise and distortion ratio (SNDR). Simulation results demonstrate that for high OSR, an LSBFQ achieves higher resolution and lower capacitor switching energy than a conventional SAR ADC using the same number of comparator bitcycles.
76854C77	Applying computer vision technology to IR (Infra-Red) images for UAV (Unmanned Aerial Vehicle) applications is difficult due to its characteristics which differ from common image processing. By combining visual categorization with low level IR image processing, this paper presents a framework for automatic labeling of IR images in probabilistic manner. We extract the features which contain temperature, texture and orientation information from the IR image, model visual categories by the distribution of features in terms of an extended visual vocabulary, and categorize IR image segments probabilistically. The proposed framework is demonstrated in experiments with high labeling accuracy, for near IR images of urban terrain taken from 100 feet altitude.
77F59E87	An ultrasonic diagnostic imaging apparatus and method are provided for segmenting signals from nonlinear targets such as microbubble contrast agents. Received echo signals are separated into their constituent linear and nonlinear components by a Doppler filter using pulse inversion separation. A threshold level is derived from an estimate made of the contributions to the echo signal from linear scattering and noise. Echo signals which exceed the threshold level are segmented as nonlinear (microbubble-originating) signals and displayed as such, whereas signals not exceeding the threshold are suppressed in the image display.
7A17AE87	This paper is about the development of the image segmentation algorithm for the industrial measurement system. Specifically, the problem of segmentation of textile yarn images is considered. The algorithm developed for yarn hairiness analyzer is introduced. It aims at extracting single fibers protruding from the yarn core. The algorithm is a region growing-based approach where the growth of the region is guided and constrained by the coherence enhancing diffusion filter. Results of the proposed method are presented and compared with the results provided by the traditional clustering approaches and recent, well-established segmentation methods. The comparison proves that the proposed segmentation algorithm provides high quality results and significantly outperforms other methods in number of fibers extracted from the background.
7B781C61	Image segmentation is a fundamental problem in computer vision. Despite many years of research, general purpose image segmentation is still a very challenging task because segmentation is inherently ill-posed. Among different segmentation schemes, graph theoretical ones have several good features in practical applications. It explicitly organizes the image elements into mathematically sound structures, and makes the formulation of the problem more flexible and the computation more efficient. In this paper, we conduct a systematic survey of graph theoretical methods for image segmentation, where the problem is modeled in terms of partitioning a graph into several sub-graphs such that each of them represents a meaningful object of interest in the image. These methods are categorized into five classes under a uniform notation: the minimal spanning tree based methods, graph cut based methods with cost functions, graph cut based methods on Markov random field models, the shortest path based methods and the other methods that do not belong to any of these classes. We present motivations and detailed technical descriptions for each category of methods. The quantitative evaluation is carried by using five indices – Probabilistic Rand (PR) index, Normalized Probabilistic Rand (NPR) index, Variation of Information (VI), Global Consistency Error (GCE) and Boundary Displacement Error (BDE) – on some representative automatic and interactive segmentation methods.
793A15A9	For the past decade, many image segmentation techniques have been proposed. These segmentation techniques can be categorized into three classes, (1) characteristic feature thresholding or clustering, (2) edge detection, and (3) region extraction. This survey summarizes some of these techniques. In the area of biomedical image segmentation, most proposed techniques fall into the categories of characteristic feature thresholding or clustering and edge detection.
7B97D449	In this work, we study ladar images of man-made objects in outdoor scenes. Our objective is to separate man-made objects from background. We explore ways to segment images from different modalities of ladar and integrate results to improve segmentation. We use planar surface fitting to segment the range image. The background usually cannot be fit into planar segments while man-made objects usually yield planar segments. The intensity image is segmented by using image statistics. Both surface fitting and statistical methods can be applied to the velocity component for segmentation. We combine these segmentation maps to generate a composite segmentation map. The final result shows strong resemblance to manual segmentation. These results can be fed into a cuing system for further processing, or into a higher level, knowledge-based system for scene interpretation.
76070AB6	Image segmentation is a process to divide an image into segments with uniform and homogeneous attributes such as graytone or texture. An image segmentation problem can be casted as a Constraint Satisfaction Problem (CSP) by interpreting the process as one of assigning labels to pixels subject to certain spatial constraints. A class of Constraint Satisfaction Neural Networks (CSNNs), different from the conventional algorithms, is proposed for image segmentation. In the network, each neuron represents one possible label of an object in a CSP and the interconnections between the neurons constitutes the constraints. In the context of image segmentation, each pixel in an n × n image can be considered as an object, i.e. there are n2 objects in the CSP. Suppose that each object is to be assigned one of m labels. Then, the CSNN consists of n × n × m neurons which can be conceived as a three-dimensional (3D) array. The connections and the topology of the CSNN are used to represent the constraints in a CSP. The initial condition for this network is set up by Kohonen's self-organizing feature map. The mechanism of the CSNN is to find a solution that satisfies all the constraints in order to achieve a global consistency. The final solution outlines segmented areas and simultaneously satisfies the given constraints. From our extensive experiments, the results show that this CSNN method is a very promising approach for image segmentation. Due to its network structure, it lends itself admirably to parallel implementation and is potentially faster than conventional image segmentation algorithms.
75958369	This paper presents an iterated region merging-based graph cuts algorithm which is a novel extension of the standard graph cuts algorithm. Graph cuts addresses segmentation in an optimization framework and finds a globally optimal solution to a wide class of energy functions. However, the extraction of objects in a complex background often requires a lot of user interaction. The proposed algorithm starts from the user labeled sub-graph and works iteratively to label the surrounding un-segmented regions. In each iteration, only the local neighboring regions to the labeled regions are involved in the optimization so that much interference from the far unknown regions can be significantly reduced. Meanwhile, the data models of the object and background are updated iteratively based on high confident labeled regions. The sub-graph requires less user guidance for segmentation and thus better results can be obtained under the same amount of user interaction. Experiments on benchmark datasets validated that our method yields much better segmentation results than the standard graph cuts and the Grabcut methods in either qualitative or quantitative evaluation.
763D12D3	This paper reviews recent developments in the use of iterative (or “relaxation”) methods in image analysis. Applications of these methods include histogram modification, noise cleaning, edge and curve detection, thinning, angle detection, template matching, and region labelling. These applications are briefly described, and references are given to papers and reports in which more detailed discussions and examples can be found.
766406D1	This paper describes some attempts to segment textured black and white images by detecting clusters of local feature values and partitioning the feature space so as to separate these clusters. This approach often works well for multispectral images, using a point's spectral signature as its feature vector. The results for grayscale images, using vectors of local feature values, are not quite as good. Factors affecting the performance of this approach are discussed, and methods of improving the results are suggested.
763D50E9	A new methodological approach to digital image processing applied to the particular case of gray-level image segmentation is introduced. The method is based on a modified and simplified version of classifier systems. The labeling function is implemented as a spatially structured set of binary-coded production rules. The labeling is iteratively modified using a distributed genetic algorithm. Results are presented which illustrate both the mechanisms underlying the functioning of the method and its performance on natural images. The relationships between this approach and other related techniques are discussed and it is shown that it compares favorably with these.
75EF76FB	This paper describes a procedure for segmenting imagery using digital methods and is based on a mathematical-pattern recognition model. The technique does not require training prototypes but operates in an "unsupervised" mode. The features most useful for the given image to be segmented are retained by the algorithm without human interaction, by rejecting those attributes which do not contribute to homogeneous clustering in N-dimensional vector space. The basic procedure is a K-means clustering algorithm which converges to a local minimum in the average squared intercluster distance for a specified number of clusters. The algorithm iterates on the number of clusters, evaluating the clustering based on a parameter of clustering quality. The parameter proposed is a product of between and within cluster scatter measures, which achieves a maximum value that is postulated to represent an intrinsic number of clusters in the data. At this value, feature rejection is implemented via a Bhattacharyya measure to make the image segments more homogeneous (thereby removing "noisy" features); and reclustering is performed. The resulting parameter of clustering fidelity is maximized with segmented imagery resulting in psychovisually pleasing and culturally logical image segments.
80DD9E38	Traditional transition region extraction methods depend much on the clip limits Llow and Lhigh. In which methods Llow and Lhigh can not often be obtained correctly from real images, which will result in incorrect extraction of transition region and finally bad quality of segmentation. A novel gradient threshold-based transition region extraction method (GT-TREM) is presented. Transition regions can steadily be extracted by GT-TREM. Experimental results demonstrated the robustness and effectiveness of the algorithm.
811A0FC5	In this paper we study the implementation issues of ultrawideband orthogonal frequency division multiplexing (UWB-OFDM) communication systems. Like narrowband OFDM, it is desirable to accomplish all modulation and demodulation process digitally in the base band. Designing such a transmitter and receiver for UWB-OFDM signal requires very fast and high-resolution digital-to-analog (D/A) and analog-to-digital (A/D) converters that operate on a very large frequency band. A modified version of sigma-delta modulator, which we call an N-tone sigma-delta, can be used for this purpose. This new structure introduces N zeros at N properly selected frequencies in the quantization noise spectrum and can be used anytime there are gaps in the spectrum of the transmitted signal. A digital transmitter and receiver for UWB-OFDM signal is proposed using this structure and its performance is studied in multipath fading channel.