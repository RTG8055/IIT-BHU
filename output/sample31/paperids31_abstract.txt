80B654FF	The main goal to extract knowledge in database is to help the user to give semantics of data and to optimize the information research. Unfortunately, this fundamental constraint is not taken into account by almost all the approaches for knowledge discovery. Indeed, these approaches generate a big number of rules that are not easily assimilated by the human brain. In this paper, we propose a new approach for Knowledge Discovery in Databases through the fusion of conceptual clustering, fuzzy logic, and formal concept analysis. While basing on the hierarchical structure offered by the lattices, we proceed to discover the Knowledge in a hierarchical way. Thus, according to the degree of detail required by the user, this approach proposes a level of knowledge and different views of this knowledge, so the user can easily exploit all knowledge generated. Moreover, this solution is extensible, the user is able to choose the fuzzy method of classification according to the domain of his data and his needs.
2D376D32	Existing closed sequential pattern mining generates a more compact yet complete resulting set compared with general sequential pattern mining. However, conventional closed sequential pattern mining algorithms pose a great challenge at spawning a large number of inefficient and redundant patterns, especially when using low support thresholds or pattern-enriched databases. Driven by wide applications of sequential patterns with contiguous constraint, we propose CCSpan (Closed Contiguous Sequential pattern mining), an efficient algorithm for mining closed contiguous sequential patterns, which contributes to a much more compact pattern set but with the same information w.r.t. closed sequential patterns. Moreover, with the shorter feature of patterns, the closed contiguous sequential patterns are preferred for feature selection and sequence classification based on the Minimum Description Length principle. CCSpan adopts a novel snippet-growth paradigm to generate a series of snippets as candidates, each of which is attached with a set of item(s) that precisely record the pattern’s occurrences in the database, and CCSpan leverages three pruning techniques to improve the computational efficiency significantly. Our experiments based on both sparse and dense datasets demonstrated that CCSpan is efficient and scalable in terms of both database size and support threshold.
79C4E44B	Knowledge discovery related theories and technologies have been applied to all kind of databases recently in growing numbers due to their abilities in converting raw data into useful knowledge for operation management, decision making and in-depth analysis and reporting. The main purpose and objective of this study was to establish a knowledge discovery model using data warehouse technique to facilitate data gathering and in-depth analysis and news reporting. This study focused on the examination data collected by the Ministry of Examination (MOEX) in charge of Taiwan's certificate examinations as the material source used in this report. The main axis of the study was based on the literature of in-depth reporting applied to the MOEX Examinations data, especially those of Professional and Technical Personnel Examinations, combined with theories and related studies of Data Mining. One of the Data Mining techniques, the Associate Rule, was carried out to explore the MOEX Data Warehouse (MOEXDW) to verify the validity of the model for Knowledge Discovery in Database (KDD). This study arrived at two important findings 1). Changes in technical categories for Professional and Technical Personnel Examinations sponsored by MOEX were numerous and frequent as Taiwan's industries evolved from 1950 to 1991; 2). Technical Categories for Professional and Technical Personnel Examinations sponsored by MOEX remained unchanged from 1992 to present. Thus, this study prompted the following suggestions for MOEX: The Technical Categories for Professional and Technical Personnel Examination should be reviewed and adjusted to cope with the rapid evolutions of various industries in Taiwan. Furthermore, various education sectors should also properly review and adjust their respective curriculums to meet the industrial trends and requirements in their technical categories. These findings indicated the Knowledge Discovery in Data Warehouse can be a viable method in support of high quality in-depth analysis and significantly improve the quality and accuracies of a special in-depth report.
81060968	This paper studied knowledge discovery in high dimensional data and temporal data based on parallel computing and GIS technology, and provided the parallel spatial-temporal knowledge discovery model which including model of natural clustering in parallel, model of querying in parallel, model of relation computing in parallel, model of spatial-temporal analysis in parallel. The model can support parallel spatial-temporal knowledge discovery which is a very urgent goal of the temporal GIS or the parallel GIS.
7A657229	The large size and the dynamic nature of the Web highlight the need for continuous support and updating of Web based information retrieval systems. Crawlers facilitate the process by following the hyperlinks in Web pages to automatically download a partial snapshot of the Web. This paper describes some details about the architecture of a fully implemented a multiagent Web search system I-Spider for the Internet. Its architecture is based on autonomous software agents and the paper is focused on the communication among them. The overall system architecture is based on a multi-agent paradigm. Agents collaborate together HTML pages from the World Wide Web and treat them in order to be able to retrieve those pages from subsequent users' queries. Crawling Agent collaboration is required in order to decide the URLs that should be first retrieved. Subsequent page treatment consists on first filtering the pages so that HTML format is transformed into XML and second indexing them so that information retrieval can be performed online
7B02A7D4	Web usage mining is the application of data mining techniques to discover usage patterns from Web data, in order to understand and better serve the needs of Web-based applications. Web usage mining consists of three phases, namely preprocessing, pattern discovery, and pattern analysis. This paper describes each of these phases in detail. Given its application potential, Web usage mining has seen a rapid increase in interest, from both the research and practice communities. This paper provides a detailed taxonomy of the work in this area, including research efforts as well as commercial offerings. An up-to-date survey of the existing work is also provided. Finally, a brief overview of the WebSIFT system as an example of a prototypical Web usage mining system is given.
790FF0D6	Domain experts should provide Intelligent Tutoring Systems (ITS) with relevant domain knowledge that enable it to guide the learner during problem-solving learning activities. However, for ill-defined domains this knowledge is hard to define explicitly. Our hypothesis is that knowledge discovery (KD) techniques can be used to extract problem-solving task models from the recorded usage of expert, intermediate and novice learners. This paper proposes a procedural-knowledge acquisition framework based on a combination of sequential pattern mining and association rules discovery techniques. The framework has been implemented and is used to discover new meta-knowledge and rules in a given domain which then extend domain knowledge and serve as problem space, allowing the Intelligent Tutoring System to guide learners in problem-solving situations. Preliminary experiments have been conducted using the framework as an alternative to a path-planning problem solver in CanadarmTutor.
812C5464	Advances in digital imaging modalities as well as other diagnosis and therapeutic techniques have generated a massive amount of diverse data for clinical research. The purpose of this study is to investigate and implement a new intuitive and space-conscious visualization framework, called DBMap, to facilitate efficient multidimensional data visualization and knowledge discovery against the large-scale data warehouses of integrated image and nonimage data. The DBMap framework is built upon the TreeMap concept. TreeMap is a space constrained graphical representation of large hierarchical data sets, mapped to a matrix of rectangles, whose size and color represent interested database fields. It allows the display of a large amount of numerical and categorical information in limited real estate of the computer screen with an intuitive user interface. DBMap has been implemented and integrated into a large brain research data warehouse to support neurologic and neuroradiologic research at the University of California, San Francisco Medical Center. For imaging specialists and clinical researchers, this novel DBMap framework facilitates another way to better explore and classify the hidden knowledge embedded in medical image data warehouses. 
78871A73	The discipline of Web Usage Mining has grown rapidly in the past few years, despite the crash of the e-commerce boom of the late 1990s. Web Usage Mining is the application of data mining techniques to Web clickstream data in order to extract usage patterns. Yet, with all of the resources put into the problem, claims of success have been limited and are often tied to specific Web site properties that are not found in general. One reason for the limited success has been a component of Web Usage Mining that is often overlooked---the need to understand the content and structure of a Web site. The processing and quantification of a Web sites content and structure for all but completely static and single frame Web sites is arguably one of the most difficult tasks to automate in the Web Usage Mining process. This article shows that, not only is the Web Usage Mining process enhanced by content and structure, it cannot be completed without it. The results of experiments run on data from a large e-commerce site are presented to show that proper preprocessing cannot be completed without the use of Web site content and structure, and that the effectiveness of pattern analysis is greatly enhanced.
5C3CF326	Knowledge frontier discovery is a novel technique for identifying interesting subpopulations of a dataset with respect to classification performance. A knowledge frontier is a collection of meaningful groups where any sub-partition with significantly different predictive accuracy is not meaningful. This research introduces knowledge frontiers and knowledge frontier discovery. The first knowledge frontier discovery algorithm, COBWEB-KFD, is also described in detail. Knowledge frontier discovery extends clustering and subgroup discovery to provide information that those techniques cannot.
5A808252	Ontologies allow us to represent knowledge and data in implicit and explicit ways. Implicit knowledge can be derived by means of several deductive logic-based processes. This paper introduces a new way for extracting implicit knowledge from ontologies by means of a link analysis of the T-box of the ontology integrated with a data mining step on the A-box. The implicit knowledge extracted is in the form of "Influence Rules" i.e. rules structured as: if property p_1 of concept c_1 has value v_1, then property p_2 of concept c_2 has value v_2 with probability π. The technique is completely general and applicable to whatever domain. The Influence Rules can be used to integrate existing knowledge or to support any other data mining process. A case study about an ontology that describes intrusion detection is used to illustrate how the method works.
7DB8F2F8	One of the impo rtant prob lems in data mining is the evaluation of subjective interestingness of the discovered rules. Past researchh as found that in many real-life applications it is easy to generate a large number of rules from the database, but most of the rules are not useful or interesting to the user. Due to the large number of rules, it is difficult for the user to analyze them manually in order to identify those interesting ones. Whether a rule is of interest to a user depends on his/her existing knowledge of the domain, and his/her interests. In this paper, we propose a technique that analyzes the discovered rules against a specific type of existing knowledge, which we call general impressions , to help the user identify interesting rules. We first propose a representation language to allow general impressions to be specified. We then present some algorithms to analyze the discovered classification rules against a set of general impressions. The results of the analysis tell us which rules conform to the general impressions and which rules are unexpected. Unexpected rules are by definition interesting.
7802FC5C	In this paper, we proposed an efficient algorithm, called PCP-Miner (Pointset Closed Pattern Miner), for mining frequent closed patterns from a pointset database, where a pointset contains a set of points. Our proposed algorithm consists of two phases. First, we find all frequent patterns of length two in the database. Second, for each pattern found in the first phase, we recursively generate frequent closed patterns by a frequent pattern tree in a depth-first search manner. Since the PCP-Miner does not generate unnecessary candidates, it is more efficient and scalable than the modified Apriori, SASMiner and MaxGeo. The experimental results show that the PCP-Miner algorithm outperforms the comparing algorithms by more than one order of magnitude.
813BA016	An efficient computer-aided-design-oriented large-signal microwave model for silicon MOSFETs is presented based on the well-founded small-signal equivalent circuit including self-heating effect and charge conservation condition. The proposed new single continuously differentiable empirical equations for drain current and gate capacitance are simple and quite accurate. The model parameters in the equations are constructed in such a way that they can be easily and straightforwardly extracted from measured data. The temperature effect is predicted by simply adopting the linear temperature-dependent model parameters for threshold voltage, saturation current, capacitance, and series resistances. The presented model is a good compromise between the simplicity of numerical calculations and the accuracy of final results that is desired by circuit designers in nonlinear circuit simulation.
77DCB86E	Class association rules (CARs) are basically used to build a classification model for prediction; they can also be used to describe correlations between itemsets and class labels. The latter is very popular in mining medical data. For example, epidemiologists often consider rules which indicate the relations between risk factors (itemsets) and HIV test results (class labels). However, in the real world, end users are often interested in a subset of class association rules. Particularly, they may consider only rules which contain at least one itemset from a user-defined set of itemsets in the rule antecedent. For example, when classifying which populations are at high risk for HIV infection, epidemiologists often concentrate on rules that include demographic information such as sex, age, and marital status in rule antecedents. Two naive strategies are to solve this problem by applying the itemset constraints into the pre-processing or post-processing step. However, such approaches are time-intensive. This paper thus proposes an efficient method for integrating the constraints into the class association rule mining process. The experimental results show that the proposed algorithm outperforms two basic approaches in the mining time and the memory consumption. The practical benefits of our method are demonstrated by a real-life application in the HIV/AIDS domain.
80F5E4C0	Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.
7AACDAF1	In the past few years there has been increased interest in using data-mining techniques to extract interesting patterns from time series data generated by sensors monitoring temporally varying phenomenon. Most work has assumed that raw data is somehow processed to generate a sequence of events, which is then mined for interesting episodes. In some cases the rule for determining when a sensor reading should generate an event is well known. However, if the phenomenon is ill-understood, stating such a rule is difficult. Detection of events in such an environment is the focus of this paper. Consider a dynamic phenomenon whose behavior changes enough over time to be considered a qualitatively significant change. The problem we investigate is of identifying the time points at which the behavior change occurs. In the statistics literature this has been called the change-point detection problem. The standard approach has been to (a) upriori determine the number of change-points that are to be discovered, and (b) decide the function that will be used for curve fitting in the interval between successive change-points. In this paper we generalize along both these dimensions. We propose an iterative algorithm that fits a model to a time segment, and uses a likelihood criterion to determine if the segment should be partitioned further, i.e. if it contains a new change- point. In this paper we present algorithms for both the batch and incremental versions of the problem, and evaluate their behavior with synthetic and real data. Finally, we present initial results comparing the change-points detected by the batch algorithm with those detected by people using visual inspection.
77120743	Large databases are becoming increasingly common in civil infrastructure applications. Although it is relatively simple to specifically query these databases at a low level, more abstract questions like ‘How does the environment affect pavement cracking?’ are difficult to answer with traditional methods. Data mining techniques can provide a solution for learning abstract knowledge from civil infrastruc-ture databases. However, data mining needs to be performed within a systematic process to ensure correct and reproducible results. Many decisions must be made during this process, making it difficult for novice analysts to apply data mining techniques thoroughly. This paper presents an application of a knowledge discovery process to data collected for an ‘intelligent’ building. The knowledge discovery process is illustrated and explained through this case study. Additionally, we discuss the importance of this case study in the context of a research effort to develop an interactive guide for the knowledge discovery process.
80398B87	Adaptively up-sampling of point-sampled models is one of key technologies to build multi-resolution point-based surfaces. In this paper, we propose an up-sampling algorithm for point-models. Our algorithm first defines a smooth manifold patch for each point in model based on a local projecting procedure, defined by the famous moving least squares (MLS) method. A valid up-sampling region is calculated for each point via constructing local Voronoi diagram of the point with its neighbors. Then up-sample the region following a grid which constructed on the local plane and commonly controlled by the local curvature of the surface and a given sampling control function. Collecting the up-sampling points, a dense multiresolution point-model can be obtained.
787582D2	In this paper, we propose an efficient algorithm, called ICMiner (Inter-transaction Closed patterns Miner), for mining closed inter-transaction itemsets. Our proposed algorithm consists of two phases. First, we scan the database once to find the frequent items. For each frequent item found, the ICMiner converts the original transaction database into a set of domain attributes, called a dataset. Then, it enumerates closed inter-transaction itemsets using an itemset–dataset tree, called an ID-tree. By using the ID-tree and datasets to mine closed inter-transaction itemsets, the ICMiner can embed effective pruning strategies to avoid costly candidate generation and repeated support counting. The experiment results show that the proposed algorithm outperforms the EH-Apriori, FITI, ClosedPROWL, and ITP-Miner algorithms in most cases.
77CA5E6E	Case-Based Reasoning (CBR) has been successfully applied in various domains over the years. Conservative adaptation provides a revision graph when partially modifying a subpar solution in contrast to null adaptation. Applying the complete CBR methodology towards financial reasoning provides a rather new domain for CBR and many opportunities to develop and improve a system when building it from scratch. Association models are gained by a target-driven method which provides an insight into the data. Rules for a reasoning prototype can be derived from the achieved associations. In this paper, initial achievements, the current state of the work and explanations regarding the system are demonstrated. Apart of that, an outlook regarding aspects within further work is presented.
808258D2	Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management.
7BCA4F38	Sequential pattern mining is crucial to data mining domains. This paper proposes a novel data mining approach for exploring hierarchical tree structures, named tree-like patterns, representing the relationships for a pair of items in a sequence. Using tree-like patterns, the relationships for a pair of items can be identified in terms of the cause and effect. A novel technique that efficiently counts support values for tree-like patterns using a queue structure is proposed. In addition, this paper addresses an efficient scheme for determining the frequency of a tree-like pattern in a sequence using a dynamic programming approach. Each tree-like pattern embedded in a sequence is considered to have a certain valuable meaning or the degree of importance used in different applications. Two addressed formulas are applied to determine the degree of significance for a specific sequence, which denotes the degree of consecutive items in a tree-like pattern for a sequence. The larger the degree of significance a tree-like pattern has, the more the tree-like pattern is compacted in the sequence. The characteristics differentiating the explored patterns from those obtained with other schemes are discussed. A simulation analysis of the proposed data mining approach is utilized to demonstrate its efficacy. Finally, the proposed approach is designed and implemented in a data mining system integrated into a novel e-learning platform.
77C7BCC5	The author has been advancing a multi-paradigm integrated approach for solution of complicated and intractable dynamic pattern recognition problems. The focus of this keynote lecture is data mining and knowledge discovery from time-series signals obtained from complex phenomena. Novel wavelet-chaos-neural network models are presented for signal processing of brain waves as recorded by electroencephalographs (EEGs) for automated EEG-based diagnosis of neurological disorders such as epilepsy and the Alzheimer’s disease (AD). Through extensive parametric studies and information reuse and integration certain combinations of parameters from the EEG sub-bands were discovered to be effective markers for seizure detection and epilepsy diagnosis. The model can distinguish among healthy, interictal, and ictal EEGs with a high accuracy of more than 96% substantially better than practicing neurologists and epileptologists. The extension the methodology for early onset diagnosis of the AD will be delineated.
7E53066B	In some domains, such as isolating problems in computer networks and discovering stock market irregularities, there is more interest in patterns consisting of infrequent, but highly correlated items rather than patterns that occur frequently (as defined by minsup, the minimum support level). We describe the m-pattern, a new pattern that is defined in terms of minp, the minimum probability of mutual dependence of items in the pattern. We show that all infrequent m-patterns can be discovered by an efficient algorithm that makes use of: (a) a linear algorithm to qualify an m-pattern; (b) an effective technique for candidate pruning based on a necessary condition for the presence of an m-pattern; and (c) a level-wise search for m-pattern discovery (which is possible because m-patterns are downward closed). Further, we consider frequent m-patterns, which are defined in terms of both minp and minsup. Using synthetic data, we study the scalability of our algorithm. Then, we apply our algorithm to data from a production computer network both to show the m-patterns present and to contrast with frequent patterns. We show that when minp=0, our algorithm is equivalent to finding frequent patterns. However, with a larger minp, our algorithm yields a modest number of highly correlated items, which makes it possible to mine for infrequent but highly correlated itemsets. To date, many actionable m-patterns have been discovered in production systems.
5D9C2125	The paper proposes a Service-oriented Knowledge Discovery (SoKD) framework and a prototype implementation named Orange4WS. To provide the proposed framework with semantics, we are using the Knowledge Discovery Ontology which defines relationships among the ingredients of knowledge discovery scenarios. It enables to reason which algorithms can be used to produce the results required by a specified knowledge discovery task, and to query the results of the knowledge discovery tasks. In addition, the ontology can also be used for automatic annotation of manually created workflows facilitating their reuse. Thus, the proposed framework provides an approach to third generation data mining: integration of distributed, heterogeneous data and knowledge resources and software into a coherent and effective knowledge discovery process. The abilities of the prototype implementation have been demonstrated on a text mining use case featuring publicly available data repositories, specialized algorithms, and third-party data analysis tools.
5CBBA2FB	The application of a connectionist model to an image binarization method called connectionist model binarization (CMB) is discussed. CMB employs a multilayer network of a connectionist model whose input and output are a histogram and a desirable threshold for binarization, respectively. This network is trained with a back-propagation algorithm to output a threshold which gives a visually suitable binarised image against any histogram. The details of CMB are described, and its learning strategy and binarization performance are discussed.
75C37E77	Frequent pattern mining is one of main concerns in data mining tasks. In frequent pattern mining, closed frequent pattern mining and weighted frequent pattern mining are two main approaches to reduce the search space. Although many related studies have been suggested, no mining algorithm considers both paradigms. Even if closed frequent pattern mining represents exactly the same knowledge and weighted frequent pattern mining provides a way to discover more important patterns, the incorporation of closed frequent pattern mining and weight frequent pattern mining may loss information. Based on our analysis of joining orders, we propose closed weighted frequent pattern mining, and present how to discover succinct but lossless closed frequent pattern with weight constraints. To our knowledge, ours is the first work specifically to consider both constraints. An extensive performance study shows that our algorithm outperforms previous algorithms. In addition, it is efficient and scalable.
78B158A8	Web Usage Mining is that area of Web Mining which deals with the extraction of interesting knowledge from logging information produced by Web servers. In this paper we present a survey of the recent developments in this area that is receiving increasing attention from the Data Mining community.
765160C5	In the field of data mining, there have been many studies on mining frequent patterns due to its broad applications in mining association rules, correlations, sequential patterns, constraint-based frequent patterns, graph patterns, emerging patterns, and many other data mining tasks. We present a new algorithm for mining maximal weighted frequent patterns from a transactional database. Our mining paradigm prunes unimportant patterns and reduces the size of the search space. However, maintaining the anti-monotone property without loss of information should be considered, and thus our algorithm prunes weighted infrequent patterns and uses a prefix-tree with weight-descending order. In comparison, a previous algorithm, MAFIA, exponentially scales to the longest pattern length. Our algorithm outperformed MAFIA in a thorough experimental analysis on real data. In addition, our algorithm is more efficient and scalable.
810D69B8	Based on the frequent pattern mining, closed frequent pattern mining and weighted frequent pattern mining have been studied to reduce the search space and discover important patterns. In the previous definition of weighted closed patterns, supports of patterns are only considered to compute the closures of the patterns. It means that the closures of weighted frequent patterns cannot be perfectly checked. Moreover, the usefulness of weighted closed frequent patterns depends on the presence of frequent patterns that have supersets with the exactly same weighted support. However, from the errors such as noise, slight changes in items' supports or weights by them have significantly negative effects on the mining results, which may prevent us from obtaining exact and valid analysis results since the errors can break the original characteristics of items and patterns. In this paper, to solve the above problems, we propose a concept of robust weighted closed frequent pattern mining, and an approximate bound is defined on the basis of the concept, which can relax requirements for precise equality among patterns' weighted supports. Thereafter, we propose a weighted approximate closed frequent pattern mining algorithm which not only considers the two approaches but also suggests fault tolerant pattern mining in the noise constraints. To efficiently mine weighted approximate closed frequent patterns, we suggest pruning and subset checking methods which reduce search space. We also report extensive performance study to demonstrate the effectiveness, efficiency, memory usage, scalability, and quality of patterns in our algorithm. 
7BD7D76E	In adversarial settings, there are those who wish to conceal their existence, properties and activities from data analysis. This substantially changes the knowledge discovery process -- finding a model that best &#x0060;fits' the data is unhelpful because it provides adversaries with predictable ways to hide, and ways to manipulate. We survey some of the implications for algorithms and process, and suggest some open problems.
84E92DCA	As one of the variations in frequent pattern mining, erasable pattern mining discovers patterns with benefits lower than or equal to a user-specified threshold from a product database. Although traditional erasable pattern mining algorithms can perform their own mining operations on static mining environments, they are not suitable for dealing with dynamic data stream environments. In such dynamic data streams, algorithms have to process them immediately with only one database scan in order to consider characteristics of data stream mining. However, previous tree-based erasable pattern mining methods have difficulty in processing dynamic data streams because they need two or more database scans to construct their own tree structures. In addition, they do not also consider specific information of each item within a product database, but they need to conduct mining operations considering such additional information of the items in order to find more useful erasable pattern results. For this reason, in this paper, we propose a weighted erasable pattern mining algorithm suitable for sliding window-based data stream environments. The algorithm employs tree and list data structures for more efficient mining processes and solves the problems of previous erasable pattern mining approaches by using a sliding window-based stream processing technique and an item weight-based pattern pruning method. We compare performance of the proposed algorithm to state-of-the-art tree-based approaches with respect to various real and synthetic datasets. Experimental results show that our method is more efficient and scalable than the competitors in terms of runtime, memory, and pattern generation.
670E0EB6	Association rule mining often generates a huge number of rules, but a majority of them either are redundant or do not reflect the true correlation relationship among data objects. We re-examine this problem and show that two interesting measures, all-confidence (denoted as /spl alpha/) and coherence (denoted as /spl gamma/), both disclose genuine correlation relationships and can be computed efficiently. Moreover, we propose two interesting algorithms, CoMine(/spl alpha/) and CoMine(/spl gamma/), based on extensions of a pattern-growth methodology. Our performance study shows that the CoMine algorithms have high performance in comparison with their Apriori-based counterpart algorithms.
7DA8EA64	With the development of knowledge economy, knowledge resource in manufacturing enterprise has become the strategic resource. The competition in manufacturing enterprises focuses on the development and management of knowledge resource. CAPP (computer aided processing planning) system is the supportable system in process planning information for discrete manufacturing enterprise. The systematic methods of process planning knowledge discovery about professional knowledge, experience and criterion in process planning work become the key problem in CAPP system. Process planning knowledge model is analyzed oriented process planning knowledge discovery problem in CAPP system. Process planning knowledge discovery technology based on knowledge model driving is researched and is applied in CAPPFramework system (a CAPP development platform supported by 863/CIMS in China). The practical application of knowledge discovery technology in CAPPFramework is described in detail, it can automatically achieve the knowledge in process planning database by the interaction with knowledge engineer.
80800A93	Weighted frequent pattern (WFP) mining is more practical than frequent pattern mining because it can consider different semantic significance (weight) of the items. For this reason, WFP mining becomes an important research issue in data mining and knowledge discovery. However, existing algorithms cannot be applied for incremental and interactive WFP mining and also for stream data mining because they are based on a static database and require multiple database scans. In this paper, we present two novel tree structures IWFPTWA (Incremental WFP tree based on weight ascending order) and IWFPTFD (Incremental WFP tree based on frequency descending order), and two new algorithms IWFPWA and IWFPFD for incremental and interactive WFP mining using a single database scan. They are effective for incremental and interactive mining to utilize the current tree structure and to use the previous mining results when a database is updated or a minimum support threshold is changed. IWFPWA gets advantage in candidate pattern generation by obtaining the highest weighted item in the bottom of IWFPTWA. IWFPFD ensures that any non-candidate item cannot appear before candidate items in any branch of IWFPTFD and thus speeds up the prefix tree and conditional tree creation time during mining operation. IWFPTFD also achieves the highly compact incremental tree to save memory space. To our knowledge, this is the first research work to perform single-pass incremental and interactive mining for weighted frequent patterns. Extensive performance analyses show that our tree structures and algorithms are very efficient and scalable for single-pass incremental and interactive WFP mining.
7A458339	Academics and practitioners have a common interest in the continuing development of methods and computer applications that support or perform knowledge-intensive engineering tasks. Operations management dysfunctions and lost production time are problems of enormous magnitude that impact the performance and quality of industrial systems as well as their cost of production. Association rule mining is a data mining technique used to find out useful and invaluable information from huge databases. This work develops a better conceptual base for improving the application of association rule mining methods to extract knowledge on operations and information management. The emphasis of the paper is on the improvement of the operations processes. The application example details an industrial experiment in which association rule mining is used to analyze the manufacturing process of a fully integrated provider of drilling products. The study reports some new interesting results with data mining and knowledge discovery techniques applied to a drill production process. Experiment’s results on real-life data sets show that the proposed approach is useful in finding effective knowledge associated to dysfunctions causes.
7B30A114	Sequential pattern mining with constraints has been developed to improve the efficiency and effectiveness in mining process. Specifically, there are two interesting constraints for sequential pattern mining. First, some sequences are more important and others are less important. Weight constraints consider the importance of sequences and items within sequences. Second, patterns including only a few items are interesting if they have high support. Meanwhile, long patterns can be interesting although their supports are relatively small. Weight constraints and length-decreasing support constraints are two paradigms aimed at finding important sequential patterns and reducing uninteresting patterns. Although weight and length-decreasing support constraints are vital elements, it is hard to consider both constraints by using previous approaches. In this paper, we integrate weight and length-decreasing support constraints by pushing two constraints into the prefix projection growth method. For pruning techniques, we define the Weighted Smallest Valid Extension property and apply the property to our pruning methods for reducing search space. In performance test, we show that our algorithm mines important sequential patterns with length-decreasing support constraints. 
7BC102B3	In this paper, we propose an efficient algorithm, called CMP-Miner, to mine closed patterns in a time-series database where each record in the database, also called a transaction, contains multiple time-series sequences. Our proposed algorithm consists of three phases. First, we transform each time-series sequence in a transaction into a symbolic sequence. Second, we scan the transformed database to find frequent patterns of length one. Third, for each frequent pattern found in the second phase, we recursively enumerate frequent patterns by a frequent pattern tree in a depth-first search manner. During the process of enumeration, we apply several efficient pruning strategies to remove frequent but non-closed patterns. Thus, the CMP-Miner algorithm can efficiently mine the closed patterns from a time-series database. The experimental results show that our proposed algorithm outperforms the modified Apriori and BIDE algorithms.
7EA3843B	Load forecasting is a traditional research field of power system. This paper analyzed the meteorological and economic characteristics of Heilongjiang Province, and put forward the load forecasting model based on knowledge discovery. First of all, generate the preliminary prediction curve by traditional method, and extract related rules from the historical load and meteorological data, then update the preliminary prediction curve using these rules, and generate the final prediction curve. The model has been used in software system of load forecasting for Heilongjiang power grid Co, Ltd., and obtained desired results.
80F0DDB9	The explosive growth in both the volume and density of geospatial data, which has resulted in the difficulty of geospatial data mining and knowledge discovery, emphasizes the need for advancing the query, analysis and computation capacity of geospatial database. In addition, distributing applications based on internet requires geospatial database and data model to accomplish the perpetually increasing demands of the query, computation and analysis distributing. Aimed at the above-mentioned issues, a new geospatial data model, X3DORGDM(X3D-based Oriented-Relation Geospatial Data Model) is designed according to the integration among the improvement of database theory, the development of computational geometry algorithms and the deployment of next generation data format of Web visualization, X3D(eXensible 3D). Through a quantitative analysis on X3DORGDM, the result shows the determinants of its spatial computing and the performance of its topological computation. X3DORGDM, as the core of spatial database, will facilitate spatial data mining and knowledge discovery, due to the robustness of its architecture, the flexibility of Structured Query Language(SQL) and the extensibility of its computational algorithm library.
7F78735A	ISO 9126 promotes a three-level model of quality (factors, criteria, and metrics) which allows one to assess quality at the top level of factors and criteria. However, it is difficult to use this model as a tool to increase software quality. In the Squale model, we add practices as an intermediate level between metrics and criteria. Practices abstract away from raw information (metrics, tool reports, audits) and provide technical guidelines to be respected. Moreover, practice marks are adjusted using formulae to suit company development habits or exigences: for example bad marks are stressed to point to places which need more attention. The Squale model has been developed and validated over the last couple of years in an industrial setting with Air France-KLM and PSA Peugeot-Citroën.
7F75B508	Several pattern discovery methods proposed in data mining literature have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that decision makers have. In this paper we propose a new method of discovery that addresses these drawbacks. In particular we propose a new method of discovering unexpected patterns that takes into consideration prior background knowledge of decision makers. This prior knowledge constitutes a set of expectations or beliefs about the problem domain. Our proposed method of discovering unexpected patterns uses these beliefs to seed the search for patterns in data that contradict the beliefs. To evaluate the practicality of our approach, we applied our algorithm to consumer purchase data from a major market research company and to web logfile data tracked at an academic Web site and present our findings in the paper.
053EE57B	This paper surveys the growing number of industrial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application.
7ED3D362	The analysis of the massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to create an intermediate level at which scientists can operate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even remotely approach humans---namely, creative data analysis, theory and hypothesis formation, and drawing insights into underlying phenomena. We give an overview of the main issues in the exploitation of scientific datasets, present five case studies where KDD tools play important and enabling roles, and conclude with future challenges for data mining and KDD techniques in science data analysis. keywords: Applications in Science, Data Analysis, overview article, large databases, automated analysis, scietific data sets, scientific discovery. 
79434C2B	Inductive representation of conditional knowledge means to complete knowledge appropriately and can be looked upon as an instance of quite a general representation problem. The crucial problem of discovering relevant conditional relationships in statistical data can also be addressed in this formal framework. The main point in this paper is to consider knowledge discovery as an operation which is inverse to inductive knowledge representation, giving rise to phrasing the inverse representation problem. This allows us to embed knowledge discovery in a theoretical framework where the vague notion of relevance can be given a precise meaning: relevance here means relevance with respect to an inductive representation method. In order to exemplify our ideas, we present an approach to compute sets of conditionals from statistical data, which are optimal with respect to the information-theoretical principle of maximum entropy.
5ACB9C10	Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.
5AA6818E	At the University of Linz a remarkable associative memory model has been developed. A neural network analogous self learning system with the capability of parallel and serial association. But, for data mining tasks it has one shortcoming. It can not reproduce how often it has seen a part of a pattern in its past — it is not able to compute frequencies. In this contribution we introduce an extension of the model with which frequencies, support and confidence are feasible. Besides, all advantages of the model could be retained. Short examples and a comparison with a common data mining tool complete the paper.
7E8AE671	Sequential pattern and inter-transaction pattern mining have long been important issues in data mining research. The former finds sequential patterns without considering the relationships between transactions in databases, while the latter finds inter-transaction patterns without considering the ordered relationships of items within each transaction. However, if we want to find patterns that cross transactions in a sequence database, called inter-sequence patterns, neither of the above models can perform the task. In this paper, we propose a new data mining model for mining frequent inter-sequence patterns. We design two algorithms, M-Apriori and EISP-Miner, to find such patterns. The former is an Apriori-like algorithm that can mine inter-sequence patterns, but it is not efficient. The latter, a new method that we propose, employs several mechanisms for mining inter-sequence patterns efficiently. Experiments show that EISP-Miner is very efficient and outperforms M-Apriori by several orders of magnitude.
60B46E6E	Web Usage Mining is the application of data mining techniques to large Web data repositories in order to extract usage patterns. As with many data mining application domains, the identification of patterns that are considered interesting is a problem that must be solved in addition to simply generating them. Aneces sary step in identifying interesting results is quantifying what is considered uninteresting in order to form a basis for comparison. Several research efforts have relied on manually generated sets of uninteresting rules. However, manual generation of a comprehensive set of evidence about beliefs for a particular domain is impractical in many cases. Generally, domain knowledge can be used to automatically create evidence for or against a set of beliefs. This paper develops a quantitative model based on support logic for determining the interestingness of discovered patterns. For Web Usage Mining, there are three types of domain information available; usage, content, and structure. This paper also describes algorithms for using these three types of information to automatically identify interesting knowledge. These algorithms have been incorporated into the Web Site Information Filter (WebSIFT) system and examples of interesting frequent itemsets automatically discovered from real Web data are presented.
5F1CC22B	 With the explosive increase in the generation and utilization of spatiotemporal data sets, many research efforts have been focused on the efficient handling of the large volume of spatiotemporal sets. With the remarkable growth of ubiquitous computing technology, mining from the huge volume of spatiotemporal data sets is regarded as a core technology which can provide real world applications with intelligence. In this paper, we propose a 3-tier knowledge discovery framework for spatiotemporal data mining. This framework provides a foundation model not only to define the problem of spatiotemporal knowledge discovery but also to represent new knowledge and its relationships. Using the proposed knowledge discovery framework, we can easily formalize spatiotemporal data mining problems. The representation model is very useful in modeling the basic elements and the relationships between the objects in spatiotemporal data sets, information and knowledge. 
7CA3DCF8	Sequential pattern mining has become an essential task with broad applications. Most sequential pattern mining algorithms use a minimum support threshold to prune the combinatorial search space. This strategy provides basic pruning; however, it cannot mine correlated sequential patterns with similar support and/or weight levels. If the minimum support is low, many spurious patterns having items with different support levels are found; if the minimum support is high, meaningful sequential patterns with low support levels may be missed. We present a new algorithm, weighted interesting sequential (WIS) pattern mining based on a pattern growth method in which new measures, sequential s-confidence and w-confidence, are suggested. Using these measures, weighted interesting sequential patterns with similar levels of support and/or weight are mined. The WIS algorithm gives a balance between the measures of support and weight, and considers correlation between items within sequential patterns. A performance analysis shows that WIS is efficient and scalable in weighted sequential pattern mining.
7DA6A13C	Periodicy detection in time series data is a challenging problem of great importance in many applications. Most previous work focused on mining synchronous periodic patterns and did not recognize the misaligned presence of a pattern due to the intervention of random noise. In this paper, we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrences may be shifted due to disturbance. Two parameters min/spl I.bar/rep and max/spl I.bar/dis are employed to specify the minimum number of repetitions that is required within each segment of nondisrupted pattern occurrences and the maximum allowed disturbance between any two successive valid segments. Upon satisfying these two requirements, the longest valid subsequence of a pattern is returned. A two-phase algorithm is devised to first generate potential periods by distance-based pruning followed by an iterative procedure to derive and validate candidate patterns and locate the longest valid subsequence. We also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency.
77EA2674	Most algorithms for frequent pattern mining use a support constraint to prune the combinatorial search space but support-based pruning is not enough. After mining datasets to obtain frequent patterns, the resulting patterns can have weak affinity. Although the minimum support can be increased, it is not effective for finding correlated patterns with increased weight and/or support affinity. Interesting measures have been proposed to detect correlated patterns but any approach does not consider both support and weight. In this paper, we present a new strategy, Weighted interesting pattern mining (WIP) in which a new measure, weight-confidence, is suggested to mine correlated patterns with the weight affinity. A weight range is used to decide weight boundaries and an h-confidence serves to identify support affinity patterns. In WIP, without additional computation cost, original h-confidence is used instead of the upper bound of h-confidence for performance improvement. WIP not only gives a balance between the two measures of weight and support, but also considers weight affinity and/or support affinity between items within patterns so more correlated patterns can be detected. To our knowledge, ours is the first work specifically to consider weight affinity between items of patterns. A comprehensive performance study shows that WIP is efficient and scalable for finding affinity patterns. Moreover, it generates fewer but more valuable patterns with the correlation. To decrease the number of thresholds, w-confidence, h-confidence and weighted support can be used selectively according to requirement of applications.
800ABE98	Sequential pattern mining is an essential research topic with broad applications which discovers the set of frequent subsequences satisfying a support threshold in a sequence database. The major problems of mining sequential patterns are that a huge set of sequential patterns are generated and the computation time is so high. Although efficient algorithms have been developed to tackle these problems, the performance of the algorithms dramatically degrades in case of mining long sequential patterns in dense databases or using low minimum supports. In addition, the algorithms may reduce the number of patterns but unimportant patterns are still found in the result patterns. It would be better if the unimportant patterns could be pruned first, resulting in fewer but important patterns after mining. In this paper, we suggest a new framework for mining weighted frequent patterns in which weight constraints are deeply pushed in sequential pattern mining. Previous sequential mining algorithms treat sequential patterns uniformly while real sequential patterns have different importance. In our approach, the weights of items are given according to the priority or importance. During the mining process, we consider not only supports but also weights of patterns. Based on the framework, we present a weighted sequential pattern mining algorithm (WSpan). To our knowledge, this is the first work to mine weighted sequential patterns. The experimental results show that WSpan detects fewer but important weighted sequential patterns in large sequence databases even with a low minimum threshold.
7A24714F	Class association rules play an important role in decision support systems and have thus been extensively studied. Recently, an efficient algorithm for mining class association rules, named CAR-Miner, has been proposed. It, however, consumes a lot of memory for storing the Obidsets (sets of object identifiers that contain itemsets) of itemsets and requires a lot of time to compute the intersection between two Obidsets, especially in the large datasets. This paper proposes an improved algorithm for mining class association rules that uses the difference between two Obidsets (d2O) to save memory usage and run time. Firstly, the d2O concept is developed. A strategy for reducing the storage space and computation time of d2O is then derived. Experimental results show that the proposed algorithm is more efficient than CAR-Miner in terms of run time and memory usage.
5EF499A6	Web usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) “aggregate usage profiles” from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users' visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.