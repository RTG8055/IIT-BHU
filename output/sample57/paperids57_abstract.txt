7DCE018B	Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case.
7AA5BE88	We  consider  the  generic  regularized  optimization  problem. Efron, Hastie, Johnstone and ibshirani [Ann.Statist.32(2004) 407–499] have shown that for the LASSO—that is, if L is squared error loss and J 1  norm of the optimal coefficient path is piecewise linear, that is,   is piecewise constant. We derive a general characterization of the properties of (loss L ,  enalty J )pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation  of   he  full  regularized  coefficient  paths.  We  investigate  the  nature of efficient path following  lgorithms which arise. We use our results to suggest robust versions of the LASSO for regression and  lassification, and to develop new, efficient algorithms for existing problems in the literature, including  ammen and van de Geer’s locally adaptive regression splines.
79AAFCF5	The application of the method of fundamental solutions to the Cauchy problem in three-dimensional isotropic linear elasticity is investigated. The resulting system of linear algebraic equations is ill-conditioned and therefore, its solution is regularized by employing the first-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both under- and equally-determined Cauchy problems in a piece-wise smooth geometry. The convergence, accuracy, and stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
815F61B2	Speech production errors characteristic of dysarthria are chiefly responsible for the low accuracy of automatic speech recognition (ASR) when used by people diagnosed with it. A person with dysarthria produces speech in a rather reduced acoustic working space, causing typical measures of speech acoustics to have values in ranges very different from those characterizing unimpaired speech. It is unlikely then that models trained on unimpaired speech will be able to adjust to this mismatch when acted on by one of the currently well-studied adaptation algorithms (which make no attempt to address this extent of mismatch in population characteristics).In this work, we propose an interpolation-based technique for obtaining a prior acoustic model from one trained on unimpaired speech, before adapting it to the dysarthric talker. The method computes a ‘background’ model of the dysarthric talker's general speech characteristics and uses it to obtain a more suitable prior model for adaptation (compared to the speaker-independent model trained on unimpaired speech). The approach is tested with a corpus of dysarthric speech acquired by our research group, on speech of sixteen talkers with varying levels of dysarthria severity (as quantified by their intelligibility). This interpolation technique is tested in conjunction with the well-known maximum a posteriori (MAP) adaptation algorithm, and yields improvements of up to 8% absolute and up to 40% relative, over the standard MAP adapted baseline.
7B1E0639	In this paper, the application of the method of fundamental solutions to the Cauchy problem associated with two-dimensional Helmholtz-type equations is investigated. The resulting system of linear algebraic equations is ill-conditioned and therefore its solution is regularized by employing the first-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both smooth and piecewise smooth geometries. The convergence and the stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
79DB33C5	Widespread use of large-vocabulary continuous speech recognition systems has recently occurred, encouraging the application of speech recognition techniques to various problems. One of the factors that adversely affect the performance of speech recognition systems is a mismatch between the acoustic properties of the speech of the system user and the acoustic model. The speech of young or middle-aged adults is generally used in constructing the acoustic model. Thus, a mismatch occurs between the model and the acoustic properties of the speech of the elderly, which may degrade the recognition rate. In this study, a large-scale elderly speech database (200 sentences ×301 subjects) is used to train the acoustic model, and the resulting elderly acoustic model is evaluated by using a large-vocabulary continuous speech recognition system. In the experiments, the word recognition rate was improved by 3 to 5% compared to the recognition results of an acoustic model trained by young or middle-aged adult speech, namely, by the JNAS speech database (150 sentences ×260 subjects, average 28.6 years). It is also verified experimentally that the recognition rate is further improved in speaker adaptation to elderly speech by making use of an acoustic model trained by elderly speech.
7E7C9F6F	Cochlear implants can provide partial restoration of hearing, even with limited spectral resolution and loss of fine temporal structure, to severely deafened individuals. Studies have indicated that background noise has significant deleterious effects on the speech recognition performance of cochlear implant patients. This study investigates the effects of noise on speech recognition using acoustic models of two cochlear implant speech processors and several predictive signal-processing-based analyses. The results of a listening test for vowel and consonant recognition in noise are presented and analyzed using the rate of phonemic feature transmission for each acoustic model. Three methods for predicting patterns of consonant and vowel confusion that are based on signal processing techniques calculating a quantitative difference between speech tokens are developed and tested using the listening test results. Results of the listening test and confusion predictions are discussed in terms of comparisons between acoustic models and confusion prediction performance.
75C78FFC	Up to now, more and more online sites have started to allow their users to build the social relationships. Take the Last.fm for example (which is a popular music-sharing site), users can not only add each other as friends, but also join online interest groups where they shall meet people with common tastes. Therefore, in this environment, users might be interested in not only receiving item recommendations (such as music), but also getting friend suggestions so they might put them in the contact list, and group recommendations that they could consider joining. To support such demanding needs, in this paper, we propose a unified framework that provides three different types of recommendation in a single system: recommending items, recommending groups and recommending friends. For each type of recommendation, we in depth investigate the contribution of fusing other two auxiliary information resources (e.g., fusing friendship and membership for recommending items, and fusing user-item preferences and friendship for recommending groups) for boosting the algorithm performance. More notably, the algorithms were developed based on the matrix factorization framework in order to achieve the ideal efficiency as well as accuracy. We performed experiments with two large-scale real-world data sets that contain users’ implicit interaction with items. The results revealed the effective fusion mechanism for each type of recommendation in such implicit data condition. Moreover, it demonstrates the respective merits of regularization model and factorization model: the factorization is more suitable for fusing bipartite data (such as membership and user-item preferences), while the regularization model better suits one mode data (like friendship). We further enhanced the friendship’s regularization by integrating the similarity measure, which was experimentally proven with positive effect.
7F8267B3	The regularization parameter choice is a fundamental problem in Learning Theory since the performance of most supervised algorithms crucially depends on the choice of one or more of such parameters. In particular a main theoretical issue regards the amount of prior knowledge needed to choose the regularization parameter in order to obtain good learning rates. In this paper we present a parameter choice strategy, called the balancing principle, to choose the regularization parameter without knowledge of the regularity of the target function. Such a choice adaptively achieves the best error rate. Our main result applies to regularization algorithms in reproducing kernel Hilbert space with the square loss, though we also study how a similar principle can be used in other situations. As a straightforward corollary we can immediately derive adaptive parameter choices for various kernel methods recently studied. Numerical experiments with the proposed parameter choice rules are also presented.
7D7B6717	In subspace identification methods, the system matrices are usually estimated by least squares, based on estimated Kalman filter state sequences and the observed inputs and outputs. For a finite number of data points, the estimated system matrix is not guaranteed to be stable, even when the true linear system is known to be stable. In this paper, stability is imposed by using regularization. The regularization term used here is the trace of a matrix which involves the dynamical system matrix and a positive (semi) definite weighting matrix. The amount of regularization can be determined from a generalized eigenvalue problem. The data augmentation method of Chui and Maciejowski (1996) is obtained by using specific choices for the weighting matrix in the regularization term.
7ECE76F2	It is well known that there is a dynamic relationship between cerebral blood flow (CBF) and cerebral blood volume (CBV). With increasing applications of functional MRI, where the blood oxygen-level-dependent signals are recorded, the understanding and accurate modeling of the hemodynamic relationship between CBF and CBV becomes increasingly important. This study presents an empirical and data-based modeling framework for model identification from CBF and CBV experimental data. It is shown that the relationship between the changes in CBF and CBV can be described using a parsimonious autoregressive with exogenous input model structure. It is observed that neither the ordinary least-squares (LS) method nor the classical total least-squares (TLS) method can produce accurate estimates from the original noisy CBF and CBV data. A regularized total least-squares (RTLS) method is thus introduced and extended to solve such an error-in-the-variables problem. Quantitative results show that the RTLS method works very well on the noisy CBF and CBV data. Finally, a combination of RTLS with a filtering method can lead to a parsimonious but very effective model that can characterize the relationship between the changes in CBF and CBV.
8118E97E	In this paper we present a novel geometric framework called geodesic active fields for general image registration. In image registration, one looks for the underlying deformation field that best maps one image onto another. This is a classic ill-posed inverse problem, which is usually solved by adding a regularization term. Here, we propose a multiplicative coupling between the registration term and the regularization term, which turns out to be equivalent to embed the deformation field in a weighted minimal surface problem. Then, the deformation field is driven by a minimization flow toward a harmonic map corresponding to the solution of the registration problem. This proposed approach for registration shares close similarities with the well-known geodesic active contours model in image segmentation, where the segmentation term (the edge detector function) is coupled with the regularization term (the length functional) via multiplication as well. As a matter of fact, our proposed geometric model is actually the exact mathematical generalization to vector fields of the weighted length problem for curves and surfaces introduced by Caselles-Kimmel-Sapiro. The energy of the deformation field is measured with the Polyakov energy weighted by a suitable image distance, borrowed from standard registration models. We investigate three different weighting functions, the squared error and the approximated absolute error for monomodal images, and the local joint entropy for multimodal images. As compared to specialized state-of-the-art methods tailored for specific applications, our geometric framework involves important contributions. Firstly, our general formulation for registration works on any parametrizable, smooth and differentiable surface, including nonflat and multiscale images. In the latter case, multiscale images are registered at all scales simultaneously, and the relations between space and scale are intrinsically being accounted for. Second, this method is, to the best of our knowledge, the first reparametrization invariant registration method introduced in the literature. Thirdly, the multiplicative coupling between the registration term, i.e. local image discrepancy, and the regularization term naturally results in a data-dependent tuning of the regularization strength. Finally, by choosing the metric on the deformation field one can freely interpolate between classic Gaussian and more interesting anisotropic, TV-like regularization.
8113BCCD	This work proposes a subspace approach that regularizes and extracts eigenfeatures from the face image. Eigenspace of the within-class scatter matrix is decomposed into three subspaces: a reliable subspace spanned mainly by the facial variation, an unstable subspace due to noise and finite number of training samples, and a null subspace. Eigenfeatures are regularized differently in these three subspaces based on an eigenspectrum model to alleviate problems of instability, overfitting, or poor generalization. This also enables the discriminant evaluation performed in the whole space. Feature extraction or dimensionality reduction occurs only at the final stage after the discriminant assessment. These efforts facilitate a discriminative and a stable low-dimensional feature representation of the face image. Experiments comparing the proposed approach with some other popular subspace methods on the FERET, ORL, AR, and GT databases show that our method consistently outperforms others.
80794964	Estimating a reliable and stable solution to many problems in signal processing and imaging is based on sparse regularizations, where the true solution is known to have a sparse representation in a given basis. Using different approaches, a large variety of regularization terms have been proposed in literature. While it seems that all of them have so much in common, a general potential function which fits most of them is still missing. In this paper, in order to propose an efficient reconstruction method based on a variational approach and involving a general regularization term (including most of the known potential functions, convex and nonconvex), we deal with i) the definition of such a general potential function, ii) the properties of the associated “proximity operator” (such as the existence of a discontinuity), and iii) the design of an approximate solution of the general “proximity operator” in a simple closed form. We also demonstrate that a special case of the resulting “proximity operator” is a set of shrinkage functions which continuously interpolate between the soft-thresholding and hard-thresholding. Computational experiments show that the proposed general regularization term performs better than ℓp -penalties for sparse approximation problems. Some numerical experiments are included to illustrate the effectiveness of the presented new potential function.
8068D18D	In enclosed environments where robots are deployed, the observed speech signal is smeared due to reverberation. This degrades the performance of the automatic speech recognition (ASR). Thus, hands-free speech recognition for human-machine communication is a difficult task. Most speech enhancement techniques used to address this problem enhance the contaminated waveform independent from that of the ASR. However, this approach does not necessarily improve ASR performance. In this paper, we expand the conventional spectral subtraction-based (SS) technique to deal with reverberation. In our proposed approach, the dereverberation parameters of SS are optimized to improve the likelihood of the acoustic model and not just the waveform signal. The system is capable of adaptively fine-tuning these parameters jointly with acoustic model training for effective use in ASR application. We have experimented using real reverberant data collected from an operational robot. Moreover, we also evaluated with reverberant data corrupted with environmental and robot internal noise. Experimental results show that the proposed method significantly improves the recognition performance over conventional approach.	
2526A3A8	This paper describes a hands-free speech recognition technique based on acoustic model adaptation to reverberant speech. In hands-free speech recognition, the recognition accuracy is degraded by reverberation, since each segment of speech is affected by the reflection energy of the preceding segment. To compensate for the reflection signal we introduce a frame-by-frame adaptation method adding the reflection signal to the means of the acoustic model. The reflection signal is approximated by a first-order linear prediction from the observation signal at the preceding frame, and the linear prediction coefficient is estimated with a maximum likelihood method by using the EM algorithm, which maximizes the likelihood of the adaptation data. Its effectiveness is confirmed by word recognition experiments on reverberant speech.
24D39B30	Acoustic modeling in speech recognition uses very little knowledge of the speech production process. At many levels our models continue to model speech as a surface phenomenon. Typically, hidden Markov model (HMM) parameters operate primarily in the acoustic space or in a linear transformation thereof; state-to-state evolution is modeled only crudely, with no explicit relationship between states, such as would be afforded by the use of phonetic features commonly used by linguists to describe speech phenomena, or by the continuity and smoothness of the production parameters governing speech. This survey article attempts to provide an overview of proposals by several researchers for improving acoustic modeling in these regards. Such topics as the controversial Motor Theory of Speech Perception, work by Hogden explicitly using a continuity constraint in a pseudo-articulatory domain, the Kalman filter based Hidden Dynamic Model, and work by many groups showing the benefits of using articulatory features instead of phones as the underlying units of speech, will be covered.
1881F04A	In recent years, the number of studies investigating new directions in speech modeling that goes beyond the conventional HMM has increased considerably. One promising approach is to use Bayesian Networks (BN) as speech models. Full recognition systems based on Dynamic BN as well as acoustic models using BN have been proposed lately. Our group at ATR has been developing a hybrid HMM/BN model, which is an HMM where the state probability distribution is modeled by a BN, instead of commonly used mixtures of Gaussian functions. In this paper, we describe how to use the hybrid HMM/BN acoustic models, especially emphasizing some design and implementation issues. The most essential part of HMM/BN model building is the choice of the state BN topology. As it is manually chosen, there are some factors that should be considered in this process. They include, but are not limited to, the type of data, the task and the available additional information. When context-dependent models are used, the state-level structure can be obtained by traditional methods. The HMM/BN parameter learning is based on the Viterbi training paradigm and consists of two alternating steps - BN training and HMM transition updates. For recognition, in some cases, BN inference is computationally equivalent to a mixture of Gaussians, which allows HMM/BN model to be used in existing decoders without any modification. We present two examples of HMM/BN model applications in speech recognition systems. Evaluations under various conditions and for different tasks showed that the HMM/BN model gives consistently better performance than the conventional HMM.
7B1AA3CE	This paper proposes a new spatial regularization of Fisher linear discriminant analysis (LDA) to reduce the overfitting due to small size sample (SSS) problem in face recognition. Many regularized LDAs have been proposed to alleviate the overfitting by regularizing an estimate of the within-class scatter matrix. Spatial regularization methods have been suggested that make the discriminant vectors spatially smooth, leading to mitigation of the overfitting. As a generalized version of the spatially regularized LDA, the proposed regularized LDA utilizes the non-uniformity of spatial correlation structures in face images in adding a spatial smoothness constraint into an LDA framework. The region-dependent spatial regularization is advantageous for capturing the non-flat spatial correlation structure within face image as well as obtaining a spatially smooth projection of LDA. Experimental results on public face databases such as ORL and CMU PIE show that the proposed regularized LDA performs well especially when the number of training images per individual is quite small, compared with other regularized LDAs.
81717C8A	The research of Mongolian speech recognition technology start comparatively late and it is still in its primary stage. In this paper, we optimized the basic resources of Mongolian speech recognition system, and we also improved the acoustic model of Mongolian speech recognition system, and this is most important. In this paper, we realized continuous HMM Gaussian mixture model and multiple data stream SCHMM model on the basis of context dependent phonetic model and decision tree method. And we compared the two models in performances. Finally, a large quantity of experiments have been taken to the testing set with HTK as an experimental platform by applying trigram language model and acoustic model which is composed of context dependent phonetic model, decision tree method and multiple data stream SCHMM model. We found system performance has been optimized, and system recognition accuracy rates of word and sentence have been greatly improved.
7EB6E8D3	Theoretical results related to properties of a regularized recursive algorithm for estimation of a high dimensional vector of parameters are presented and proved. The recursive character of the procedure is proposed to overcome the difficulties with high dimension of the observation vector in computation of a statistical regularized estimator. As to deal with high dimension of the vector of unknown parameters, the regularization is introduced by specifying a priori non-negative covariance structure for the vector of estimated parameters. Numerical example with Monte-Carlo simulation for a low-dimensional system as well as the state/parameter estimation in a very high dimensional oceanic model is presented to demonstrate the efficiency of the proposed approach.
84349968	The optimization problems associated with various regularization techniques for supervised learning from data (e.g., weight-decay and Tikhonov regularization) are described in the context of Reproducing Kernel Hilbert Spaces. Suboptimal solutions expressed by sparse kernel models with a given upper bound on the number of kernel computational units are investigated. Improvements of some estimates obtained in Comput. Manag. Sci., vol. 6, pp. 53-79, 2009 are derived. Relationships between sparseness and generalization are discussed.
7BD96528	A useful strategy to deal with complex classification scenarios is the “divide and conquer” approach. The mixture of experts (MoE) technique makes use of this strategy by jointly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weighs their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, in the case of high dimensional data. In general, subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using regularization. Experiments using artificial and real-world datasets provide evidence that the proposed method improves the classical MoE technique, in terms of accuracy and sparseness of the solution. Furthermore, our results indicate that the advantages of the proposed technique increase with the dimensionality of the data.
7CC97970	Training a classifier with good generalization capability is a major issue for pattern classification problems. A novel training objective function for Radial Basis Function (RBF) network using a localized generalization error model (L-GEM) is proposed in this paper. The localized generalization error model provides a generalization error bound for unseen samples located within a neighborhood that contains all training samples. The assumption of the same width for all dimensions of a hidden neuron in L-GEM is relaxed in this work. The parameters of RBF network are selected via minimization of the proposed objective function to minimize its localized generalization error bound. The characteristics of the proposed objective function are compared with those for regularization methods. For weight selection, RBF networks trained by minimizing the proposed objective function consistently outperform RBF networks trained by minimizing the training error, Tikhonov Regularization, Weight Decay or Locality Regularization. The proposed objective function is also applied to select center, width and weight in RBF network simultaneously. RBF networks trained by minimizing the proposed objective function yield better testing accuracies when compared to those that minimizes training error only.
79F45FF9	The  vast  majority  of  automatic  speech  recognition systems  use  Hidden  Markov  Models  (HMMs)  as  the  underlying acoustic  model.  Initially  these  models  were  trained  based  on the maximum likelihood criterion. Significant performance gains have been obtained by using discriminative training criteria, such as  maximum  mutual  information  and  minimum  phone  error. However,  the  underlying  acoustic  model  is  still  generative,  with the associated constraints on the state and transition probability distributions, and classification is based on Bayes’ decision rule. Recently, there has been interest in examining discriminative, or direct, models for speech recognition. This paper briefly reviews the  forms  of  discriminative  models  that  have  been  investigated. These  include  maximum  entropy  Markov  models,  hidden  con- ditional  random  fields  and  conditional  augmented  models.  The relationships between the various models and issues with applying them  to  large  vocabulary  continuous  speech  recognition  will  be discussed.	
804433CC	Regularization looks for an interpolating function which is close to the data and also "smooth". This function is obtained by minimizing an error functional which is the weighted sum of a "fidelity term" and a "smoothness term". However, using only one set of weights does not guarantee that this function will be the MAP estimate. One has to consider all possible weights in order to find the MAP function. Also, using only one combination of weights makes the algorithm very sensitive to the data. The solution suggested here is through the Bayesian approach: a probability distribution over all weights is constructed and all weights are considered when reconstructing the function or computing the expectation of a linear functional on the function space.
7CEE7EDE	In this paper, we propose a novel order-recursive training algorithm for kernel-based discriminants which is computationally efficient. We integrate this method in a hybrid HMM-based speech recognition system by translating the outputs of the kernel-based classifier into class-conditional probabilities and using them instead of Gaussian mixtures as production probabilities of a HMM-based decoder for speech recognition. The performance of the described hybrid structure is demonstrated on the DARPA resource management (RMI) corpus
7F8F53F9	In this paper, the method of fundamental solutions is applied to solve some inverse boundary value problems associated with the steady-state heat conduction in an anisotropic medium. Since the resulting matrix equation is severely ill-conditioned, a regularized solution is obtained by employing the truncated singular value decomposition, while the optimal regularization parameter is chosen according to the L-curve criterion. Numerical results are presented for both two- and three-dimensional problems, as well as exact and noisy data. The convergence and stability of the proposed numerical scheme with respect to increasing the number of source points and the distance between the fictitious and physical boundaries, and decreasing the amount of noise added into the input data, respectively, are analysed. A sensitivity analysis with respect to the measure of the accessible part of the boundary and the distance between the internal measurement points and the boundary is also performed. The numerical results obtained show that the proposed numerical method is accurate, convergent, stable and computationally efficient, and hence it could be considered as a competitive alternative to existing methods for solving inverse problems in anisotropic steady-state heat conduction.
828B43F4	We investigate both theoretically and numerically the so-called invariance property, see e.g. Sun and Ma (2015a,b), of the solution of boundary value problems associated with the anisotropic heat conduction equation (or Laplace–Beltrami’s equation) in two dimensions with respect to elementary transformations of the solution domain, e.g. dilations or contractions. We also show that the standard method of fundamental solutions (MFS) does not satisfy the invariance property. Motivated by these reasons, we introduce, in a natural manner, a modified version of the MFS that remains invariant under elementary transformations of the solution domain and is referred to as the invariant MFS (IMFS). Five two-dimensional examples are thoroughly investigated to assess the numerical accuracy, convergence and stability of the proposed IMFS, in conjunction with the Tikhonov regularization method (Tikhonov and Arsenin, 1986) and Morozov’s discrepancy principle (Morozov, 1966), for Laplace–Beltrami’s equation with perturbed boundary conditions.
7D4E550B	The application of the method of fundamental solutions to the Cauchy problem for steady-state heat conduction in two-dimensional functionally graded materials (FGMs) is investigated. The resulting system of linear algebraic equations is ill-conditioned and, therefore, regularization is required in order to solve this system of equations in a stable manner. This is achieved by employing the zeroth-order Tikhonov functional, while the choice of the regularization parameter is based on the L-curve method. Numerical results are presented for both smooth and piecewise smooth geometries. The convergence and the stability of the method with respect to increasing the number of source points and the distance between the source points and the boundary of the solution domain, and decreasing the amount of noise added into the input data, respectively, are analysed.
7BD64644	The reconstruction of solutions in statistical inverse problems in Hilbert spaces requires regularization, which is often based on a parametrized family of proposal estimators. The choice of an appropriate parameter in this family is crucial. We propose a modification of the classical discrepancy principle as an adaptive parameter selection. This varying discrepancy principle evaluates the misfit in some weighted norm, and it also has an incorporated emergency stop. These ingredients allow the order optimal reconstruction when the solution owns nice spectral resolution. Theoretical analysis is accompanied with numerical simulations, which highlight the features of the proposed varying discrepancy principle.
770EB885	Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie for the selection of groups of correlated variables. To investigate on the statistical properties of this scheme and in particular on its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combination of elements ({\em features}) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular ``{\em elastic-net representation}'' of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed by Zou and Hastie 
79F1432A	The aim of this article is to propose a procedure to cluster functional observations in a subspace of reduced dimension. The dimensional reduction is obtained by constraining the cluster centroids to lie into a subspace which preserves the maximum amount of discriminative information contained in the original data. The model is estimated by using penalized least squares to take into account the functional nature of the data. The smoothing is carried out within the clustering and its amount is adaptively calibrated. A simulation study shows how the combination of these two elements, feature-extraction and automatic data-driven smoothing, improves the performance of clustering by reducing irrelevant and redundant information in the data. The effectiveness of the proposal is demonstrated by an application to a real dataset regarding a speech recognition problem. Implementation details of the algorithm together with a computer code are available in the online supplements.
80F92BE9	We investigate the classical integrability of the Alday-Arutyunov-Frolov model, and show that the Lax connection can be reduced to a simpler 2 x 2 representation. Based on this result, we calculate the algebra between the L-operators and find that it has a highly non-ultralocal form. We then employ and make a suitable generalization of the regularization technique proposed by Maillet for a simpler class of non-ultralocal models, and find the corresponding r- and s-matrices. We also make a connection between the operator-regularization method proposed earlier for the quantum case, and the Maillet's symmetric limit regularization prescription used for non-ultralocal algebras in the classical theory
7FD65373	We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.
79F8471E	The proportional odds model (POM) is commonly used in regression analysis to predict the outcome for an ordinal response variable. The maximum likelihood estimation (MLE) approach is typically used to obtain the parameter estimates. The likelihood estimates do not exist when the number of parameters, p, is greater than the number of observations n. The MLE also does not exist if there are no overlapping observations in the data. In a situation where the number of parameters is less than the sample size but p is approaching to n, the likelihood estimates may not exist, and if they exist they may have quite large standard errors. An estimation method is proposed to address the last two issues, i.e. complete separation and the case when p approaches n, but not the case when p>n. The proposed method does not use any penalty term but uses pseudo-observations to regularize the observed responses by downgrading their effect so that they become close to the underlying probabilities. The estimates can be computed easily with all commonly used statistical packages supporting the fitting of POMs with weights. Estimates are compared with MLE in a simulation study and an application to the real data
7B37D69B	We consider efficient construction of nonlinear solution paths for general ℓ1-regularization. Unlike the existing methods that incrementally build the solution path through a combination of local linear approximation and recalibration, we propose an efficient global approximation to the whole solution path. With the loss function approximated by a quadratic spline, we show that the solution path can be computed using a generalized Lars algorithm. The proposed methodology avoids high-dimensional numerical optimization and thus provides faster and more stable computation. The methodology also can be easily extended to more general regularization framework. We illustrate such flexibility with several examples, including a generalization of the elastic net and a new method that effectively exploits the so-called “support vectors” in kernel logistic regression.
7D3C2BD7	In this study , we introduce a path-follo wing algorithm for L 1 regularized general- ized linear mo dels. The L 1 regularization pro cedure is useful esp ecially because it, in e ect, selects variables according to the amoun t of penalization on the L 1 norm of the coecien ts, in a manner less greedy than forw ard selection/bac kward deletion. The GLM path algorithm ecien tly computes solutions along the entire regularization path using the predictor-corrector metho d of con vex-optimization. Selecting the step length of the regularization parameter is critical in con trolling the overall accuracy of the paths; we suggest intuitiv e and  exible strategies for choosing appropriate values. We demonstrate the implemen tation with several sim ulated and real datasets.
799E0C6C	Multimedia event detection (MED) is one of the most important branches of multimedia content analysis. Current research work on MED focuses mainly on detecting specific events, such as sport events, news events and suspicious events, which is far from achieving a complicated and generic MED due to the fact that these events usually contain a lot of visual attributes, such as objects, scenes and human actions. Being different from visual features, visual attributes are hidden classes to event detectors and event classifiers. Hence, proper representation of these visual attributes could be helpful in building a sophisticated and generic MED. In this paper, we use Gaussian mixture model (GMM) for representing video events with the motivation that the individual component densities of GMM could model some underlying hidden visual attributes and propose a ℓ2-regularized logistic Gaussian mixture regression approach, which is also called LLGMM classifier, for a more generic and complicated MED. We also propose an efficient iterative algorithm, which uses gradient descent, a standard convex optimization method, to solve the objective function of LLGMM. Finally, extensive experiments are conducted on the challenging TRECVID MED 2012 development dataset. The results demonstrate the effectiveness of the proposed LLGMM classifier for MED.
7881B8BF	This paper proposes a regularized locality preserving discriminant analysis (RLPDA) approach for facial feature extraction and recognition. The RLPDA approach decomposes the eigenspace of the locality preserving within-class scatter matrix into three subspaces, i.e., the face space, the noise space and the null space, and then regularizes the three subspaces differently according to their predicted eigenvalues. As a result, the proposed approach integrates discriminative information in all of the three subspaces, de-emphasizes the effect of the eigenvectors corresponding to the small eigenvalues, and meanwhile suppresses the small sample size problem. Extensive experiments on ORL face database, FERET face subset and UMIST face database illustrate the effectiveness of the proposed approach.
7FC0C78E	It is well-known that the applicability of linear discriminant analysis (LDA) to high-dimensional pattern classification tasks such as face recognition (FR) often suffers from the so-called "small sample size" (SSS) problem arising from the small number of available training samples compared to the dimensionality of the sample space. In this paper, we propose a new LDA method that effectively addresses the SSS problem using a regularization technique. In addition, a scheme of expanding the representational capacity of the face database is introduced to overcome the limitation that the LDA based algorithms require at least two samples per class available for learning. Extensive experimentation performed on the FERET database indicates that the proposed methodology outperforms traditional methods such as eigenfaces and direct LDA in a number of SSS setting scenarios.
7DA92E38	It is well-known that the applicability of both linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) to high-dimensional pattern classification tasks such as face recognition (FR) often suffers from the so-called “small sample size” (SSS) problem arising from the small number of available training samples compared to the dimensionality of the sample space. In this paper, we propose a new QDA like method that effectively addresses the SSS problem using a regularization technique. Extensive experimentation performed on the FERET database indicates that the proposed methodology outperforms traditional methods such as Eigenfaces, direct QDA and direct LDA in a number of SSS setting scenarios.
7B0C0D3F	We discuss the low-energy analysis of models involving quarks and four-fermion couplings. The relation with QCD and with other models of mesons and meson plus quarks at low energies is discussed. A short description of how the heat-kernel expansion can be used to get regularization independent information, is given
7AB5539E	In this paper we present a new algorithm to recover a sparse signal from a noisy register. The algorithm assumes a new prior distribution for the sparse signal that consists of a mixture of a narrow and a broad Gaussian both with zero mean. A penalty term which favors solutions driven from this model is added to the usual error cost function and the resultant global cost function is minimized by means of a gradient-type algorithm. A condition is derived for the step-size parameter in order to ensure convergence. In the paper we also propose a method (based on the Expectation-Maximization algorithm) to update the mixture parameters. The estimation of the sparse signal and the optimization of the Gaussian mixture are combined in the proposed algorithm: in each iteration a new signal estimate and a new model (which approximates the distribution of the new estimate) are obtained. In this way, the proposed method can be used without any statistical knowledge about the signal. Simulation experiments show that the accuracy of the proposed method is competitive with classical statistical detectors with a lower computational load.
7B2D3282	In probabilistic speech recognition it is often interesting to evaluate the contribution of the language model and that of the acoustic model. We propose an information theoretical approach which takes into account the interaction between the two sources of information. Experimental results are presented concerning the IBM prototype real-time recognizer of the Italian language based on a 20,000-word vocabulary.