7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
77D497A2	The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results. Online search engines have used other features beyond pure IR features to return relevant matching documents. However, over-emphasis on relevance could lead to redundancy in search results. In document search, diversity is simply the variety of documents that span the result set. In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options. For such a marketplace, in order to minimize query abandonment and the risk of dissatisfaction to the average user, several factors like diversity, trust and value need to be taken into account. Previous work in this field [4] has shown an impossibility result that there exists no such function that can optimize for all these factors. Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user. In this paper we describe an interface which enables users to have more control over the optimization function used to present the results. We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory. We use an algorithm based on bounded greedy selection [5] to construct the result set based on parameters specified by the user.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
7FEBCDC1	This paper analyzes the computational complexity of finding relevant documents on the Web. Given a search query that has n significant terms, relevant documents retrieved by search engines will contain at least a number k of the significant terms. The threshold k chosen will depend on the collection of documents and is determined experimentally upon formation of the collection. Algorithms are then provided to compute a similarity ranking. The fundamental analysis is based on combinatorial theory and theorems providing bounds on the runtime complexity of the algorithms are proven.
77B1FCBA	Providing an effective mobile search service is a difficult task given the unique characteristics of the mobile space. Small-screen devices with limited input and interaction capabilities do not make ideal search devices. In addition, mobile content, by its concise nature, offers limited indexing opportunities, which makes it difficult to build high-quality mobile search engines and indexes. In this paper we consider the issue of limited page content by evaluating a heuristic content enrichment framework that uses standard Web resources as a source of additional indexing knowledge. We present an evaluation using a mobile news service that demonstrates significant improvements in search performance compared to a benchmark mobile search engine.
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
02426620	Code Search Engines (CSE) can serve as powerful resources of open source code, as they can search in billions of lines of open source code available on the web. The strength of CSEs can be used for several tasks like searching relevant code samples, identifying hotspots, and finding bugs. However, the major limitations in using CSEs for these tasks are that the returned samples are too many and they are often partial. Our framework addresses the preceding limitations and thereby helps in using CSEs for these tasks. We showed the effectiveness of our framework with two tools developedbased on our framework.
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
7F48E77D	With the proliferation of social media, consumers’ cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers’ search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
58FB967D	A part of a larger user study conducted within the scope of the EU project IRIS was to investigate the preferences of people with disabilities in regard to interface design of Web applications. User requirements of online help and search engines were also in the focus of this study, since the target user group depends heavily upon powerful tools to support them in the process of seeking information. The results showed that user preferences for information presentation vary a great deal, which could be solved by comprehensive user profiling. However, the functionalities of online help and search functions, as presented in the Internet today, need to be enhanced so they can be used more flexibly and be adjusted to the users’ mental representation of problems and information needs to help them fulfill their tasks.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
7BB9B487	The world wide web of today publishes a great number of real-time content, causing the increasing need for a differentiated way of searching. In this paper, three issues related to retrieving real-time content are presented, and their applications are proposed. First, the characteristics of real-time content, as well as the concept of real-time search are introduced. Second, the real-time technologies that enable real-time search are described. Finally, a platform for application services utilizing real-time search is proposed.
75B65C35	We investigate search engines' mechanism for allocating impressions generated from different search terms. This mechanism is equivalent to running an independent GSP auction for each search term only when the number of search terms is small. In practice, the number of search terms is so large that an advertiser cannot possibly communicate to the search engine all the GSP auctions that he wishes to participate in. For example, a travel agency is interested in all search terms pertaining to flight, including "flight to boston", "ticket to SFO", "cheap airfare", etc. Therefore, the search engine introduces broad match keywords as a bidding language that allows an advertiser to submit a bid for multiple GSP auctions at once. However, with broad match keywords, the GSP auctions are no longer independent, i.e. an advertiser's bid in one auction may depend on his bid in another auction.We propose the broad match mechanism as a model that captures this aspect of the multi-keyword sponsored search mechanism. We study the performance of this mechanism under the price of anarchy (POA) framework. We identify two properties of broad match keywords, namely expressiveness and homogeneity, that characterize the POA and we prove almost tight bounds on the POA. The bounds allow us to explore trade-offs between the two properties. We introduce the exact-match-only mechanism whose performance, when compared to that of broad match mechanisms, gives us an insight into the net benefit of broad match keywords. The broad match mechanism can also be viewed as a mechanism that copes with severe communication constraint i.e. the valuation of an advertiser is described by many more numbers than the search engine can solicit.
7FBD8242	The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.
80BE578A	Structured documents (predominantly encoded in XML) utilize markup dialects for several purposes, such as conveying logical structure, or providing rendering instructions. XML structure can also help users to navigate within documents to satisfy their information needs. However, including the user's structural preferences in the ranking of retrieved elements remains a key challenge in XML retrieval. In this paper, we propose an approach for including structural preferences in the ranking of XML elements by improving the structural relevance (SR) of results. SR is an evaluation measure which relies on graphical navigation models to capture the structural preferences of users. We propose several algorithms to post-process search engine output to improve the SR of the output. Experimental results (using data, assessments, and search engines from INEX 2007 and 2008) demonstrate the effect of different combinations of post-processing algorithms and navigation models on the effectiveness of systems.
7B60E99F	Understanding the functional mechanisms of the complex biological system as a whole is drawing more and more attention in global health care management. Traditional Chinese Medicine (TCM), essentially different from Western Medicine (WM), is gaining increasing attention due to its emphasis on individual wellness and natural herbal medicine, which satisfies the goal of integrative medicine. However, with the explosive growth of biomedical data on the Web, biomedical researchers are now confronted with the problem of large-scale data analysis and data query. Besides that, biomedical data also has a wide coverage which usually comes from multiple heterogeneous data sources and has different taxonomies, making it hard to integrate and query the big biomedical data. Embedded with domain knowledge from different disciplines all regarding human biological systems, the heterogeneous data repositories are implicitly connected by human expert knowledge. Traditional search engines cannot provide accurate and comprehensive search results for the semantically associated knowledge since they only support keywords-based searches. In this paper, we present BioTCM-SE, a semantic search engine for the information retrieval of modern biology and TCM, which provides biologists with a comprehensive and accurate associated knowledge query platform to greatly facilitate the implicit knowledge discovery between WM and TCM.
7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
75CC9391	With much information available from open sources on the internet, computer generated databases have become commonplace. The question whether computer generated databases can be protected under the sui generis database right has hitherto received little attention. This article investigates this question and finds that the substantiality of investment, the definition of the rights holder and the interpretation of exclusive rights raise fundamental issues.
7F2BF256	Today, Web browsers can interpret an enormous amount of different file types, including time-continuous data. By consuming an audio or video, however, the hyperlinking functionality of the Web is "left behind" since these files are typically unsearchable, thus not indexed by common text-based search engines. Our XML-based CMML annotation format and the Annodex file format presented in this paper are designed to solve this problem of "dark matter" on the Internet: Continuous media files are annotated and indexed (i.e., Annodexed), enabling hyperlinks to and from the media. Furthermore, the hyperlinks do not typically point to an entire media file, but to and from arbitrary fragments or intervals. The standards proposed in to create the Continuous Media Web have been submitted to the IETF for review.
753A28D6	Document similarity search is to find documents similar to a given query document and return a ranked list of similar documents to users, which is widely used in many text and web systems, such as digital library, search engine, etc. Traditional retrieval models, including the Okapi's BM25 model and the Smart's vector space model with length normalization, could handle this problem to some extent by taking the query document as a long query. In practice, the Cosine measure is considered as the best model for document similarity search because of its good ability to measure similarity between two documents. In this paper, the quantitative performances of the above models are compared using experiments. Because the Cosine measure is not able to reflect the structural similarity between documents, a new retrieval model based on TextTiling is proposed in the paper. The proposed model takes into account the subtopic structures of documents. It first splits the documents into text segments with TextTiling and calculates the similarities for different pairs of text segments in the documents. Lastly the overall similarity between the documents is returned by combining the similarities of different pairs of text segments with optimal matching method. Experiments are performed and results show: 1) the popular retrieval models (the Okapi's BM25 model and the Smart's vector space model with length normalization) do not perform well for document similarity search; 2) the proposed model based on TextTiling is effective and outperforms other models, including the Cosine measure; 3) the methods for the three components in the proposed model are validated to be appropriately employed.
7F561450	The market for Internet search is not only economically and socially important, it is also highly concentrated. Is this a problem? We study the question whether "competition is only a free click away". We argue that the market for Internet search is characterized by indirect network externalities and construct a simple model of search engine competition, which produces a market share development that fits the empirically observed development since 2003 well. We find that there is a strong tendency towards market tipping and, subsequently, monopolization, with negative consequences on economic welfare. Therefore, we propose to require search engines to share their data on previous searches. We compare the resulting "competitive oligopoly" market structure with the less competitive current situation and show that our proposal would spur innovation, search quality, consumer surplus, and total welfare. We also discuss the practical feasibility of our policy proposal and sketch the legal issues involved.
75F71181	One of the enabling technologies of the World Wide Web, along with browsers, domain name servers, and hypertext markup language, is the search engine. Although the Web contains over 100 million pages of information, those millions of pages are useless if you cannot find the pages you need. All major Web search engines operate the same way: a gathering program explores the hyperlinked documents of the Web, foraging for Web pages to index. These pages are stockpiled by storing them in some kind of database or repository. Finally, a retrieval program takes a user query and creates a list of links to Web documents matching the words, phrases, or concepts in the query. Although the retrieval program itself is correctly called a search engine, by popular usage the term now means a database combined with a retrieval program. For example, the Lycos search engine comprises the Lycos Catalog of the Internet and the Pursuit retrieval program. This paper describes the Lycos system for collecting, storing, and retrieving information about pages on the Web. After outlining the history and precursors of the Lycos system, the paper discusses some of the design choices made in building this Web indexer and touches briefly on the economic issues involved in working with very large retrieval systems.
7D600956	Web search engines are powerful tools used to satisfy specific information needs on the web. Their purpose is to maximize user satisfaction when performing this task. Although there are other sources of evidence, besides text, to characterize document relevance for a specific need, especially for HTML documents, current search engines do not allow users to explore these features when posing a query. Search engine queries are based almost exclusively on keywords. We believe that it is possible to improve user satisfaction if HTML tags and document metadata are available to users at query time. In this paper we present Xearch, a meta-search system that wraps public search engines in a framework that improves both the expressiveness of the language available for the user to specify information needs and the control over the answer format. Xearch converts HTML pages to a specific XML Schema, covering text and metadata derived from HTML. User queries are then submitted on this schema and can be specified through keywords but also explore documents’ HTML tags and metadata. Results from our experimental evaluation confirm that it is possible to improve the answer quality with this framework.
77C4F21A	In  this  work,  we  investigate  how  to  propagate  annotated labels for a given single image from the image-level to their corresponding semantic regions, namely Label-to-Region  (L2R),  by  utilizing  the  auxiliary  knowledge  from Internet image search with the annotated image labels as queries.  A nonparametric solution is proposed to perform L2R for single image with complete labels.  First, each la- bel of the image is used as query for online image search engines to obtain a set of semantically related and visually similar images, which along with the input image are encoded as Bags-of-Hierarchical-Patches.   Then,  an efficient  two-stage feature  mining procedure is  presented  to discover  those  input-image  specific,  salient  and  descriptive features for each label from the proposed Interpolation SIFT (iSIFT) feature pool. These features consequently constitute a patch-level representation, and the continuity biased sparse coding is proposed to select few patches from the online images with preference to larger patches to reconstruct a candidate region, which randomly merges the spatially connected patches of the input image.  Such candidate regions are further ranked according to the reconstruction errors, and the top regions are used to derive the label confidence vector for each patch of the input image. Finally, a patch clustering procedure is performed as postprocessing to finalize L2R for the input image.  Extensive experiments on three public databases demonstrate the encouraging performance of the proposed nonparametric L2R solution.
7E48A49D	People often want to know expected future events related to given real world entities. For supporting users in the process of future scenario analysis, we propose several methods that enable to retrieve and analyze future-related opinions from large text collections. In particular, we focus on time-unreferenced predictions, which do not contain any explicit future time reference and hence are more difficult to be retrieved. As a second contribution, we propose estimating validity of predictions by automatically searching for real world events corresponding to the predictions. This kind of analysis aims to help detect predictions that are no longer valid as well as help estimating prediction accuracy of information sources.
5F6157DD	The ability to rapidly locate useful on-line services (e.g. software applications, software components, process models, or service organizations), as opposed to simply useful documents, is becoming increasingly critical in many domains. Current service retrieval technology is, however, notoriously prone to low precision. This paper describes a novel service retrieval approached based on the sophisticated use of process ontologies. Our preliminary evaluations suggest that this approach offers qualitatively higher retrieval precision than existing (keyword and table-based) approaches without sacrificing recall and computational tractability/scalability.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
00B17298	he search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
59CE7EA6	Semantic search seems to be an elusive and fuzzy target to many researchers. One of the reasons is that the task lies in between several areas of specialization. In this extended abstract we review some of the ideas we have been investigating while approaching this problem. First, we present how we understand semantic search, the Web and the current challenges. Second, how to use shallow semantics to improve Web search. Third, how the usage of search engines can capture the implicit semantics encoded in the queries and actions of people. To conclude, we discuss how these ideas can create virtuous feedback circuit for machine learning and, ultimately, better search.
5A9D2B23	We present an approach to increasing the effectiveness of ranked-output retrieval systems that relies on graphical display and user manipulation of “views” of retrieval results, where a view is the subset of retrieved documents that contain a specified subset of query terms. This approach has been implemented in a system named VIEWER (VIEwing WEb Results), acting as an interface to available search engines. An experimental evaluation of the performance of VIEWER in contrast to AltaVista is the major focus of the paper. We first report the results of an experiment on single, short query searches where VIEWER, used as an interactive ranking system, markedly outperformed AltaVista. We then concentrate on a more realistic searching scenario, involving free query formulation, unconstrained selection of retrieval results, and possibility of query reformulation. We report the results of an experiment where the use of VIEWER, compared to AltaVista, seemed to shift the user effort from inspection to evaluation of results, increasing retrieval effectiveness, and user satisfaction. In particular, we found that the VIEWER users retrieved half as many nonrelevant documents as the AltaVista users while retrieving a comparable number of relevant documents.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
80BF1EF4	The information explosion on the Internet makes it hard for users to obtain required information from the Web searched results in a more personalized way. For the same input word, most search engines return the same result to each user without taking into consideration user preference. For many users, it is no longer sufficient to get non-customized results. It is crucial to analyze users' search and browsing behaviors based on searching keywords entered by users, the clicking rate of each link in the result and the time they spend on each site. To this end, we have proposed a method to derive user searching profiles. We have also proposed a mechanism to derive document profiles, based on similarity score of documents. In this paper, we discuss how to use our model to combine the user searching profiles and the document profile, with a view to presenting customized search results to the users.
76244FC6	We conduct a broad survey of query-adaptive search strategies in a variety of application domains, where the internal retrieval mechanisms used for search are adapted in response to the anticipated needs for each individual query experienced by the system. While these query-adaptive approaches can range from meta-search over text collections to multimodal search over video databases, we propose that all such systems can be framed and discussed in the context of a single, unified framework. In our paper, we keep an eye towards the domain of video search, where search cues are available from a rich set of modalities, including textual speech transcripts, low-level visual features, and high-level semantic concept detectors. The relative efficacy of each of the modalities is highly variant between many types of queries. We observe that the state of the art in query-adaptive retrieval frameworks for video collections is highly dependent upon the definition of classes of queries, which are groups of queries that share similar optimal search strategies, while many applications in text and Web retrieval have included many advanced strategies, such as direct prediction of search method performance and inclusion of contextual cues from the searcher. We conclude that such advanced strategies previously developed for text retrieval have a broad range of possible applications in future research in multimodal video search.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
803D2737	In the current information age, the dominant method for information search is by providing few keywords to a search engine. Keyword search is currently one of the most important operations in search engines and numerous other applications. In this paper we propose a new text indexing technique for improving the performance of keyword search. Our proposed technique not only speeds up searching operations but also the operations for inserting and for deleting keywords, which are particularly important for the ever increasing and dynamic changing databases such as that for search engines. We propose to partition all keywords into search trees based on the first character and the length of the keywords. Our partitioning scheme creates a much more even distribution of keywords and results in a 32% speedup in the worst cases and a 1% speedup in the average cases in comparing to one of the leading text indexing techniques called burst tries. In addition, our proposed technique stores document indexes only at the leaf nodes of the search trees and results in efficient algorithms for searching, insertion, and deletion of keywords. We successfully integrated the technique into our Information Classification and Search Engine system and showed its potential and feasibility.
6E411207	This study examines existent and new methods for evaluating the success of information retrieval systems. The theory underlying current methods is not robust enough to allow testing retrieval using different meta-tagging schemas. Traditional measures rely on judgments of whether a document is relevant to a particular question. A good system returns all the relevant documents and no extraneous documents. There is a rich literature questioning the efficacy of relevance judgments. Such questions as, Relevant to whom? When? and To what purpose? are not well-answered in traditional theory. In this study, two new measures (Spink's Information Need and Cooper's Utility) are used in evaluating two search tools (tag-based and text-based), comparing these new measures with traditional measures and each other. The open-source Swish text-based search engine and a self-constructed tag-based search tool were used. Thirty-four educators searched for information using both search engines and evaluated the information retrieved by each. Construct measures, derived by multiplying each of the three measures (traditional, information need, and utility) by a rating of satisfaction were compared using two way analysis of variance. This study specifically analyzes small information systems. The design concepts would be untenable for large systems. Results indicated that there was a significant correlation between the three measures, indicating that the new measures provide an equivalent method of evaluating systems and have some significant advantages, which include not requiring relevance judgments and the ability to use the measures in situ
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7EE419B7	Information Retrieval (IR) approaches for semantic web search engines have become very populars in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
78A4DC3B	The phenomenon of sponsored search advertising where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7DEA9D12	An information retrieval (IR) process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In IR a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.An object is an entity which keeps or stores information in a database. User queries are matched to objects stored in the database. Depending on the application the data objects may be, for example, text documents, images or videos. The documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates.Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.In this paper we try to explain IR methods and asses them from two view points and finally propose a simple method for ranking terms and documents on IR and implement the method and check the result.
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
72030340	Users often express confusion and frustration in trying to locate the full-text availability of a journal and having to check multiple resources and interfaces on a library's Web site. The challenge was to seek an interim practical solution which would address this need effectively. The Social Science Journals Database and Search Engine (Soc-dbase) project demonstrates a low-cost one-stop search solution that can be easily and quickly adopted and implemented. The project involves creating a single Web interface to search a database of selected social science and sociology journal titles that include full-text online availability information. This paper presents the design and creation of the social sciences journals database that can be searched to find a journal's full-text availability at the Rutgers University Libraries or on the Web.
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the document side(type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
01D30712	Personalized search engines must be able to cope with various user preferences, retrieving the best matches to a query. For SQL and XML applications new methods for such preference-based searches have been implemented recently. Here we adopt this approach to keyword search in full-text search engines. We propose to augment the vector space model (VSM) by preference constructors having an intuitive partial order semantics: Pareto-accumulation and prioritization. We show that prioritization can be interpreted as subspace preference in the VSM. Using a preprocessor approach we succeed to map prioritization onto the VSM. A first query benchmark, using the standard Time-collection, revealed promising results. The retrieval quality, measured by average expected search length, could be slightly improved. Using the proposed meta engine approach, this gain in retrieval quality is accompanied by a substantial speed up of query runtimes. Thus subspace preferences can be integrated efficiently into existing full-text search engines. 
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.