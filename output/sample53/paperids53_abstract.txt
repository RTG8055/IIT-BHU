80039747	This paper presents a new algorithm for detecting and tracking multiple moving objects in both outdoor and indoor environments. The proposed method measures the change of a combined color-texture feature vector in each image block to detect moving objects. The texture feature is extracted from DCT frequency domain. An attributed relational graph (ARG) is used to represent each object, in which vertices are associated to an object’s sub-regions and edges represent spatial relations among them. Multiple cues including color, texture, and spatial position are integrated to describe each object’s sub-regions. Object tracking and identification are accomplished by inexact graph matching, which enables us to track partially occluded objects and to cope with object articulation. An ARG adaptation scheme is incorporated into the system to handle the changes in object scale and appearance. The experimental results prove the efficiency of the proposed method.
7EF0AE42	Real-time human face detection and recognition from video sequences in surveillance applications is a challenging task due to the variances in background, facial expression and illumination. The face detection approach is based on modest AdaBoost  algorithm and can achieve fast, accurate face detection that is robust to changes in illumination and background. The detection stage provides good results maintaining a low computational cost. The recognition stage is based on an improved independent components Analysis approach which has been modified to cope with the video surveillance application. In the recognition stage, the Hausdorff distance is used as a similarity measure between a general face model and possible instances of the object within the image. After the integration of the two stages, several improvements are proposed which increase the face detection and recognition rate and the overall performance of the system. The experimental results demonstrate the significant performance improvement using the proposed approach over others. It can be seen that the proposed method is very efficient and has significant value in application.
7F4FBA42	We present a method for the detection of instances of an object class, such as cars or pedestrians, in natural images. Similarly to some previous works, this is accomplished via generalized Hough transform, where the detections of individual object parts cast probabilistic votes for possible locations of the centroid of the whole object; the detection hypotheses then correspond to the maxima of the Hough image that accumulates the votes from all parts. However, whereas the previous methods detect object parts using generative codebooks of part appearances, we take a more discriminative approach to object part detection. Towards this end, we train a class-specific Hough forest, which is a random forest that directly maps the image patch appearance to the probabilistic vote about the possible location of the object centroid. We demonstrate that Hough forests improve the results of the Hough-transform object detection significantly and achieve state-of-the-art performance for several classes and datasets.
81790792	Model-based 3D tracker estimate the position, rotation, and joint angles of a given model from video data of one or multiple cameras. They often rely on image features that are tracked over time but the accumulation of small errors results in a drift away from the target object. In this work, we address the drift problem for the challenging task of human motion capture and tracking in the presence of multiple moving objects where the error accumulation becomes even more problematic due to occlusions. To this end, we propose an analysis-by-synthesis framework for articulated models. It combines the complementary concepts of patch-based and region-based matching to track both structured and homogeneous body parts. The performance of our method is demonstrated for rigid bodies, body parts, and full human bodies where the sequences contain fast movements, self-occlusions, multiple moving objects, and clutter. We also provide a quantitative error analysis and comparison with other model-based approaches.
7FDC9A31	One of the major challenges that visual tracking algorithms face nowadays is being able to cope with changes in the appearance of the target during tracking. Linear sub-space models have been extensively studied recently and are possibly the most popular way of modeling target appearance. Unfortunately, efficiency is one of the limitations of present linear subspace models, and this is a key feature for a good tracker. In this paper we present an efficient procedure for tracking based on a linear subspace model of target appearance (grey levels). A set of motion templates is built from the subspace base, which is used to efficiently compute target motion and appearance parameters. It differs from previous works in that we impose no restrictions on the subspace used for modeling appearance. In the experiments conducted we have built a modular PCA-based face tracker which shows that video-rate tracking performance can be achieved with a non optimized implementation of our algorithm.
7E7285EB	In background subtraction, cast shadows induce silhouette distortions and object fusions hindering performance of high level algorithms in scene monitoring. We introduce a nonparametric framework to model surface behavior when shadows are cast on them. Based on physical properties of light sources and surfaces, we identify a direction in RGB space on which background surface values under cast shadows are found. We then model the posterior distribution of lighting attenuation under cast shadows and foreground objects, which allows differentiation of foreground and cast shadow values with similar chromaticity. The algorithms are completely unsupervised and take advantage of scene activity to learn model parameters. Spatial gradient information is also used to reinforce the learning process. Contributions are two-fold. Firstly, with a better model describing cast shadows on surfaces, we achieve a higher success rate in segmenting moving cast shadows in complex scenes. Secondly, obtaining such models is a step toward a full scene parametrization where light source properties, surface reflectance models and scene 3D geometry are estimated for low-level segmentation.
7FAD2C94	While low-dimensional image representations have been very popular in computer vision, they suffer from two limitations: (i) they require collecting a large and varied training set to learn a low-dimensional set of basis functions, and (ii) they do not retain information about the 3D geometry of the object being imaged. In this paper, we show that it is possible to estimate low-dimensional manifolds that describe object appearance while retaining the geometrical information about the 3D structure of the object. By using a combination of analytically derived geometrical models and statistical learning methods, this can be achieved using a much smaller training set than most of the existing approaches. Specifically, we derive a quadrilinearmanifold of object appearance that can represent the effects of illumination, pose, identity and deformation, and the basis functions of the tangent space to this manifold depend on the 3D surface normals of the objects. We show experimental results on constructing this manifold and how to efficiently track on it using an inverse compositional algorithm.
8075F3AF	Moving cast shadows are a major concern for foreground detection algorithms. Processing of foreground images in surveillance applications typically requires that such shadows have been identified and removed from the detected foreground. This paper presents a novel pixel-based statistical approach to model moving cast shadows of non-uniform and varying intensity. This approach uses the Gaussian mixture model (GMM) learning ability to build statistical models describing moving cast shadows on surfaces. This statistical modeling can deal with scenes with complex and time-varying illumination, and prevent false detection in regions where shadows cannot be detected. Gaussian mixture shadow models (GMSM) are automatically constructed and updated over time, are easily added to GMM architecture for foreground detection, and require only a small number of parameters. Results obtained with different scene types show the robustness of the approach.
7E3F3BFE	Cast shadows induced by moving objects often cause serious problems to many vision applications. We present in this paper an online statistical learning approach to model the background appearance variations under cast shadows. Based on the bi-illuminant (i.e. direct light sources and ambient illumination) dichromatic reflection model, we derive physics-based color features under the assumptions of constant ambient illumination and light sources with common spectral power distributions. We first use one Gaussian mixture model (GMM) to learn the color features, which are constant regardless of the background surfaces or illuminant colors in a scene. Then, we build up one pixel based GMM for each pixel to learn the local shadow features. To overcome the slow convergence rate in the conventional GMM learning, we update the pixel-based GMMs through confidence-rated learning. The proposed method can rapidly learn model parameters in an unsupervised way and adapt to illumination conditions or environment changes. Furthermore, we demonstrate that our method is robust to scenes with few foreground activities and videos captured at low or unsteady frame rates.
80DC8ED6	Illumination models of the image set of an object (e.g., human face) under varying lighting conditions have been either empirically or analytically explored. However, the theoretical dimensionality of video bricks of an object under varying illumination is still unknown. In this paper, we focus on this question concretely and give both analytical and empirical results. We derive the theoretical upper bound of the dimensionality of video bricks by investigating the analytical formula of appearance changes due to motion variables of light sources. Theoretical results show in real-world scenes video bricks of an object under varying illumination could be expressed well by a low-dimensional linear subspace. Empirical results of the principal component analysis on the YaleB Face database and our video database are consistent with the theoretical results completely. The application of the low-dimensional linear models of video bricks is demonstrated by the foreground detection task in visual surveillance with drastic illumination changes.
7D5CF113	Historically, SSD or correlation-based visual tracking algorithms have been sensitive to changes in illumination and shading across the target region. This paper describes methods for implementing SSD tracking that is both insensitive to illumination variations and computationally efficient. We first describe a vector-space formulation of the tracking problem, showing how to recover geometric deformations. We then show that the same vector space formulation can be used to account for changes in illumination. We combine geometry and illumination into an algorithm that tracks large image regions on live video sequences using no more computation than would be required to trade with no accommodation for illumination changes. We present experimental results which compare the performance of SSD tracking with and without illumination compensation.
80AE788F	In this paper, we formulate object tracking in a particle filter framework as a multi-task sparse learning problem, which we denote as Multi-Task Tracking (MTT). Since we model particles as linear combinations of dictionary templates that are updated dynamically, learning the representation of each particle is considered a single task in MTT. By employing popular sparsity-inducing ℓp, q mixed norms (p ∈ {2, ∞} and q = 1), we regularize the representation problem to enforce joint sparsity and learn the particle representations together. As compared to previous methods that handle particles independently, our results demonstrate that mining the interdependencies between particles improves tracking performance and overall computational complexity. Interestingly, we show that the popular L1 tracker [15] is a special case of our MTT formulation (denoted as the L11 tracker) when p = q = 1. The learning problem can be efficiently solved using an Accelerated Proximal Gradient (APG) method that yields a sequence of closed form updates. As such, MTT is computationally attractive. We test our proposed approach on challenging sequences involving heavy occlusion, drastic illumination changes, and large pose variations. Experimental results show that MTT methods consistently outperform state-of-the-art trackers.
7D19FB73	We propose a novel tracking algorithm that can work robustly in a challenging scenario such that several kinds of appearance and motion changes of an object occur at the same time. Our algorithm is based on a visual tracking decomposition scheme for the efficient design of observation and motion models as well as trackers. In our scheme, the observation model is decomposed into multiple basic observation models that are constructed by sparse principal component analysis (SPCA) of a set of feature templates. Each basic observation model covers a specific appearance of the object. The motion model is also represented by the combination of multiple basic motion models, each of which covers a different type of motion. Then the multiple basic trackers are designed by associating the basic observation models and the basic motion models, so that each specific tracker takes charge of a certain change in the object. All basic trackers are then integrated into one compound tracker through an interactive Markov Chain Monte Carlo (IMCMC) framework in which the basic trackers communicate with one another interactively while run in parallel. By exchanging information with others, each tracker further improves its performance, which results in increasing the whole performance of tracking. Experimental results show that our method tracks the object accurately and reliably in realistic videos where the appearance and motion are drastically changing over time.
7DA8D19C	This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed, thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in real time even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outperforms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.
816C1AA9	In this paper, we present a novel re-texturing approach using intrinsic video. Our approach begins with indicating the regions of interests by contour-aware layer segmentation. Then, the intrinsic video (including reflectance and illumination components) within the segmented region is recovered by our weighted energy optimization. After that, we compute the normals for the re-textured region, and the texture coordinates in key frames through our newly developed optimization approach. At the same time, the texture coordinates in non-key frames are optimized by our proposed energy function. Finally, when the target sample texture is specified, the re-textured video is created by multiplying the re-textured reflectance component by the original illumination component within the replaced region. As demonstrated in our experimental results, our method can produce high quality video re-texturing results with preserving the lighting and shading effect of the original video.
7DB09023	Many methods have been introduced to detect shot boundaries, e.g. pixel-by-pixel comparisons and histogram comparisons. But they ignore the problem of illumination variation inherent in the video production process especially in TV news reports. So they often can be confused when the incident illumination varies. The Color Ratio Histogram is proposed as frame content measure to solve the problem. However, it is computationally expensive. Different methods have both advantages and disadvantages, in this paper, the merits of different methods are associated to acquire a better performance. A good compromise between speed and accuracy is achieved. The detection is divided into two steps in which different metrics are used to identify different transitions. The effectiveness of our methods has been validated by experiments on some real-world video sequences.
75A72BF2	The objective of this work was to evaluate the effectiveness of commercial video image fire detection systems for small, cluttered spaces as would be found on Navy ships. The primary goal was to establish an understanding of the performance sensitivity and limitations of the video image detection (VID) systems to various setup and environmental conditions that may occur onboard ship while exposed to a range of flaming and smoldering fire sources and potential nuisance alarm sources. The response of the VID systems was benchmarked against standard fire alarm systems using addressable ionization and photoelectric smoke detectors.
8151AE60	In consumer video conferencing, lighting conditions are usually not ideal thus the image qualities are poor. Lighting affects image quality on two aspects: brightness and skin tone. While there has been much research on improving the brightness of the captured images including contrast enhancement and noise removal (which can be thought of as components for brightness improvement), little attention has been paid to the skin tone aspect. In contrast, it is a common knowledge for professional stage lighting designers that lighting affects not only the brightness but also the color tone which plays a critical role in the perceived look of the host and the mood of the stage scene. Inspired by stage lighting design, we propose an active lighting system which automatically adjusts the lighting so that the image looks visually appealing. The system consists of computer controllable light emitting diode light sources of different colors so that it improves not only the brightness but also the skin tone of the face. Given that there is no quantitative formula on what makes a good skin tone, we use a data driven approach to learn a good skin tone model from a collection of photographs taken by professional photographers. We have developed a working system and conducted user studies to validate our approach.
793127B8	Recently, most background modeling approaches represent distributions of background changes by using parametric models such as Gaussian mixture models. Because of significant illumination changes and dynamic moving backgrounds with time, variations of background changes are hard to be modeled by parametric background models. Moreover, how to efficiently and effectively update parameters of parametric models to reflect background changes remains a problem. In this paper, we propose a novel coarse-to-fine detection theory algorithm to extract foreground objects on the basis of nonparametric background and foreground models represented by binary descriptors. We update background and foreground models by a first-in-first-out strategy to maintain the most recent observed background and foreground instances. As shown in the experiments, our method can achieve better foreground extraction results and fewer false alarms of surveillance videos with lighting changes and dynamic backgrounds in both collected and CDnet 2012 benchmark data sets.
788A8E2C	We present a novel algorithm that exploits joint optimization of representation and classification for robust tracking in which the goal is to minimize the least-squares reconstruction errors and discriminative penalties with regularized constraints. In this formulation, an object is represented by the sparse coefficients of local patches based on an overcomplete dictionary, and a classifier is learned to discriminate the target object from the background. To locate the target object in each frame, we propose a deterministic approach to solve the optimization problem. We show that the proposed algorithm can be considered as a generalization of several tracking methods with effectiveness. To account for appearance change of the target and the background, the classifier is adaptively updated with new tracking results. Compared with the most recent tracking algorithms based on sparse representation, the proposed formulation has more discriminative power due to the use of background information and is much faster due to the use of deterministic optimization. Qualitative and quantitative experiments on a variety of challenging sequences show favorable performance of the proposed algorithm against several state-of-the-art methods.
80D8263D	Segmenting and tracking of objects in video is of great importance for video-based encoding, surveillance, and retrieval. However, the inherent difficulty of object segmentation and tracking is to distinguish changes in the displacement of objects from disturbing effects such as noise and illumination changes. Therefore, in this paper, we formulate a color-based deformable model which is robust against noisy data and changing illumination. Computational methods are presented to measure color constant gradients. Further, a model is given to estimate the amount of sensor noise through these color constant gradients. The obtained uncertainty is subsequently used as a weighting term in the deformation process. Experiments are conducted on image sequences recorded from three-dimensional scenes. From the experimental results, it is shown that the proposed color constant deformable method successfully finds object contours robust against illumination, and noisy, but homogeneous regions.
7D394E2F	This paper describes a novel framework for detection and suppression of properly shadowed regions for most possible scenarios occurring in real video sequences. Our approach requires no prior knowledge about the scene, nor is it restricted to specific scene structures. Furthermore, the technique can detect both achromatic and chromatic shadows even in the presence of camouflage that occurs when foreground regions are very similar in color to shadowed regions. The method exploits local color constancy properties due to reflectance suppression over shadowed regions. To detect shadowed regions in a scene, the values of the background image are divided by values of the current frame in the RGB color space. We show how this luminance ratio can be used to identify segments with low gradient constancy, which in turn distinguish shadows from foreground. Experimental results on a collection of publicly available datasets illustrate the superior performance of our method compared with the most sophisticated, state-of-the-art shadow detection algorithms. These results show that our approach is robust and accurate over a broad range of shadow types and challenging video conditions.
76E3B663	Foreground/background segmentation via change detection in video sequences is often used as a stepping stone in high-level analytics and applications. Despite the wide variety of methods that have been proposed for this problem, none has been able to fully address the complex nature of dynamic scenes in real surveillance tasks. In this paper, we present a universal pixel-level segmentation method that relies on spatiotemporal binary features as well as color information to detect changes. This allows camouflaged foreground objects to be detected more easily while most illumination variations are ignored. Besides, instead of using manually set, frame-wide constants to dictate model sensitivity and adaptation speed, we use pixel-level feedback loops to dynamically adjust our method's internal parameters without user intervention. These adjustments are based on the continuous monitoring of model fidelity and local segmentation noise levels. This new approach enables us to outperform all 32 previously tested state-of-the-art methods on the 2012 and 2014 versions of the ChangeDetection.net dataset in terms of overall F-Measure. The use of local binary image descriptors for pixel-level modeling also facilitates high-speed parallel implementations: our own version, which used no low-level or architecture-specific instruction, reached real-time processing speed on a midlevel desktop CPU. A complete C++ implementation based on OpenCV is available online.
7EC8840E	This paper presents a technique for motion detection that incorporates several innovative mechanisms. For example, our proposed technique stores, for each pixel, a set of values taken in the past at the same location or in the neighborhood. It then compares this set to the current pixel value in order to determine whether that pixel belongs to the background, and adapts the model by choosing randomly which values to substitute from the background model. This approach differs from those based upon the classical belief that the oldest values should be replaced first. Finally, when the pixel is found to be part of the background, its value is propagated into the background model of a neighboring pixel. We describe our method in full details (including pseudo-code and the parameter values used) and compare it to other background subtraction techniques. Efficiency figures show that our method outperforms recent and proven state-of-the-art methods in terms of both computation speed and detection rate. We also analyze the performance of a downscaled version of our algorithm to the absolute minimum of one comparison and one byte of memory per pixel. It appears that even such a simplified version of our algorithm performs better than mainstream techniques.
769D105E	This letter presents a new method for background subtraction and shadow removal for grayscale video sequences. The background image is modeled using robust statistical descriptors, and a noise estimate is obtained. Foreground pixels are extracted, and a statistical approach combined with geometrical constraints are adopted to detect and remove shadows.
7F224147	Road  scene  analysis  is  a  challenging  problem  that has  applications  in  autonomous  navigation  of  vehicles.  An  integral component of this system is the robust detection and tracking of  lane  markings.  It  is  a  hard  problem  primarily  due  to  large appearance variations in lane markings caused by factors such as occlusion (traffic on the road), shadows (from objects like trees), and  changing  lighting  conditions  of  the  scene  (transition  from day  to  night).  In  this  paper,  we  address  these  issues  through a  learning-based  approach  using  visual  inputs  from  a  camera mounted  in  front  of  a  vehicle. We  propose,  (i)  a  pixel-hierarchy feature descriptor to model contextual information shared by lane markings with the surrounding road region, (ii) a robust boosting algorithm to select relevant contextual features for detecting lane markings,  and  (iii)  particle  filters  to  track  the  lane  markings, without  the  knowledge  of  vehicle  speed,  by  assuming  the  lane markings  to  be  static  through  the  video  sequence  and  then learning  the  possible  road  scene  variations  from  the  statistics of tracked model parameters. We investigate the effectiveness of our algorithm on challenging daylight and night-time road video sequences.
801C8D0A	To prevent moving shadows being misclassified as moving objects or parts of moving objects, this paper presents an explicit method for detection of moving cast shadows on a dominating scene background. Those shadows are generated by objects moving between a light source and the background. Moving cast shadows cause a frame difference between two succeeding images of a monocular video image sequence. For shadow detection, these frame differences are detected and classified into regions covered and regions uncovered by a moving shadow. The detection and classification assume plane background and a nonnegligible size and intensity of the light sources. A cast shadow is detected by temporal integration of the covered background regions while subtracting the uncovered background regions. The shadow detection method is integrated into an algorithm for two-dimensional (2-D) shape estimation of moving objects from the informative part of the description of the international standard ISO/MPEG-4. The extended segmentation algorithm compensates first apparent camera motion. Then, a spatially adaptive relaxation scheme estimates a change detection mask for two consecutive images. An object mask is derived from the change detection mask by elimination of changes due to background uncovered by moving objects and by elimination of changes due to background covered or uncovered by moving cast shadows. Results obtained with MPEG-4 test sequences and additional sequences show that the accuracy of object segmentation is substantially improved in presence of moving cast shadows. Objects and shadows are detected and tracked separately.
7FDBF311	In this paper, we propose a graph-based approach for detecting and tracking multiple players in broadcast soccer videos. In the first stage, the position of the players in each frame is determined by removing the non player regions. The remaining pixels are then grouped using a region growing algorithm to identify probable player candidates. A directed weighted graph is constructed, where probable player candidates correspond to the nodes of the graph while each edge links candidates in a frame with the candidates in next two consecutive frames. Finally, dynamic programming is applied to find the trajectory of each player. Experiments with several sequences from broadcasted videos of international soccer matches indicate that the proposed approach is able to track the players reasonably well even under varied illumination and ground conditions.
78D0A0EB	Moving objects segmentation plays a very important role in real-time image analysis. However, as one of the common parts in the natural scenes, shadows severely interfere with the accuracy of moving objects detection in video surveillance. In this paper, we present a novel method for moving cast shadows detection. Based on the analysis of the physical model of moving shadows, we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each moving pixel into foreground object or moving shadow. Intensity constraint and geometric heuristics are imposed to further improve the performance. Experiments on various typical scenes exhibit the robustness of the proposed method. Extensively quantitative evaluation and comparison demonstrate that the proposed method significantly outperforms state-of-the-art methods.
803B72DE	Shot change detection is an essential step in video content analysis. However, automatic shot change detection often suffers from high false detection rates due to camera or object movements. To solve this problem, we propose an approach based on local keypoint matching of video frames. This approach aims to detect both abrupt and gradual transitions between shots without modeling different kinds of transitions. Our experiment results show that the proposed algorithm is effective for most kinds of shot changes.
80A1F632	In this paper, we present a theory for combining the effects of motion, illumination, 3D structure, albedo, and camera parameters in a sequence of images obtained by a perspective camera. We show that the set of all Lambertian reflectance functions of a moving object, at any position, illuminated by arbitrarily distant light sources, lies "close" to a bilinear subspace consisting of nine illumination variables and six motion variables. This result implies that, given an arbitrary video sequence, it is possible to recover the 3D structure, motion, and illumination conditions simultaneously using the bilinear subspace formulation. The derivation builds upon existing work on linear subspace representations of reflectance by generalizing it to moving objects. Lighting can change slowly or suddenly, locally or globally, and can originate from a combination of point and extended sources. We experimentally compare the results of our theory with ground truth data and also provide results on real data by using video sequences of a 3D face and the entire human body with various combinations of motion and illumination directions. We also show results of our theory in estimating 3D motion and illumination model parameters from a video sequence
7DF2F63C	This paper proposes an automatic and robust method to detect human faces from video sequences that combines feature extraction and face detection based on local normalization, Gabor wavelets transform and Adaboost algorithm. The key step and the main contribution of this work is the incorporation of a normalization technique based on local histograms with optimal adaptive correlation (OAC) technique to alleviate a common problem in conventional face detection methods: inconsistent performance due to the sensitivity to illumination variations such as local shadowing, noise and occlusion. This approach uses a cascade of classifiers to adopt a coarse-to-fine strategy to achieve higher detection rate with lower false positives. The experimental results demonstrate a significant performance improvement by local normalization over method without normalizations in real video sequences with a wide range of facial variations in color, position, scale, and varying lighting conditions.
7F4EC590	The paper investigates the interaction between tracking and recognition of human faces from video under a framework proposed earlier (Shaohua Zhou et al., Proc. 5th Int. Conf. on Face and Gesture Recog., 2002; Shaohua Zhou and Chellappa, R., Proc. European Conf. on Computer Vision, 2002), where a time series model is used to resolve the uncertainties in both tracking and recognition. However, our earlier efforts employed only a simple likelihood measurement in the form of a Laplacian density to deal with appearance changes between frames and between the observation and gallery images, yielding poor accuracies in both tracking and recognition when confronted by pose and illumination variations. The interaction between tracking and recognition was not well understood. We address the interdependence between tracking and recognition using a series of experiments and quantify the interacting nature of tracking and recognition.
816A1DCF	This paper provides a comprehensive quantitative comparison of metrics for detecting visual anomalies between two videos that are recorded along same path but at different times by a camera on a patrolling platform. The metrics used in this paper are histogram based metrics, statistic based metrics and pixel differences based metrics. We test the metrics for the detection of mobile and stationary anomalies between videos. The two videos are brought to spatial temporal alignment by a two step process. For each frame in the first video the closest matching frame from the second video is found manually and the matched pair of frames are registered using a feature based registration method. Laws texture kernels are used to extract texture energy measures from the images and nine different metrics are applied to generate a difference image sequence which is followed by thresholding to get a binary image sequence. The binary images are compared with the actual ground truth and the performance of each metric are presented for four videos taken in different environments.
7FF0C0FE	We develop an efficient algorithm to track point features supported by image patches undergoing affine deformations and changes in illumination. The algorithm is based on a combined model of geometry and photometry, that is used to track features as well as to detect outliers in a hypothesis testing framework. The algorithm runs in real time on a personal computer; and is available to the public.
7F020A02	This paper presents an algorithm for computing optical flow, shape, motion, lighting, and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination. The problem is formulated in a manner that subsumes structure from motion, multi-view stereo, and photometric stereo as special cases. The algorithm utilizes both spatial and temporal intensity variation as cues: the former constrains flow and the latter constrains surface orientation; combining both cues enables dense reconstruction of both textured and texture-less surfaces. The algorithm works by iteratively estimating affine camera parameters, illumination, shape, and albedo in an alternating fashion. Results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera
7D5EE484	This paper proposes a method for background modeling and foreground detection in video. This method divides the background into two layers, the dynamic layer and the static layer. An energy descriptor is proposed to analysis the motion state in dynamic layer while a grid filter is proposed to reduce the negative impact of sudden illumination change such as light switching off. Experiment results compared with four typical algorithms show that this method outperforms others in most challenging videos including sudden illumination change and some complex backgrounds.
7EB39671	In this paper, we show how to estimate, accurately and efficiently, the 3D motion of a rigid or non-rigid object, and time-varying lighting in a dynamic scene. This is achieved in an inverse compositional tracking framework with a novel warping function that involves a 2D → 3D → 2D transformation. The method is guaranteed to converge, is able to work with rigid and non-rigid objects, and estimates the lighting and motion from a video sequence. Experimental analysis on multiple face video sequences shows impressive speed-up over existing methods while retaining a high level of accuracy.	
7EA28D97	We propose a new robust head detection algorithm that is capable of handling significantly different conditions in terms of viewpoint, tilt angle, scale and resolution. To this aim, we built a new model for the head based on appearance distributions and shape constraints. We construct a categorical model for hair and skin, separately, and train the models for four categories of hair (brown, red, blond and black) and three categories of skin representing the different illumination conditions (bright, standard and dark). The shape constraint fits an elliptical model to the candidate region and compares its parameters with priors based on human anatomy. The experimental results validate the usability of the proposed algorithm in various video surveillance and multimedia applications.
81592FB1	Recreating the temporal illumination variations of natural scenes has great potential for realistic synthesis of video sequences. In this paper, we present a 3D (model-based) approach that achieves this goal. The approach requires a training sequence to learn the time-varying illumination models, which can then be used for synthesis in another sequence. The motion and illumination parameters in the training sequence are estimated alternately by projecting onto appropriate basis functions of a bilinear space defined in terms of the 3D surface normals of the objects. The motion is represented in terms of 3D translation and rotation of the object centroid in the camera frame, and the illumination is represented using a spherical harmonics linear basis. We show video synthesis results using the proposed approach.
80550AB9	This paper proposes a new way to achieve feature point tracking using the entropy of the image. Sum of Squared Differences (SSD) is widely considered in differential trackers such as the KLT. Here, we consider another metric called Mutual Information (MI), which is far less sensitive to changes in the lighting condition and to a wide class of non-linear image transformation. Since mutual-information is used as an energy function to be maximized to track each points, a new feature selection, which is optimal for this metric, is proposed. Results under various complex conditions are presented. Comparison with the classical KLT tracker are proposed.
76B40489	Facial landmark detection has proved to be a very challenging task in biometrics due to the numerous sources of variation. In this work, we present an algorithm for robust detection of facial component-landmarks. Specifically, we address the variation due to extreme pose and illumination. To achieve robust detection for extreme poses, we use a set of independent pose and landmark specific detectors. Each component-landmark detector is applied independently and the information obtained is used to make inferences about the layout of multiple components. In addition, we incorporate a multi-view representation based on an aspect graph approach. The performance of our algorithm is assessed using data from a publicly available database. The failure rate of our method is lower than that of commercially available software.
80D7C0B1	One basic observation for pedestrian detection in video sequences is that both appearance and motion information are important to model the moving people. Based on this observation, we propose a new kind of features, 3D Haar-like (3DHaar) features. Motivated by the success of Haar-like features in image based face detection and differential-frame based pedestrian detection, we naturally extend this feature by defining seven types of volume filters in 3D space, instead of using rectangle filter in 2D space. The advantage is that it can not only represent pedestrian's appearance, but also capture the motion information. To validate the effectiveness of the proposed method, we combine the 3DHaar with support vector machine (SVM) for pedestrian detection. Our experiments demonstrate the 3DHaar are more effective for video based pedestrian detection.
7F50262E	The paper presents a statistical adaptive realtime background subtraction algorithm that is very robust to moving shadows and dynamic scene environment. The algorithm enhances the previously developed method reported by T. Horprascrt et al. (see Proc. IEEE ICCV'99 Frame-rate Workshop, 1999) by adding adaptation of the model corresponding to a dynamic background using adaptive brightness and color distortion. In addition, we propose a novel "vivacity factor" to measure the activities of foreground objects. It is used to delay the adaptation rate for the area of often-occurring moving foregrounds. Our method provides a solution to real-time moving object and shadow detection in the dynamic background scene of a video stream. We also develop the learning-rate control mechanism that is not addressed by most background subtraction algorithms
75974390	We present a novel method to relight video sequences given known surface shape and illumination. The method preserves fine visual details. It requires single view video frames, approximate 3D shape and standard studio illumination only, making it applicable in studio production. The technique is demonstrated for relighting video sequences of faces
7D3B6BBC	In many real applications traditional superresolution methods fail to provide high-resolution images due to objectionable blur and inaccurate registration of input low-resolution images. In this paper, we present a method of superresolution and blind deconvolution of video sequences and address problems of misregistration, local motion and change of illumination. The method processes the video by applying temporal windows, masking out regions of misregistration, and minimizing a regularized energy function with respect to the high-resolution frame and blurs, where regularization is carried out in both the image and blur domains. Experiments on real video sequences illustrate robustness of the method.
7F7F452D	Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object’s appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination. 
80DF8849	The perception of 2nd-order, texture-contrast-defined motion was studied for apparent-motion stimuli composed of a pair of spatially displaced, simultaneously visible checkerboards. It was found that background-relative, counter-changing contrast provided the informational basis for the perception of 2nd-order apparent motion; motion began where contrast changed toward the contrast value of the background checkerboard and ended where contrast changed away from the background value. The perceived apparent motion was not attributable to either postrectification motion-energy analysis or salience-mapping/feature-tracking mechanisms. Parallel results for 1st-order, luminance-defined motion (H. S. Hock, L. A. Gilroy, & G. Harnett, 2002) suggest that counter-changing activation provides a common basis for the perception of both luminance- and texture-contrast-defined apparent motion.
7E49514F	The pupil detection and tracking is an important step for developing a human-computer interaction system. To develop a human eye-computer interaction system, we examine pupil detection and tracking by image processing techniques. In the image processing techniques, the illumination directly influences the image quality in general. If influences of illumination is little, we can obtain an image of good image quality. The subsequent image processing techniques are expected almost to succeed. In this paper, in order to avoid the influences of illumination, we have tried to combine the hardware constitution of an infrared light-emitting diode (LED) light, a sensitive infrared camera, and an infrared (IR) filter. In the experiment with this hardware constitution, we investigate the effects of the pupil detection and tracking by image processing techniques for a human eye-computer interaction system.
70678280	When an airbag deploys on a rear-facing infant seat, it can injure or kill the infant. When an airbag deploys on an empty seat, the airbag and the money to replace it are wasted. We have shown that video images can be used to determine whether or not to deploy the passenger-side airbag in a crash. Images of the passenger seat, taken from a video camera mounted inside the vehicle, can be used to classify the seat as either empty, containing a rear-facing infant seat, or occupied. Our first experiment used a single, monochrome video camera. The system was automatically trained on a series of test images. Using a principle components (eigenimages) nearest neighbor classifier, it achieved a correct classification rate of 99.5% on a test of 910 images. Our second experiment used a pair of monochrome video cameras to compute stereo disparity (a function of 3D range) instead of intensity images. Using a similar algorithm, the second approach achieved a correct classification rate of 95.1% on a test of 890 images. The stereo technique has the advantage of being less sensitive to illumination, and would likely work best in a real system.