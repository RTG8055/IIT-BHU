761A6464	Tuning game difficulty prior to release requires careful consideration. Players can quickly lose interest in a game if it is too hard or too easy. Assessing how players will cope prior to release is often inaccurate. However, modern games can now collect sufficient data to perform large scale analysis post deployment and update the product based on these insights. AI Factory Spades is currently the top rated Spades game in the Google Play store. In collaboration with the developers, we have collected gameplay data from 27 592 games and statistics regarding wins/losses for 99 866 games using Google Analytics. Using the data collected, this study analyses the difficulty and behavior of an Information Set Monte Carlo Tree Search player we developed and deployed in the game previously. The methods of data collection and analysis presented in this study are generally applicable. The same workflow could be used to analyze the difficulty and typical player or opponent behavior in any game. Furthermore, addressing issues of difficulty or nonhuman-like opponents postdeployment can positively affect player retention.
7A3CAFFC	Reactive navigation based on task decomposition is an effective means for producing robust navigation in complex domains. By incorporating various forms of knowledge, this technique can be made considerably more flexible. Behavioral and perceptual strategies which are represented in a modular form and configured to meet the robot's mission and environment add considerable versatility. A priori world knowledge, when available, can be used to configure these strategies in an efficient form. Dynamically acquired world models can be used to circumvent certain pitfalls that representationless methods are subject to.The Autonomous Robot Architecture (AuRA) is the framework within which experiments in the application of knowledge to reactive control are conducted. Actual robot experiments and simulation studies demonstrate the flexibility and feasibility of this approach over a wide range of navigational domains.
5DF09841	Game playing and genetic algorithms (GAs) are two important topics in artificial intelligence (AI). In this work we employ network tournament to assist in teaching these concepts associated with AI. Three exercises that implement a game-playing program are designed to help students learn relevant topics in AI. The first exercise involves game theory, e.g. mini-max search and alpha-beta pruning. The second exercise helps students understand the critical nature of a good heuristic function in game playing. And, the third exercise introduces GAs to learn a heuristic function. In addition to these exer- cises, this work also designs several programming toolkits to help students complete their exercises, such as a network tournament interface, a graphical man-machine inter- face, and a genetic algorithm based game-playing engine. These exercises encompass pertinent topics involving artificial intelligence and the network tournament. The net- work tournament provides a relatively easy means of Othello competition, and has merit in improving students' motivation for learning relevant topics. 
8592E419	We try to help Paul and Pinocchio to deserve their trip to Venice where they are planning to continue a 50-year long competition in the 20 question game with lies.
81060467	Reinforcement learning is an unsupervised machine learning method in the area of Artificial Intelligence. It presents well performance in simulation of the thinking ability of human. However, it needs a trial-and-error process to achieve the goal. In the research field of game AI, it is a good approach to allow the non-player-characters (NPCs) of digital games to become more humanity. In this paper, we try to build a Tank-battle computer game and use the methodology of reinforcement learning for the NPCs (tanks). The goal of this paper is to make this game become more interesting from the enhanced interactions with these intelligent NPCs.
7BCB7705	Designing fighting game AI has been a challenging problem because the program should react in real-time and require expert knowledge on the combination of actions. In fact, most of entries in 2013 fighting game AI competition were based on expert rules. In this paper, we propose an automatic policy learning method for the fighting game AI bot. In the training stage, the AI continuously plays fighting games against 12 bots (10 from 2013 competition entries and 2 examples) and stores massive play data (about 10 GB). UCB1 is used to collect the data actively. In the testing stage, the agent searches for the similar situations from the logs and selects skills with the highest rewards. In this way, it is possible to construct the fighting game AI with minimum expert knowledge. Experimental results show that the learned agent can defeat two example bots and show comparable performance against the winner of 2013 competition.
640E635F	Maximum Satisfiability (MaxSAT) is a well-known optimization pro- blem, with several practical applications. The most widely known MAXS AT algorithms are ineffective at solving hard problems instances from practical application domains. Recent work proposed using efficient Boolean Satisfiability (SAT) solvers for solving the MaxSAT problem, based on identifying and eliminating unsatisfiable subformulas. However, these algorithms do not scale in practice. This paper analyzes existing MaxSAT algorithms based on unsatisfiable subformula identification. Moreover, the paper proposes a number of key optimizations to these MaxSAT algorithms and a new alternative algorithm. The proposed optimizations and the new algorithm provide significant performance improvements on MaxSAT instances from practical applications. Moreover, the efficiency of the new generation of unsatisfiability-based MaxSAT solvers becomes effectively indexed to the ability of modern SAT solvers to proving unsatisfiability and identifying unsatisfiable subformulas. 
791CE782	Machine learning techniques can increase the power/expertise and improve the problem-solving efficiency of artificial intelligence applications for complex problems such as reactive scheduling in manufacturing operations management. Reactivity requires adaptively changing behaviour of a performance system that can best be supported by learning from experience. Reactive scheduling is considered in this paper with two options for reactive repair/proactive schedule adjustment to compensate for/prevent the effects of disturbances during the execution time of a current schedule. With a supervisory control perspective to shop floor operations, reactive scheduling of the two options above is embodied as one major intelligent supervisory function in a supervisory reactive scheduler SUPREACT and complemented by a cognitive learning approach to improve search space/control pattern-matching knowledge for guiding iterative schedule repair actions. In a brief overview, some learning approaches that comprise past experience for future re-use in reactive scheduling are referred to. The paper then presents a combined rule- and case-based reasoning/learning approach to opportunistic reactive scheduling based on a blackboard framework of the system's Expert Supervisor Unit, which also supports human integration into supervisory control of executed processes. Inherent learning ability of the case-based component allows to capture new schedule repair/search control knowledge, including also human preferences, and thus improve the system's reactive/proactive schedule repair efficiency in response to unexpected events/performance deterioration trends during the execution of predictive schedules in manufacturing shop floors.
7AFE5DB2	 General game players are computer systems able to play strategy games based solely on formal game descriptions supplied at "runtime" (n other words, they don't know the rules until the game starts). Unlike specialized game players, such as Deep Blue, general game players cannot rely on algorithms designed in advance for specific games; they must discover such algorithms themselves. General game playing expertise depends on intelligence on the part of the game player and not just intelligence of the programmer of the game player.GGP is an interesting application in its own right. It is intellectually engaging and more than a little fun. But it is much more than that. It provides a theoretical framework for modeling discrete dynamic systems and defining rationality in a way that takes into account problem representation and complexities like incompleteness of information and resource bounds. It has practical applications in areas where these features are important, e.g., in business and law. More fundamentally, it raises questions about the nature of intelligence and serves as a laboratory in which to evaluate competing approaches to artificial intelligence.
765C5ADD	Othello is a recent addition to the collection of games that have been examined within artificial intelligence. Advances have been rapid, yielding programs that have reached the level of world-championship play. This article describes the current champion Othello program, iago. The work described here includes: (1) a task analysis of Othello; (2) the implementation of a program based on this analysis and state-of-the-art AI game-playing techniques; and (3) an evaluation of the program's performance through games played against other programs and comparisons with expert human play.
7E64BC71	This paper describes a novel fuzzy rule-based modeling approach for some industrial processes. Structure identification is realized by clustering and support vector machines. When the process is slow, fuzzy rules can be obtained automatically. Parameters identification uses the techniques of fuzzy neural networks. A time-varying learning rate assures stability of the modeling error.
7D9EA8FF	The expected-outcome model, in which the proper evaluation of a game-tree node is the expected value of the game's outcome given random play from that node on, is proposed. Expected outcome is considered in its ideal form, where it is shown to be a powerful heuristic. The ability of a simple random sampler that estimates expected outcome to outduel a standard Othello evaluator is demonstrated. The sampler is combined with a linear regression procedure to produce efficient expected-outcome estimators. Overall, the expected-outcome model of two-player games is shown to be precise, accurate, easily estimable, efficiently calculable, and domain-independent.
75D6F8E1	In this paper, we are proposing a method which is different from many practical computer programs have been developed to exhibit useful types of learning. For problems such as speech recognition, different algorithms based on machine learning outperform all other approaches that have been attempted to date. In the field known as data mining, machine learning algorithms are being used commonly to discover valuable knowledge from large commercial databases containing equipment maintenance records, loan applications, financial transactions, medical records etc. Thus, it seems inevitable that machine learning will play an integral role in computer science and computer technology.In this paper, modeling and designing of a general learning system is proposed that presents new machine learning procedures used to arrive at "knowledgeable" static evaluators for checker board positions. The static evaluators are compared with each other, and with the linear polynomial using two different numerical indices reflecting the extent to which they agree with the choices of checker experts in the course of tabulated book games. The new static evaluators are found to perform about equally well, despite the relative simplicity of the second; and they perform noticeably better than the linear polynomial.
7ECBDEA2	We present a new local strategy to solve incremental learning tasks. It allows to avoid re‐learning of all the parameters by selecting a working subset where the incremental learning is performed. While this procedure can be applied to various schemes (hybrid decision trees, committee machines), we illustrate it with Support Vector Machines based on local kernel. We derive and compare three methods to perform the selection procedure: two of them take advantage of the estimation of generalization error by using theoretical error bounds devoted to SVM. Experimental simulations on three typical datasets of machine learning give promising results.
59ACF6B9	Monte-Carlo Tree Search and Upper Confidence Bounds provided huge improvements in computer-Go. In this paper, we test the generality of the approach by experimenting on the game, Havannah, which is known for being especially difficult for computers. We show that the same results hold, with slight differences related to the absence of clearly known patterns for the game of Havannah, in spite of the fact that Havannah is more related to connection games like Hex than to territory games like Go.
80B16C7A	The present article is a novel attempt in providing an exhaustive survey of neuro-fuzzy rule generation algorithms. Rule generation from artificial neural networks is gaining in popularity in recent times due to its capability of providing some insight to the user about the symbolic knowledge embedded within the network. Fuzzy sets are an aid in providing this information in a more human comprehensible or natural form, and can handle uncertainties at various levels. The neuro-fuzzy approach, symbiotically combining the merits of connectionist and fuzzy approaches, constitutes a key component of soft computing at this stage. To date, there has been no detailed and integrated categorization of the various neuro-fuzzy models used for rule generation. We propose to bring these together under a unified soft computing framework. Moreover, we include both rule extraction and rule refinement in the broader perspective of rule generation. Rules learned and generated for fuzzy reasoning and fuzzy control are also considered from this wider viewpoint. Models are grouped on the basis of their level of neuro-fuzzy synthesis. Use of other soft computing tools like genetic algorithms and rough sets are emphasized. Rule generation from fuzzy knowledge-based networks, which initially encode some crude domain knowledge, are found to result in more refined rules. Finally, real-life application to medical diagnosis is provided.
7AA410BA	Geometric assumption and verification with RANSAC has become a crucial step for corresponding to local features due to its wide applications in biomedical feature analysis and vision computing. However, conventional RANSAC is very time-consuming due to redundant sampling times, especially dealing with the case of numerous matching pairs. This paper presents a novel preprocessing model to explore a reduced set with reliable correspondences from initial matching dataset. Both geometric model generation and verification are carried out on this reduced set, which leads to considerable speedups. Afterwards, this paper proposes a reliable RANSAC framework using preprocessing model, which was implemented and verified using Harris and SIFT features, respectively. Compared with traditional RANSAC, experimental results show that our method is more efficient.
752E19F8	Cellular contrast was demonstrated for the first time in lensfree on-chip holography by employing antibody-conjugated gold and silver nanoparticles (NPs) as contrast agents. Cytometric differentiation of CD4+ and CD8+ lymphocytes was achieved, after NP labeling, using machine learning algorithms.
5E0CB5E5	A functional MOS transistor called a neuron MOSFET (vMOS) which simulates the function of biological neurons is discussed. A method of constructing neural network LSIs that have a self-learning capability using the neuron MOSFET is given. The key is the implementation of a synaptic connection which changes its weight according to various learning algorithms. In addition, the synapse must be free from standby power dissipation and be as small as possible.
783F2E58	This paper presents some aspects of a special multimodality in a two-person game-renju (a Japanese alignment game on a board); the authors believe such a renju multimodality will surely be a new base of ideas and understanding of adaptive processes in artificial intelligence requiring multimodal optimum search. After introducing renju rules, and basic renju technical terms and strategies, this paper develops the necessary concepts such as distance between any two points, four directions, string, jumstring, costring, dead (jum-)string,and criterion function of a placed stone (piece or chip) on a renju board. The essential aspects of renju multimodality studied here are as follows: possible values of the criterion function, the realized number of the highest peak (lowest valley) formed by the criterion function within a game, height of maximum peak vs. the issue of the game, possible number of the highest peak (lowest valley), illustration of the criterion function, developing processes of peak and valley, adaptive mobility of the optimum point, and practical appearance of the multimodality of renju problems.
7986A229	We attack the problem of game balancing by using a coevolutionary algorithm to explore the space of possible game strategies and counter strategies. We define balanced games as games which have no single dominating strategy. Balanced games are more fun and provide a more interesting strategy space for players to explore. However, proving that a game is balanced mathematically may not be possible and industry commonly uses extensive and expensive human testing to balance games. We show how a coevolutionary algorithm can be used to test game balance and use the publicly available continuous state, capture-the-flag CaST game as our testbed. Our results show that we can use coevolution to highlight game imbalances in CaST and provide intuition towards balancing this game. This aids in eliminating dominating strategies, thus making the game more interesting as players must constantly adapt to opponent strategies.
808AC5BF	MLF (Machine Learning Framework) is a tool for data mining developed at Software Competence Center Hagenberg, Austria. MLF is a library of algorithms and cannot be used as a standalone program and it uses Mathematica/spl reg/ as a front-end. In this paper, we present a possibility of using Microsoft Excel as a front-end for MLF. The benefits of choosing Microsoft Excel are also presented.
5B14EB95	With the recent success of Monte-Carlo tree search algorithms in Go and other games, and the increasing number of cores in standard CPUs, the efficient parallelization of the search has become an important issue. We present a new lock-free parallel algorithm for Monte-Carlo tree search which takes advantage of the memory model of the IA-32 and Intel-64 CPU architectures and intentionally ignores rare faulty updates of node values. We show that this algorithm significantly improves the scalability of the Fuego Go program
7C5096E9	In this paper, a method of hyperspectral unmixing for the linear regression model is introduced. The proposed algorithm employs an adaptive lasso problem using the alternating direction method of multipliers (ADMM) for unmixing process. Indeed, we formulate a weighted l1 norm problem under the reasonable given error to reconstruct the fractional abundances and to avoid inconsistent endmember selection in a sparse semi-supervised hyperspectral imaging process. We show that this problem can be efficiently solved by appropriate selection of functions and parameters appearing in the ADMM approach. First, we enforce both non-negativity and full additivity constraints of the abundance fractions in the objective function. Then, we apply the ADMM algorithm to solve the acquired optimization problem. Our simulations show that the proposed algorithms outperform the state-of-the-art methods in terms of mean square error and reconstruction signal-to-noise-ratio with reasonably reduced computational costs.
7636FDD8	This paper presents a concept of predicting lightning with the data from radiosonde and a more reliable dataset of lightning occurances from Tenaga Nasionl Berhad Research Sdn Berhad (TNBR). The location of interest in this research is Kuala Lumpur International Airport (KLIA). The engine used for prediction is of a Neural Network Back Propogation type (ANN-BP). The initial results show that the combination of datasets and engine are workable, however the prediction results seem to be more biased towards lightning days as compared to non-lightning days.
633C1E93	Multi-agent systems arise in several domains of engineering and can be used to solve problems which are difficult for an individual agent to solve. Strategies for team decision problems, including optimal control, -player games (-infinity control and non-zero sum), and so on are normally solved for off-line by solving associated matrix equations such as the coupled Riccati equations or coupled Hamilton–Jacobi equations. However, using that approach players cannot change their objectives online in real time without calling for a completely new off-line solution for the new strategies. Therefore, in this paper we bring together cooperative control, reinforcement learning, and game theory to present a multi-agent formulation for the online solution of team games. The notion of graphical games is developed for dynamical systems, where the dynamics and performance indices for each node depend only on local neighbor information. It is shown that standard definitions for Nash equilibrium are not sufficient for graphical games and a new definition of “Interactive Nash Equilibrium” is given. We give a cooperative policy iteration algorithm for graphical games that converges to the best response when the neighbors of each agent do not update their policies, and to the cooperative Nash equilibrium when all agents update their policies simultaneously. This is used to develop methods for online adaptive learning solutions of graphical games in real time along with proofs of stability and convergence.
826382AD	Online search in games has been a core interest of artificial intelligence. Search in imperfect information games (e.g., Poker, Bridge, Skat) is particularly challenging due to the complexities introduced by hidden information. In this paper, we present Online Outcome Sampling, an online search variant of Monte Carlo Counterfactual Regret Minimization, which preserves its convergence to Nash equilibrium. We show that OOS can overcome the problem of non-locality encountered by previous search algorithms and perform well against its worst-case opponents. We show that exploitability of the strategies played by OOS decreases as the amount of search time increases, and that preexisting Information Set Monte Carlo tree search (ISMCTS) can get more exploitable over time. In head-to-head play, OOS outperforms ISMCTS in games where non-locality plays a significant role, given a sufficient computation time per move.
7BAAD65C	The article develops the decision rules to win each set of the Chinese chess game using artificial intelligent and evaluation algorithm, and presents the movement scenarios of the chesses using mobile robots on the grid based platform. Artificial intelligent is often applied in computer chess game, and programs the movement method to win each set for a player (red side or black side). We play the Chinese chess game using the mouse according to the game rules on the user interface. The supervised computer controls mobile robots according to the programmed motion paths of the chess moving on the platform via wireless radio frequency (RF) interface, and selects which chess moving to the best position to win the set using evaluation algorithm and artificial intelligent method. Then we use simulation method to display the motion paths of the assigned chesses on the user interface. The supervised computer implements the simulation results on the chessboard platform using mobile robots. Mobile robots move on the chessboard platform to be guided to move on the centre line of the corridor, and avoid the obstacles (chesses), and detect the cross point of the platform using three reflective infrared (IR) modules.
5DC974BD	Monte-Carlo Tree Search (MCTS) is a new best-first search method that started a revolution in the field of Computer Go. Parallelizing MCTS is an important way to increase the strength of any Go program. In this article, we discuss three parallelization methods for MCTS: leaf parallelization, root parallelization, and tree parallelization. To be effective tree parallelization requires two techniques: adequately handling of (1) local mutexes and (2) virtual loss. Experiments in 13×13 Go reveal that in the program Mango root parallelization may lead to the best results for a specific time setting and specific program parameters. However, as soon as the selection mechanism is able to handle more adequately the balance of exploitation and exploration, tree parallelization should have attention too and could become a second choice for parallelizing MCTS. Preliminary experiments on the smaller 9×9 board provide promising prospects for tree parallelization.
7F96DA91	This paper presents a novel method for background estimation in a video sequence from the function estimation point of view. The proposed algorithm, called Kernel-based Background Learning (KBL), is designed based on kernel machine joint with learning schemes. In order to estimate background using KBL algorithm, we first interpret foreground samples as outliers relative to the background ones and so propose an Outlier Separator (OS). Then, the obtained results of OS algorithm are employed in the KBL method in order to train and estimate background in each pixel. Experimental results show the high accuracy and effectiveness of the proposed method in background estimation and foreground detection for the scenes including moving backgrounds, camera shakes, and non-empty backgrounds. Index Terms—Background estimation, outlier separator, kernel-based background learning. 
8025D261	NeuroFAST is an on-line fuzzy modeling learning algorithm, featuring high function approximation accuracy and fast convergence. It is based on a first-order Takagi-Sugeno-Kang (TSK) model, where the consequence part of each fuzzy rule is a linear equation. Structure identification is performed by a fuzzy adaptive resonance theory (ART)-like mechanism, assisted by fuzzy rule splitting and adding procedures. The well known /spl delta/ rule continuously performs parameter identification on both premise and consequence parameters. Simulation results indicate the potential of the algorithm. It is worth noting that NeuroFAST achieves a remarkable performance in the Box and Jenkins gas furnace process, outperforming all previous approaches compared.
7DF54FD2	Monte Carlo tree search (MCTS) is an AI technique that has been successfully applied to many deterministic games of perfect information. This paper investigates the application of MCTS methods to games with hidden information and uncertainty. In particular, three new information set MCTS (ISMCTS) algorithms are presented which handle different sources of hidden information and uncertainty in games. Instead of searching minimax trees of game states, the ISMCTS algorithms search trees of information sets, more directly analyzing the true structure of the game. These algorithms are tested in three domains with different characteristics, and it is demonstrated that our new algorithms outperform existing approaches to handling hidden information and uncertainty in games.
7E219278	This paper addresses the control problem of masterslave systems which involve severe modeling errors and other high-level uncertainties, using neural networks. The solution approach is based on a recent teleoperator control scheme of S. Lee and H.S. Lee (1993, 1994), which is suitably enhanced such that to become capable of compensating the uncertainties. The class of radial-basis functions (RBF) neural networks are employed in a multipartitioned neural network architecture, and a special learning scheme is adopted which distributes the learning error to each subnetwork and allows online learning. The effectiveness of the present RBF neurocontroller was investigated through extensive simulation and compared to that of MLP (multilayer perceptron) neurocontroller and a robust sliding-mode controller representative.
7DA1591F	This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
7E1A2F16	Fuzzy Actor-Critic Learning (FACL) and Fuzzy Q-Learning (FQL) are reinforcement learning methods based on dynamic programming (DP) principles. In the paper, they are used to tune online the conclusion part of fuzzy inference systems (FIS). The only information available for learning is the system feedback, which describes in terms of reward and punishment the task the fuzzy agent has to realize. At each time step, the agent receives a reinforcement signal according to the last action it has performed in the previous state. The problem involves optimizing not only the direct reinforcement, but also the total amount of reinforcements the agent can receive in the future. To illustrate the use of these two learning methods, we first applied them to a problem that involves finding a fuzzy controller to drive a boat from one bank to another, across a river with a strong nonlinear current. Then, we used the well known Cart-Pole Balancing and Mountain-Car problems to be able to compare our methods to other reinforcement learning methods and focus on important characteristic aspects of FACL and FQL. We found that the genericity of our methods allows us to learn every kind of reinforcement learning problem (continuous states, discrete/continuous actions, various type of reinforcement functions). The experimental studies also show the superiority of these methods with respect to the other related methods we can find in the literature.
7E7B544D	This paper studies a method for learning a discriminative visual codebook for various computer vision tasks such as image categorization and object recognition. The performance of various computer vision tasks depends on the construction of the code book which is a table of visual-words (i.e. codewords). This paper proposed a learning criterion for constructing a discriminative codebook, and it is solved by the homonym scheme which splits codeword regions by labels. A codebook is learned based on the proposed homonym scheme such that its histogram can be used to discriminate objects of different labels. The traditional codebook based on the k-means is compared against the learned codebook on two well-known datasets (Caltech 101, ETH-80) and a dataset we constructed using google images. We show that the learned codebook consistently outperforms the traditional codebook.
814FC0FA	The aim of general game playing (GGP) is to create intelligent agents that can automatically learn how to play many different games at an expert level without any human intervention. The traditional design model for GGP agents has been to use a minimax-based game-tree search augmented with an automatically learned heuristic evaluation function. The first successful GGP agents all followed that approach. In this paper, we describe CadiaPlayer, a GGP agent employing a radically different approach: instead of a traditional game-tree search, it uses Monte Carlo simulations for its move decisions. Furthermore, we empirically evaluate different simulation-based approaches on a wide variety of games, introduce a domain-independent enhancement for automatically learning search-control knowledge to guide the simulation playouts, and show how to adapt the simulation searches to be more effective in single-agent games. CadiaPlayer has already proven its effectiveness by winning the 2007 and 2008 Association for the Advancement of Artificial Intelligence (AAAI) GGP competitions.
662CC8CF	Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations, and can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a MonteCarlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity methods. This algorithm was implemented in a 9×9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.
7E18DF77	This work proposes the use of machine learning for the creation of a basic library of experiences, which will be used for the generation of emergent behaviors for characters in a strategy game. In order to create a high diversification of the agents' story elements, the characteristics of the agents are manipulated based on their adaptation to the environment and interaction with enemies. We start by defining important requirements that should be observed when modeling the instances. Then, we propose a new architecture paradigm and suggest what would be the most appropriate classification algorithm for this architecture. Results are obtained with an implementation of a prototype strategy game, called Darwin Kombat, which validated the definition of the best classifier.
5A53E63D	In Artificial Intelligence, for practical applications, we often have to manage over-constrained systems of constraints. So, a model based on the formalism of finite Constraint Satisfaction Problems (CSPs) [14] has been proposed with Dynamic CSPs (DCSPs) to handle this kind of problems [10][11]. Some classical techniques defined in the field of CSPs are usable in DCSPs, but the management of over-constrained system with DCSPs induces new problems. The purpose of this paper is to introduce an efficient way to solve DCSPs based on a logical approach. We use Ordered Binary Decision Diagrams (OBDDs) [3] and propose a particular coding for dynamicity. We show that our approach allows to solve some major questions in the field of DCSP, particularly consistency maintenance. This kind of problems is naturally expressed as a problem of optimal path computing in weighted graphs. Moreover, we shall see that the problem of finding optimal solutions can be solved easily and efficiently by our approach. One important problem in OBDD is the amount of memory required to represent the OBDD. In the worst case, this amount is in O(2N) where N is the number of propositional variables for static CSPs. We prove here that, if the number of dynamic constraints is m and if n is the number of variables in the problem, the size of OBDD is bounded by O(m×2n). First experimental results attest the interest of the approach.
7E71B409	Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple non-linear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMs) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.
80B93A3F	An Emotion-based Synthetic Character (ESC) is proposed to enhance game entertainment and to highly personalize human-machine communication in games. The model of the ESC integrates emotional system with attention, intention and facial expression system. Forgetting characteristic of human is also simulated to make the ESC be equal to player characters. Fuzzy representation is used to capture the inherent uncertainty of emotion and cognition. As a result, an experiment implements the model.
5856016D	In this paper we describe our experience in applying KAoS services to ensure policy compliance for Semantic Web Services workflow composition and enactment. We are developing these capabilities within the context of two applications: Coalition Search and Rescue (CoSAR-TS) and Semantic Firewall (SFW). We describe how this work has uncovered requirements for increasing the expressivity of policy beyond what can be done with description logic (e.g., role-value-maps), and how we are extending our representation and reasoning mechanisms in a carefully controlled manner to that end. Since KAoS employs OWL for policy representation, it fits naturally with the use of OWL-S workflow descriptions generated by the AIAI I-X planning system in the CoSAR-TS application. The advanced reasoning mechanisms of KAoS are based on the JTP inference engine and enable the analysis of classes and instances of processes from a policy perspective. As the result of analysis, KAoS concludes whether a particular workflow step is allowed by policy and whether the performance of this step would incur additional policy-generated obligations. Issues in the representation of processes within OWL-S are described. Besides what is done during workflow composition, aspects of policy compliance can be checked at runtime when a workflow is enacted. We illustrate these capabilities through two application examples. Finally, we outline plans for future work.
7FE9C903	Explanations for decisions made by a policy framework allow end users to understand how the results were obtained, increase trust in the policy decision and enforcement process, and enable policy administrators to ensure the correctness of the policy. In our framework, an explanation for any statement including a policy decision is a representation of the list of reasons (known as dependencies) associated with its derivation. Dependency tracking involves maintaining the list of reasons (statements and rules) for the derivation of a new statement. In this paper, we describe our policy approach that (i) provides explanations for policy decisions, (ii) provides more efficient and expressive reasoning through the use of nested sub-rules and goal direction, and (iii) is grounded in Semantic Web technologies. We discuss the characteristics of our approach and provide a brief overview of the AIR policy language that implements it. We also discuss how relevant explanation information is identified and presented to end users and describe our preliminary graphical user interface.
717F35F8	We consider how cue-reading, sensory-manipulation, and signalling games may initially evolve from ritualized decisions and how more complex games may evolve from simpler games by polymerization, template transfer, and modular composition. Modular composition is a process that combines simpler games into more complex games. Template transfer, a process by which a game is appropriated to a context other than the one in which it initially evolved, is one mechanism for modular composition. And polymerization is a particularly salient example of modular composition where simpler games evolve to form more complex chains. We also consider how the evolution of new capacities by modular composition may be more efficient than evolving those capacities from basic decisions. 
77C70C3D	Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory.
77C70C3D	Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory.
7ED6D936	Equilibrium solutions are characterized for a class of two-layer differential games in which one player, P/sub 2/, controls a continuous-time deterministic system while another player, P/sub 1/, intervenes only at random stopping times when he or she can impulsively modify the state and the mode of the lower layer system. They explore the case in which the objectives of the two layers do not coincide totally. This model is also related to the theory of piecewise deterministic systems. Equilibria can be defined and characterized by reformulating this differential game as a noncooperative sequential Markov game with continuous state and action spaces. A precise definition of this class of systems is given and a reformulation in a sequential game format is presented. An existence proof for epsilon -equilibria and a computational approach are presented.
5F1925FD	Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest.This dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task. The results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.
775D7C28	Frolog is a text-adventure game whose goal is to serve as a laboratory for testing pragmatic theories of accommodation. To this end, rather than implementing ad-hoc mechanisms for each task that is neces sary in such a conversational agent, Frolog integrates recently developed tools from computational linguistics, theorem proving and artiﬁcial intelligence planning. 
7F785279	Monte-Carlo tree search is a new best-first tree search algorithm that triggered a revolution in the computer Go world. Developing good parallel Monte-Carlo tree search algorithms is importan because single processor’s performance cannot be expected to increase as used to. A novel parallel Monte-Carlo tree search algorithm is proposed. A tree searcher runs on a client computer and multiple Monte-Carlo simulators run on other computers (simulation servers) on a network. The tree searcher broadcasts a position being simulated to every simulator, which then simulates the game from the position to the end and sends the result back to the searcher, if not busy. The statistical information in the search tree is updated by the searcher according to the result. This algorithm can run on a loosely coupled heterogeneous computer cluster, consists of inexpensive personal computers and game consoles, on a moderate speed network and allows users to connect or disconnect the servers on-the-fly, in contrast to ones run on an expensive HPC cluster. Experiments using four quad-core Linux personal computers on a private Gigabit Ethernet LAN show its performance scales well.
8045FD94	Requirements engineering for video games must address a wide range of functional and non-functional requirements. Video game designers are most concerned with capturing and representing the player experience: the means by which the player's consciousness is cognitively engaged while simultaneously inducing emotional responses. We show that emotional requirements can be expressed in two parts: as the emotional intent of the designer and the means by which the designer expects to induce the target emotional state. Spatial and temporal qualifiers on intent and means may also be required. We introduce emotional terrain maps, emotional intensity maps, and emotion timelines as visual mechanisms for capturing and expressing emotional requirements. Using a first-person shooter example, we show that these mechanisms can express the desired emotional requirements while providing support for spatial and temporal qualifiers.
7714A1B4	A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.
7EF58F8A	We present a robust and efficient technique for matching dense sets of points undergoing non-rigid spatial transformations. Our main intuition is that the subset of points that can be matched with high confidence should be used to guide the matching procedure for the rest. We propose a novel algorithm that incorporates these high-confidence matches as a spatial prior to learn a discriminative subspace that simultaneously encodes both the feature similarity as well as their spatial arrangement. Conventional subspace learning usually requires spectral decomposition of the pair-wise distance matrix across the point-sets, which can become inefficient even for moderately sized problems. To this end, we propose the use of random projections for approximate subspace learning, which can provide significant time improvements at the cost of minimal precision loss. This efficiency gain allows us to iteratively find and remove high-confidence matches from the point sets, resulting in high recall. To show the effectiveness of our approach, we present a systematic set of experiments and results for the problem of dense non-rigid image-feature matching.
7EF20A17	In this paper, a modified TS-type neuro-fuzzy system (MTSNFS) for on-line identification is proposed, which possesses six layers of neurons to perform the fuzzy inference. A modified self-organizing competitive learning algorithm with capabilities of dynamical rules recruitment and cancellation is proposed for structure identification. A hybrid learning algorithm combining recursive least squares (RLS) estimation and ordered derivative learning is used for parameter estimation. Both the structure and parameters could be automatically determined online without a priori knowledge. Comparisons with other related works are made via identification of Box-Jenkins furnance. Identification of bed temperature of a circulating fluidized bed boiler using the MTSNFS is also presented in this paper. The results demonstrate that the proposed identification approach is of high accuracy and compactness, and suitable for on-line modeling and prediction.
7D3C4469	FUEGO is both an open-source software framework and a state-of-the-art program that plays the game of Go. The framework supports developing game engines for full-information two-player board games, and is used successfully in a substantial number of projects. The FUEGO Go program became the first program to win a game against a top professional player in 9 × 9 Go. It has won a number of strong tournaments against other programs, and is competitive for 19 × 19 as well. This paper gives an overview of the development and current state of the FUEGO project. It describes the reusable components of the software framework and specific algorithms used in the Go engine.
7FA26BB1	In reinforcement learning applications such as autonomous robot navigation, the use of options (macro-operators) instead of low level actions has been reported to produce learning speedup due to a more aggressive exploration of the state space. In this paper we present an evaluation of the use of option policies O/sub S/. Each option policy in this framework is a fixed sequence of actions, depending exclusively on the state in which the option is initiated. This contrasts with option policies O/sub /spl Pi//, more common in the literature and that correspond to action sequences that depend on the states visited during the execution of the options. One of our goals was to analyse the effects of a variation of the action sequence length for O/sub S/ policies. The main contribution of the paper, however, is a study on the use of a termination improvement (TI) technique which allows for the abortion of option execution if a more promising one is found. Experimental results show that TI for O/sub S/ options, whose benefits had already been reported for O/sub /spl Pi// options, can be much more effective - due to its adaptation of the size of the action sequence depending on the state where the option is initiated - than indiscriminately augmenting the option size in order to increase exploration of the state space.