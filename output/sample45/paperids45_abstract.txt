76F2F40D	We propose a new approach for the segmentation of 3-D point clouds into geometric surfaces using adaptive surface models. Starting from an initial configuration, the algorithm converges to a stable segmentation through a new iterative split-and-merge procedure, which includes an adaptive mechanism for the creation and removal of segments. This allows the segmentation to adjust to changing input data along the movie, leading to stable, temporally coherent, and traceable segments. We tested the method on a large variety of data acquired with different range imaging devices, including a structured-light sensor and a time-of-flight camera, and successfully segmented the videos into surface segments. We further demonstrated the feasibility of the approach using quantitative evaluations based on ground-truth data.
80DD0E0A	In this paper a new and efficient supervised method for color image segmentation is presented. This method improves a part of the automatic extraction problem. The basic technique consists in fusing information streaming from three different sources for the same image. The first source uses information coming from only one pixel, using the Mahalanobis distance. The second uses the multidimensional distribution of the three bands in a window centered in each pixel, using the Bhattacharyya distance. And the third employs cooccurrence matrices over the texture cube built around one pixel, using the Bhattacharyya distance again. The Dempster–Shafer theory of evidence is applied in order to fuse the information from the three sources which represent different orders of statistics. This method reveals the importance of applying context and textural properties for the segmentation process. The results show the potential of the method for real images starting from the three RGB bands only
7ECD2584	This paper proposes a novel matching method for realtime finding the correspondences among different images containing the same object. The method utilizes an efficient Kernel Projection scheme to descript the image patch around a detected feature point. In order to achieve invariance and tolerance to geometric distortions, it combines a training stage based on generated synthetic views of the object. The two reliable and efficient methods cooperate together, resulting the core part of our novel multiple view kernel projection method (MVKP). Finally, considering the properties and distribution of the described feature vectors, we search for the best correspondence between two sets of features using a fast filtering vector approximation (FFVA) algorithm, which can be viewed as a fast lower-bound rejection scheme. Extensive experimental results on both synthetic and real data have demonstrated the effectiveness of the proposed approach.
7D57BEC3	This paper describes an approach to the recognition of stacked objects with planar and curved surfaces. The system works in two phases. In the learning phase, a scene containing a single object is shown one at a time. The range data of a scene are obtained by a range finder. The description of each scene is built in terms of properties of regions and relations between them. This description is stored as an object model. In the recognition phase, an unknown scene is described in the same way as in the learning phase. Then the description is matched to the object models so that the stacked objects are recognized sequentially. Efficient matching is achieved by a combination of data-driven and model-driven search processes. Experimental results for blocks and machine parts are shown.
76F730DC	Interactive segmentation, in which a user provides a bounding box to an object of interest for image segmentation, has been applied to a variety of applications in image editing, crowdsourcing, computer vision, and medical imaging. The challenge of this semi-automatic image segmentation task lies in dealing with the uncertainty of the foreground object within a bounding box. Here, we formulate the interactive segmentation problem as a multiple instance learning (MIL) task by generating positive bags from pixels of sweeping lines within a bounding box. We name this approach MILCut. We provide a justification to our formulation and develop an algorithm with significant performance and efficiency gain over existing state-of-the-art systems. Extensive experiments demonstrate the evident advantage of our approach.
780ED305	We present a novel framework for tree-structure embedded density estimation and its fast approximation for mode seeking. The proposed method could find diverse applications in computer vision and feature space analysis. Given any undirected, connected and weighted graph, the density function is defined as a joint representation of the feature space and the distance domain on the graph's spanning tree. Since the distance domain of a tree is a constrained one, mode seeking can not be directly achieved by traditional mean shift in both domain. we address this problem by introducing node shifting with force competition and its fast approximation. Our work is closely related to the previous literature of nonparametric methods. One shall see, however, that the new formulation of this problem can lead to many advantages and new characteristics in its application, as will be illustrated later in this paper.	
58B34D41	Applying computer vision technology to IR (Infra-Red) images for UAV (Unmanned Aerial Vehicle) applications is difficult due to its characteristics which differ from common image processing. By combining visual categorization with low level IR image processing, this paper presents a framework for automatic labeling of IR images in probabilistic manner. We extract the features which contain temperature, texture and orientation information from the IR image, model visual categories by the distribution of features in terms of an extended visual vocabulary, and categorize IR image segments probabilistically. The proposed framework is demonstrated in experiments with high labeling accuracy, for near IR images of urban terrain taken from 100 feet altitude.
7E085EB4	This paper utilizes the periodicity of swing distances to estimate gait period. It shows good adaptability to low quality silhouette images. Gait moment image (GMI) is implemented based on the estimated gait period. GMI is the gait probability image at each key moment in gait period. It reduces the noise of the silhouettes extracted from low quality videos by gait probability distribution at each key moment. Moment deviation image (MDI) is generated by using silhouette images and GMIs. As a good complement of gait energy image (GEI), MDI provides more motion features than the basic GEI. MDI is utilized together with GEI to represent a subject. The nearest neighbor classifier is adopted to recognize subjects. The proposed algorithm is evaluated on the USF gait database, and the performance is compared with the baseline algorithm and two other algorithms. Experimental results show that this algorithm achieves a higher total recognition rate than the other algorithms
78698E02	A dynamic multi-scale neural model for enhancing regions and extracting contours in the colour image segmentation process is proposed. This model combines colour and textural information to coherently enhance images through the operation of two main components: the colour opponent system (COS) and the chromatic segmentation system (CSS). First, the COS module transforms the RGB chromatic input signals into long- (L), middle- (M) and short- (S) wavelength cone activations and luminance signals and then it generates the luminance (black–white), L–M and S–(L+M) opponent channels. The CSS module incorporates contour extraction, double opponency mechanisms and diffusion processes to yield coherent enhancing regions in colour image segmentation. This enhancement allows the region labelling to be more efficient due to a reduction in the uncertainty of the images' region allocation. The CSS module is based on the boundary contour system/feature contour system, extending it to allow colour stimuli processing to obtain general purpose architecture for image segmentation with later applications in computer vision and object recognition. Simulations show the good visual results obtained and the robustness of the architecture when processing images presenting different levels of noise. A benchmark using the Berkeley Segmentation Dataset is included in order to quantify and compare the results obtained. 
7E4E679E	This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine-invariant local patches is extracted from the image. This spatial selection process permits the computation of characteristic scale and neighborhood shape for every texture element. The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces taken from different viewpoints.
813E448B	An approach to the analysis and representation of facial dynamics for recognition of facial expressions from image sequences is presented. The algorithms utilize optical flow computation to identify the direction of rigid and nonrigid motions that are caused by human facial expressions. A mid-level symbolic representation motivated by psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, is demonstrated on a large set of image sequences.
7D563D7B	A strategy for recognizing and locating three-dimensional objects in range data is presented. The strategy combines information derived from models of the objects and edges and surfaces detected in the data to efficiently match objects. Given a set of objects to be found, the set of object features are partitioned into subsets having similar intrinsic properties. An ordered tree of features to be considered is set up for each subset. These search trees are designed to maximize the use of the information as it is obtained and minimize the time required to recognize objects. A detailed example of this approach being used to recognize moderately complex castings in a jumble is presented.
7DBA7924	We propose an efficient online real-time solution for single-camera 3D tracking of rigid objects that can handle large camera displacements, drastic aspect changes, and partial occlusions. While the offline camera registration problem can be considered as essentially solved, robust online tracking remains an open issue because many real-time algorithms described in the literature still lack robustness and are prone to drift and jitter. To solve these problems, we have developed a robust approach to 3D feature matching that can handle wide-baseline matching: our method merges the information from preceding frames in traditional recursive tracking fashion with that provided by a very limited number of keyframes created during an offline stage. This combination results in a system that does not suffer from the above difficulties and can deal with drastic aspect changes. We use augmented reality applications to demonstrate its behavior because they are particularly demanding in terms of tracking performance.
76C4FC8C	This paper describes a scheme for seamlessly stitching together images captured from an aerial platform, in real-time, in order to provide an operator with a larger field-of-view. Both recent images, and images from earlier in a flight are used. To obtain real-time performance several of the latest computer vision techniques are applied: firstly the Bag-of-Words image representation allows overlapping images to be found efficiently, and provides cheap wide-baseline correspondences between them. Secondly the BaySAC robust estimation framework allows images to be registered efficiently from a prior motion model combined with large numbers of potential matches between cheap image patch descriptors. Thirdly an efficient seam-placement algorithm allows the rendering of a visually attractive mosaic. Results are presented on a sequence of high-resolution images captured from a microlight.
7EBFAFB8	Shape retrieval/matching is a very important topic in computer vision. The recent progress in this domain has been mostly driven by designing smart features for providing better similarity measure between pairs of shapes. In this paper, we provide a new perspective to this problem by considering the existing shapes as a group, and study their similarity measures to the query shape in a graph structure. Our method is general and can be built on top of any existing shape matching algorithms. It learns a better metric through graph transduction by propagating the model through existing shapes, in a way similar to computing geodesics in shape manifold. However, the proposed method does not require learning the shape manifold explicitly and it does not require knowing any class labels of existing shapes. The presented experimental results demonstrate that the proposed approach yields significant improvements over the state-of-art shape matching algorithms. We obtained a retrieval rate of 91% on the MPEG-7 data set, which is the highest ever reported in the literature.
7EA7FCB9	The complex extended Gaussian image (CEGI), a 3D object representation that can be used to determine the pose of an object, is described. In this representation, the weight associated with each outward surface normal is a complex weight. The normal distance of the surface from the predefined origin is encoded as the phase of the weight, whereas the magnitude of the weight is the visible area of the surface. This approach decouples the orientation and translation determination into two distinct least-squares problems. The justification for using such a scheme is twofold: it not only allows the pose of the object to be extracted, but it also distinguishes a convex object from a nonconvex object having the same EGI representation. The CEGI scheme has the advantage of not requiring explicit spatial object-model surface correspondence in determining object orientation and translation. Experiments involving synthetic data of two polyhedral and two smooth objects are presented to illustrate the feasibility of this method.
7D02CBD3	In this paper, we propose a novel approach to model shape variations. It encodes sparsity, exploits geometric redundancy, and accounts for the different degrees of local variation and image support. In this context we consider a control-point based shape representation. Their sparse distribution is derived based on a shape model metric learned from the training data, and the ambiguity of local appearance with regard to segmentation changes. The resulting sparse model of the object improves reconstruction and search behavior, in particular for data that exhibit a heterogeneous distribution of image information and shape complexity. Furthermore, it goes beyond conventional image-based segmentation approaches since it is able to identify reliable image structures which are then encoded within the model and used to determine the optimal segmentation map. We report promising experimental results comparing our approach with standard models on MRI data of calf muscles - an application where traditional image-based methods fail - and CT data of the left heart ventricle.
811198B6	Part structure and articulation are of fundamental importance in computer and human vision. We propose using the inner-distance to build shape descriptors that are robust to articulation and capture part structure. The inner-distance is defined as the length of the shortest path between landmark points within the shape silhouette. We show that it is articulation insensitive and more effective at capturing part structures than the Euclidean distance. This suggests that the inner-distance can be used as a replacement for the Euclidean distance to build more accurate descriptors for complex shapes, especially for those with articulated parts. In addition, texture information along the shortest path can be used to further improve shape classification. With this idea, we propose three approaches to using the inner-distance. The first method combines the inner-distance and multidimensional scaling (MDS) to build articulation invariant signatures for articulated shapes. The second method uses the inner-distance to build a new shape descriptor based on shape contexts. The third one extends the second one by considering the texture information along shortest paths. The proposed approaches have been tested on a variety of shape databases, including an articulated shape data set, MPEG7 CE-Shape-1, Kimia silhouettes, the ETH-80 data set, two leaf data sets, and a human motion silhouette data set. In all the experiments, our methods demonstrate effective performance compared with other algorithms
7E46A206	The state-of-the-art in image segmentation builds hierarchical segmentation structures based on analyzing local feature cues in spectral settings. Due to their impressive performance, such segmentation approaches have become building blocks in many computer vision applications. Nevertheless, the main bottlenecks are still the computationally demanding processes of local feature processing and spectral analysis. In this paper, we demonstrate that based on a discrete-continuous optimization of oriented gradient signals, we are able to provide segmentation performance competitive to state-of-the-art on BSDS 500 (even without any spectral analysis) while reducing computation time by a factor of 40 and memory demands by a factor of 10.
8064C0D7	This paper proposes a new object representation, called Connected Segmentation Tree (CST), which captures canonical characteristics of the object in terms of the photometric, geometric, and spatial adjacency and containment properties of its constituent image regions. CST is obtained by augmenting the object’s segmentation tree (ST) with inter-region neighbor links, in addition to their recursive embedding structure already present in ST. This makes CST a hierarchy of region adjacency graphs. A region’s neighbors are computed using an extension to regions of the Voronoi diagram for point patterns. Unsupervised learning of the CST model of a category is formulated as matching the CST graph representations of unlabeled training images, and fusing their maximally matching subgraphs. A new learning algorithm is proposed that optimizes the model structure by simultaneously searching for both the most salient nodes (regions) and the most salient edges (containment and neighbor relationships of regions) across the image graphs. Matching of the category model to the CST of a new image results in simultaneous detection, segmentation and recognition of all occurrences of the category, and a semantic explanation of these results.
5F6E00BA	During the last years, there has been a significant increase in the level of interest in image morphology, full-color image processing, image data compression, image recognition and knowledge based analysis systems for medical images. The present paper describes the implementation and tests the efficiency of algorithms dealing with the issues of segmentation and registration of digital images containing skin lesions. Those steps are considered of great importance in computer based characterization systems as they are responsible for the isolation of pathological findings and the matching of sequential images during follow-up studies in medical imaging.
806FEB8F	Image analysis using texture as a spatial feature can be employed to segment regions of a complex scene or in the classification of surface materials. The relationship between most textural images and their description is mathematically intractable. In this paper the authors propose a new statistical measure, which is not based on a pre-defined formulation. Here, the local information in all directions around a pixel and its neighbourhood is represented in a 'directional RANK-strength' vector. The proposed method leads to texture classification and segmentation methods. Both algorithms have been tested on natural images with results in agreement with perceived ones.
7B527168	Image mosaicing is widely used in computer vision applications. Accurate and consistent alignment of sequence images is the key issue to image mosaicing. In this paper, a globally consistent image mosaicing is proposed by taking account of various uncertainties. The problem of global alignment of a sequence of images is considered as a stochastic estimation problem. The transformation parameters of images are considered as system state. System augmentation model and system observation model are constructed. The global homographies parameters of sequence images are estimated recursively with augmented Kalman filter in a common state vector and covariance matrix. The proposed image alignment method can handle the uncertainty efficiently and is globally consistent. Some experimental results are provided to validate the performance of the proposed method.
7EB2C241	An approach to estimating the motion of the head and facial expressions in model-based facial image coding is presented. An affine nonrigid motion model is set up. The specific knowledge about facial shape and facial expression is formulated in this model in the form of parameters. A direct method of estimating the two-view motion parameters that is based on the affine method is discussed. Based on the reasonable assumption that the 3-D motion of the face is almost smooth in the time domain, several approaches to predicting the motion of the next frame are proposed. Using a 3-D model, the approach is characterized by a feedback loop connecting computer vision and computer graphics. Embedding the synthesis techniques into the analysis phase greatly improves the performance of motion estimation. Simulations with long image sequences of real-world scenes indicate that the method not only greatly reduces computational complexity but also substantially improves estimation accuracy.
7CF79669	Model-based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. The author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved, parameterized objects.
7E7A5930	We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.
7F115563	The motion history image (MHI) approach is a view-based temporal template method which is simple but robust in representing movements and is widely employed by various research groups for action recognition, motion analysis and other related applications. In this paper, we provide an overview of MHI-based human motion recognition techniques and applications. Since the inception of the MHI template for motion representation, various approaches have been adopted to improve this basic MHI technique. We present all important variants of the MHI method. This paper points some areas for further research based on the MHI method and its variants.
7F0EF4AB	We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.
815C8B6C	Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we "plug-in" human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much "head room" there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.
81163150	A view-based approach to the representation and recognition of human movement is presented. The basis of the representation is a temporal template-a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: The first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence. We then develop a recognition method matching temporal templates against stored instances of views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on standard platforms.
805CA599	Forensic terrestrial photogrammetry is one of the most valuable and low-cost resources of spatial data available today. Due to the ephemeral crime scene characteristics, these photographs can often capture information that is never to be seen again. This paper presents a novelty approach for the documentation, analysis, and visualization of crime scenes for which only a single perspective image is available. The photogrammetric process consists of a few well-known steps in close-range photogrammetry: features extraction, vanishing points computation, camera self-calibration, 3D metric reconstruction, dimensional analysis, and interactive visualization. Likewise, the method incorporates a quality control of the different steps accomplished sequentially. As a result, several cases of study are presented in the experimental results section in order to test their viability. The full approach can be applied easily through the free software, sv3DVision, which has been evaluated by a number of police officers, forensic scientists, and forensic educators satisfactorily.
7E6F011B	A research project initiated to study the modeling, analysis, and visualization of nonrigid motion in terms of three important types of objects-heart, human face/head, and fluid-is discussed. Attention is given to modeling and analysis of heart wall motion, human face/head motion analysis and synthesis in model-based image compression, and the evolution of coherent structures in fluid motion. It is believed that techniques from computer vision, can be useful for solving these problems. In particular, the following computer vision techniques appear relevant: 3-D solid modeling, motion estimation, curvature analysis, shape from shading, relational structures, model-based systems, aspect graphs, and silhouette-slide theorems. The goal of the research is to develop concepts, methodologies, and techniques that will be widely applicable to many types of nonrigid motion.
785BBDD3	Algorithms that encode images using a sparse set of basis functions have previously been shown to explain aspects of the physiology of a primary visual cortex (V1), and have been used for applications, such as image compression, restoration, and classification. Here, a sparse coding algorithm, that has previously been used to account for the response properties of orientation tuned cells in primary visual cortex, is applied to the task of perceptually salient boundary detection. The proposed algorithm is currently limited to using only intensity information at a single scale. However, it is shown to out-perform the current state-of-the-art image segmentation method (Pb) when this method is also restricted to using the same information.
6F09A36D	An approach to image feature extraction is proposed. Complex moments of the Gabor power spectrum are used to detect linear, rectangular, hexagonal/triangular, and other structures with very fine to very coarse resolutions. When the method is applied to texture segmentation, good results are obtained.
7C28B8BF	Representing complex three-dimensional objects in a computer involves more than just evaluating its display capabilities. Other factors are the uses and costs of the representation, what operations can be performed on it and, ultimately, how useful it is for computer recognition or description or three-dimensional objects. Many of the questions which are posed arise from the joint consideration of computer graphics and computer vision, and a specific representation hierarchy is proposed for complex objects which makes them amenable to display, manipulation, measurement, and analysis.
7A5332F2	The recognition in image data of viewed patches of spheres, cylinders, and planes in the 3-D world is discussed as a first step to complex object recognition or complex object location and orientation estimation. Accordingly, an image is partitioned into small square windows, each of which is a view of a piece of a sphere, or of a cylinder, or of a plane. Windows are processed in parallel for recognition of content. New concepts and techniques include approximations of the image within a window by 2-D quadric polynomials where each approximation is constrained by one of the hypotheses that the 3-D surface shape seen is either planar, cylindrical, or spherical; a recognizer based upon these approximations to determine whether the object patch viewed is a piece of a sphere, or a piece of a cylinder, or a piece of a plane; lowpass filtering of the image by the approximation. The shape recognition is computationally simple, and for large windows is approximately Bayesian minimum-probability-of-error recognition. These classifications are useful for many purposes. One such purpose is to enable a following processor to use an appropriate estimator to estimate shape, and orientation and location parameters for the 3-D surface seen within a window.
7F10C9C1	A computer vision system has been implemented that can recognize three-dimensional objects from unknown viewpoints in single gray-scale images. Unlike most other approaches, the recognition is accomplished without any attempt to reconstruct depth information bottom-up from the visual input. Instead, three other mechanisms are used that can bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization is used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Second, a probabilistic ranking method is used to reduce the size of the search space during model-based matching. Finally, a process of spatial correspondence brings the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. A high level of robustness in the presence of occlusion and missing data can be achieved through full application of a viewpoint consistency constraint. It is argued that similar mechanisms and constraints form the basis for recognition in human vision.
7E7A0AC6	This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.
7FB1ABBD	Suppose a set of arbitrary (unlabeled) images contains frequent occurrences of 2D objects from an unknown category. This paper is aimed at simultaneously solving the following related problems: 1) unsupervised identification of photometric, geometric, and topological properties of multiscale regions comprising instances of the 2D category, 2) learning a region-based structural model of the category in terms of these properties, and 3) detection, recognition, and segmentation of objects from the category in new images. To this end, each image is represented by a tree that captures a multiscale image segmentation. The trees are matched to extract the maximally matching subtrees across the set, which are taken as instances of the target category. The extracted subtrees are then fused into a tree union that represents the canonical category model. Detection, recognition, and segmentation of objects from the learned category are achieved simultaneously by finding matches of the category model with the segmentation tree of a new image. Experimental validation on benchmark data sets demonstrates the robustness and high accuracy of the learned category models when only a few training examples are used for learning without any human supervision.
585F98C4	Developing effective content recognition methods for diverse imagery continues to challenge computer vision researchers. We present a new approach for document image content categorization using a lexicon of shape features. Each lexical word corresponds to a scale and rotation invariant shape feature that is generic enough to be detected repeatably and segmentation free. We learn a concise, structurally indexed shape lexicon from training by clustering and partitioning feature types through graph cuts. We demonstrate our approach on two challenging document image content recognition problems: 1) The classification of 4,500 Web images crawled from Google Image Search into three content categories — pure image, image with text, and document image, and 2) Language identification of 8 languages (Arabic, Chinese, English, Hindi, Japanese, Korean, Russian, and Thai) on a 1,512 complex document image database composed of mixed machine printed text and handwriting. Our approach is capable to handle high intra-class variability and shows results that exceed other state-of-the-art approaches, allowing it to be used as a content recognizer in image indexing and retrieval systems.
7F7F547B	In this paper, we present a novel generalized Segment-Forest Model (SFM) to segment an object as well as label all the object's semantic parts simultaneously. Segment-Forest is composed by various generated segment trees that act directly on super pixels. Unlike recent works, SFM does not need the prior information like skeleton to capture the core structure of an object, but actively learns the structure from semantic parts during the learning stage. The prediction is an Inference-On-Tree-Voting-On-Forest process, which precludes the expensive computation of the multi-objective optimization. Both training and inference in SFM are extremely efficient, especially compared with traditional multi-objective optimization approaches using in segmentation. We demonstrate superior performance particularly in object articulation, non-rigid deformation on two standard datasets over current state-of-the-art methods. Through further occlusion experiments, we show that SFM can work robustly in the real world.
7F4471F4	The research topic of looking at people, that is, giving machines the ability to detect, track, and identify people and more generally, to interpret human behavior, has become a central topic in machine vision research. Initially thought to be the research problem that would be hardest to solve, it has proven remarkably tractable and has even spawned several thriving commercial enterprises. The principle driving application for this technology is "fourth generation" embedded computing: "smart" environments and portable or wearable devices. The key technical goals are to determine the computer's context with respect to nearby humans (e.g., who, what, when, where, and why) so that the computer can act or respond appropriately without detailed instructions. The paper examines the mathematical tools that have proven successful, provides a taxonomy of the problem domain, and then examines the state of the art. Four areas receive particular attention: person identification, surveillance/monitoring, 3D methods, and smart rooms/perceptual user interfaces. Finally, the paper discusses some of the research challenges and opportunities
7EB8966A	In Content Based Image Retrieval (CBIR), images are segmented to synthesize image information. Among several characteristics like color or edges, texture is useful for segmenting. This paper proposes an intensive multiresolution approach to texture segmentation based on a wavelet transform. The technique delivers schematic descriptions of images. That is to say, it provides the main regions of interest (ROIs) according to image information. Firstly, the process divides images into 2 × 2 blocks. Then, it tracks texture through the multiresolution offered by the wavelet transform to form featuring vectors. Next, a K-means algorithm partitions the texture vector space into clusters. Finally, a connected component extraction delivers the image schema.
7691E99E	A nonlinear weighted averaging filter called bilateral filter depends mainly on two parameters. The spatial distance indicates the size, and the intensity distance indicates the contrast of the features to be preserved, with respect to a center pixel. Its simplicity in behavior and implementation makes it easily adaptable to many applications with minor variations. This paper brings out one such variation by using different scales of the image for computing the parametric functions of the filter. Experimental results demonstrate the efficacy of the proposed technique.
8032F81D	In this paper we present a method for objects tracking in images sequence. This approach is achieved into two main steps. In the first one, we constructed the Local Binary Pattern (LBP) histogram pattern of each image in the sequence and the reference pattern. In the second one, we perform the algorithm by the pattern selected based on a distance measures to find similarity between two histograms. The maximum LBP histogram distance gives best results than the chi-square one. The proposed approach has been tested on synthetic and real sequence images and the results are satisfactory.
7704589C	A general-purpose computer vision system must be capable of recognizing three-dimensional (3-D) objects. This paper proposes a precise definition of the 3-D object recognition problem, discusses basic concepts associated with this problem, and reviews the relevant literature. Because range images (or depth maps) are often used as sensor input instead of intensity images, techniques for obtaining, processing, and characterizing range data are also surveyed.
7EA966C4	Pattern matching is a widely used procedure in signal processing, computer vision, image and video processing. Recently, methods using Walsh Hadamard Transform (WHT) and Gray-Code kernels (GCK) are successfully applied for fast transform domain pattern matching. This paper introduces strip sum on the image. The sum of pixels in a rectangle can be computed by one addition using the strip sum. Then we propose to use the orthogonal Haar transform (OHT) for pattern matching. Applied for pattern matching, the algorithm using strip sum requires O(log u) additions per pixel to project input data of size N × N onto u 2-D OHT basis while existing fast algorithms require O(u) additions per pixel to project the same data onto u 2-D WHT or GCK basis. Experimental results show the efficiency of pattern matching using OHT.
80705B75	Both image enhancement and image segmentation are most practical approaches among virtually all automated image recognition systems. Feature extraction and recognition have numerous applications on telecommunication, weather forecasting, environment exploration and medical diagnosis. The adaptive image contrast stretching is a typical image enhancement approach and watershed segmentation is a typical image segmentation approach. Under conditions of an improper or disturbed illumination, the adaptive contrast stretching should be conducted, which adapts to intensity distributions. Watershed segmentation is a feasible approach to separate different objects automatically, where watershed lines separate the catchment basins. The erosion and dilation operations are essential procedures involved in watershed segmentation. To avoid over-segmentation, the markers for foreground and background can be selected accordingly. Quantitative measures (gray level energy, discrete entropy, relative entropy and mutual information) are proposed to evaluate the actual improvement via two techniques. These methodologies can be easily expanded to many other image processing approaches.
7D19259C	This paper makes two contributions. It provides (1) an operational definition of textons, the putative elementary units of texture perception, and (2) an algorithm for partitioning the image into disjoint regions of coherent bright-ness and texture, where boundaries of regions are defined by peaks in contour orientation energy and differences in texton densities across the contour.Julesz introduced the term texton, analogous to a phoneme in speech recognition, but did not provide an operational definition for gray-level images. Here we re-invent textons as frequently co-occurring combinations of oriented linear filter outputs. These can be learned using a K-means approach. By mapping each pixel to its nearest texton, the image can be analyzed into texton channels, each of which is a point set where discrete techniques such as Voronoi diagrams become applicable.Local histograms of texton frequencies can be used with a X2 test for significant differences to find texture boundaries. Natural images contain both textured and untextured regions, so we combine this cue with that of the presence of peaks of contour energy derived from outputs of odd- and even-symmetric oriented Gaussian derivative filters. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on a statistical test for isotropy of Delaunay neighbors. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.
043987B9	The problem of denoising images is one of the most important and widely studied problems in image processing and computer vision.  Various image filtering strategies based on linear systems, statistics, information theory, and variational calculus, have been effective, but invariably make strong assumptions about the properties of the signal and/or noise.Therefore, they lack the generality to be easily applied to new applications or diverse image collections. This paper describes a novel unsupervised, information-theoretic, adaptive filter (UINTA) that improves the predictability of pixel intensities from their neighborhoods by decreasing the joint entropy between them.  In this way UINTA automatically discovers the statistical properties of the signal and can thereby reduce noise in a wide spectrum of  images  and  applications.   The  paper  describes  the  formulation  required  to  minimize the joint entropy measure, presents several important practical considerations in estimating image-region statistics, and then presents a series of results and comparisons on both real and synthetic data.
812F43D4	Camera calibration and the acquisition of Euclidean 3D measurements have so far been considered necessary requirements for overlaying three-dimensional graphical objects with live video. We describe a new approach to video-based augmented reality that avoids both requirements: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four fiducial points that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is based on the following observation: given a set of four or more noncoplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by: tracking regions and color fiducial points at frame rate; and representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points. Experimental results on two augmented reality systems, one monitor-based and one head-mounted, demonstrate that the approach is readily implementable, imposes minimal computational and hardware requirements, and generates real-time and accurate video overlays even when the camera parameters vary dynamically.
8028B109	Eigendecomposition is a common technique that is performed on sets of correlated images in a number of computer vision and robotics applications. Unfortunately, the computation of an eigendecomposition can become prohibitively expensive when dealing with very high-resolution images. While reducing the resolution of the images will reduce the computational expense, it is not known a priori how this will affect the quality of the resulting eigendecomposition. The work presented here provides an analysis of how different resolution reduction techniques affect the eigendecomposition. A computationally efficient algorithm for calculating the eigendecomposition based on this analysis is proposed. Examples show that this algorithm performs well on arbitrary video sequences.
7D257585	We present a 3D shape-based object recognition system for simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching surfaces by matching points using the spin image representation. The spin image is a data level shape descriptor that is used to match surfaces represented as surface meshes. We present a compression scheme for spin images that results in efficient multiple object recognition which we verify with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trials on 100 scenes.
7EC4E633	This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine Harris and Laplacian regions is found in the image. Each of these regions can be thought of as a texture element having a characteristic elliptic shape and a distinctive appearance pattern. This pattern is captured in an affine-invariant fashion via a process of shape normalization followed by the computation of two novel descriptors, the spin image and the RIFT descriptor. When affine invariance is not required, the original elliptical shape serves as an additional discriminative feature for texture recognition. The proposed approach is evaluated in retrieval and classification tasks using the entire Brodatz database and a publicly available collection of 1,000 photographs of textured surfaces taken from different viewpoints
7C769452	Hybrid image is the image that changes interpretation according to viewing distance. By simply extracting the low and the high spatial frequency bands from two source images, the combination result image can be interpreted differently by viewing distance. This research finds the way to allow construction of hybrid image regardless of the source image's shape. Without the need to carefully pick the two images to be superimposed, hybrid image can be extended to use with any kind of image contents. There are two approaches for accomplishing shape-free hybrid image. Noise-inserted approach forces observers to perceive alternative low frequency image as meaningless noises in a close viewing distance, by manipulating contrast and details in the high frequency image. Color-inserted approach helps attract the visual attention for the high frequency image perception is also introduced in this research. Finally, hybrid image recognition experiment proves that our proposed method yield a better recognition rate over the original method while preserving hybrid image characteristic.
8127172B	Image smoothing, segmentation and registration are three key processing steps in many computer vision applications. In this paper, we present a novel framework for achieving all three seemingly disparate goals simultaneously across multiple images in a unified framework via a single variational principle. The proposed method ensures that the estimated registration is unbiased and all compositions of registration maps are compatible. The solution to the variational problem is achieved efficiently by solving a coupled system of partial differential equations over the common domain on which the registration maps are defined. The effectiveness of the proposed framework is demonstrated on sets of real images.
7DC6766A	This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.
7CFA456A	We introduce a novel method for embedding and detecting a chaotic watermark in the digital spatial image domain, based on segmenting the image and locating regions that are robust to several image manipulations. The robustness of the method is confirmed by experimental results that display the immunity of the embedded watermark to several kinds of attacks, such as compression, filtering, scaling, cropping, and rotation.
7EC290A2	In this paper, we propose a new spatio-temporal gait representation, called Gait Energy Image (GEI), to characterize human walking properties for individual recognition by gait. To address the problem of the lack of training templates, we also propose a novel approach for human recognition by combining statistical gait features from real and synthetic templates. We directly compute the real templates from training silhouette sequences, while we generate the synthetic templates from training sequences by simulating silhouette distortion. We use a statistical approach for learning effective features from real and synthetic templates. We compare the proposed GEI-based gait recognition approach with other gait recognition approaches on USF HumanID Database. Experimental results show that the proposed GEI is an effective and efficient gait representation for individual recognition, and the proposed approach achieves highly competitive performance with respect to the published gait recognition approaches
7D2ADFBF	A new approach to model based object recognition employing multiple views is described. The emphasis is given on the determination of camera viewpoints for succesive views looking for distinguishing features of objects. The distance and direction of the camera are determined separately. The distance is determined by the size of the object and the feature, while the direction is determined by the shape of the feature and the presence of the occluding objects.
7EF9355F	The pose detection of three-dimensional (3-D) objects from two-dimensional (2-D) images is an important issue in computer vision and robotics applications. Specific examples include automated assembly, automated part inspection, robotic welding, and human robot interaction, as well as a host of others. Eigendecomposition is a common technique for dealing with this issue and has been applied to sets of correlated images for this purpose. Unfortunately, for the pose detection of 3-D objects, a very large number of correlated images must be captured from many different orientations. As a result, the eigendecomposition of this large set of images is very computationally expensive. In this work, we present a method for capturing images of objects from many locations by sampling S2 appropriately. Using this spherical sampling pattern, the computational burden of computing the eigendecomposition can be reduced by using the Spherical Harmonic Transform to “condense” information due to the correlation in S2. We propose a computationally efficient algorithm for approximating the eigendecomposition based on the spherical harmonic transform analysis. Experimental results are presented to compare and contrast the algorithm against the true eigendecomposition, as well as quantify the computational savings.
7EC39680	It is widely conjectured that the excellent ROC performance of biological vision systems is due in large part to the exploitation of context at each of many levels in a part/whole hierarchy. We propose a mathematical framework (a "composition machine") for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, and we build a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport. The demonstration system detects and correctly reads more than 98% of the plates, with a negligible rate of false detection. Unlike a formal grammar, the architecture of a composition machine does not exclude the sharing of sub-parts among multiple entities, and does not limit interpretations to single trees (e.g. a scene can have multiple license plates, or no plates at all). In this sense, the architecture is more like a general Bayesian network than a formal grammar. On the other hand, unlike a Bayesian network, the distribution is non-Markovian, and therefore more like a probabilistic context-sensitive grammar. The conceptualization and construction of a composition machine is facilitated by its formulation as the result of a series of non-Markovian perturbations of a "Markov backbone."
7B101C77	In this paper, we propose a novel shape description method for mobile retrieval of leaf images. In this method, termed multiscale arch height (MARCH), hierarchical arch height features at different chord spans are extracted from each contour point to provide a compact, multiscale shape descriptor. Both the global and detailed features of the leaf shape can be effectively captured by the proposed algorithm. MARCH descriptors are compared using a simple L1-norm based dissimilarity measurement providing very fast shape matching. The algorithm has been tested on four publicly available leaf image datasets including the Swedish leaf dataset, the Flavia leaf dataset, the ICL leaf dataset and the scanned subset of the ImageCLEF leaf dataset. The experiments indicate that the proposed method can achieve a higher classification rate and retrieval accuracy than the state-of-the-art benchmarks with a more than 500 times faster retrieval speed. A mobile retrieval system embedding the proposed algorithms has been developed for the real application of leaf image retrieval.
75F60D62	Identifying the same physical point in more than one image, the correspondence problem, is vital in motion analysis. Most research for establishing correspondence uses only two frames of a sequence to solve this problem. By using a sequence of frames, it is possible to exploit the fact that due to inertia the motion of an object cannot change instantaneously. By using smoothness of motion, it is possible to solve the correspondence problem for arbitrary motion of several nonrigid objects in a scene. We formulate the correspondence problem as an optimization problem and propose an iterative algorithm to find trajectories of points in a monocular image sequence. A modified form of this algorithm is useful in case of occlusion also. We demonstrate the efficacy of this approach considering synthetic, laboratory, and real scenes.
7FDB6B1D	ACRONYM is a comprehensive domain independent model-based system for vision and manipulation related tasks. Many of its submodules and representations have been described elsewhere. Here the derivation and use of invariants for image feature prediction is described. Predictions of image features and their relations are made from three-dimensional geometric models. Instructions are generated which teli the interpretation algorithms how to make use of image feature measurements to derive three-dimensional size, structural, and spatial constraints on the original three-dimensional models. Some preliminary examples of ACRONYM's interpretations of aerial images are shown.
7E751047	Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research
75B465C2	Shift-map image processing is a new framework based on energy minimization over a large space of labels. The optimization utilizes alpha-expansion moves and iterative refinement over a Gaussian pyramid. In this paper we extend the range of applications to image registration. To do this, new data and smoothness terms have to be constructed. We note a great improvement when we measure pixel similarities with the dense DAISY descriptor. The main contributions of this paper are: * The extension of the shift-map framework to include image registration. We register images for which SIFT only provides 3 correct matches. * A publicly available implementation of shift-map image processing (e.g. in painting, registration). We conclude by comparing shift-map registration to a recent method for optical flow with favorable results
5F6D546E	Computer vision tasks such as learning, recognition, classification or segmentation applied to spatial data often requires spatial normalization of repeated features and structures. Spatial normalization, or in other words, image registration, is still a big hurdle for the image processing community. Its formulation often relies on the fact that correspondence is achieved when a similarity measure is maximized. This paper presents a novel similarity measuring technique based on a coupling function inside a template matching framework. It allows using any entropy-based similarity metric, which is crucial for registration using different acquisition devices. Results are presented using this technique on a multiresolution incremental scheme.