808B43EF	The large prevalence of multimedia systems in recent years makes the security of multimedia communications an important and critical issue. We study the problem of securing the delivery of scalable video streams so that receivers can ensure the authenticity of the video content. Our focus is on recent scalable video coding (SVC) techniques, such as H.264/SVC, which can provide three scalability types at the same time: temporal, spatial, and visual quality. This three-dimensional scalability offers a great flexibility that enables customizing video streams for a wide range of heterogeneous receivers and network conditions. This flexibility, however, is not supported by current stream authentication schemes in the literature. We propose an efficient and secure authentication scheme that accounts for the full scalability of video streams, and enables verification of all possible substreams that can be extracted from the original stream. In addition, we propose an algorithm for minimizing the amount of authentication information that need to be attached to streams. The proposed authentication scheme supports end-to-end authentication, in which any third-party entity involved in the content delivery process, such as stream adaptation proxies and caches, does not have to understand the authentication mechanism. Our simulation study with real video traces shows that the proposed authentication scheme is robust against packet losses, incurs low computational cost for receivers, has short delay, and adds low communication overhead. Finally, we implement the proposed authentication scheme as an open source library called svcAuth, which can be used as a transparent add-on by any multimedia streaming application.
74E8AB32	Policy-based networking (PBN) is gaining a wide acceptance in IP management, resulting in a more unified control and management approach toward complexity of IP management. QoS interworking in IP management based on PBN is going to provide QoS guaranteed/differentiated IP connection services. QoS interworking issues between enterprise network and public IP network are studied. The exponential growth of intranet/Internet interworking itself may put PBN in jeopardy. To counter the increasing management complexity, two approaches, policy abstraction and hierarchy organization with precedence rules are proposed and studied.
7B9D73D9	Hyperspectral imaging instruments capture and collect hundreds of different wavelength data corresponding to the same surface. As a result, tons of information must be stored, processed and transmitted to ground. However, the downlink bandwidth is limited, and transmitting all data from the satellite to ground is a slow task that jeopardizes the use of this information for applications under real-time or near real-time constraints. This is the reason why most of the research activity is moving towards developing solutions which are able to process this data on-board, sending back to ground only relevant information.In this context, this paper presents three different FPGA-based architectures, implemented on an FPGA, which perform the Modified Vertex Component Analysis (MVCA) algorithm, as part of the hyperspectral linear unmixing processing chain. As a consequence, not only high performance but flexible and reusable designs have been designed. Moreover, final results demonstrate the influence of the implemented parallelization methodology over the final performance which motivates the system adaptivity through the scalability feature.
7D996511	In this paper, we derive the rate distortion lower bounds of spatially scalable video coding techniques. The methods we evaluate are subband and pyramid motion compensation where temporal redundancies in the same spatial layer as well as interlayer spatial redundancies are exploited in the enhancement layer encoding. The rate distortion bounds are derived from rate distortion theory for stationary Gaussian signals where mean square error is used as the distortion criteria. Assuming that the base layer is encoded by a nonscalable video coder, we derive the rate distortion functions for the enhancement layer, which depend upon the power spectral density of the input signal, the motion prediction error probability density function and the base layer encoding performance. We will show that pyramid and subband methods are expected to outperform independently encoding the enhancement layer using motion-compensated prediction, in terms of rate distortion efficiency, when the base layer is encoded at a relatively higher quality or less accurate displacement estimation happens in the enhancement layer.
7E6B030D	In this paper, we propose a scalable service scheme for secure group communication in grid. In the service scheme, a series of methods and strategies are presented, such as the initialization methods for group member, administrative domain and virtual organization, the key distribution strategy and the rekeying strategy. In order to improve the scalability of this service scheme, the services for a group are logically divided into two hierarchical levels, which is in accordance with the characteristics of group communication in grid. In addition, in order to show the efficiency and the scalability of the service scheme, simulation experiments are done. The results show that the service scheme is efficient and scalable. Thus, the service scheme can satisfy the requirement of people in large-scale, dynamic grid environment.
75B7139F	The new vision of cloud computing demands scalable, available and autonomic software platforms in order to deploy applications and services accessible anywhere and anytime. Multi-tier architectures are an important building block for many applications that are deployed in the cloud. This paper presents a novel caching and replication infrastructure that facilitates the scalable and elastic deployment of multi-tier architectures. Our Elastic SI-Cache is a novel multi-version cache that attains high performance and consistency in multi-tier systems. In contrast to most existing caches, Elastic SI-Cache provides snapshot isolation coherently across all tiers. Furthermore, Elastic SI-Cache supports scalable replication of the different tiers where replicas can be added or removed dynamically as needed, making the cache amenable for cloud computing environments. Elastic SI-Cache has been implemented and integrated into an open source JEE application server and its performance evaluated with the industrial benchmark SPECjAppServer.
7A792280	The public-key cryptography is indispensable for securing the smart grid communications. In this paper, we propose a hierarchical and fully-connected public key infrastructure that considers the smart grid characteristics. In the proposed public key infrastructure, each certificate authority is responsible for managing the public-key certificates for a geo-bounded small area. We also propose a novel format for the certificates that does not only bind a node's identity to its public key but also to its privileges and permissions. Finally we propose efficient and scalable certificate- renewing scheme that can much reduce the overhead of renewing certificates. Our verifications and evaluations demonstrate that using public key cryptography is essential for securing the smart grid and our proposals are scalable. Moreover, the simulation results demonstrate that the certificate-renewing scheme can significantly reduce the overhead of certificate renewals.
7B964191	Advanced metering infrastructure (AMI) is a major part of a smart grid system, and it deals with both data collection from smart meters and processing of those data. The traditional AMI architecture uses a centralized operation center with a centralized meter data management system (MDMS), which makes this system non-scalable. The system needs to be scalable so that with increased demand, it can be expanded at minimal cost. In this paper, we used two types of scalable distributed communication architectures, as initially proposed by Zhou et al. [1], namely, communication architecture with distributed MDMS and fully distributed communication architecture to minimize the deployment cost. We modified the analysis approach and used MATLAB-based code incorporating a Heuristic algorithm to determine close-to-optimal solutions for optimization problems. The unique feature of our work is the process of calculating accumulated bandwidth distance, in which distances between different components of an AMI were calculated according to the practical grid system layout of a city's infrastructure system. Theoretically developed scalability analysis was performed following [1], and the results were compared with the simulated results to indicate the validity of the asymptotic theoretical analysis. In our simulation, we found that the average distance between MDMS and the operation center was significantly different from that of Zhou et al. [1]. Our simulation results also indicated that both of the proposed architectures were scalable with significantly lower total deployment cost compared to the existing centralized communication architecture.
7F7C98E5	In this paper we propose a wavelet based coding algorithm for color images using a luminance/chrominance color space. Data rate scalability is achieved by using an embedded coding scheme, which is similar to Shapiro's (1993) embedded zerotree wavelet (EZW) algorithm. In a luminance/chrominance color space, the three color components have little statistical correlation. However, observations are made that at the spatial locations where chrominance signals have large transitions, it is highly likely for the luminance signal to have large transitions. This interdependence between the color components is exploited in the algorithm.
7EAADF32	In s-to-p broadcasting, s processors in a processor machine contain a message to be broadcast to all the processors, 1/spl les/s/spl les/p. We present a number of different broadcasting algorithms that handle all ranges of s. We show how the performance of each algorithm is influenced by the distribution of the s source processors and by the relationships between the distribution and the characteristics of the interconnection network. For the Intel Paragon we show that for each algorithm and machine dimension there exist ideal distributions and distributions on which the performance degrades. For the Cray T3D we also demonstrate dependencies between distributions and machine sizes. To reduce the dependence of the performance on the distribution of sources, we propose a repositioning approach. In this approach, the initial distribution is turned into an ideal distribution of the target broadcasting algorithm. We report experimental results for the Intel Paragon and Cray T3D and discuss scalability and performance.
6E42AD7F	There are many scenarios in which the same data must be delivered over a packet switched network to a large set of receivers. The Internet enables efficient multipoint transmissions through IP multicast by allowing data transmission to all receivers with a single send. Most approaches to scalable reliable multicast utilize receiver-oriented retransmissions. Defining an API for receiver-oriented reliable multicast is difficult because it is not clear how to manage the sender's cache and to schedule repairs. We outline an approach to defining an API based on logical cache persistence that addresses these problems. We also we explore the issues involved in defining an API for reliable multicast protocol on the Internet that can scale to millions of receivers.
763CE745	The content authenticity of the multimedia delivery is important issue with rapid development and widely used of multimedia technology. Till now many authentication solutions had been proposed, such as cryptology and watermarking based methods. However, in latest heterogeneous network the video stream transmission has been coded in scalable way such as H.264/SVC, there is still no good authentication solution. In this paper, we firstly summarized related works and proposed a scalable content authentication scheme using a ratio of different energy (RDE) based perceptual hashing in Q/S dimension, which is used Dempster-Shafer theory and combined with the latest scalable video coding (H.264/SVC) construction. The idea of “sign once and verify in scalable way” can be realized. Comparing with previous methods, the proposed scheme based on perceptual hashing outperforms previous works in uncertainty (robustness) and efficiencies in the H.264/SVC video streams. At last, the experiment results verified the performance of our scheme.
78114AC2	In both the regular and the irregular MPI (Message-Passing Interface) collective communication and reduction interfaces there is a correspondence between the argument lists and certain MPI derived datatypes. As a means to address and alleviate well-known memory and performance scalability problems in the irregular (or vector) collective interface definitions of MPI we propose to push this correspondence to its natural limit, and replace the interfaces of the MPI collectives with a different set of interfaces that specify all data sizes and displacements solely by means of derived datatypes. This reduces the number of collective (communication and reduction) interfaces from 16 to 10, significantly generalizes the operations, unifies regular and irregular collective interfaces, makes it possible to decouple certain algorithmic decisions from the collective operation, and moves the interface scalability issue from the collective interfaces to the MPI derived datatypes. To complete the proposal we discuss the memory scalability of the derived datatypes and suggest a number of alternative datatypes for MPI, some of which should be of independent interest. A running example illustrates the benefits of this alternative set of collective interfaces. Implementation issues are discussed showing that an implementation can be undertaken within any reasonable MPI library implementation.
80E27171	We propose an efficient strategy for resource management for scalable QoS guaranteed real-time communication services. This strategy is based on sink trees, and is particularly well suited for differentiated-services based architectures. We first show that finding a set of sink-trees in a given network is NP-complete. Then we propose a heuristic algorithm that always efficiently produces a set of sink-trees for a given network. Sink-tree based resource management integrates routing and resource reservation along the routes, and therefore has a number of advantages over other resource management scheme, in terms of: admission probability, link resource utilization, flow set up latency, signaling overhead, and routing over-head. In this paper we show by simulation experiments that even for simple cases the sink-tree based approach shows excellent results in terms of admission probability.
7E101CFA	The IBM Cyclops-64 (C64) chip employs a multithreaded architecture that integrates a large number of hardware thread units on a single chip. A cellular supercomputer is being developed based on a 3D-mesh connection of the C64 chips. This paper introduces the Cyclops datagram protocol (CDP) developed for the C64 supercomputer system. CDP is inspired by the TCP/IP protocol, yet simpler and more compact. The implementation of CDP leverages the abundant hardware thread-level parallelism provided by the C64 multithreaded architecture. The main contributions of this paper are: (1) We have completed a design and implementation of CDP that is used as the fundamental communication infrastructure for the C64 supercomputer system. (2) CDP successfully exploits the massive thread-level parallelism provided on the C64 hardware, achieving good performance scalability; (3) CDP is quite efficient. Its peak throughput reaches 884Mbps on the gigabit Ethernet, even it is running at the user-level on a single-processor Linux machine; (4) Extensive application test cases are passed and no reliability problems have been reported.
80485FDF	Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes an overlay-based Internet Indirection Infrastructure ( I3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows I3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.
7D11BF0C	In this paper, we investigate the scalability of three communication architectures for advanced metering infrastructure (AMI) in smart grid. AMI in smart grid is a typical cyber-physical system (CPS) example, in which large amount of data from hundreds of thousands of smart meters are collected and processed through an AMI communication infrastructure. Scalability is one of the most important issues for the AMI deployment in smart grid. In this study, we introduce a new performance metric, accumulated bandwidthdistance product (ABDP), to represent the total communication resource usages. For each distributed communication architecture, we formulate an optimization problem and obtain the solutions for minimizing the total cost of the system that considers both the ABDP and the deployment cost of the meter data management system (MDMS). The simulation results indicate the significant benefits of the distributed communication architectures over the traditional centralized one. More importantly, we analyze the scalability of the total cost of the communication system (including MDMS) with regard to the traffic load on the smart meters for both the centralized and the distributed communication architectures. Through the closed form expressions obtained in our analysis, we demonstrate that the total cost for the centralized architecture scales linearly as O(λN), with N being the number of smart meters, and λ being the average traffic rate on a smart meter. In contrast, the total cost for the fully distributed communication architecture is O(λ2/3 N2/3), which is significantly lower.
81565DAF	Scalable Video Coding is an H.264/AVC scalable extension that is used to provide various network-friendly scalability using a single bit stream. In SVC, the newly adapted predictive coding techniques can achieve high encoding efficiency, but they increase the computational complexity. To reduce the computational complexity, we present a fast mode decision algorithm based on the block complexity function considering the degree of correlation between base layer and enhancement layer. The simulation results show that the encoding time of the proposed fast mode decision algorithm for the combined scalability is about 54.72% compared with normal method although the loss of visual quality is negligible.
7F3DE411	A novel frame-level rate control (RC) algorithm is presented in this paper for temporal scalability of scalable video coding. First, by introducing a linear quality dependency model, the quality dependency between a coding frame and its references is investigated for the hierarchical B-picture prediction structure. Second, linear rate-quantization (R-Q) and distortion-quantization (D-Q) models are introduced based on different characteristics of temporal layers. Third, according to the proposed quality dependency model and R-Q and D-Q models for each temporal layer, adaptive weighting factors are derived to allocate bits efficiently among temporal layers. Experimental results on not only traditional quarter common intermediate format/common intermediate format but also standard definition and high definition sequences demonstrate that the proposed algorithm achieves excellent coding efficiency as compared to other benchmark RC schemes.
7EFFF39D	Scalable video coding is attractive due to the capability of reconstructing lower resolution or lower quality signals from partial bit streams. This allows for simple solutions in adaptation to network and terminal capabilities. Different modalities of scalability are specified by video coding standards like MPEG-2 and MPEG-4. This paper gives a short overview over these techniques and analyzes in more detail the encoder/decoder drift problem, which is the major reason why scalable coding has been significantly less efficient than single-layer coding in most of these implementations. Only recently, new scalable video coding technology has evolved, which seems to close the gap of compression performance compared to state of the art single-layer video coding. New methods of efficient enhancement layer prediction were developed to improve traditional (motion-compensated hybrid) scalable coders, providing more flexible compromises on the drift problem. As a new technology trend, motion-compensated spatiotemporal wavelet coding has matured which entirely discards the drift and allows most flexible combinations of spatial, temporal, and signal-to-noise ratio (SNR) scalability with fine granularity over a broad range of data rates.
80FB17B6	This paper describes the file format defined for scalable video coding. Techniques in the file format enable rapid extraction of scalable data, corresponding to the desired operating point. Significant assistance to file readers can be provided, and there is also great flexibility in the ways that the techniques can be used and combined, corresponding to different usages and application scenarios.
7D194C2D	Scalable video coding (SVC) is currently developed as an extension of H.264/AVC. In the SVC encoder, an exhaustive search technique is employed to select the best coding mode for each macro block (MB). This technique achieves an optimal trade-off between rate and distortion, but it requires an extremely large encoding time. In this paper, we propose a fast mode decision algorithm for inter-layer coding at the enhancement layer. The proposed algorithm predicts the mode of each MB at the enhancement layer using the modes of a co-located MB and its neighboring MBs at the base layer. The proposed method can achieve a time saving of up to 74% in spatial scalability and 63% in coarse grain quality scalability with negligible loss of quality and bit rate increment.
8086CE01	We propose a novel architecture for scalable video coding, namely, progressive fine granularity scalable (PEGS) coding, which can provide a high coding efficiency along with good bandwidth adaptation and error recovery properties. Unlike the fine granularity scalable (FGS) coding in the MPEG-4 proposal, some of the enhancement layers in a current frame are predicted from a high quality enhancement layer in a reference frame, rather than always from the base layer. Using a high quality enhancement layer as the reference makes the motion prediction more accurate to improve the coding efficiency. On the other hand, the use of multiple layers of different quality references may also result in increases and fluctuations of the prediction residues to be coded when switching the references, which may limit the coding efficiency improvement. A multiple-layer conditional replenishment approach is used to eliminate this kind of fluctuation. Experimental results show that our coding scheme can improve the coding efficiency up to 0.5 dB compared with fine granularity scalability coding.
8043E91D	In this paper, we present a new wavelet based rate scalable video compression algorithm. We will refer to this new technique as the scalable adaptive motion compensated wavelet (SAMCoW) algorithm. SAMCoW uses motion compensation to reduce temporal redundancy. The prediction error frames and the intracoded frames are encoded using an approach similar to the embedded zerotree wavelet (EZW) coder. An adaptive motion compensation (AMC) scheme is described to address error propagation problems. We show that, using our AMC scheme, the quality of the decoded video can be maintained at various data rates. We also describe an EZW approach that exploits the interdependency between color components in the luminance/chrominance color space. We show that, in addition to providing a wide range of rate scalability, our encoder achieves comparable performance to the more traditional hybrid video coders, such as MPEG1 and H.263. Furthermore, our coding scheme allows the data rate to be dynamically changed during decoding, which is very appealing for network-oriented applications.
7D640F01	Scalable video coding is an ongoing standard, and the current working draft (WD) is an extension of H.264/AVC. In the WD, an exhaustive search technique is employed to select the best coding mode for each macroblock. This technique achieves the highest possible coding efficiency, but it results in extremely large encoding time which obstructs it from practical use. This paper proposes a fast mode decision algorithm for inter-frame coding for spatial, coarse grain signal-to-noise ratio, and temporal scalability. It makes use of the mode-distribution correlation between the base layer and enhancement layers. Specifically, after the exhaustive search technique is performed at the base layer, the candidate modes for enhancement layers can be reduced to a small number based on the correlation. Experimental results show that the fast mode decision scheme reduces the computational complexity significantly with negligible coding loss and bit-rate increases
7B639BB8	This paper proposes GIA, a scalable architecture for global IP-anycast. Existing designs for providing IP-anycast must either globally distribute routes to individual anycast groups, or confine each anycast group to a pre-configured topological region. The first approach does not scale because of excessive growth in the routing tables, whereas the second one severely limits the utility of the service. Our design scales by dividing inter-domain anycast routing into two components. The first component builds inexpensive default anycast routes that consume no bandwidth or storage space. The second component, controlled by the edge domains, generates enhanced anycast routes that are customized according to the beneficiary domain's interests. We evaluate the performance of our design using simulation, and prove its practicality by implementing it in the Multi-threaded Routing Toolkit.
7D12F2C0	This paper proposes an architecture for inter-domain routing, called DTIA - Dynamic Topological Information Architecture. DTIA separates the issues of reachability and routing, and this paper addresses the first one. One major requirement has been not to change IP packets and the commercial relations in the Internet. DTIA is based on the knowledge of a static network formed by the Autonomous Systems (AS) and an algorithm to manage link failures. We use the concept of a region as a mechanism to sustain scale. DTIA supports the most important functionalities of BGP: some of them are built in and others can be implemented on top of the reachability level or the routing level. The main concerns we aim to solve are taking advantage of multihoming, increase the robustness in terms of convergence, reduce the churn rate and range of routing events, and due to forwarding packets by AS identifiers and topologic links (as opposed to prefix policy defined) reduce the growth of the routing table.
772B6552	The IP-based addressing scheme currently supporting the whole routing architecture embeds some well-known limitations that may significantly hinder the deployment of new applications and services on the Internet. Indeed, it is widely accepted that the unstoppable growth of Internet users is producing two well-known problems: (1) depletion of addresses, motivated by a design limitation of the currently deployed addressing scheme, and (2) the semantic overload of addresses. The main negative consequences of these problems may be summarized as: (i) exacerbating the geometrical growth of the routing tables, and (ii) affecting other network features, such as traffic engineering and mobility, in terms of resilience and disruption tolerant communications.The relevant consequences that addressing brings to the overall network operation is pushing the networking community to study and propose new addressing architectures that may limit or even remove the negative effects (affecting network performance) stemmed from the currently deployed addressing architecture. To this end, researchers working on this area must have a perfect understanding of the weaknesses and limitations coming up from the nowadays architecture as well as a comprehensive knowledge of the alternatives proposed so far along with the most appealing research trends. Aligned to this scenario, this paper comes up with the aim of assisting the reader to both: (i) get insights about the most prominent limitations of the currently deployed addressing architecture, and (ii) survey the existing proposals based on ID/Locator Split Architectures (ILSAs) including an analysis of pros and cons, as well as a taxonomy aiming at formulating a design space for evaluating and designing existing and future ILSAs.
77C866FE	Internet is facing many challenges that cannot be solved easily through ad hoc patches. To address these challenges, many research programs and projects have been initiated and many solutions are being proposed. However, before we have a new architecture that can motivate Internet service providers (ISPs) to deploy and evolve, we need to address two issues: 1) know the current status better by appropriately evaluating the existing Internet; and 2) find how various incentives and strategies will affect the deployment of the new architecture. For the first issue, we define a series of quantitative metrics that can potentially unify results from several measurement projects using different approaches and can be an intrinsic part of future Internet architecture (FIA) for monitoring and evaluation. Using these metrics, we systematically evaluate the current interdomain routing system and reveal many “autonomous-system-level” observations and key lessons for new Internet architectures. Particularly, the evaluation results reveal the imbalance underlying the interdomain routing system and how the deployment of FIAs can benefit from these findings. With these findings, for the second issue, appropriate deployment strategies of the future architecture changes can be formed with balanced incentives for both customers and ISPs. The results can be used to shape the short- and long-term goals for new architectures that are simple evolutions of the current Internet (so-called dirty-slate architectures) and to some extent to clean-slate architectures.
7D873E94	Rate scalable video compression is appealing for low bit rate applications, such as video telephony and wireless communication, where bandwidth available to an application cannot be guaranteed. In this paper, we investigate a set of strategies to increase the performance of SAMCoW, a rate scalable encoder. These techniques are based on based on wavelet decomposition, spatial orientation trees, and motion compensation.
7E663076	Attempts to generalize the Internet’s point-to-point communication abstraction to provide services like multicast, any- cast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes a general, overlay-based Internet Indirection In- frastructure ( 3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows 3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this ap- proach, we have designed and built a prototype based on the Chord lookup protocol.
7CC2095B	Hashing has enjoyed a great success in large-scale similarity search. Recently, researchers have studied the multi-modal hashing to meet the need of similarity search across different types of media. However, most of the existing methods are applied to search across multi-views among which explicit bridge information is provided. Given a heterogeneous media search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH simultaneously learns hash functions embedding heterogeneous media into different Hamming spaces, and translators aligning these spaces. Unlike almost all existing methods that map heterogeneous data in a common Hamming space, mapping to different spaces provides more flexible and discriminative ability. We empirically verify the effectiveness and efficiency of our algorithm on two real world large datasets, one publicly available dataset of Flickr and the other MIRFLICKR-Yahoo Answers dataset.
80A1F6BB	Especially for large Network Operators (NOs), eliminating routing anomalies is an important aspect for the internal BGP design. To avoid such unwanted effects, classical architectures [1], [2], [3] have need of certain restrictions kept in both, the network design and the iBGP peering. However, these restrictions are in conflict with the optimized network designs NOs seek. In this study we develop an inherently anomaly-free iBGP architecture that takes the demands of NOs into account. It is based on a central server that guarantees consistent local views in the entire system. This is done by exchanging additional routing information with the Border Routers of the Autonomous System. The architecture implements the results of iBGP analyses made in [4]. Despite not being that as flexible as classical information reduction techniques, our design scales equal or better in practice. All required protocol extensions are supported by IETF Internet Drafts from persons affiliated with important router vendors.
7EAA2130	This paper presents a novel architecture of MPEG-4 zerotree encoder. Under the architecture, a fast technique of label coefficients is proposed to reduce the recursive scan. It is achieved by exploiting the feature of the MPEG-4 zerotree symbol alphabet. A new combined structure of ZTR address buffer and the significant flag bit is described. It can simplify the skipping of the significant coefficients and locating the descendent coefficients of ZTR/VZTR. Furthermore, a preprocessor is given for independent encoding of each tree in individual bitplane.
812477A0	In s-to-p broadcasting, s processors in a processor machine contain a message to be broadcast to all the processors, 1/spl les/s/spl les/p. We present a number of different broadcasting algorithms that handle all ranges of s. We show how the performance of each algorithm is influenced by the distribution of the s source processors and by the relationships between the distribution and the characteristics of the interconnection network. For the Intel Paragon we show that for each algorithm and machine dimension there exist ideal distributions and distributions on which the performance degrades. For the Cray T3D we also demonstrate dependencies between distributions and machine sizes. To reduce the dependence of the performance on the distribution of sources, we propose a repositioning approach. In this approach, the initial distribution is turned into an ideal distribution of the target broadcasting algorithm. We report experimental results for the Intel Paragon and Cray T3D and discuss scalability and performance.
8152D1DA	Many challenges to the Internet including global routing scalability have drawn significant attention from both industry and academia, and have generated several new ideas for the next generation. MILSA (Mobility and Multihoming supporting Identifier Locator Split Architecture) and related enhancements are designed to address the naming, addressing, and routing scalability challenges, provide mobility and multihoming support, and easy transition from the current Internet. In this paper, we synthesize our research into a multiple-tier realm-based framework and present the fundamental principles behind the architecture. Through detailed presentation of these principles and different aspects of our architecture, the underlying design rationale is justified. We also discuss how our proposal can meet the IRTF RRG design goals. As an evolutionary architecture, MILSA balances the high-level long-run architecture design with ease of transition considerations. Additionally, detailed evaluation of the current inter-domain routing system and the achievable improvements deploying our architecture is presented that reveals the roots of the current difficulties and helps to shape our deployment strategy.
7F4A214A	This paper describes the first scalable implementation of a text processing engine used in visual analytics tools. These tools aid information analysts in interacting with and understanding large textual information content through visual interfaces. By developing a parallel implementation of the text processing engine, we enabled visual analytics tools to exploit cluster architectures and handle massive datasets. The paper describes key elements of our parallelization approach and demonstrates virtually linear scaling when processing multi-gigabyte data sets such as Pubmed. This approach enables interactive analysis of large datasets beyond capabilities of existing state-of-the art visual analytics tools.
7F7FB540	Motion-compensated fine-granularity scalability (MC-FGS) with leaky prediction has been shown to provide an efficient tradeoff between compression gain and error resilience, facilitating the transmission of video over dynamic channel conditions. In this paper, we propose an n-channel symmetric motion-compensated multiple description (MD) coding and transmission scheme for the delivery of scalable video over orthogonal frequency division multiplexed systems, utilizing the concepts of partial and leaky predictions. We investigate the proposed MD coding and transmission scheme using a cross-layer design perspective. In particular, we construct the symmetric motion-compensated MD codes based on the diversity order of the channel, defined as the ratio of the overall bandwidth of the system to the coherence bandwidth of the channel. We show that knowing the diversity order of a physical channel can assist an MC-FGS video coder in selecting the motion-compensation prediction point, as well as on the use of leaky prediction. More importantly, we illustrate how the side information can reduce the drift management problem associated with the construction of symmetric motion-compensated MD codes. We provide results based on both an information-theoretic approach and simulations.
5D3D697B	Designing massively scalable, highly available big data systems is an immense challenge for software architects. Big data applications require distributed systems design principles to create scalable solutions, and the selection and adoption of open source and commercial technologies that can provide the required quality attributes. In big data systems, the data management layer presents unique engineering problems, arising from the proliferation of new data models and distributed technologies for building scalable, available data stores. Architects must consequently compare candidate database technology features and select platforms that can satisfy application quality and cost requirements. In practice, the inevitable absence of up-to-date, reliable technology evaluation sources makes this comparison exercise a highly exploratory, unstructured task. To address these problems, we have created a detailed feature taxonomy that enables rigorous comparison and evaluation of distributed database platforms. The taxonomy captures the major architectural characteristics of distributed databases, including data model and query capabilities. In this paper we present the major elements of the feature taxonomy, and demonstrate its utility by populating the taxonomy for nine different database technologies. We also briefly describe QuABaseBD, a knowledge base that we have built to support the population and querying of database features by software architects. QuABaseBD links the taxonomy to general quality attribute scenarios and design tactics for big data systems. This creates a unique, dynamic knowledge resource for architects building big data systems.
815BA7A5	This paper presents the design of a new Internet routing architecture (NIRA). In today's Internet, users can pick their own ISPs, but once the packets have entered the network, the users have no control over the overall routes their packets take. NIRA aims at providing end users the ability to choose the sequence of Internet service providers a packet traverses. User choice fosters competition, which imposes an economic discipline on the market, and fosters innovation and the introduction of new services.This paper explores various technical problems that would have to be solved to give users the ability to choose: how a user discovers routes and whether the dynamic conditions of the routes satisfy his requirements, how to efficiently represent routes, and how to properly compensate providers if a user chooses to use them. In particular, NIRA utilizes a hierarchical provider-rooted addressing scheme so that a common type of domain-level route can be efficiently represented by a pair of addresses. In NIRA, each user keeps track of the topology information on domains that provide transit service for him. A source retrieves the topology information of the destination on demand and combines this information with his own to discover end-to-end routes. This route discovery process ensures that each user does not need to know the complete topology of the Internet.
7DF1DA2B	MPAs (massively parallel architectures) should address two fundamental issues for scalability: synchronization and communication latency. Dataflow architectures cause problems of excessive synchronization costs and inefficient execution of sequential programs while they offer the ability to exploit massive parallelism inherent in programs. In contrast, MPAs based on the von Neumann computational model may suffer from inefficient synchronization mechanism and communication latencies. DAVRID (Dataflow von Neumann, RISC Hybrid) is a massively parallel multithreaded architecture. By combining the advantages of the von Neumann model and the dataflow model, DAVRID preserves good single thread performance and tolerates latency and synchronization costs. We describe the DAVRID architecture and evaluate it through simulation results over several benchmarks.
809FFAF6	The scalable video coding (SVC) standard as an extension of H.264/AVC allows efficient, standard-based temporal, spatial, and quality scalability of video bit streams. Scalability of a video bit stream allows for media bit rate as well as for device capability adaptation. Moreover, adaptation of the bit rate of a video signal is a desirable key feature, if limitation in network resources, mostly characterized by throughput variations, varying delay or transmission errors, need to be considered. Typically, in mobile networks the throughput, delay and errors of a connection (link) depend on the current reception conditions, which are largely influenced by a number of physical factors. In order to cope with the typically varying characteristics of mobile communication channels in unicast, multicast, or broadcast services, different methods for increasing robustness and achieving quality of service are desirable. We will give an overview of SVC and its relation to mobile delivery methods. Furthermore, innovative use cases are introduced which apply SVC in mobile networks.
810F8A3E	A challenge faced by smart grid systems is providing highly reliable transmissions to better serve different types of electrical applications and improve the energy efficiency of the system. Although wireless networking technologies can provide highspeed and cost-effective solutions, their performance may be impaired by various factors that affect the reliability of smart grid networks. Here, we first suggest the use of IEEE 802.11s-based wireless LAN mesh networks as high-speed wireless backbone networks for smart grid infrastructure to provide high scalability and flexibility while ensuring low installation and management costs. Thereafter, we analyze some vital problems of the IEEE 802.11s default routing protocol (named hybrid wireless mesh protocol; HWMP) from the perspective of transfer reliability, and propose appropriate solutions with a new routing method called HWMP-reliability enhancement to improve the routing reliability of 802.11s-based smart grid mesh networking. A simulation study using ns-3 was conducted to demonstrate the superiorityof the proposed schemes.
7D48C9A2	This article proposes a scalable wavelength-routed optical Network on Chip (NoC) based on the Spidergon topology, named Power-efficient Scalable Wavelength-routed Network-on-chip (PeSWaN). The key idea of the proposed all-optical architecture is the utilization of per-receiver wavelengths in the data network to prevent network contention and the adoption of per-sender wavelengths in the control network to avoid end-point contention. By performing a series of simulations, we study the efficiency of the proposed architecture, its power and energy consumption, and the data transmission delay. Moreover, we compare the proposed architecture with electrical NoCs and alternative ONoC architectures under various traffic patterns.
7DB5CC40	We present a control scheme for a rate scalable video codec. We describe a wavelet based video codec with motion compensation used to reduce temporal redundancy. The prediction error frames are encoded using an embedded zerotree wavelet (EZW) approach which allows data rate scalability. Since motion compensation is used in the algorithm, the duality of the decoded video may decay due to the propagation of errors in the temporal domain. An adaptive motion compensation scheme is proposed to address this problem. We show that using our control scheme the quality of the decoded video can be maintained at any data rate.
80168B22	Generally speaking, rate scalable video systems today are evaluated operationally, meaning that the algorithm is implemented and the rate-distortion performance is evaluated for an example set of inputs. However, in these cases it is difficult to separate the artifacts caused by the compression algorithm and data set with general trends associated with scalability. In this paper, we derive and evaluate theoretical rate-distortion performance bounds for both layered and continuously rate scalable video compression algorithms which use a single motion-compensated prediction (MCP) loop. These bounds are derived using rate-distortion theory based on an optimum mean-square error (MSE) quantizer, and are thus applicable to all methods of intraframe encoding which use MSE as a distortion measure. By specifying translatory motion and using an approximation of the predicted error frame power spectral density, it is possible to derive parametric versions of the rate-distortion functions which are based solely on the input power spectral density and the accuracy of the motion-compensated prediction. The theory is applicable to systems which allow prediction drift, such as the data-partitioning and SNR-scalability schemes in MPEG-2, as well as those with zero prediction drift such as fine granularity scalability MPEG-4. For systems which allow prediction drift we show that optimum motion compensation is a sufficient condition for stability of the decoding system
80DCC6AC	Layer 5 switching-based transparent Web caching intercepts HTTP requests and redirects requests according to their contents. This technique makes the deployment and configuration of a caching system easier and improves its performance by ensuring that non-cacheable HTTP requests bypass the cache servers. We propose a Load Balancing Layer 5 switching-based (LB-L5) Web caching scheme that uses the Layer 5 switching-based technique to support distributed Web caching. We present simulation results that show that LB-L5 outperforms existing Web caching schemes, namely ICP, Cache Digest, and basic L5 transparent Web caching, in terms of cache server workload balancing and response time. LB-L5 is also shown to be more adaptable to high HTTP request intensity than the other schemes.
817736CD	With the introduction of the H.264/AVC video coding standard, significant improvements have recently been demonstrated in video compression capability. The Joint Video Team of the ITU-T VCEG and the ISO/IEC MPEG has now also standardized a Scalable Video Coding (SVC) extension of the H.264/AVC standard. SVC enables the transmission and decoding of partial bit streams to provide video services with lower temporal or spatial resolutions or reduced fidelity while retaining a reconstruction quality that is high relative to the rate of the partial bit streams. Hence, SVC provides functionalities such as graceful degradation in lossy transmission environments as well as bit rate, format, and power adaptation. These functionalities provide enhancements to transmission and storage applications. SVC has achieved significant improvements in coding efficiency with an increased degree of supported scalability relative to the scalable profiles of prior video coding standards. This paper provides an overview of the basic concepts for extending H.264/AVC towards SVC. Moreover, the basic tools for providing temporal, spatial, and quality scalability are described in detail and experimentally analyzed regarding their efficiency and complexity.
5BA87494	In this paper, we propose a linear dependent rate-quantization model for video enhancement layers encoding in H.264/AVC based scalable video coding (SVC). It is noted that the proposed model is applicable for different scalable structures, such as temporal, quality, spatial and combined scalability. Leveraging the base layer information (such as bitrate and quantization parameter), proposed model can accurately predict the number of bit required for the enhancement layer encoding. Such linear model demonstrates the high accuracy for bitrate estimation at enhancement layers, with the average prediction accuracy over 94%. It has the noticeable improvement from the existing works, without requiring additional complexity increase. Meanwhile, proposed model is applied to do the rate control for enhancement layers encoding. Experimental results show that the average bitrate mismatch error can be significantly reduced compared with the existing algorithms.
7FE8D880	Swarm intelligence forms the core of a new class of algorithms inspired by the social behavior of insects that live in swarms. Its attractive features include adaptation, robustness and a distributed, decentralized nature, rendering swarm-based algorithms well-suited for routing in wireless or satellite networks, where it is difficult it implement centralized network control. We propose one such routing algorithm, dubbed adaptive swarm-based distributed routing (adaptive-SDR), which is scalable, robust and suitable to handle large amounts of network traffic, while minimizing delay and packet loss.
7136DB48	In this paper, we present a scalable three dimensional hybrid parallel Delaunay image-to-mesh conversion algorithm (PDR.PODM) for distributed shared memory architectures. PDR.PODM is able to explore parallelism early in the mesh generation process because of the aggressive speculative approach employed by the Parallel Optimistic Delaunay Mesh generation algorithm (PODM). In addition, it decreases the communication overhead and improves data locality by making use of a data partitioning scheme offered by the Parallel Delaunay Refinement algorithm (PDR). PDR.PODM utilizes an octree structure to decompose the initial mesh and to distribute the bad elements to different octree leaves (subregions). A set of independent subregions are selected and refined in parallel without any synchronization among them. In each subregion, a group of threads is assigned to insert or delete multiple points based on the refinement rules offered by PODM. We tested PDR.PODM on Blacklight, a distributed shared memory (DSM) machine in the Pittsburgh Supercomputing Center, and observed a weak scaling speedup of 163.8 and above for up to 256 cores as opposed to PODM whose weak scaling speedup is only 44.7 on 256 cores. The end result is that we can generate 18 million elements per second as opposed to 14 million per second in our earlier work. To the best of our knowledge, PDR.PODM exhibits the best scalability among parallel guaranteed quality Delaunay mesh generation algorithms running on DSM supercomputers.
80C21360	In tree-based hierarchical key management schemes, scalability is achieved by reducing the number of messages exchanged during a rekeying operation. A single server manages the entire tree structure in such schemes. Failure of that server leads to single point failure which interrupts the group communication. In this paper we propose a method to avoid single point failure by distributing user information among set of X number of servers and use (t,X) threshold scheme to reconstruct the tree. The new auxiliary keys and group key are computed partly by the users which reduces number of encryptions required to communicate new set of keys to the remaining group members.
79C99A78	This article presents the progressive evolution of NFV from the initial SDN-agnostic initiative to a fully SDN-enabled NFV solution, where SDN is not only used as infrastructure support but also influences how virtual network functions (VNFs) are designed. In the latest approach, when possible, stateless processing in the VNF shifts from the computing element to the networking element. To support these claims, the article presents the implementation of a flow-based network access control solution, with an SDN-enabled VNF built on IEEE 802.1x, which establishes services as sets of flow definitions that are authorized as the result of an end user authentication process. Enforcing the access to the network is done at the network element, while the authentication and authorization state is maintained at the compute element. The application of this proposal allows the performance to be enhanced, while traffic in the control channel is reduced to a minimum. The SDN-enabled NFV approach sets the foundation to increase the areas of application of NFV, in particular in those areas where massive stateless processing of packets is expected.