59300DF4	Software systems embed in them knowledge about the domain in which they operate. However, this knowledge is "latent". Making such knowledge accessible could be of great value to the organization both as a source of explicit knowledge and to systems development and maintenance. We propose a framework aimed at making domain knowledge embedded in software explicit. The framework is based on identifying domain knowledge acquired during the development process (especially in requirements analysis) and formalizing it. The software architecture is then partitioned into two parts: one represents the domain knowledge and the other responsible for the actual processing (using this knowledge). A specific object-oriented design approach is suggested to accomplish this partitioning.
08660FF1	Inheritance is a key feature of object-oriented programming. Overriding is one of the most important parts of inheritance, allowing a subclass to replace methods implemented in its superclass. Unfortunately, the way programmers use overriding in practise is not well understood.We present the first large-scale empirical study of overriding. We describe a suite of metrics that measure overriding and present a corpus analysis that uses those metrics to analyse 100 open-source applications, containing over 100,000 separate classes and interfaces. We found substantial overriding: most subclasses override at least one method and many classes that only declare overriding methods. We also found questionable uses of overriding, such as removing superclass method implementations by overriding them with empty method bodies.
7B4DA32F	Automated analysis of object-oriented design models can provide insight into the quality of a given software design. Data obtained from automated analysis, however, is often too complex to be easily understood by a designer. This paper examines the use of an automated analysis tool on industrial software UML class models, where one set of models was created as part of the design process and the other was obtained from reverse engineering code. The analysis was performed by DesignAdvisor, a tool developed by Siemens Corporate Research, that supports metrics-based analysis and detection of design guideline violations. The paper describes the lessons learned from using the automated analysis techniques to assess the quality of these models. We also assess the impact of design pattern use in the overall quality of the models. Based on our lessons learned, identify design guidelines that would minimize the occurrence of these errors.
7B07BAE5	This paper aims at empirically exploring the relationships between most of the existing coupling and cohesion measures for object-oriented (OO) systems, and the fault-proneness of OO system classes. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The study described here is a replication of an analogous study conducted in an university environment with systems developed by students. In order to draw more general conclusions and to (dis)confirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals. Results show that many of our findings are consistent across systems, despite the very disparate nature of the systems under study. Some of the strong dimensions captured by the measures in each data set are visible in both the university and industrial case study. For example, the frequency of method invocations appears to be the main driving factor of fault-proneness in all systems. However, there are also differences across studies which illustrate the fact that quality does not follow universal laws and that quality models must be developed locally, wherever needed.
726FC96C	This paper presents the application of neural networks in software quality estimation using object-oriented metrics. In this paper, two kinds of investigation are performed. The first on predicting the number of defects in a class and the second on predicting the number of lines changed per class. Two neural network models are used, they are Ward neural network and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model
76A0F499	Presents a novel way of using object-oriented design metrics to support the incremental development of object-oriented programs. Based on a quality model (the factor-criteria-metrics model), so-called multi-metrics relate a number of simple structural measurements to design principles and rules. Single components of an object-oriented program like classes or subsystems are analyzed to determine whether they conform to specific design goals. Concise measurement reports, together with detailed explanations of the obtained values, identify problem spots in system design and give hints for improvement. This allows the designer to measure and evaluate programs at an appropriate level of abstraction. This paper details the use of the multi-metrics approach for the design and improvement of a framework for industry and its use for graphical applications. Multi-metrics tools were used with several versions of the framework. The measurement results were used in design reviews to quantify the effects of efforts to reorganize the framework. The results showed that this approach was very effective at giving good feedback, even to very experienced software developers. It helped them to improve their software and to create stable system designs.
7C02205D	The object-oriented software package OOQP for solving convex quadratic programming problems (QP) is described. The primal-dual interior point algorithms supplied by OOQP are implemented in a way that is largely independent of the problem structure. Users may exploit problem structure by supplying linear algebra, problem data, and variable classes that are customized to their particular applications. The OOQP distribution contains default implementations that solve several important QP problem types, including general sparse and dense QPs, bound-constrained QPs, and QPs arising from support vector machines and Huber regression. The implementations supplied with the OOQP distribution are based on such well known linear algebra packages as MA27/57, LAPACK, and PETSc. OOQP demonstrates the usefulness of object-oriented design in optimization software development, and establishes standards that can be followed in the design of software packages for other classes of optimization problems. A number of the classes in OOQP may also be reusable directly in other codes.
7BA53FFB	Repeated code modification lowers code quality and impacts object-oriented system design. Object-oriented metrics have proven as indicators of problems in system design. They have been grouped in minimal sets, known as quality models, to assess object-oriented system quality. Improvement can be gained by establishing relationships between quality characteristics and metrics computed from object-oriented diagrams. Quality models include metrics that can produce better code in object-oriented systems. Code quality can also be gained with refactoring. Refactoring is used to reduce complexity and eliminate redundant code. It is important to identify when and where to use refactoring. There are many different approaches. This work presents early stage analysis and focuses on exploring whether object-oriented metrics can be used as indicators where in the code refactoring can be used. Through multiple itera tions of successive measurement and refactoring, relation between metric values and need of refactoring can be concluded.
2DBF3E95	The goals of the Ada Performance Study are described. The methods used are explained. Guidelines for future Ada development efforts are given. The goals and scope of the study are detailed, and the background of Ada development in the Flight Dynamics Division (FDD) is presented. The organization and overall purpose of each test are discussed. The purpose, methods, and results of each test and analyses of these results are given. Guidelines for future development efforts based on the analysis of results from this study are provided. The approach used on the performance tests is discussed. 
7B027B55	Software engineers of multi-agent systems (MASs) are faced with different concerns such as autonomy, adaptation, interaction, collaboration, learning, and mobility, which are essentially different from classical concerns addressed in object-oriented software engineering. MAS developers, however, have relied mostly on object-oriented design techniques and programming languages, such as Java. This often leads to a poor separation of MAS concerns and in turn to the production of MASs that are difficult to maintain and reuse. This paper discusses software engineering approaches for MASs, and presents a new method for integrating agents into object-oriented software engineering from an early stage of design. The proposed approach encourages the separate handling of MAS concerns, and provides a disciplined scheme for their composition. Our proposal explores the benefits of aspect-oriented software development for the incorporation of agents into object-oriented systems. We also illustrate our aspect-oriented approach through the Portalware multi-agent system, a Web-based environment for the development of e-commerce portals. Copyright Â© 2004 John Wiley & Sons, Ltd.
76F99AE3	The Ada programming language has been criticized for lacking essential runtime system capabilities which support the generation of verifiable, real-time multitasking software. Such a capability is critical to the development of application software for high performance embedded avionic systems. The Ada Real-Time Machine Interface (ARTMI) software component of the F-16 Modular Mission Computer (MMC) provides Ada runtime environment (RTE) extensions that support development of real-time multitasking application software. The extensions eliminate the problem of mutual deadlock and unbounded priority inversion at critical sections through use of the priority ceiling protocol. This allows application software to be designed with task prioritization schemes that permit application of Rate Monotonic Analysis techniques. In addition, the extensions support execution time modification of task priorities. Such a capability can be used to reallocate processing bandwidth to critical system functions as the demands of a mission change.
6D9B5EDE	SMARTNet-2 was developed to provide a high-speed shared memory network for support of real-time parallel processing. Rapid prototyping put early versions of the SMARTNet-2 Diagnostic Software on the various computers being used for hardware board development and testing. This provided quick feedback to the hardware design engineers and ensured that all addressable hardware could be accessed by the software. As the hardware matured and new parts were added it was fairly simple to update the software to accommodate the new hardware. The authors detail the unique aspects of the design and implementation of the SMARTNet-2 diagnostic software. In addition, they discuss the command language and the methods used to achieve fault isolation testing.
6B8C5D5C	Lockheed Martin Tactical Aircraft Systems and Texas Instruments recompiled over 2 million non-comment non-blank source lines of code (SLOC) in the avionics and vehicle management services subdomains, originally written in Ada 83, using four Ada 95 compilers. Two avionics applications were linked and executed using a workstation-based simulator to verify gross functionality. A Distributed Systems Annex (DSA) version of this code was created and executed successfully on multiple workstations across a TCP/IP network. Translating Ada 83 code to Ada 95 compatibility proceeded much faster than expected, about 2000 lines per hour, and was easily accomplished with only a text editor. Once translated, the code proved highly portable between compilers. The greatest challenge was not in Ada version incompatibilities, but in assumptions inherent in the legacy code. Ada 95 compilers will require at least 6 go 12 months of maturation to be capable of supporting a large avionics development effort. Particular weaknesses in is the lack of task-aware pre-processors for handling multiple aircraft configurations from a common source code base, and support for optional language annexes. However, the Ada 95 tools resolve major problem areas of Ada 83 tools, including compilation order dependencies and lack of object-oriented features, and in general compile much faster. When mature, these Ada 95 tools are expected to provide significant productivity gains over their Ada 83 counterparts.
7E3716D0	With the increasing use of object-oriented methods in new software development, there is a growing need to both document and improve current practice in object-oriented design and development. In response to this need, a number of researchers have developed various metrics for object-oriented systems as proposed aids to the management of these systems. In this research, an analysis of a set of metrics proposed by Chidamber and Kemerer (1994) is performed in order to assess their usefulness for practising managers. First, an informal introduction to the metrics is provided by way of an extended example of their managerial use. Second, exploratory analyses of empirical data relating the metrics to productivity, rework effort and design effort on three commercial object-oriented systems are provided. The empirical results suggest that the metrics provide significant explanatory power for variations in these economic variables, over and above that provided by traditional measures, such as size in lines of code, and after controlling for the effects of individual developers.
7F9BE01F	The adoption of and adherence to object-oriented design and programming principles have allowed the software industry to create applications of ever-increasing complexity. A concomitant need arises for strategies to identify, manage, and, wherever possible, reduce this software complexity. One such strategy is the systematic collection, interpretation, and analysis of software metrics, mappings from software objects or constructs to sets of numerical features that quantify relevant software attributes. We describe a novel approach that employs various graph theoretic algorithms to analyze the higher level, application-wide class relationship graphs that emerge from object-oriented software. In addition to the software's overall inheritance tree characteristics, these algorithms will use metrics that reflect information on the import and export coupling of class-attribute and class-method relationships. Further, we incorporate information relating to the response sets for each object in the software, that is, the number of methods that can be executed in response to messages being received by objects.
7884E4DF	Technical, management, and political decisions made in the design phase of this shipboard command and weapons control system (more than 1 million LOC), have led to a significant percentage of reuse on subsequent projects. The practical issues and lessons learned in trying to design for massive software reuse will be discussed. 
7E66A794	A technique for automatically inserting software mechanisms to detect single event upset (SEU) in distributed Ada systems is presented. SEUs may cause information corruption, leading to a change in program flow or causing a program to execute an infinite loop. Two cooperative software mechanisms for detecting the presence of these upsets are described. Automatic insertion of these mechanisms is discussed in relation to the structure of Ada software systems. A program, software modifier for upset detection (SMUD), has been written to automatically modify Ada application software and insert software upset detection mechanisms. As an example, the mechanisms have been incorporated into a system model that employs the MIL-STD-1553B communications protocol. This system model is used as a testbed for verifying that SMUD properly inserts the detection mechanisms. Ada is used for creating the simulation environment to exercise and verify the protocol. Simulation has been used to test and verify the proper functioning of the detection mechanisms. The testing methodology, a short description of the 1553B testbed, and a set of performance measures are presented.
7DDCF7CF	Software metrics is one of the well-known topics of research in software engineering. Metrics are used to improve the quality and validity of software systems. Research in this area focus mainly on static metrics obtained by static analysis of the software. However modern software systems without object oriented design are incomplete. Every system has its own complexity which should be measured to improve the quality of the system. This paper describes the different types of metrics along with the static code metrics and Object oriented metrics. Then the metrics are summarized on the basis of relevance in finding the complexity and hence help in better maintainability of the software code, retaining the quality and making it cost effective.
7A7F3308	An object-oriented approach has become a commonly-used method in software-related activities. Many design metrics for object-oriented systems have been proposed and also employed for predicting and managing the quality of processes and products. To enhance the practical utility of object-oriented metrics in software industry, various researchers have tried to find relations between these metrics and fault proneness, but very few focus on relating them with the number-offaults in different levels as per their severity rating. In this study, empirical validation is carried out on object-oriented design metrics (i.e. Chidamber and Kemerer CK-metrics suite and source lines of codes) for predicting number-of-faults in different severity levels. Different statistical methods are used to analyze the data, including correlation.
79C57B7D	An empirical study of the relationship between object-oriented (OO) metrics and error-severity categories is presented. The focus of the study is to identify threshold values of software metrics using receiver operating characteristic curves. The study used the three releases of the Eclipse project and found threshold values for some OO metrics that separated no-error classes from classes that had high-impact errors. Although these thresholds cannot predict whether a class will definitely have errors in the future, they can provide a more scientific method to assess class error proneness and can be used by engineers easily. Copyright Â© 2009 John Wiley & Sons, Ltd.
7A9076C5	
The need to develop and maintain large complex software systems in a competitive and dynamic environment has driven interest in new approaches to software design and development. The problems with the classical waterfall model have been cataloged in almost every software engineering text [19,23]. In response, alternative models such as the spiral [2], and fountain [9] have been proposed. Problems with traditional development using the classical life cycle include no iteration, no emphasis on reuse, and no unifying model to integrate the phases. The difference in point of view between following data flows in structured analysis and building hierarchies of tasks in structured design has always been a major problem [4]. Each system is built from scratch and maintenance costs account for a notoriously large share of total system costs. The object-oriented paradigm addresses each of these issues. A look at the object-oriented software life cycle, as described by Meyer [5], Coad and Yourdon [4], and Henderson-Sellers and Edwards [9], identifies the three traditional activities of analysis, design, and implementation. However, each of the referenced descriptions eliminates the distinct boundaries between the phases. The primary reason for this blurring of boundaries is that the items of interest in each phase are the same: objects. Objects and the relationships between objects are identified in both the analysis and design phases. Objects and relationships identified and documented in the analysis phase serve not only as input to the design phase, but as an initial layer in the design. This continuity provides for a much more seamless interface between the phases. Analysts, designers and programmers are working with a common set of items upon which to build. A second reason for the blurring of these boundaries is that the object-oriented development process is iterative. Henderson-Sellers and Edwards further refine this idea by replacing the waterfall model of software development with a fountain model. Development reaches a high level only to fall back to a previous level to begin the climb once again. As an example of the blurring of the traditional boundaries of the life cycle phases, Coad and Yourdon recommend that classification relationships between objects be captured and documented during the object-oriented analysis (OOA) phase. This classification will be directly reflected in the class inheritance structure developed in the design and in the code. This classification is in no way required in order to document the system requirements. In other words, Coad and Yourdon are recommending a traditional design activity in the analysis phase. The blurring of the traditional design and implementation phases has been fueled by the development of encapsulation and abstraction mechanisms in object-oriented and object-based languages. For example, Meyer claims [14] that Eiffel is both a design and an implementation language. He goes on to say that software design is sometimes mistakenly viewed as an activity totally secluded from actual implementation. From his point of view, much is to be gained from an approach that integrates both activities within the same conceptual framework. The object-oriented design paradigm is the next logical step in a progression that has led from a purely procedural approach to an object-based approach and now to the object-oriented approach. The progression has resulted from a gradual shift in point of view in the development process. The procedural design paradigm utilizes functional decomposition to specify the tasks to be completed in order to solve a problem. The object-based approach, typified by the techniques of Yourdon, Jackson and Booth, gives more attention to data specifications than the procedural approach but still utilizes functional decomposition to develop the architecture of a system. The object-oriented approach goes beyond the object-based technique in the emphasis given to data by utilizing the relationships between objects as a fundamental part of the system architecture. The goal in designing individual software components is to represent a concept in what will eventually be an executable form. The Abstract Data Type (ADT) is the object-based paradigm's technique for capturing this conceptual information. The class is the object-oriented paradigm's conceptual modeling tool. The design pieces resulting from the object-oriented design technique represent a tighter coupling of data and functionality than traditional ADTs. These artifacts of the design process used in conjunction with a modeling-based decomposition approach yield a paradigm, a technique, which is very natural and flexible. It is natural in the sense that the design pieces are closely identified with the real-world concepts which they model. It is flexible in the sense of quickly adapting to changes in the problem specifications. Object-oriented remains a term which is interpreted differently by different people. Before presenting an overview of a set of techniques for the design process, we will give our perspective so the reader may judge the techniques in terms of those definitions. Briefly, we adapt Wegner's [27] definition for object-oriented languages to object-oriented design. The pieces of the design are objects which are grouped into classes for specification purposes. In addition to traditional dependencies between data elements, an inheritance relation between classes is used to express specializations and generalizations of the concepts represented by the classes. As natural and flexible as the object-oriented technique is, it is still possible to produce a bad design when using it. We will consider a number of general design criteria and will discuss how the object-oriented approach assists the designer in meeting these criteria. We will refer to a number of design guidelines developed specifically for the object-oriented design paradigm and will discuss how these properties reinforce the concepts of good design. The paradigm sprang from language, has matured into design, and has recently moved into analysis. The blurring of boundaries between these phases has led us to include topics in this article that are outside the realm of design, but which we consider important to understanding the design process. Since the paradigm sprang from language, we define the concepts basic to object-oriented programming in the following section.
77FC0E58	The portability and scalability benefits of Java [1, 2] combined with improved economies of scale resulting from its popularity are motivating many organizations to switch to Java. As organizations switch to Java for new development, many face difficult challenges with respect to maintenance and evolution of their existing legacy systems. In certain critical embedded and real-time domains, important legacy systems are implemented in the Ada language. This paper describes a middleware approach to enable efficient and robust integration of Ada and Java software into mixed-language software systems. Though the technology is designed to generalize to all Ada-Java mixed-language applications, this paper focuses attention on the special challenges unique to stack management of temporary objects, as characterized by the Ravenscar profile of Ada 95 and the JSR-302 subset of traditional Java. The middleware design, known as Ada-Java Method Invocation (AJMI), simplifies inter-language calls, extends object-oriented abstractions across language boundaries, and enables reliable sharing of stack-allocated objects which are integral to the use of Ada and Java in safety-critical systems.
7DA5D03D	This paper describes a design study that explores how multi-touch devices can provide support for developers when carrying out modeling tasks in software development. We investigate how well a multi-touch augmented approach performs compared to a traditional approach and if this new approach can be integrated into existing software engineering processes. For that, we have implemented a fully-functional prototype, which is concerned with agreeing on a good object-oriented design through the course of a Class Responsibility Collaboration (CRC) modeling session. We describe how multi-touch technology helps with integrating CRC cards with larger design methodologies, without loosing their unique physical interaction aspect. We observed high-potential in augmenting such informal sessions in software engineering with novel user interfaces, such as those provided by multi-touch devices.
7F8F8A2A	Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.
80545B71	This paper examines some of the underpinnings of software reliability and software development concerns. A survey was conducted of one company's employees regarding an unsuccessful large-scale program to identify some lessons learned. These lessons point out that one of the largest origins of software problems lies in communication deficiencies. The quality of the development process ultimately determines the quality of the delivered software product. The primary causes of customer problems often stem from difficulties in capturing true customer requirements. An additional challenge is assuring strong integrated product team understanding and communications. This paper identifies structured development process initiatives that assure the building of more reliable code. Then two of the more significant initiatives, object-oriented design and the defect prevention process, are discussed in detail.
2EBA97B6	A case study of the impact of Ada on a Command and Control project completed at the Jet Propulsion Laboratory (JPL) is given. The data for this study was collected as part of a general survey of software costs and productivity at JPL and other NASA sites. The task analyzed is a successful example of the use of rapid prototyping as applied to command and control for the U.S. Air Force and provides the U.S. Air Force Military Airlift Command with the ability to track aircraft, air crews and payloads worldwide. The task consists of a replicated database at several globally distributed sites. The local databases at each site can be updated within seconds after changes are entered at any one site. The system must be able to handle up to 400,000 activities per day. There are currently seven sites, each with a local area network of computers and a variety of user displays; the local area networks are tied together into a single wide area network. Using data obtained for eight modules, totaling approximately 500,000 source lines of code, researchers analyze the differences in productivities between subtasks. Factors considered are percentage of Ada used in coding, years of programmer experience, and the use of Ada tools and modern programming practices. The principle findings are the following. Productivity is very sensitive to programmer experience. The use of Ada software tools and the use of modern programming practices are important; without such use Ada is just a large complex language which can cause productivity to decrease. The impact of Ada on development effort phases is consistent with earlier reports at the project level but not at the module level. 
799CA085	This paper presents a state-of-the-art review of empirical research on object-oriented (OO) design. Many claims about the cognitive benefits of the OO paradigm have been made by its advocates. These claims concern the ease of designing and reusing software at the individual level as well as the benefits of this paradigm at the team level. Since these claims are cognitive in nature, it seems important to assess them empirically. After a brief presentation of the main concepts of the OO paradigm, the claims about the superiority of OO design are outlined.The core of this paper consists of a review of empirical studies of OO design (OOD). We first discuss results concerning OOD by individuals. On the basis of empirical work, we (1) analyse the design activity of novice OO designers, (2) compare OOD with procedural design and (3) discuss a typology of problems relevant for the OO approach. Then we assess the claims about naturalness and ease of OOD. The next part discusses results on OO software reuse. On the basis of empirical work, we (1) compare reuse in the OO versus the procedural paradigm, (2) discuss the potential for OO software reuse and (3) analyse reuse activity in the OO paradigm. Then we assess claims on reusability. The final part reviews empirical work on OOD by teams. We present results on communication, coordination, knowledge dissemination and interactions with clients. Then we assess claims about OOD at the software design team level.
79916FCC	The state of object-oriented is evolving rapidly. This survey describes what are currently thought to be the key ideas. Although it is necessarily incomplete, it contains both academic and industrial efforts and describes work in both the United States and Europe. It ignores well-known ideas, like that of Coad and Meyer [34], in favor of less widely known projects.Research in object-oriented design can be divided many ways. Some research is focused on describing a design process. Some is focused on finding rules for good designs. A third approach is to build tools to support design. Most of the research described in this article does all three.We first present work from Alan Snyder at Hewlett-Packard on developing a common framework for object-oriented terminology. The goal of this effort is to develop and communicate a corporate-wide common language for specifying and communicating about objects.We next look into the research activity at Hewlett-Packard, led by Dennis de Champeaux. De Champeaux is developing a model for object-based analysis. His current research focuses on the use of a trigger-based model for inter-object communications and development of a top-down approach to analysis using ensembles.We then survey two research activities that prescribe the design process. Rebecca Wirfs-Brock from Tektronix has been developing an object-oriented design method that focuses on object responsibilities and collaborations. The method includes graphical tools for improving encapsulation and understanding patterns of object communication. Trygve Reenskaug at the Center for Industriforskning in Oslo, Norway has been developing an object-oriented design method that focuses on roles, synthesis, and structuring. The method, called Object-Oriented Role Analysis, Syntheses and Structuring, is based on first modeling small sub-problems, and then combining small models into larger ones in a controlled manner using both inheritance (synthesis) and run-time binding (structuring).We then present investigations by Ralph Johnson at the University of Illinois at Urbana-Champaign into object-oriented frameworks and the reuse of large-scale designs. A framework is a high-level design or application architecture and consists of a suite of classes that are specifically designed to be refined and used as a group. Past work has focused on describing frameworks and how they are developed. Current work includes the design of tools to make it easier to design frameworks.Finally, we present some results from the research group in object-oriented software engineering at Northeastern University, led by Karl Lieberherr. They have been working on object-oriented Computer Assisted Software Engineering (CASE) technology, called the Demeterm system, which generates language-specific class definitions from language-independent class dictionaries. The Demeter system include tools for checking design rules and for implementing a design.
5C9BCB40	About ten years ago, the first serious applications of concept lattices in software analysis were published. Today, a wide range of applications of concept lattices in static and dynamic analysis of software artefacts is known. This overview summarizes important papers from the last ten years, and presents three methods in some detail: 1. methods to extract classes and modules from legacy software; 2. the Snelting/Tip algorithm for application-specific, semantics-preserving refactoring of class hierarchies; 3. Ballâs method for infering dynamic dominators and control flow regions from program traces. We conclude with some perpectives on further uses of concept lattices in software technology.
74707DC0	The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts.
8025DE28	Agent-oriented software engineering (AOSE) has become an active area of research in recent years. We look at the use of agent-oriented concepts for software analysis. Using agent-oriented analysis may offer benefits even if the system is implemented without an agent-based language or framework (e.g. using an object-oriented detailed design and language). We examine the software analysis components of a number of existing agent-oriented methodologies. We discuss the benefits that can be gained by using agent-oriented concepts, and where the concepts require further development. Based on this analysis, we present the agent-oriented methodology that we are developing, and describe an example of how it may be applied for software analysis.
795ECF43	This study aims at developing and empirically testing hypotheses about professional designers' cognitive activities when using object-oriented methodology (OOD) versus using traditional functional decomposition methodologies (TFD). Our preliminary results indicate that OOD may achieve substantial time savings over TFD in logical design. The verbal protocols from a pilot study show that OOD may achieve these time savings: 1) by simplifying rule induction processes used in functional decomposition; 2) by guiding designers on how to build more effective problem spaces; and 3) by allowing designers to run mental simulation more efficiently and more effectively.
7E67E534	In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes
7F62DF4A	Many software development organizations are adopting object-oriented methodologies as their primary paradigm for software development. The object-oriented method appears to increase programmer productivity, reduce the overall cost of the software and, perhaps most importantly, creates software that promotes reuse and subsequently is easier to modify. Consistent with the change in industry, many universities and industry training organizations are currently in the process of integrating object orientation into their curriculum. There are several approaches including horizontal integration (integrating a small dose of the object orientation into many courses) and vertical integration (having a large dose of the concepts in a single course). In 1996, the Systems Analysis department of Miami University, USA, opted for the latter approach and added a new course to its curriculum. It is a course that is intended to provide some in-depth exposure to object-oriented design and implementation. It should be of particular value to faculty in computer science and information systems departments (both at the 4-year and 2-year institutions) as well as those in industry training organizations who are looking for ways to incorporate the object orientation into their curriculum. In this paper, the authors describe the choices their department made, what worked well, and what needs to be improved.
60E0AAD7	This paper presents an empirical study of the software reuse activity by expert designers in the context of object-oriented design. Our study focuses on the three following aspects of reuse : (1) the interaction between some design processes, e.g. constructing a problem representation, searching for and evaluating solutions, and reuse processes, i.e. retrieving and using previous solutions, (2) the mental processes involved in reuse, e.g. example-based retrieval or bottom-up versus top-down expanding of the solution, and (3) the mental representations constructed throughout the reuse activity, e.g. dynamic versus static representations. Some implications of these results for the specification of software reuse support environments are discussed. 
7D1C7594	This study provides an initial assessment of the U.S.'s industrial capacity to produce MCCR software. A Survey of senior government and industry people showed that 90 percent of them expected a serious problem with the nation's capacity to produce military software over the next 5 years. They ranked acquisition and labor factors as contributing most to the failure of military system development contracts to meet schedule or costs. The study team also analyzed available data about the supply of labor (new graduates and experienced scientists and engineers) and three aspects of demand (Ada systems, PDSS, and related commercial applications) before concluding there is a serious capacity problem. The report describes, organizational, and technological issues affecting software production capacity and concludes with some preliminary recommendations for DoD and industry initiatives. (Author) (kr)
2FBC050F	The SEL is an organization sponsored by NASA/GSFC to investigate the effectiveness of software engineering technologies applied to the development of applications software. The SEL was created in 1977 and has three organizational members: NASA/GSFC, Systems Development Branch; The University of Maryland, Computer Sciences Department; and Computer Sciences Corporation, Systems Development Operation. The goals of the SEL are as follows: (1) to understand the software development process in the GSFC environments; (2) to measure the effect of various methodologies, tools, and models on this process; and (3) to identify and then to apply successful development practices. The activities, findings, and recommendations of the SEL are recorded in the Software Engineering Laboratory Series, a continuing series of reports that include the Ada Performance Study Report. This paper describes the background of Ada in the Flight Dynamics Division (FDD), the objectives and scope of the Ada Performance Study, the measurement approach used, the performance tests performed, the major test results, and the implications for future FDD Ada development efforts. 
8120C03C	Loud discussions concerning various ways of teaching object-orientation have taken place without much empirical evidence for any position. This paper reports qualitative observations of learning of object-ori-ented programming in an introductory course. The students were found to cope reasonably well with the object-oriented concepts, and they had learnt procedural programming first. However, they modeled the real world domain to be represented in the program by imagination and through coding. Their problems may be attributed to the high complexity generated by having to relate to four - six areas of attention. Three ways of improving teaching are suggested, making the areas of attention and the ways to relate them more explicit for the students, forcing modeling by means of a tool, and reducing complexity by means of programming environments that visualize objects and their behavior. 
77A26FC8	Object oriented design brought up improvement of productivity and software quality by adopting some concepts such as inheritance and encapsulation. However, both the number of software`s classes and object couplings are increasing as the software volume is becoming larger. The object coupling between classes is closely related with software complexity, and high complexity causes decreasing software quality. In order to solve the object coupling issue, IT-field researchers adopt a component based development and software quality metrics. The component based development requires explicit representation of dependencies between classes and the software quality metrics evaluates quality of software. As part of the research, we intend to gain a basic data that will be used on decomposing software. We focused on properties of the linkage between classes rather than previous studies evaluated and accumulated the qualities of individual classes. Our method exploits machine learning technique to analyze the properties of linkage and predict the strength of dependency between classes, as a new perspective on analyzing software property.
7E017F8E	The role of reuse within software engineering is examined. The benefits of reuse are stated, inhibitors to successful reuse are pointed out, and technical issues for achieving reuse are examined. Specific attention is paid to how object-oriented software engineering enhances or reduces the potential for reuse as compared with traditional (i.e. not based on objects or object-oriented concepts) software engineering. Three major activities for supporting reuse are described. The first is the intentional development of reusable artifacts, the second is the representation and classification of artifacts into repositories, and the last is the utilization of the artifacts from repositories. The object-oriented model is found to be significantly similar to the traditional model in many aspects. However, differences are found insofar as object-oriented design supports the design techniques of abstraction and factorization, which utilize object inheritance to yield reusable objects.
0B76970E	This paper aims at empirically exploring the relationships between most of the existing design coupling, cohesion, and inheritance measures for object-oriented (OO) systems, and the fault-proneness of OO system classes. The underlying goal of this study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. In addition, we aim at assessing whether such relationships, once modeled, can be used to effectively drive and focus inspections or testing. The study described here is a replication of an analogous study conducted in a university environment with systems developed by students. In order to draw more general conclusions and to (dis)confirm the results obtained there, we now replicated the study using data collected on an industrial system developed by professionals. Results show that many of our findings are consistent across systems, despite the very disparate nature of the systems under study. Some of the strong dimensions captured by the measures in each data set are visible in both the university and industrial case study. For example, the frequency of method invocations appears to be the main driving factor of fault-proneness in all systems. However, there are also differences across studies, which illustrate the fact that, although many principles and techniques can be reused, quality does not follow universal laws and quality models must be developed locally, wherever needed.
7DA864F8	The Submarine Message Buffer (SMB) is a ml time embedded message processing system developed at the Naval Command, Control and Ocean Surveillance Center, Research, Development, Test and Evaluation Division (NRaD). The SMB is sponsored by the Space and Naval Warfare Systems Command (SPAWAR) to support modernization of SSN (Los Angels and Seawolf class submarines) radio rooms. The development strategy adopted for the SMB concentrated on the reuse of Ada software. This paper will focus on the design, processes and methodologies, which were used in the development of this system. Metrics will also be provided showing why this system has been identified as an Ada success story by the Ada Joint Program Office among others
5FBCD285	Based on our experience using active learning methods to teach object-oriented software design we propose a game-based approach to take the classroom experience into a virtual environment.The different pedagogical approaches that our active method supports, have motivated us to tailor an architecture that supports the creation of different variations of role-play environments, ranging from open-ended trial and error approaches to highly constrained settings where students can not get very far from the solution. We also describe a prototype that instantiates this architecture called ViRPlay3D2.
13ACDC2A	This study investigates the needs of software developers in the area of software reuse. The study formulates recommendations on how a software repository can meet the needs of the software development community. The Naval Software Reuse System (NSRS) is the focus of this research and the findings are intended for their use. The study focuses on eight core questions that are the central issues for the NSRS. On the basis of these eight core questions, a survey was developed that elicited the opinions of Department of the Navy software developers. The analysis of the survey provides conclusions for seven of the eight core questions. These conclusions lead to recommendations that for the most part are completely within the control of Naval Computers and Telecommunications Station (NCTS) Washington. NCTS Washington is the agency presently building the NSRS.
75E04CFE	Software designers in the object-oriented paradigm can make use of modeling tools and standard notations such as UML. Nevertheless, casual observations from collocated design collaborations suggest that teams tend to use physical mediums to sketch a plethora of informal diagrams in varied representations that often diverge from UML. To better understand such collaborations and support them with tools, we need to understand the origins, roles, uses, and implications of these alternate representations. To this end we conducted observational studies of collaborative design exercises, in which we focused on representation use.Our primary finding is that teams intentionally improviserepresentations and organize design information in responseto ad-hoc needs, which arise from the evolution of the design, and which are difficult to meet with fixed standard notations. This behavior incurs orientation and grounding difficulties for which teams compensate by relying on memory, other communication mediums, and contextual cues. Without this additional information the artifacts are difficult to interpret and have limited documentation potential. Collaborative design tools and processes should therefore focus on preserving contextual information while permitting unconstrained mixing and improvising of notations.
7D53B04F	A modeling and prototyping approach for defining system and software requirements and validating them through rapid prototyping is described, and the results of a case study for generating prototype software (representing the total system hardware and software) directly and automatically from the requirements are presented. System/software definitions, which include the requirements and design (architecture), are addressed. The case study applied this approach to selected systems of Boeing 747-400 aircraft. A formal model of system specification was generated. The rapid prototyping task automatically generated thousands of lines of Ada source code from the specification model. The software was executed successfully the first time. The functions and behavior of the system were demonstrated and validated by its users. This study indicated that early execution and validation of system requirements, through the use of formal modeling and rapid prototyping with direct user involvement, can be accomplished.
78154E04	The AFHRL has a long term research project to develop an Integrated Maintenance Information System (IMIS). The project was initiated in 1979 and is expected to be completely by 1991. The objective of IMIS is to give maintenance technicians a very small size portable computer/display that will interface with on-aircraft systems and other computer systems to provide a single, integrated source of the information needed to perform maintenance on the line and in the shop. The system will display graphic instructions, provide intelligent diagnostic advice, provide aircraft battle damage assessment aids, analyze in-flight access and interrogate on-board built-in-test capabilities. It will also provide the technician with easy and efficient methods to receive work orders, report maintenance actions, order parts from supply, and offer complete computer-aided training lessons and simulations.The objectives of the study were to review and evaluate the requirements specifications and to make recommendations to accomplish total compatibility of IMIS with current and future requirements of the users with an emphasis on methodologies/tools for software requirements specification. The study concentrated on structured tools which could be used for IMIS. The various reports submitted by contractors and those developed in-house related to concepts of IMIS, feasibility aspects and information requirements of maintenance technicians were reviewed by the researcher, and it was observed that structured tools have not been used in defining user requirements or in development of software. The information requirements of maintenance technicians were presented using process flowcharts and narration. A great deal of mental effort is required to understand these charts. Checking for consistency and completeness of requirements is a formidable task when these charts are used.It was recommended that structured techniques should be used at the various phases of IMIS software development life cycle. They would provide efficient documentation of various phases and also lead to solutions in successive phases. They could also be used as management tools in assigning tasks to various contractors and in monitoring the progress of IMIS project. Some of the tools which could be used in the IMIS project are: Data Flow Diagrams, Structure Charts, Warnier-Orr Methodology, Nassi-Shneiderman Charts, Decision Tables, and Entity-Relationship Diagrams.It was also recommended that automated structured tools should be adapted as it is not possible to enforce uniformity and consistency if the documentation is done manually. An automated tool for DFD was obtained and some of the information requirements were presented through the DFD. An effort is in progress to select appropriate automated tools for various phases of IMIS life cycle.
5C5C50B8	Thispaper aims at empirically exploring the relationships betweenmost of the existing design coupling, cohesion, and inheritancemeasures for object-oriented (OO) systems, and the fault-pronenessof OO system classes. The underlying goal of this study is tobetter understand the relationship between existing design measurementin OO systems and the quality of the software developed. in addition,we aim at assessing whether such relationships, once modeled,can be used to effectively drive and focus inspections or testing.The study described here is a replication of an analogous studyconducted in a university environment with systems developedby students. In order to draw more general conclusions and to(dis)confirm the results obtained there, we now replicated thestudy using data collected on an industrial system developedby professionals. Results show that many of our findings areconsistent across systems, despite the very disparate natureof the systems under study. Some of the strong dimensions capturedby the measures in each data set are visible in both the universityand industrial case study. For example, the frequency of methodinvocations appears to be the main driving factor of fault-pronenessin all systems. However, there are also differences across studies,which illustrate the fact that, although many principles andtechniques can be reused, quality does not follow universal lawsand quality models must be developed locally, wherever needed.
7B86871D	This paper describes an Essential Software Framework for Meshfree Methods (ESFM). Through thorough analyses of many existing meshfree methods, their common elements and procedures are identified, and a general procedure is formulated into ESFM that can facilitate their implementations and accelerate new developments in meshfree methods. ESFM also modulates performance-critical components such as neighbor-point searching, sparse-matrix storage, and sparse-matrix solver enabling developed meshfree analysis programs to achieve high-performance. ESFM currently consists of 21 groups of classes and 94 subclasses, and more algorithms can be easily incorporated into ESFM. Finally, ESFM provides a common ground to compare various meshfree methods, enabling detailed analyses of performance characteristics.