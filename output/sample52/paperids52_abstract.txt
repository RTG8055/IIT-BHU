5E672001	In this paper, we propose a new manifold representation for visual speech recognition. The developed system consists of three main steps: a) lip extraction from input video data, b) generate the expectation-maximization PCA (EMPCA) manifolds for the entire image sequence and perform manifold interpolation and re-sampling, c) classify the manifolds using a HMM classifier to identify the words described by the lips motions in the input video sequence.
80118637	We extend our earlier work on deep-structured conditional random field (DCRF) and develop deep-structured hidden conditional random field (DHCRF). We investigate the use of this new sequential deep-learning model for phonetic recognition. DHCRF is a hierarchical model in which the final layer is a hidden conditional random field (HCRF) and the intermediate layers are zero-th-order conditional random fields (CRFs). Parameter estimation and sequence inference in the DHCRF are developed in this work. They are carried out layer by layer so that the time complexity is linear to the number of layers. In the DHCRF, the training label is available only at the final layer and the state boundary is unknown. This difficulty is addressed by using unsupervised learning for the intermediate layers and lattice-based supervised learning for the final layer. Experiments on the standard TIMIT phone recognition task show small performance improvement of a three-layer DHCRF over a two-layer DHCRF; both are significantly better than the single-layer DHCRF and are superior to the discriminatively trained tri-phone hidden Markov model (HMM) using identical input features.
0000AB99	In this paper we have developed a novel technique to deal with the problem of feeding a temporal variable speech signal to Multi-Layered Perceptron (MLP), which generally only accept fixed-dimension input pattern, for speech recognition. Instead of using conventional linear or nonlinear interpolation methods, this method is based on the integration of an MLP with Finite-State Vector Quantizer (FSVQ) which is characterized by the ability to memorize the correlations between successive speech feature vectors. FSVQ is designed to map the variable length input pattern into an activation trace on a set of sub-codebooks, each of which corresponds to a 'frame' input unit of the MLP. Experiments show that for the multi-speaker English Alphabet E-set task it can achieve better performance than an MLP with a non linearly interpolated input of fixed dimension. 
7843D7EA	Pattern classification is an instinct in human, and artificial neural networks (ANN) can be its biotical simulation. Therefore, a pattern classification based on Pi calculus is firstly discussed, which use the same basic principles of concurrent computation as neural networks; secondly this paper introduces a method to use Pi calculus for ANN and illustrating equivalence between them. Finally its superiority is also discussed.
7E8C6BD1	We develop and present the deep-structured conditional random field (CRF), a multi-layer CRF model in which each higher layer's input observation sequence consists of the previous layer's observation sequence and the resulted frame-level marginal probabilities. Such a structure can closely approximate the long-range state dependency using only linear-chain or zeroth-order CRFs by constructing features on the previous layer's output (belief). Although the final layer is trained to maximize the log-likelihood of the state (label) sequence, each lower layer is optimized by maximizing the frame-level marginal probabilities. In this deep-structured CRF, both parameter estimation and state sequence inference are carried out efficiently layer-by-layer from bottom to top. We evaluate the deep-structured CRF on two natural language processing tasks: search query tagging and advertisement field segmentation. The experimental results demonstrate that the deep-structured CRF achieves word labeling accuracies that are significantly higher than the best results reported on these tasks using the same labeled training set.
7BD8C65F	Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems.
7D5D90EF	A structured generative model of speech coarticulation and reduction is described with a novel two-stage implementation. At the first stage, the dynamics of formants or vocal tract resonances (VTRs) in fluent speech is generated using prior information of resonance targets in the phone sequence, in absence of acoustic data. Bidirectional temporal filtering with finite-impulse response (FIR) is applied to the segmental target sequence as the FIR filter's input, where forward filtering produces anticipatory coarticulation and backward filtering produces regressive coarticulation. The filtering process is shown also to result in realistic resonance-frequency undershooting or reduction for fast-rate and low-effort speech in a contextually assimilated manner. At the second stage, the dynamics of speech cepstra are predicted analytically based on the FIR-filtered and speaker-adapted VTR targets, and the prediction residuals are modeled by Gaussian random variables with trainable parameters. The combined system of these two stages, thus, generates correlated and causally related VTR and cepstral dynamics, where phonetic reduction is represented explicitly in the hidden resonance space and implicitly in the observed cepstral space. We present details of model simulation demonstrating quantitative effects of speaking rate and segment duration on the magnitude of reduction, agreeing closely with experimental measurement results in the acoustic-phonetic literature. This two-stage model is implemented and applied to the TIMIT phonetic recognition task. Using the N-best (N=2000) rescoring paradigm, the new model, which contains only context-independent parameters, is shown to significantly reduce the phone error rate of a standard hidden Markov model (HMM) system under the same experimental conditions.
800458B9	We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.
78DF895A	Modeling dynamic structure of speech is a novel paradigm in speech recognition research within the generative modeling framework, and it offers a potential to overcome limitations of the current hidden Markov modeling approach. Analogous to structured language models where syntactic structure is exploited to represent long-distance relationships among words , the structured speech model described in this paper makes use of the dynamic structure in the hidden vocal tract resonance space to characterize long-span contextual influence among phonetic units. A general overview is provided first on hierarchically classified types of dynamic speech models in the literature. A detailed account is then given for a specific model type called the hidden trajectory model, and we describe detailed steps of model construction and the parameter estimation algorithms. We show how the use of resonance target parameters and their temporal filtering enables joint modeling of long-span coarticulation and phonetic reduction effects. Experiments on phonetic recognition evaluation demonstrate superior recognizer performance over a modern hidden Markov model-based system. Error analysis shows that the greatest performance gain occurs within the sonorant speech class
7D658A7A	This paper introduces an enhanced phoneme-based myoelectric signal (MES) speech recognition system. The system can recognize new words without retraining the phoneme classifier, which is considered to be the main advantage of phoneme-based speech recognition. It is shown that previous systems experience severe performance degradation when new words are added to a testing dataset. To maintain high accuracy with new words, several improvements are proposed. In the proposed MES speech recognition approach, the raw MES is processed by class-specific rotation matrices to spatially decorrelate the data prior to feature extraction in a preprocessing stage. Then, an uncorrelated linear discriminant analysis is used for dimensionality reduction. The resulting data are classified through a hidden Markov model classifier to obtain the phonemic log likelihoods of the phonemes, which are mapped to corresponding words using a word classifier. An average word classification accuracy of 98.533% is achieved over six subjects. The system offers dramatically improved accuracy when expanding a vocabulary, offering promise for robust large-vocabulary myoelectric speech recognition.
7F4B2665	Traditional acoustic speech recognition accuracies have been shown to deteriorate in highly noisy environments. A secondary information source is exploited using surface myoelectric signals (MES) collected from facial articulatory muscles during speech. Words are classified at the phoneme level using a hidden Markov model (HMM) classifier. Acoustic and MES data was collected while the words "zero" through "nine" were spoken. An acoustic expert classified the 18 formative phonemes in low noise levels [signal-to-noise ratio (SNR) of 17.5 dB] with an accuracy of 99%, but deteriorated to approximately 38% under simulations with SNR approaching 0 dB. A fused acoustic-myoelectric multiexpert system, without knowledge of SNR, improved on acoustic classification results at all noise levels. A multiexpert system, incorporating SNR information, obtained accuracies of 99% at low noise levels while maintaining accuracies above 94% during low SNR (0 dB) simulations. Results improve on previous full word MES speech recognition accuracies by almost 10%
7C2E106F	Many alternative models have been proposed to address some of the shortcomings of the hidden Markov model (HMM), which is currently the most popular approach to speech recognition. In particular, a variety of models that could be broadly classified as segment models have been described for representing a variable-length sequence of observation vectors in speech recognition applications. Since there are many aspects in common between these approaches, including the general recognition and training problems, it is useful to consider them in a unified framework. The paper describes a general stochastic model that encompasses most of the models proposed in the literature, pointing out similarities of the models in terms of correlation and parameter tying assumptions, and drawing analogies between segment models and HMMs. In addition, we summarize experimental results assessing different modeling assumptions and point out remaining open questions.
8068D18D	In enclosed environments where robots are deployed, the observed speech signal is smeared due to reverberation. This degrades the performance of the automatic speech recognition (ASR). Thus, hands-free speech recognition for human-machine communication is a difficult task. Most speech enhancement techniques used to address this problem enhance the contaminated waveform independent from that of the ASR. However, this approach does not necessarily improve ASR performance. In this paper, we expand the conventional spectral subtraction-based (SS) technique to deal with reverberation. In our proposed approach, the dereverberation parameters of SS are optimized to improve the likelihood of the acoustic model and not just the waveform signal. The system is capable of adaptively fine-tuning these parameters jointly with acoustic model training for effective use in ASR application. We have experimented using real reverberant data collected from an operational robot. Moreover, we also evaluated with reverberant data corrupted with environmental and robot internal noise. Experimental results show that the proposed method significantly improves the recognition performance over conventional approach.
12F4604F	This paper describes a novel speaker-independent speech recognition method, called speaker-consistent parsing", which is based on an intra-speaker correlation called the speaker-consistency principle. We focus on the fact that a sentence or a string of words is uttered by an individual speaker even in a speaker-independent task. Thus, the proposed method searches through speaker variations in addition to the contents of utterances. As a result of the recognition process, an appropriate standard speaker is selected for speaker adaptation. This new method is experimentally compared with a conventional speaker-independent speech recognition method. Since the speaker-consistency principle best demonstrates its effect with a large number of training and test speakers, a small-scale experiment may not fully exploit this principle. Nevertheless, even the results of our small-scale experiment show that the new method significantly outperforms the conventional method. In addition, this framework's speaker selection mechanism can drastically reduce the likelihood map computation.
7EF2B2E3	One of the major drawbacks to using speech as the input to any pervasive environment is the requirement to balance accuracy with the high processing overheads involved. This paper presents an Arabic speech recognition system (called UbiqRec), which address this issue by providing a natural and intuitive way of communicating within ubiquitous environments, while balancing processing time, memory and recognition accuracy. A hybrid approach has been used which incorporates spectrographic information, singular value decomposition, concurrent self-organizing maps (CSOM) and pitch contours for Arabic phoneme recognition. The approach employs separate self-organizing maps (SOM) for each Arabic phoneme joined in parallel to form a CSOM. The performance results confirm that with suitable preprocessing of data, including extraction of distinct power spectral densities (PSD) and singular value decomposition, the training time for CSOM was reduced by 89%. The empirical results also proved that overall recognition accuracy did not fall below 91%.
7DB61D52	Lipreading is an efficient method among those proposed to improve the performance of speech recognition systems, especially in acoustic noisy environments. This paper proposes a simple audio-visual speech recognition (AVSR) system, which could improve the robustness and accuracy of audio speech recognition by integrating the synchronous audio and visual information. We propose a hidden Markov model (HMM) based on the probabilistic principal component analysis (PCA) for the visual-only speech recognition and the visual modality of the audio-visual speech recognition. The probabilistic PCA based HMM directly uses the images which only contain the speaker's mouth region without pre-processing (mouth corner detection, contour marking, etc), and takes probabilistic PCA as the observation probability density function (PDF). Then we integrate these two modalities information (audio and visual) together and obtain a multi-stream hidden Markov model (MSHMM). We found that, without extracting the specialized features before processing, probabilistic PCA could capture the principal components during the training and describe the visual part of the materials. It is also verified by the experiments that the integration of the audio and visual information could help to improve the recognition accuracy even at a low acoustic signal-to-noisy ratio (SNR).
8029ACB7	An improved BP neural network classifier integration method was mainly described, by which using k-means clustering a group of value of weights and thresholds with some differences were gotten, and then as the value of individuals of integrated network to improve the performance of integrated learning, and be successfully applied to non-specific human isolated word speech recognition system. By comparing the experimental result and the traditional Adaboost integration algorithm, the validity of the method was confirmed.
79B47D09	A pre-processing of linear predictive coefficient (LPC) features for preparation of reliable reference templates for the set of words to be recognized using the artificial neural network is presented in this paper. The paper also proposes the use of pitch feature derived from the recorded speech data as another input feature. The Dynamic Time Warping algorithm (DTW) is the back-bone of the newly developed algorithm called DTW fixing frame algorithm (DTW-FF) which is designed to perform template matching for the input preprocessing. The purpose of the new algorithm is to align the input frames in the test set to the template frames in the reference set. This frame normalization is required since NN is designed to compare data of the same length, however same speech varies in their length most of the time. By doing frame fixing, the input frames and the reference frames are adjusted to the same number of frames according to the reference frames. Another task of the study is to extract pitch features using the Harmonic Filter algorithm. After pitch extraction and linear predictive coefficient (LPC) features fixed to a desired number of frames, speech recognition using neural network can be performed and results showed a very promising solution. Result showed that as high as 98% recognition can be achieved using combination of two features mentioned above. At the end of the paper, a convergence comparison between conjugate gradient descent (CGD), Quasi-Newton, and steepest gradient descent (SGD) search direction is performed and results show that the CGD outperformed the Newton and SGD.
82F9BBC3	Voice activity detection (VAD) is an important topic in audio signal processing. Contextual information is important for improving the performance of VAD at low signal-to-noise ratios. Here we explore contextual information by machine learning methods at three levels. At the top level, we employ an ensemble learning framework, named multi-resolution stacking (MRS), which is a stack of ensemble classifiers. Each classifier in a building block inputs the concatenation of the predictions of its lower building blocks and the expansion of the raw acoustic feature by a given window (called a resolution). At the middle level, we describe a base classifier in MRS, named boosted deep neural network (bDNN). bDNN first generates multiple base predictions from different contexts of a single frame by only one DNN and then aggregates the base predictions for a better prediction of the frame, and it is different from computationally-expensive boosting methods that train ensembles of classifiers for multiple base predictions. At the bottom level, we employ the multi-resolution cochleagram feature, which incorporates the contextual information by concatenating the cochleagram features at multiple spectrotemporal resolutions. Experimental results show that the MRS-based VAD outperforms other VADs by a considerable margin. Moreover, when trained on a large amount of noise types and a wide range of signal-to-noise ratios, the MRS-based VAD demonstrates surprisingly good generalization performance on unseen test scenarios, approaching the performance with noise-dependent training.
7FD6EED9	We apply the recently proposed Context-Dependent Deep- Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-text transcription. For single-pass speaker-independent recognition on the RT03S Fisher portion of phone-call transcription benchmark (Switchboard), the word-error rate is reduced from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs, to 18.5%?aa 33% relative improvement. CD-DNN-HMMs combine classic artificial-neural-network HMMs with traditional tied-state triphones and deep-beliefnetwork pre-training. They had previously been shown to reduce errors by 16% relatively when trained on tens of hours of data using hundreds of tied states. This paper takes CD-DNNHMMs further and applies them to transcription using over 300 hours of training data, over 9000 tied states, and up to 9 hidden layers, and demonstrates how sparseness can be exploited. On four less well-matched transcription tasks, we observe relative error reductions of 22¨C28%.
80C3AB1B	We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third-from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%-using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.
5E0BE9BB	We study the problem of how to recognize a person only by his eyebrow based on hidden Markov models (HMM). By experiments on a small-scale eyebrow image database taken from 27 subjects, we show that our HMM-based eyebrow recognition method can achieve the highest accuracy of 92.6%, based on the relation of its accuracy to the number of observation symbols and that of states. Hence, we conclude that human eyebrow can work as a biometric with certain possibility and feasibility.
71BFFE83	This paper presents a way of improving the recognition rate of a typical Hidden Markov Model (HMM)-based Automatic Speech Recognition (ASR) system by integrating the l1 - least absolute deviation (LAD) algorithm and the l0 - least square (LS) algorithm in a framework designed to selectively use them based on the level of impulse noise present in speech signal. We present the overall architecture of the model, as well as experimental results and compare our enhanced noise-robust HMM-based ASR system with state-of-the-art proving the improvements brought by this approach as well as future directions of research.
7D6812CD	JANUS-II is a research system for investigating various issues in speech-to-speech translations and has been implemented for translations in many languages. In this paper, we address the Spanish speech recognition part of JANUS-II. First, we report the bootstrapping and optimization of the recognition system. Then we investigate the difference between push-to-talk and cross-talk dialogs, which are two different kinds of data in our database. We give a detailed noise analysis for the push-to-talk and cross-talk dialogs and present some recognition results for comparison. We have observed that the cross-talk dialogs are harder than the push-to-talk dialogs for speech recognition, because they are more noisy than the latter. Currently, the error rate of our Spanish recognizer is 27% for the push-to-talk test set and 32% for the cross-talk test set.
0B91B818	Czech (like other Slavic languages) is well known for its complex morphology. Text processing (e.g., automatic translation, syntactic analysis...) usually requires unambiguous selection of grammatical categories (so called morphological tag) for every word in a text. Morphological tagging consists of two parts - assigning all possible tags to every word in a text and selecting the right tag in a given context. Project Morče attempts to solve the second part, usually called disambiguation. Using a statistical method based on the combination of a Hidden Markov Model and the AveragedAveraged Perceptron algorithm, a number of experiments have been made exploring different parameter settings of the algorithm in order to obtain the best success rate possible. Final accuracy of Morče on data from PDT 2.0 was 95.431% (results of March 2006). So far, it is the best result for a standalone tagger.
7D030924	Speech recognition is a key element of diverse applications in communication systems, medical transcription systems, security systems etc. However, there has been very little research in the domain of speech processing for African languages, thus, the need to extend the frontier of research in order to port in, the diverse applications based on speech recognition. Hausa language is an important indigenous lingua franca in west and central Africa, spoken as a first or second language by about fifty million people. Speech recognition of Hausa Language is presented in this paper. A pattern recognition neural network was used for developing the system.
7940F51A	Pitch determination is a fundamental problem in speech processing, which has been studied for decades. However, it is challenging to determinate pitch in strong noise because the harmonic structure is corrupted. In this paper, we estimate pitch using supervised learning, where the probabilistic pitch states are directly learned from noisy speech data. We investigate two alternative neural networks modeling pitch state distribution given observations. The first one is a feedforward deep neural network (DNN), which is trained on static frame-level acoustic features. The second one is a recurrent deep neural network (RNN) which is trained on sequential frame-level features and capable of learning temporal dynamics. Both DNNs and RNNs produce accurate probabilistic outputs of pitch states, which are then connected into pitch contours by Viterbi decoding. Our systematic evaluation shows that the proposed pitch tracking algorithms are robust to different noise conditions and can even be applied to reverberant speech. The proposed approach also significantly outperforms other state-of-the-art pitch tracking algorithms.
80BD42AB	Artificial intelligence — one of the most interesting theoretical and applied areas of computer science. There is a wide range of techniques and approaches for creation of artificial intelligence in the applications and real-time control systems. In this paper, we propose a realization of neuro-automata controlling based on neural networks and automaton paradigm and considering an example of how this paradigm could be used in real application.
8179B46E	A robust speech recognition system for videoconference applications is presented based on a microphone array. By means of a microphone array, the speech recognition system is able to know the position of the users and increase the signal-to-noise ratio (SNR) between the desired speaker signal and the interference from the other users. The user positions are estimated by means of the combination of a direction of arrival (DOA) estimation method with a speaker identification system. The beamforming is performed by using the spatial references of the desired speaker and the interference locations. A minimum variance algorithm with spatial constraints working in the frequency domain is used to design the weights of the broadband microphone array. Results of the speech recognition system are reported in a simulated environment with several users asking questions to a geographic data base.
0D759238	This paper considers the robust tracking control problem for output stochastic distributions of dynamic non-Gaussian systems. By using the square root B-spline approximations with modelling errors, a robust constrained tracking control strategy with proportional-integral (PI) structure is investigated for a nonlinear weighting system in the presence of exogenous disturbances. The main objective is to make the output probability density functions (PDFs) to follow a target PDF. An LMI-based PI control algorithm is proposed to track the desired weight dynamics, where the robust peak-to-peak measure is applied to optimize the tracking performance and the state constraints system related to the B-spline expansion can be guaranteed. Rigorous stability and performance analysis is provided for the constrained weight tracking control problem
72C1B620	As for the problem of too long training time of convolution neural network (CNN), this paper proposes a fast training method for CNN in SAR automatic target recognition (ATR). The CNN is divided into two parts: one that contains all the convolution layers and sub-sampling layers is considered as convolutional auto-encoder (CAE) for unsupervised training to extract high-level features; the other that contains fully connected layers is regarded as shallow neural network (SNN) to work as a classifier. The experiment based on MSATR database shows that the proposed method can tremendously reduce the training time with little loss of recognition rate.
0889039D	This paper introduces a method for regularization of HMM systems that avoids parameter overfitting caused by insufficient training data.  Regularization is done by augmenting the EM trainicng method by a penalty term that favors simple and smooth HMM systems.  The penalty term is constructed as a mixture model ofnegative exponential distributions that is assumed to generate the state dependent emission probabilities of the HMMs.  This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HcMM systems. The effect of regularization is demonstrated for contcinuous speech recognition tasks by improving overfitted triphone modelsand by speaker adaptation with limited training data.
659F446A	The paper presents a new variant of parameter estimation methods for discrete hidden Markov models (HMM) in speech recognition. This method makes use of a codeword dependent distribution normalization (CDDN) and a distance weighting by fuzzy contribution in dealing with the problems of robust state modeling in FVQ based modeling. The proposed method is compared with the existing techniques using speaker-independent phonetically balanced isolated word recognition. The results have shown that the recognition rate of the proposed method is improved 4.5% over the conventional NQ based method and the distance weighting to the smoothing of output probability is more efficient than the distance based codeword weighting.
7DEC73B6	This paper presents a statistical approach to developing multimodal recognition systems and, in particular, to integrating the posterior probabilities of parallel input signals involved in the multimodal system. We first derive the performance bounds of multimodal recognition probabilities, and identify the primary factors that influence multimodal recognition performance. We then develop a technique, a members-teams-committee (MTC) recognition approach, designed to optimize accurate recognition during the multimodal integration process. We evaluate these methods using Quickset, a speech/gesture multimodal system, and report evaluation results based on an empirical corpus collected with Quickset. From an architectural perspective, the integration technique presented offers enhanced robustness. It also is premised on more realistic assumptions than previous multimodal systems using semantic fusion. From a methodological standpoint, the evaluation techniques that we describe provide a valuable tool for evaluating multimodal systems.
7EDFCD19	The paper mainly discusses the speech keyword recognition system dealing with the audio streaming media. With the help of the Microsoft Windows Media Format SDK (WMFSDK), a powerful front-end interface module is designed to extract audio stream from different streaming media and convert it to the audio format supported by the speech-recognizer. In order to rapidly spot keywords and reject out-of-vocabulary (OOV) words, the keyword-spotting strategy is put forward based on on-line garbage models. Studies show that this strategy works well in utterance verification. On the utterance verification stage, mixed with multi-confident measures, three classifiers are designed and compared, with the test results proved by different classifiers, the support vector machine (SVM) method is proved superior in performance to the Fisher and neural network (NN) method
5E976DF6	 We have developed two different methods for using auditory, telephone speech to drive the movements of a synthetic face. In the first method, Hidden Markov Models (HMMs) were trained on a phonetically transcribed telephone speech database. The output of the HMMs was then fed into a rule-based visual speech synthesizer as a string of phonemes together with time labels. In the second method, Artificial Neural Networks (ANNs) were trained on the same database to map acoustic parameters directly to facial control parameters. These target parameter trajectories were generated by using phoneme strings from a database as input to the visual speech synthesis The two methods were evaluated through audio-visual intelligibility tests with ten hearing impaired persons, and compared to "ideal" articulations (where no recognition was involved), a natural face, and to the intelligibility of the audio alone. It was found that the HMM method performs considerably better than the audio alone condition (54% and 34% keywords correct respectively), but not as well as the "ideal" articulating artificial face (64%). The intelligibility for the ANN method was 34% keywords correct.
0A18E4E8	This paper describes a text independent continuous speech recognition system, which is built up from many steps. The method of the process is different in all steps, adapted to the particular problems of that level. The whole system is a combination of the application of neural nets and rule based processes, using segment based approach. The segmentation technics is language independent, and all the other processes can be adapted easily to other languages. The recognition works sentence by sentence. At the input the sound pressure is measured, and at the output the half-syllable series (first and second candidates) of the sentences are given. The recognition time of the sentences is about the pronunciation time of them, running the program on a PC 486.
5BB77197	In automatic speech recognition (ASR) systems, hidden Markov models (HMMs) have been widely used for modeling the temporal speech signal. As discussed in Part I, the conventional acoustic models used for ASR have many drawbacks like weak duration modeling and poor discrimination. This paper (Part II) presents a review on the techniques which have been proposed in literature for the refinements of standard HMM methods to cope with their limitations. Current advancements related to this topic are also outlined. The approaches emphasized in this part of review are connectionist approach, explicit duration modeling, discriminative training and margin based estimation methods. Further, various challenges and performance issues such as environmental variability, tied mixture modeling, and handling of distant speech signals are analyzed along with the directions for future research.
7B96DDC7	We study the problem of how to recognize a person only by his eyebrow based on hidden Markov models (HMM). By experiments on a small-scale eyebrow image database taken from 27 subjects, we show that our HMM-based eyebrow recognition method can achieve the highest accuracy of 92.6%, based on the relation of its accuracy to the number of observation symbols and that of states. Hence, we conclude that human eyebrow can work as a biometric with certain possibility and feasibility.
5B5286A0	Binary weights are favored in electronic and optical hardware implementations of neural networks as they lead to improved system speeds. Optical neural networks based on fast ferroelectric liquid crystal binary level devices can benefit from the many orders of magnitudes improved liquid crystal response times. An optimized learning algorithm for all-positive perceptrons is simulated on a limited data set of handwritten digits and the resultant network implemented optically. First, gray-scale and then binary inputs and weights are used in recall mode. On comparing the results for the example data set, the binarized inputs and weights network shows almost no loss in performance.
7DC4D8A8	In this contribution we introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared throughout the paper. Within the first method a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines and results achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English language is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86% recognition rate. As a basis of comparison the similar judgment of human deciders classifying the same corpus at 79.8% recognition rate was analyzed.
7D1C079F	When adaptive arrays are applied to practical problems, the performances of the existing adaptive algorithms are known to degrade substantially in the presence of even slight mismatches between the actual and presumed array responses to the desired signal. Similar types of performance degradation can occur when the signal array response is known precisely but the training sample size is small. In this paper, we propose a novel neural network approach to robust adaptive beamforming. The proposed algorithm is based on explicit modeling of uncertainties in the desired signal array response and a three-layer radial basis function neural network (RBFNN). In the proposed algorithm, the computation of the optimum weight vector is viewed as a mapping problem, which can be modeled using a RBFNN trained with input/output pairs. Our proposed approach offers fast convergence rate, provides excellent robustness against some types of mismatches and makes the mean output array SINR consistently close to the optimal one. Computer simulation results are presented, which show that the proposed algorithm yields significantly better performance as compared with the existing adaptive beamforming algorithms.
5B7FD1FC	Hidden Markov Model (HMM), which is widely used in acoustic modeling, has powerful dynamic time-series modeling capability; Support Vector Machine (SVM) still has strong classification ability when the training samples are limited. This paper proposes an improved speech recognition algorithm based on a hybrid SVM/HMM architecture. We use the algorithm to extract the speech features and apply the features to the Speech Recognition (SR) interface of Microsoft Speech SDK (SAPI) to improve the interface data type. The experimental results show that the recognition rate increases greatly.
623973B2	Speech production knowledge has been used to enhance the phonetic representation and the performance of automatic speech recognition (ASR) systems successfully. Representations of speech production make simple explanations for many phenomena observed in speech. These phenomena can not be easily analyzed from either acoustic signal or phonetic transcription alone. One of the most important aspects of speech production knowledge is the use of articulatory knowledge, which describes the smooth and continuous movements in the vocal tract. In this paper, we present a new articulatory model to provide available information for rescoring the speech recognition lattice hypothesis. The articulatory model consists of a feature front-end, which computes a voicing feature based on a spectral harmonics correlation (SHC) function, and a back-end based on the combination of deep neural networks (DNNs) and hidden Markov models (HMMs). The voicing features are incorporated with standard Mel frequency cepstral coefficients (MFCCs) using heteroscedastic linear discriminant analysis (HLDA) to compensate the speech recognition accuracy rates. Moreover, the advantages of two different models are taken into account by the algorithm, which retains deep learning properties of DNNs, while modeling the articulatory context powerfully through HMMs. Mandarin speech recognition experiments show the proposed method achieves significant improvements in speech recognition performance over the system using MFCCs alone.
5950EF37	The Rosenblatt perceptron was used for handwritten digit recognition. For testing its performance the MNIST database was used. 60,000 samples of handwritten digits were used for perceptron training, and 10,000 samples for testing. A recognition rate of 99.2% was obtained. The critical parameter of Rosenblatt perceptrons is the number of neurons N in the associative neuron layer. We changed the parameter N from 1,000 to 512,000. We investigated the influence of this parameter on the performance of the Rosenblatt perceptron. Increasing N from 1,000 to 512,000 involves decreasing of test errors from 5 to 8 times. It was shown that a large scale Rosenblatt perceptron is comparable with the best classifiers checked on MNIST database (98.9%-99.3%).
7CEF18CC	Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition. This paper first provides a brief overview of graphical models and their uses as statistical models. It is then shown that the statistical assumptions behind many pattern recognition techniques commonly used as part of a speech recognition system can be described by a graph — this includes Gaussian distributions, mixture models, decision trees, factor analysis, principle component analysis, linear discriminant analysis, and hidden Markov models. Moreover, this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph, including many at the acoustic-, pronunciation-, and language-modeling levels. A number of speech recognition techniques born directly out of the graphical-models paradigm are also surveyed. Additionally, this paper includes a novel graphical analysis regarding why derivative (or delta) features improve hidden Markov model-based speech recognition by improving structural discriminability. It also includes an example where a graph can be used to represent language model smoothing constraints. As will be seen, the space of models describable by a graph is quite large. A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model.
7D373DBB	In recent years there has been a significant body of work, both theoretical and experimental, that has established the viability of artificial neural networks (ANN's) as a useful technology for speech recognition. It has been shown that neural networks can be used to augment speech recognizers whose underlying structure is essentially that of hidden Markov models (HMM's). In particular, we have demonstrated that fairly simple layered structures, which we lately have termed big dumb neural networks (BDNN's), can be discriminatively trained to estimate emission probabilities for an HMM. Recently simple speech recognition systems (using context-independent phone models) based on this approach have been proved on controlled tests, to be both effective in terms of accuracy (i.e., comparable or better than equivalent state-of-the-art systems) and efficient in terms of CPU and memory run-time requirements. Research is continuing on extending these results to somewhat more complex systems. In this paper, we first give a brief overview of automatic speech recognition (ASR) and statistical pattern recognition in general. We also include a very brief review of HMM's, and then describe the use of ANN's as statistical estimators. We then review the basic principles of our hybrid HMM/ANN approach and describe some experiments. We discuss some current research topics, including new theoretical developments in training ANN's to maximize the posterior probabilities of the correct models for speech utterances. We also discuss some issues of system resources required for training and recognition. Finally, we conclude with some perspectives about fundamental limitations in the current technology and some speculations about where we can go from here.
80B76035	An overview of a statistical paradigm for speech recognition is given where phonetic and phonological knowledge sources, drawn from the current understanding of the global characteristics of human speech communication, are seamlessly integrated into the structure of a stochastic model of speech. A consistent statistical formalism is presented in which the submodels for the discrete, feature-based phonological process and the continuous, dynamic phonetic process in human speech production are computationally interfaced. This interface enables global optimization of a parsimonious set of model parameters that accurately characterize the symbolic, dynamic, and static components in speech production and explicitly separates distinct sources of the speech variability observable at the acoustic level. The formalism is founded on a rigorous mathematical basis, encompassing computational phonology, Bayesian analysis and statistical estimation theory, nonstationary time series and dynamic system theory, and nonlinear function approximation (neural network) theory. Two principal ways of implementing the speech model and recognizer are presented, one based on the trended hidden Markov model (HMM) or explicitly defined trajectory model, and the other on the state-space or recursively defined trajectory model. Both implementations build into their respective recognition and model-training algorithms a continuity constraint on the internal, production-affiliated trajectories across feature-defined phonological units. The continuity and the parameterized structure in the dynamic speech model permit a joint characterization of the contextual and speaking-style variations manifested in speech acoustics, thereby holding promises to overcome some key limitations of the current speech recognition technology
7ECCA8AC	A topic dependent class (TDC) language model (LM) is a topic-based LM that uses a semantic extraction method to reveal latent topic information from noun-document relation. Then a clustering for a given context is performed to define topics. Finally, a fixed window of word history is observed to decide the topic of the current event through voting in online manner. Previously, we have shown that TDC overperforms several state-of-the-art baselines in terms of perplexity. In this paper we evaluate TDC on automatic speech recognition experiment (ASR) for rescoring task. Experiments on read speech Wall Street Journal (English ASR system) and Mainichi Shimbun (Japanese ASR system) show that TDC LM improves both perplexity and word-error-rate (WER). The result shows that the proposed model gives improvements 3.0% relative on perplexity and 15.2% relative on WER for English ASR system, and 16.4% relative on perplexity and 24.3% relative on WER for Japanese ASR system.