761A6464	Tuning game difficulty prior to release requires careful consideration. Players can quickly lose interest in a game if it is too hard or too easy. Assessing how players will cope prior to release is often inaccurate. However, modern games can now collect sufficient data to perform large scale analysis post deployment and update the product based on these insights. AI Factory Spades is currently the top rated Spades game in the Google Play store. In collaboration with the developers, we have collected gameplay data from 27 592 games and statistics regarding wins/losses for 99 866 games using Google Analytics. Using the data collected, this study analyses the difficulty and behavior of an Information Set Monte Carlo Tree Search player we developed and deployed in the game previously. The methods of data collection and analysis presented in this study are generally applicable. The same workflow could be used to analyze the difficulty and typical player or opponent behavior in any game. Furthermore, addressing issues of difficulty or nonhuman-like opponents postdeployment can positively affect player retention
7A3CAFFC	Reactive navigation based on task decomposition is an effective means for producing robust navigation in complex domains. By incorporating various forms of knowledge, this technique can be made considerably more flexible. Behavioral and perceptual strategies which are represented in a modular form and configured to meet the robot's mission and environment add considerable versatility. A priori world knowledge, when available, can be used to configure these strategies in an efficient form. Dynamically acquired world models can be used to circumvent certain pitfalls that representationless methods are subject to.The Autonomous Robot Architecture (AuRA) is the framework within which experiments in the application of knowledge to reactive control are conducted. Actual robot experiments and simulation studies demonstrate the flexibility and feasibility of this approach over a wide range of navigational domains.
81539B6B	In this paper, we use computational intelligence techniques to built quantitative models of player experience for a platform game. The models accurately predict certain key affective states of the player based on both gameplay metrics that relate to the actions performed by the player in the game, and on parameters of the level that was played. For the experiments presented here, a version of the classic Super Mario Bros game is enhanced with parameterizable level generation and gameplay metrics collection. Player pairwise preference data is collected using forced choice questionnaires, and the models are trained using this data and neuroevolutionary preference learning of multilayer perceptrons (MLPs). The derived models will be used to optimize design parameters for particular types of player experience, allowing the designer to automatically generate unique levels that induce the desired experience for the player.
633C1E93	Multi-agent systems arise in several domains of engineering and can be used to solve problems which are difficult for an individual agent to solve. Strategies for team decision problems, including optimal control, -player games (-infinity control and non-zero sum), and so on are normally solved for off-line by solving associated matrix equations such as the coupled Riccati equations or coupled Hamilton–Jacobi equations. However, using that approach players cannot change their objectives online in real time without calling for a completely new off-line solution for the new strategies. Therefore, in this paper we bring together cooperative control, reinforcement learning, and game theory to present a multi-agent formulation for the online solution of team games. The notion of graphical games is developed for dynamical systems, where the dynamics and performance indices for each node depend only on local neighbor information. It is shown that standard definitions for Nash equilibrium are not sufficient for graphical games and a new definition of “Interactive Nash Equilibrium” is given. We give a cooperative policy iteration algorithm for graphical games that converges to the best response when the neighbors of each agent do not update their policies, and to the cooperative Nash equilibrium when all agents update their policies simultaneously. This is used to develop methods for online adaptive learning solutions of graphical games in real time along with proofs of stability and convergence.
7FCE5B98	This paper presents a first attempt at evolving the rules for a game. In contrast to almost every other paper that applies computational intelligence techniques to games, we are not generating behaviours, strategies or environments for any particular game; we are starting without a game and generating the game itself. We explain the rationale for doing this and survey the theories of entertainment and curiosity that underly our fitness function, and present the details of a simple proof-of-concept experiment.
7E3D3A30	We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs train more effectively than restricted Boltzmann machines (RBM) with the same number of weights. Since sampling from Boltzmann distributions of a DBM is not classically feasible, this is evidence of advantage of a non-Turing sampling oracle. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This further improves the reinforcement learning method using DBMs. 
7A5C8DAA	This paper attempts to give a high-level overview of the field of artificial and computational intelligence (AI/CI) in games, with particular reference to how the different core research areas within this field inform and interact with each other, both actually and potentially. We identify ten main research areas within this field: NPC behavior learning, search and planning, player modeling, games as AI benchmarks, procedural content generation, computational narrative, believable agents, AI-assisted game design, general game artificial intelligence and AI in commercial games. We view and analyze the areas from three key perspectives: 1) the dominant AI method(s) used under each area; 2) the relation of each area with respect to the end (human) user; and 3) the placement of each area within a human-computer (player-game) interaction perspective. In addition, for each of these areas we consider how it could inform or interact with each of the other areas; in those cases where we find that meaningful interaction either exists or is possible, we describe the character of that interaction and provide references to published studies, if any. We believe that this paper improves understanding of the current nature of the game AI/CI research field and the interdependences between its core areas by providing a unifying overview. We also believe that the discussion of potential interactions between research areas provides a pointer to many interesting future research projects and unexplored subfields.
796DEA1C	This paper introduces Rank-based Interactive Evolution (RIE) which is an alternative to interactive evolution driven by computational models of user preferences to generate personalized content. In RIE, the computational models are adapted to the preferences of users which, in turn, are used as fitness functions for the optimization of the generated content. The preference models are built via ranking-based preference learning, while the content is generated via evolutionary search. The proposed method is evaluated on the creation of strategy game maps, and its performance is tested using artificial agents. Results suggest that RIE is both faster and more robust than standard interactive evolution and outperforms other state-of-the-art interactive evolution approaches.
7D65C922	Collaborative filtering is a technique to predict users’ interests for items by exploiting the behavior patterns of a group of users with similar preferences. This technique has been widely used for recommender systems and has a number of successful applications in E-commerce. In practice, a major challenge when applying collaborative filtering is that a typical user provides ratings for just a small number of items, thus the amount of training data is sparse with respect to the size of the domain. In this paper, we present a method to address this problem. Our method formulates the collaborative filtering problem in a multi-task learning framework by treating each user rating prediction as a classification problem and solving multiple classification problems together. By doing this, the method allows sharing information among different classifiers and thus reduces the effect of data sparsity.
77EC4D4C	Estimating affective and cognitive states in conditions of rich human-computer interaction, such as in games, is a field of growing academic and commercial interest. Entertainment and serious games can benefit from recent advances in the field as, having access to predictors of the current state of the player (or learner) can provide useful information for feeding adaptation mechanisms that aim to maximize engagement or learning effects. In this paper, we introduce a large data corpus derived from 58 participants that play the popular Super Mario Bros platform game and attempt to create accurate models of player experience for this game genre. Within the view of the current research, features extracted both from player gameplay behavior and game levels, and player visual characteristics have been used as potential indicators of reported affect expressed as pairwise preferences between different game sessions. Using neuroevolutionary preference learning and automatic feature selection, highly accurate models of reported engagement, frustration, and challenge are constructed (model accuracies reach 91%, 92%, and 88% for engagement, frustration, and challenge, respectively). As a step further, the derived player experience models can be used to personalize the game level to desired levels of engagement, frustration, and challenge as game content is mapped to player experience through the behavioral and expressivity patterns of each player.
7F22EB71	This paper compares three strategies in using reinforcement learning algorithms to let an artificial agent learn to play the game of Othello. The three strategies that are compared are: Learning by self-play, learning from playing against a fixed opponent, and learning from playing against a fixed opponent while learning from the opponent's moves as well. These issues are considered for the algorithms Q-learning, Sarsa and TD-learning. These three reinforcement learning algorithms are combined with multi-layer perceptrons and trained and tested against three fixed opponents. It is found that the best strategy of learning differs per algorithm. Q-learning and Sarsa perform best when trained against the fixed opponent they are also tested against, whereas TD-learning performs best when trained through self-play. Surprisingly, Q-learning and Sarsa outperform TD-learning against the stronger fixed opponents, when all methods use their best strategy. Learning from the opponent's moves as well leads to worse results compared to learning only from the learning agent's own moves
7EBBA383	This research develops and evaluates a new multi-agent adaptive traffic signal control system based on swarm intelligence and the neural-fuzzy actor-critic reinforcement learning (NFACRL) method. The proposed method combines the better attributes of swarm intelligence and the NFACRL method. Two scenarios are used to evaluate the method and the new NFACRL-Swarm method is compared with its NFACRL counterpart. First, the proposed control model is applied to isolated intersection signal adaptive control to evaluate its learning performance. Then, the control system is implemented in signal control coordination in a typical arterial. In the isolated intersection, the proposed hybrid method outperforms its previous counterpart by improving the learning speed and is shown to be insensitive to reward function parameters. In the network, by introducing a coordination scheme inspired by swarm intelligence, the proposed method improves the performance by up to 12% and has a faster learning speed
806F5667	This research develops and evaluates a new multi-agent adaptive traffic signal control system based on swarm intelligence and the neural-fuzzy actor-critic reinforcement learning (NFACRL) method. The proposed method combines the better attributes of swarm intelligence and the NFACRL method. Two scenarios are used to evaluate the method and the new NFACRL-Swarm method is compared with its NFACRL counterpart. First, the proposed control model is applied to isolated intersection signal adaptive control to evaluate its learning performance. Then, the control system is implemented in signal control coordination in a typical arterial. In the isolated intersection, the proposed hybrid method outperforms its previous counterpart by improving the learning speed and is shown to be insensitive to reward function parameters. In the network, by introducing a coordination scheme inspired by swarm intelligence, the proposed method improves the performance by up to 12% and has a faster learning speed
7739CAF2	This paper describes an architecture for controlling non-player characters (NPC) in the First Person Shooter (FPS) game Unreal Tournament 2004. Specifically, the DRE-Bot architecture is made up of three reinforcement learners, Danger, Replenish and Explore, which use the tabular Sarsa(λ) algorithm. This algorithm enables the NPC to learn through trial and error building up experience over time in an approach inspired by human learning. Experimentation is carried to measure the performance of DRE-Bot when competing against fixed strategy bots that ship with the game. The discount parameter, γ, and the trace parameter, λ, are also varied to see if their values have an effect on the performance.
75A72F45	Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary properties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD. 
7ED6D936	Equilibrium solutions are characterized for a class of two-layer differential games in which one player, P/sub 2/, controls a continuous-time deterministic system while another player, P/sub 1/, intervenes only at random stopping times when he or she can impulsively modify the state and the mode of the lower layer system. They explore the case in which the objectives of the two layers do not coincide totally. This model is also related to the theory of piecewise deterministic systems. Equilibria can be defined and characterized by reformulating this differential game as a noncooperative sequential Markov game with continuous state and action spaces. A precise definition of this class of systems is given and a reformulation in a sequential game format is presented. An existence proof for epsilon -equilibria and a computational approach are presented.
7ECBDEA2	In this paper we propose a reinforcement connectionist learning architecture that allows an autonomous robot to acquire efficient navigation strategies in a few trials. Besides rapid learning, the architecture has three further appealing features. First, the robot improves its performance incrementally as it interacts with an initially unknown environment, and it ends up learning to avoid collisions even in those situations in which its sensors cannot detect the obstacles. This is a definite advantage over nonlearning reactive robots. Second, since it learns from basic reflexes, the robot is operational from the very beginning and the learning process is safe. Third, the robot exhibits high tolerance to noisy sensory data and good generalization abilities. All these features make this learning robot's architecture very well suited to real-world applications. We report experimental results obtained with a real mobile robot in an indoor environment that demonstrate the appropriateness of our approach to real autonomous robot control.
5E5CD462	Most machine learning algorithms are essentially generalized learning schemes. When used incrementally, conflict between the concepts already learned and the new events to be learned need be eliminated. This paper presents a new incremental learning scheme that divides induction into two phases: incrementally accurate learning and generalization. The new learning scheme is conflict-free thus it is suitable for incremental induction.
7CFF69C3	Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions. 
7C42DF48	A major cause of energy waste in wireless networks is the interference between nodes working in the same frequency band. This problem appears to be more serious in Wireless Body Area Networks (WBANs) in which energy is the most valuable resource. In order to cope with this issue, power control techniques can be employed. Amongst potential approaches, those with intelligence which allow WBANs to improve their performance by learning from experience can remarkably increase flexibility and adaptability. Besides, approaches with less inter-node negotiation and cooperation are more attractive in WBANBs due to their low overhead and superior scalability. In this paper, we propose a power controller which employs Reinforcement Learning (RL) to enable WBANs to learn from experience and coordinate their power levels in a distributed manner with no inter-node negotiation and cooperation. We evaluate the performance of the proposed power controller with different RL algorithms and compare them to a counterpart approach based on game theory without learning. Their performances are evaluated in terms of optimality of the solution and convergence. We show that the RL-based power controller can trade off throughput for transmission power level and achieve lower energy consumption compared to the counterpart game.
7ACFCB1B	Computational agents of commercial Real Time Strategic (RTS) games mostly have their behaviors designed via simple ad hoc and static techniques, which require manual definition of actions. Thus, such agents are not able to adapt themselves to diverse situations and their behavior becomes predictable along the game, enabling human players to eventually discover the strategies used by them. This work proposes a modeling approach for the use of SARSA reinforcement learning technique applied to combat situations in RTS games. This technique enables that computational agents evolve their combat behavior according to actions of opponents. The performance of this technique was evaluated using a Starcraft based simulator. The experiments showed that agents were able to improve their behavior, developing knowledge to decide about the best actions during different game states and using this knowledge in an efficient way to obtain better results in later battles.
79AA9C17	More than a decade after the early research efforts on the use of artificial intelligence (AI) in computer games and the establishment of a new AI domain the term ``game AI'' needs to be redefined. Traditionally, the tasks associated with game AI revolved around non player character (NPC) behavior at different levels of control, varying from navigation and pathfinding to decision making. Commercial-standard games developed over the last 15 years and current game productions, however, suggest that the traditional challenges of game AI have been well addressed via the use of sophisticated AI approaches, not necessarily following or inspired by advances in academic practices. The marginal penetration of traditional academic game AI methods in industrial productions has been mainly due to the lack of constructive communication between academia and industry in the early days of academic game AI, and the inability of academic game AI to propose methods that would significantly advance existing development processes or provide scalable solutions to real world problems. Recently, however, there has been a shift of research focus as the current plethora of AI uses in games is breaking the non-player character AI tradition. A number of those alternative AI uses have already shown a significant potential for the design of better games.This paper presents four key game AI research areas that are currently reshaping the research roadmap in the game AI field and evidently put the game AI term under a new perspective. These game AI flagship research areas include the computational modeling of player experience, the procedural generation of content, the mining of player data on massive-scale and the alternative AI research foci for enhancing NPC capabilities.
7E1F94AD	A number of data-driven fuzzy measure (FM) learning techniques have been put forth for the fuzzy integral (FI). Examples include quadratic programming, Gibbs sampling, gradient descent, reward and punishment and evolutionary optimization. However, most approaches focus solely on the minimization of the sum of squared error (SSE). Limited attention has been placed on characterizing and subsequently minimizing model (i.e., FM) complexity. Furthermore, the vast majority of learning techniques are highly susceptible to over-fitting and noise. Herein, we explore a regularization approach to learning the FM for the Choquet FI. We investigate the mathematical motivation for such an approach, its applicability and impact on different types of FMs, and its desirable properties for quadratic programming (QP) based optimization. We show that L\ regularization has a distinct meaning for measure learning and aggregation operators. Experiments are performed and validated with respect to the Shapley index. Specifically, we show that it is possible to reduce the effect of overfitting, we can identify higher quality measures and, if desired, force the learning of fewer numbers of sources.
7F98460E	In most modern video games, character behavior is scripted; no matter how many times the player exploits a weakness, that weakness is never repaired. Yet, if game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. This paper introduces the real-time Neuroevolution of Augmenting Topologies (rtNEAT) method for evolving increasingly complex artificial neural networks in real time, as a game is being played. The rtNEAT method allows agents to change and improve during the game. In fact, rtNEAT makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. To demonstrate this concept, the Neuroevolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players' teams. This paper describes results from this novel application of machine learning, and demonstrates that rtNEAT makes possible video games like NERO where agents evolve and adapt in real time. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.
69271ADD	Machine learning has proven useful for producing solutions to various problems, including the creation of controllers for autonomous intelligent agents. However, the control requirements for an intelligent agent sometimes go beyond the simple ability to complete a task, or even to complete it efficiently: An agent must sometimes complete a task in style. For example, if an autonomous intelligent agent is embedded in a game where it is visible to human observers, and plays a role that evokes human intuitions about how that role should be fulfilled, then the agent must fulfill that role in a manner that does not dispel the illusion of intelligence for the observers. Such visibly intelligent behavior is a subset of general intelligent behavior: a subset that we must be able to provide if our methods are to be adopted by the developers of games and simulators. This dissertation continues the tradition of using neuroevolution to train artificial neural networks as controllers for agents embedded in strategy games or simulators, expanding that work to address selected issues of visibly intelligent behavior. A test environment is created and used to demonstrate that modified methods can create desirable behavioral traits such as flexibility, consistency, and adherence to a doctrine, and suppress undesirable traits such as seemingly erratic behavior and excessive predictability. These methods are designed to expand a program of work leading toward adoption of neuroevolution by the commercial gaming industry, increasing player satisfaction with their products, and perhaps helping to set AI forward as The Next Big Thing in that industry. As the capabilities of research-grade machine learning converge with the needs of the commercial gaming industry, work of this sort can be expected to expand into a broad and productive area of research into the nature of intelligence and the behavior of autonomous agents.
7A5654F9	This paper presents the method of image texture recognition by Volume Trace Transform-VTT, based on several Trace function. All of 10 trace functions will be selected by the reinforcement learning process and constructed to produce noticeable features. The next process is to sum the results of each function together by “NDFT - Neighbor Discriminant Feature Transform”, this process will all the results from the different positions of image results and construct 2-D histogram. The histogram is evaluated by chi-square statistic test and the process works on the basis of brodatz texture and vision texture database.
768B06CC	Person re-identification is defined as to find the same person who re-occurred in a multi-camera surveillance system. Existing machine learning approaches focus on extracting or learning discriminative features followed by template matching using a distance measure. However, labeling images for a training set is a time consuming task. In this paper, the person re-identification is considered as a binary classification problem. The active learning framework with SVM is applied to person re-identification problem in this paper. Rather than learning from all the training samples, the proposed method selects the most valuable sample according to the current knowledge of the classifier. Experimental results show that our proposed method not only can reduce the number of sample labeling but also achieve a higher accuracy with using less training samples.
8129B174	One important aspect of creating game bots is adversarial motion planning: identifying how to move to counter possible actions made by the adversary. In this paper, we examine the problem of opponent interception, in which the goal of the bot is to reliably apprehend the opponent. We present an algorithm for motion planning that couples planning and prediction to intercept an enemy on a partially-occluded Unreal Tournament map. Human players can exhibit considerable variability in their movement preferences and do not uniformly prefer the same routes. To model this variability, we use inverse reinforcement learning to learn a player-specific motion model from sets of example traces. Opponent motion prediction is performed using a particle filter to track candidate hypotheses of the opponent's location over multiple time horizons. Our results indicate that the learned motion model has a higher tracking accuracy and yields better interception outcomes than other motion models and prediction methods.
7D46B46D	In this paper, we propose a new learning method called “self-enhancement learning.” In this model, a network enhances its state by itself, and this enhanced state is to be imitated by another state of the network. The word “target” in our model means that a target is created spontaneously by a network, which must try to attain the target. Enhancement is realized by changing the Gaussian width or enhancement parameter. With different enhancement parameters, we can set up the different states of a network. In particular, we set up an enhanced and a relaxed state, and the relaxed state tries to imitate the enhanced state as much as possible. To demonstrate the effectiveness of this method, we apply the self-enhancement learning to the SOM. For this purpose, we introduce collectiveness into an enhanced state in which all neurons collectively respond to input patterns. Then, this enhanced and collective state should be imitated by the other non-enhanced and relaxed state. We applied the method to the Iris problem. Experimental results showed that the U-matrices obtained were significantly similar to those produced by the conventional SOM. However, much better performance could be obtained in terms of quantitative and topological errors. The experimental results suggest the possibility for the self-enhancement learning to be applied to many different neural network models.
7CBED1AA	Semi supervised multitask learning has been one of the hotest problems in the machine learning field in recent years. In this paper a dynamic fuzzy semi supervised multitask learning algorithm has been proposed to deal with the dynamic fuzzy problems. Our goal is to learn dynamic fuzzy possibility of each class with a limited initial data set, and the dynamic fuzzy possibility is numerically equal to the membership functions of each class. So the dynamic fuzzy possibility needs to be adapted with new data incoming. Experiment results shows that our method has performed much better, compared with the semi supervised fuzzy pattern matching algorithm proposed in
7DA1591F	This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. 
7FEAB69B	One of the standing challenging aspects in mobile robotics is the ability to navigate autonomously. It is a difficult task, which requiring a complete modeling of the environment and intelligent controllers. This paper presents an intelligent navigation method for an autonomous mobile robot which requires only a scalar signal likes a feedback indicating the quality of the applied action. Instead of programming a robot, we will let it only learn its own strategy. The Q-learning algorithm of reinforcement learning is used for the mobile robot navigation by discretizing states and actions spaces. In order to improve the mobile robot performances, an optimization of fuzzy controllers will be discussed for the robot navigation; based on prior knowledge introduced by a fuzzy inference system so that the initial behavior is acceptable. The effectiveness of this optimization method is verified by simulation.
76393999	In this paper, we propose a new double-layered state space construction method, which consists of Fritzke's Growing Neural Gas algorithm and a class management mechanism of GNG units. The classification algorithm yields a new class by referring to anticipation error, anticipation vectors of an originated class, and anticipation vectors GNG units belonging in the originated class.
813D20A7	Open-ended systems and unknown dynamical environments present challenges to the traditional machine learning systems, and in many cases traditional methods are not applicable. Lifelong reinforcement learning is a special case of dynamic (process-oriented) reinforcement learning. Multi-task learning is a methodology that exploits similarities and patterns across multiple tasks. Both can be successfully used for open-ended systems and automated learning in unknown environments. Due to its unique characteristics, lifelong reinforcement presents both challenges and potential capabilities that go beyond traditional reinforcement learning methods. In this article, we present the basic notions of lifelong reinforcement learning, introduce the main methodologies, applications and challenges. We also introduce a new model of lifelong reinforcement based on the Evolvable Virtual Machine architecture (EVM).
79879ED3	This article proposes an application using a reinforcement learning (RL) approach to the card game Wizard. The aim is to create a computer player that is able to learn a winning strategy for the game by himself. Wizard is a partially observable competitive multiplayer game that consists of two game phases, forecasting and trick playing. The biggest challenges in creating a strong player are dealing with multiple rounds which have a different grade of imperfection and the decision on the forecast at the beginning of every game round. We introduce an RL approach to the problem by adopting an existing RL algorithm to the playing phase of the game and by implementing an evaluator of the player's hand card using a Multi-Player-Perceptron to conduct the forecast. The results of our experiments show that the player is able to improve his playing strategy through learning. At the beginning the performance of the learning agent is very bad due to the bad forecasting behavior, but he is able to improve his performance over a few training episodes from 0% won games to approximately 25.68% won games in an experiment with 4 players. Therefore he plays equally strong as his opponents and even outperforms one of them.
7A87E660	In this paper we employ a steady state genetic algorithm to evolve different types of behaviour for bots in the Unreal Tournament 2004™ computer game. For this purpose we define three fitness functions which are based on the number of enemies killed, the lifespan of the bot and a combination of both. Long run experiments were carried out, in which the evolved bots' behaviours outperform those of standard bots supplied by the game, particularly in those cases where the fitness involves a measure of the bot's lifespan. Also, there is an increase in the number of items collected, and the behaviours tend to become less aggressive, tending instead towards a more optimised combat style. Further “short run” experiments were carried out with a further type of fitness function defined, based on the number of items picked. In these cases the bots evolve performances towards the goal they have been aimed, with no other behaviours arising, except in the case of the multiple objective one. We conclude that in order to evolve interesting behaviours more complex fitness functions are needed, and not necessarily ones that directly include the goal we are aiming for.
7D11376A	Game theory is the best mathematical tool to study human society interaction. Learning approach has an important influence on interaction. The Traffic Signal Control Agent (TSCA) interaction model is the basis of research on coordinated control of urban area traffic signal. As for the interactive intersection, this research constructed structure models of two TSCAs, such as intersection Agent and management Agent. Based on this, the TSCA interactive frame model was established. This research constructed TSCA interaction mathematical model via game theory. In the interaction mathematical model, the renewed Q-values in the distributed reinforcement Q-learning was used to build the payoff values. Therefore, interaction has taken on from the action selection between TSCAs. Next, an algorithm of distributed Q-learning based on distributed weight function is brought forward. The interaction model paves the way for traffic control simulation in the future.
54941CF5	Reinforcement learning methods are not yet widely used in computer games, at least not for demanding online learning tasks. This is in part because such methods often require excessive number of training samples before converging. This can be particularly troublesome in mobile game devices where both storage and CPU are limited and valuable resources. In this paper we describe a new AI-based game for mobile phones that we are currently developing. We address some of the main challenges of incorporating efficient on-line reinforcement learning methods into such gaming platforms. Furthermore, we introduce two simple methods for interactively incorporating user feed-back into reinforcement learning. These methods not only have the potential of increasing the entertainment value of games, but they also drastically reduce the number of training episodes needed for the learning to converge. This enhancement made it possible for us to use otherwise standard reinforcement learning as the core part of the learning AI in our game.
7D2CED69	Partially observable environments pose a major challenge to the application of reinforcement learning algorithms. In such environments, due to the Markov property frequently being violated in the system state representation, situations can occur where an agent has insufficient information to decide on the optimal action. In such cases, it is necessary to determine when information gathering actions should be executed, that is, when the agent needs to reduce uncertainty about the current state before deciding on how to act. One possible solution that has been proposed in past research is to manually code rules for execution of information gathering actions in the policy using heuristic (and likely faulty) knowledge. However, such a solution requires explicit expert knowledge about actions which are information gathering. In this paper a flexible solution is proposed which automatically learns when to execute information gathering actions and furthermore to automatically discover which actions gather information. We present an evaluation in the Robo{C}up Keep{A}way domain that empirically shows the robustness of the proposed approach and its success in learning under varying degrees of partial observability. Hence, it eliminates the need for hand-coded rules, is flexible in different situations and does not require knowledge about information gathering actions.
7F33D26E	This correspondence presents a two-stage classification learning algorithm. The first stage approximates the class-conditional distribution of a discrete space using a separate mixture model, and the second stage investigates the class posterior probabilities by training a network. The first stage explores the generative information that is inherent in each class by using the Chow-Liu (CL) method, which approximates high-dimensional probability with a tree structure, namely, a dependence tree, whereas the second stage concentrates on discriminative learning to distinguish between classes. The resulting learning algorithm integrates the advantages of both generative learning and discriminative learning. Because it uses CL dependence-tree estimation, we call our algorithm CL-Net. Empirical tests indicate that the proposed learning algorithm makes significant improvements when compared with the related classifiers that are constructed by either generative learning or discriminative learning.
76331AE0	Recently there has been much research focus on the use of Reinforcement Learning (RL) algorithms for game agent control. However, although it has been shown that such agents are capable of learning in real time, the high dimensionality of agent sensor state spaces still prove to be a significant barrier to progress. This paper outlines an approach to dealing with this issue by using a modular RL architecture with a fine granularity of modules. The modular approach enables a reduction of the dimensionality in complex game-like environments by dividing the state space into smaller, more manageable sub tasks. While this approach is successful in reducing dimensionality, challenges with action selection, exploration and reward allocation arise. This paper discusses approaches to overcoming these issues.
02E8D5A1	Open-text (or open-domain) semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose here a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words, which are mapped to more than 40,000 entities) thanks to a training scheme that combines learning from WordNet and ConceptNet with learning from raw text. The model learns structured embeddings of words, entities and MRs via a multi-task training process operating on these diverse sources of data that integrates all the learnt knowledge into a single system. This work ends up combining methods for knowledge acquisition, semantic parsing, and word-sense disambiguation. Experiments on various tasks indicate that our approach is indeed successful and can form a basis for future more sophisticated systems. 
801D18DD	A methodology for optimizing player satisfaction in games on the "playware" physical interactive platform is demonstrated in this paper. Previously constructed artificial neural network user models, reported in the literature, map individual playing characteristics to reported entertainment preferences for augmented-reality game players. An adaptive mechanism then adjusts controllable game parameters in real time in order to improve the entertainment value of the game for the player. The basic approach presented here applies gradient ascent to the user model to suggest the direction of parameter adjustment that leads toward games of higher entertainment value. A simple rule set exploits the derivative information to adjust specific game parameters to augment the entertainment value. Those adjustments take place frequently during the game with interadjustment intervals that maintain the user model's accuracy. Performance of the adaptation mechanism is evaluated using a game survey experiment. Results indicate the efficacy and robustness of the mechanism in adapting the game according to a user's individual playing features and enhancing the gameplay experience. The limitations and the use of the methodology as an effective adaptive mechanism for entertainment capture and augmentation are discussed.
7E69F95D	In this study a landing control of an acrobat robot is considered and Q-learning method is applied for falling avoidance control. Since the dynamics of the system is changed according to contact conditions to the ground, the system is a typical variable constraint and hybrid system. The state space for the Q-learning consists of discrete mode variable and continuous states. It is shown by numerical simulations that taking a step motion is automatically generated and falling down is avoided properly.
7C36AE69	In current state-of-the-art commercial first person shooter games, computer controlled bots, also known as nonplayer characters, can often be easily distinguishable from those controlled by humans. Tell-tale signs such as failed navigation, “sixth sense” knowledge of human players' whereabouts and deterministic, scripted behaviors are some of the causes of this. We propose, however, that one of the biggest indicators of nonhumanlike behavior in these games can be found in the weapon shooting capability of the bot. Consistently perfect accuracy and “locking on” to opponents in their visual field from any distance are indicative capabilities of bots that are not found in human players. Traditionally, the bot is handicapped in some way with either a timed reaction delay or a random perturbation to its aim, which doesn't adapt or improve its technique over time. We hypothesize that enabling the bot to learn the skill of shooting through trial and error, in the same way a human player learns, will lead to greater variation in game-play and produce less predictable nonplayer characters. This paper describes a reinforcement learning shooting mechanism for adapting shooting over time based on a dynamic reward signal from the amount of damage caused to opponents.
7F9EA83D	This paper investigates the relationship between level design parameters of platform games, individual playing characteristics and player experience. The investigated design parameters relate to the placement and sizes of gaps in the level and the existence of direction changes; components of player experience include fun, frustration and challenge. A neural network model that maps between level design parameters, playing behavior characteristics and player reported emotions is trained using evolutionary preference learning and data from 480 platform game sessions. Results show that challenge and frustration can be predicted with a high accuracy (77.77% and 88.66% respectively) via a simple single-neuron model whereas model accuracy for fun (69.18%) suggests the use of more complex non-linear approximators for this emotion. The paper concludes with a discussion on how the obtained models can be utilized to automatically generate game levels which will enhance player experience.
7CFD6D48	The paper deals with a modification in the learning phase of AntNet routing algorithm, which improves the system adaptability in the presence of undesirable events. Unlike most of the ACO algorithms which consider reward-inaction reinforcement learning, the proposed strategy considers both reward and penalty onto the action probabilities. As simulation results show, considering penalty in AntNet routing algorithm increases the exploration towards other possible and sometimes much optimal selections, which leads to a more adaptive strategy. The proposed algorithm also uses a self-monitoring solution called Occurrence-Detection, to sense traffic fluctuations and make decision about the level of undesirability of the current status. The proposed algorithm makes use of the two mentioned strategies to prepare a self-healing version of AntNet routing algorithm to face undesirable and unpredictable traffic conditions.
811E80EC	Much of artificial intelligence research is focused on devising optimal solutions for challenging and well-defined but highly constrained problems. However, as we begin creating autonomous agents to operate in the rich environments of modern videogames and computer simulations, it becomes important to devise agent behaviors that display the visible attributes of intelligence, rather than simply performing optimally. Such visibly intelligent behavior is difficult to specify with rules or characterize in terms of quantifiable objective functions, but it is possible to utilize human intuitions to directly guide a learning system toward the desired sorts of behavior. Policy induction from human-generated examples is a promising approach to training such agents. In this paper, such a method is developed and tested using Lamarckian neuroevolution. Artificial neural networks are evolved to control autonomous agents in a strategy game. The evolution is guided by human-generated examples of play, and the system effectively learns the policies that were used by the player to generate the examples. I.e., the agents learn visibly intelligent behavior. In the future, such methods are likely to play a central role in creating autonomous agents for complex environments, making it possible to generate rich behaviors derived from nothing more formal than the intuitively generated examples of designers, players, or subject-matter experts. 
816F54E5	The complexity of most multi-agent systems prohibits a hand-coded approach to decision-making. In addition to that a complex, dynamic, adversarial environment like the one of a football game makes decision-making and cooperation even more difficult. This paper addresses these problems by using machine learning techniques and agent technology. By gathering useful experience from earlier stages, an agent can significantly improve performance. The method used requires no previous knowledge regarding the environment. Since cooperation in adversarial domains is a very challenging task, the proposed learning algorithm assigns each agent a role to play to achieve a certain goal. By distributing the responsibilities among the agents and linking their goals, an efficient way of cooperation emerges.
7C21D5BD	Elevator Group Supervisory Control System (EGSCS) is a very large scale stochastic dynamic optimization problem. Due to its vast state space, significant uncertainty and numerous resource constraints such as finite car capacities and registered hall/car calls, it is hard to manage EGSCS using conventional control methods. Recently, many solutions for EGSCS using Artificial Intelligence (AI) technologies have been reported. Genetic Network Programming (GNP), which is proposed as a new evolutionary computation method several years ago, is also proved to be efficient when applied to EGSCS problem. In this paper, we propose an extended algorithm for EGSCS by introducing Reinforcement Learning (RL) into GNP framework, and an improvement of the EGSCS' performances is expected since the efficiency of GNP with RL has been clarified in some other studies like tile-world problem. Simulation tests using traffic flows in a typical office building have been made, and the results show an actual improvement of the EGSCS' performances comparing to the algorithms using original GNP and conventional control methods. Furthermore, as a further study, an importance weight optimization algorithm is employed based on GNP with RL and its efficiency is also verified with the better performances. 
80A0D729	In order to improve the efficiency and intelligence of negotiation, the paper applies the technology about agent and the mechanism of reinforcement learning to electronic commerce negotiation. Through presenting negotiation protocol and analyzing negotiation flow based on multi-attribute utility theory, the paper builds an open and dynamic automated negotiation model, and imports Q-learning into the negotiation to quicken the process of negotiation. Compared with no learning mechanism in negotiation, the negotiation efficiency of the model has been improved and the negotiation results are acceptable.
80DBB141	Inspired by popular co-training and domain adaptation methods, we propose a co-adaptation algorithm. The goal is improving the performance of a dialog act segmentation model by exploiting the vast amount of unlabeled data. This task provides a nice framework for multiview learning, as it has been shown that lexical and prosodic features provide complementary information. Instead of simply adding machine-labeled data to the set of manually labeled data, co-adaptation technique adapts the existing models. While both co-training and domain adaptation techniques have been employed for dialog act segmentation, our experiments show that the proposed co-adaptation algorithm results in significantly better performance.
8155AF62	We study the problem of learning a non-parametric mapping between two continuous spaces without having access to input-output pairs for training, but rather to groups of input-output pairs, where the correspondence structure within each group is unknown and where outliers may be present. This problem is solved by transforming each space using the channel representation, and finding a linear mapping on the transformed domain. The asymptotical behavior of the method for a large number of training samples is found to be very related to the case of known correspondences. The results are evaluated on simulated data
7869F722	This paper presents a new learning method to attack two significant sub-problems in reinforcement learning at the same time: continuous space and linguistic rewards. Linguistic reward-oriented Takagi-Sugeno fuzzy reinforcement learning (LRTSFRL) is constructed by combining Q-learning with Takagi-Sugeno type fuzzy inference systems. The proposed paradigm is capable of solving complicated learning tasks of continuous domains, also can be used to design Takagi-Sugeno fuzzy logic controllers. Experiments on the double inverted pendulum system demonstrate the performance and applicability of the presented scheme. Finally, the conclusion remark is drawn.
7919ABFB	Current zero-shot learning methods relied on attributes to describe the unseen class characteristics, using the learned seen class model. However, these approaches required extensive attribute labels on each object class, and a well-defined, attributes relationship between the seen and unseen class with the aid of human knowledge. In this work, we avoid these with a novel learning process using the probabilistic Latent Semantic Analysis (pLSA). We replace the attributes with topic model and extend the representation as a mapping algorithm to object classes, so that zero-shot learning would be possible. With this, less annotated class information is required to achieve similar performance. Evaluations on three public datasets had shown the effectiveness of our proposed method.
7C36AE69	In current state-of-the-art commercial first person shooter games, computer controlled bots, also known as nonplayer characters, can often be easily distinguishable from those controlled by humans. Tell-tale signs such as failed navigation, “sixth sense” knowledge of human players' whereabouts and deterministic, scripted behaviors are some of the causes of this. We propose, however, that one of the biggest indicators of nonhumanlike behavior in these games can be found in the weapon shooting capability of the bot. Consistently perfect accuracy and “locking on” to opponents in their visual field from any distance are indicative capabilities of bots that are not found in human players. Traditionally, the bot is handicapped in some way with either a timed reaction delay or a random perturbation to its aim, which doesn't adapt or improve its technique over time. We hypothesize that enabling the bot to learn the skill of shooting through trial and error, in the same way a human player learns, will lead to greater variation in game-play and produce less predictable nonplayer characters. This paper describes a reinforcement learning shooting mechanism for adapting shooting over time based on a dynamic reward signal from the amount of damage caused to opponents.