78E95A6F	In this paper, the control of a planar three-link musculoskeletal arm by using a revolutionary actor–critic reinforcement learning (RL) method during a reaching movement to a stationary target is presented. The arm model used in this study included three skeletal links (wrist, forearm, and upper arm), three joints (wrist, elbow, and shoulder without redundancy), and six non-linear monoarticular muscles (with redundancy), which were based on the Hill model. The learning control system was composed of actor, critic, and genetic algorithm (GA) parts. Two single-layer neural networks were used for each part of the actor and critic. This learning control system was used to apply six activation commands to six monoarticular muscles at each instant of time. It also used a reinforcement (reward) feedback for the learning process and controlling the direction of arm movement. Also, the GA was implemented to select the best learning rates for actor–critic neural networks. The results showed that mean square error (MSE) and average episode time gradually decrease and average reward gradually increases to constant values during the learning of the control policy. Furthermore, when learning was complete, optimal values of learning rates were selected.
81601B48	The aim of our study is to have a hopping module to control the height of hopping in an environment where the control parameters are unknown. This will lead to the development of a system for building dynamic walking robots. Assuming that a hopping module can be controlled by a spring and a DC motor, we placed a built-in learning system in the module that consists of reinforcement learning (RL) for identification and layered neural networks (NN) for generalization. By using this learning system, we simulated autonomous adjustment control in order to obtain the optimum DC motor angular velocity, which enables the module to hop to an arbitrary height. As a result, we can design a regulator that has the advantage of both RL and NN, and have laid the foundation for further developments to apply the algorithms of learning to practical walking robots.
8061BD25	Several studies have demonstrated that reward from a human trainer can be a powerful feedback signal for control-learning algorithms. However, the space of algorithms for learning from such human reward has hitherto not been explored systematically. Using model-based reinforcement learning from human reward, this article investigates the problem of learning from human reward through six experiments, focusing on the relationships between reward positivity, which is how generally positive a trainer's reward values are; temporal discounting, the extent to which future reward is discounted in value; episodicity, whether task learning occurs in discrete learning episodes instead of one continuing session; and task performance, the agent's performance on the task the trainer intends to teach. This investigation is motivated by the observation that an agent can pursue different learning objectives, leading to different resulting behaviors. We search for learning objectives that lead the agent to behave as the trainer intends.We identify and empirically support a “positive circuits” problem with low discounting (i.e., high discount factors) for episodic, goal-based tasks that arises from an observed bias among humans towards giving positive reward, resulting in an endorsement of myopic learning for such domains. We then show that converting simple episodic tasks to be non-episodic (i.e., continuing) reduces and in some cases resolves issues present in episodic tasks with generally positive reward and—relatedly—enables highly successful learning with non-myopic valuation in multiple user studies. The primary learning algorithm introduced in this article, which we call “vi-tamer”, is the first algorithm to successfully learn non-myopically from reward generated by a human trainer; we also empirically show that such non-myopic valuation facilitates higher-level understanding of the task. Anticipating the complexity of real-world problems, we perform further studies—one with a failure state added—that compare (1) learning when states are updated asynchronously with local bias—i.e., states quickly reachable from the agent's current state are updated more often than other states—to (2) learning with the fully synchronous sweeps across each state in the vi-tamer algorithm. With these locally biased updates, we find that the general positivity of human reward creates problems even for continuing tasks, revealing a distinct research challenge for future work.
77734AA3	In this paper, the design of a network of real-time close-loop wide-area decentralized power system stabilizers (WD-PSSs) is investigated. In this approach, real-time wide-area measurement data are processed and utilized to design a set of stability agents based on a Reinforcement Learning (RL) method. Recent technological breakthroughs in wide-area measurement system (WAMS) make the use of the system-wide signals possible in designing power system controllers. The main design objectives of these controllers are to stabilize the system after severe disturbances and mitigate the oscillations afterward. The proposed stability agents are decentralized and autonomous. The proposed method extends the stability boundary of the system and achieves the above goals without losing any generator or load area and without any knowledge of the disturbances causing the response. This paper describes the developed framework and addresses different challenges in designing such a network. A case study is provided to illustrate and verify the performance and robustness of the proposed approach.
5BD1BADE	Reinforcement learning is a class of model-free learning control method that can solve Markov decision problems. But it has some problems in applications, especially in MDPs of continuous state spaces. In this paper, based on the vague neural networks, we propose a Q-learning algorithm which is comprehensively considering the reward and punishment of the environment. Simulation results in cart-pole balancing problem illustrate the effectiveness of the proposed method.
7DC6C462	Although fuzzy control was initially introduced as a model-free control design method based on the knowledge of a human operator, current research is almost exclusively devoted to model-based fuzzy control methods that can guarantee stability and robustness of the closed-loop system. State-of-the-art techniques for identifying fuzzy models and designing model-based controllers are reviewed in this article. Attention is also paid to the role of fuzzy systems in higher levels of the control hierarchy, such as expert control, supervision and diagnostic systems. Open issues are highlighted and an attempt is made to give some directions for future research.
801563B6	Fuzzy logic control was originally introduced and developed as a model free control design approach. However, it unfortunately suffers from criticism of lacking of systematic stability analysis and controller design though it has a great success in industry applications. In the past ten years or so, prevailing research efforts on fuzzy logic control have been devoted to model-based fuzzy control systems that guarantee not only stability but also performance of closed-loop fuzzy control systems. This paper presents a survey on recent developments (or state of the art) of analysis and design of model based fuzzy control systems. Attention will be focused on stability analysis and controller design based on the so-called Takagi-Sugeno fuzzy models or fuzzy dynamic models. Perspectives of model based fuzzy control in future are also discussed
5CA11DB0	Most common applications using neural networks for control problems are the automatic controls using the artificial perceptual function. These control mechanisms are similar to those of the intelligent and pattern recognition control of an adaptive method frequently performed by the animate nature. Many automated buildings are using HVAC(Heating Ventilating and Air Conditioning) by PI that has simple and solid characteristics. However, to keep up good performance, proper tuning and re-tuning are necessary.In this paper, as the one of method to solve the above problems and improve control performance of controller, using reinforcement learning method for the one of neural network learning method(supervised/unsupervised/reinforcement learning), reinforcement learning controller is proposed and the validity will be evaluated under the real operating condition of AHU(Air Handling Unit) in the environment chamber.
7D752A1A	In this paper we consider the problem of reinforcement learning in a dynamically changing environment. In this context, we study the problem of adaptive control of finite-state Markov chains with a finite number of controls. The transition and payoff structures are unknown. The objective is to find an optimal policy which maximizes the expected total discounted payoff over the infinite horizon. A stochastic neural network model is suggested for the controller. The parameters of the neural net, which determine a random control strategy, are updated at each instant using a simple learning scheme. This learning scheme involves estimation of some relevant parameters using an adaptive critic. It is proved that the controller asymptotically chooses an optimal action in each state of the Markov chain with a high probability.
7E14100A	This paper gives a matrix expression of logic. Under matrix expression a general description of the logical operations is proposed, which is very convenient in logical inference. Then based on matrix expression the logic operators have been extended to multi-valued logic, which provides a foundation for fuzzy systems. Finally, the logic-based fuzzy control is considered.
80EF5C14	Most of the fuzzy literature presents stability and performance results in fuzzy control of Takagi-Sugeno models via LMI conditions; such conditions are independent of the particular shape of the LMIs. Shape-dependent conditions may be used to relax the results: some of them are only conditions on the memberships themselves; others include relations between state variables and memberships. The latter approach, then, enters into the realm of actual nonlinear control, departing from the conventional analysis which proves stability of nonlinear systems via proving stability of some linear time-variant convex-hull models (the Takagi-sugeno systems). The conditions may be made polynomial with the sum-of-squares approach. With the ideas in this paper, the gap between fuzzy and nonlinear control gets smaller.
7DFFC28A	The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, thereplacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the "Mountain-Car" task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator.
779E2A18	This paper proposes a Bayesian-game-based fuzzy reinforcement learning (RL) controller for decentralized partially observable Markov decision processes (Dec-POMDPs). Dec-POMDPs have recently emerged as a powerful platform for optimizing multiagent sequential decision making in partially observable stochastic environments. However, finding exact optimal solutions to a Dec-POMDP is provably intractable (NEXP-complete), necessitating the use of approximate/suboptimal solution approaches. This approach proposes an approximate solution by employing fuzzy inference systems (FISs) in a game-based RL setting. It uses the powerful universal approximation capability of fuzzy systems to compactly represent a Dec-POMDP as a fuzzy Dec-POMDP, allowing the controller to progressively learn and update an approximate solution to the underlying Dec-POMDP. The proposed controller envisages an FIS-based RL controller for Dec-POMDPs modeled as a sequence of Bayesian games (BGs). We implement the proposed controller for two scenarios: 1) Dec-POMDPs with free communication between agents; and 2) Dec-POMDPs without communication. We empirically evaluate the proposed approach on three standard benchmark problems: 1) multiagent tiger; 2) multiaccess broadcast channel; and 3) recycling robot. Simulation results and comparative evaluation against other Dec-POMDP solution approaches elucidate the effectiveness and feasibility of employing FIS-based game-theoretic RL for designing Dec-POMDP controllers.
24D2E0E1	In this paper, we propose a new architecture for decentralized control of discrete event systems, whose basic principle is as follows: for every event which is controllable by several supervisors, each of the latter takes an enabling/disabling decision only when it is sure that this is the right decision which can be applied to the plant. Otherwise, the supervisor transmits its local information to the fusion system which will thus be involved in the enabling/disabling decision-making. We compare our approach with the previous decentralized approaches.
79F5B21B	This paper is concerned with the stabilization problem for a class of discrete-time networked control systems (NCSs) with bounded time delays and packet losses. The controlled plant is represented by a Takagi-Sugeno fuzzy model, and both the state feedback control and output feedback control cases are considered. By guaranteeing the decrement of Lyapunov functional at each control signal updating step, a less conservative stability condition for the state feedback NCSs is derived, and the corresponding stabilizing controller design method is also presented. Under an observer-based framework, the output feedback stabilization problem is further studied, where the main contribution is the development of the separation principle for NCSs. Illustrative examples are provided to show the advantage and effectiveness of the developed results.
755905FC	We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.
7E79B63B	An extension of earlier work in the refinement of robotic motor control using reinforcement learning is described. It is no longer assumed that the magnitude of the state-dependent nonlinear torque is known. The learning controller learns about not only the presence of the torque, but also its magnitude. The ability of the learning system to learn this real-valued mapping from output feedback and reference input to control signal is facilitated by a stochastic algorithm that uses reinforcement feedback. A learning controller that can learn nonlinear mappings holds many possibilities for extending existing adaptive control research.
77CFD07E	This paper investigates the problem of Hankel-norm output feedback controller design for a class of T–S fuzzy stochastic systems. The full-order output feedback controller design technique with the Hankel-norm performance is proposed by the fuzzy-basis-dependent Lyapunov function approach and the conversion on the Hankel-norm controller parameters. Sufficient conditions are established to design the controllers such that the resulting closed-loop system is stochastically stable and satisfies a prescribed performance. The desired output feedback controller can be obtained by solving a convex optimization problem, which can be efficiently solved by standard numerical algorithms. Finally, a Henon map system is used to illustrate the effectiveness of the proposed techniques.
7C9F6684	A supervisor proposed by Ramadge and Wonham controls a discrete event system (DES) so as to satisfy logical control specifications. However a precise description of both the specifications and the DES is needed for the control. This paper proposes a synthesis method of the supervisor for decentralized DESs based on reinforcement learning. In decentralized DESs, several local supervisors exist and control the DES jointly. Costs for disabling and enabling events as well as control specifications are considered. By using reinforcement learning, the proposed method is applicable under imprecise specifications and uncertain environment.
7E04A1A7	This paper presents the stability analysis of polynomial fuzzy-model-based (PFMB) control systems using the sum-of-squares (SOS) approach. The PFMB control system under consideration requires that the polynomial fuzzy model and polynomial fuzzy controller share neither the same premise membership functions nor the same number of fuzzy rules. This class of PFMB control systems offers a greater design flexibility to the polynomial fuzzy controller. However, due to the imperfectly matched membership functions, it usually produces more conservative stability conditions by following the traditional stability-analysis approach for the FMB control systems. To facilitate the stability analysis, piecewise-linear membership functions (PLMFs) are proposed, which offer a nice property that the grades of membership are governed by a finite number of sample points. Thus, it allows the PLMFs to be brought to the SOS-based stability conditions, which are applied to the PFMB control systems with the specified PLMFs rather than any shapes. The system stability can be examined by checking only the PFMB control system at the sample points. It is worth mentioning that the PLMFs, which are not necessarily implemented physically, are a mathematical tool to carry out the stability analysis. To verify the stability-analysis result, a simulation example is given to demonstrate the effectiveness of the proposed approach.
7D66A0FD	Global exponential stability of fuzzy control systems with delays is studied. These delays in the fuzzy control systems are assumed to be any uncertain bounded continuous functions. Stability of systems with uncertain delays is interesting since in practical applications it is not easy to know the delays exactly. Conditions for global exponential stability of free fuzzy systems with uncertain delays are derived. Criteria for design of nonlinear fuzzy controllers to feedback control the stability of global nonlinear fuzzy systems are given. Theorems are proved via the method of functional differential inequalities analysis.
7DC88153	Reinforcement learning agents typically require a significant amount of data before performing well on complex tasks. Transfer learning methods have made progress reducing sample complexity, but they have primarily been applied to model-free learning methods, not more data-efficient model-based learning methods. This paper introduces timbrel, a novel method capable of transferring information effectively into a model-based reinforcement learning algorithm. We demonstrate that timbrel can significantly improve the sample efficiency and asymptotic performance of a model-based algorithm when learning in a continuous state space. Additionally, we conduct experiments to test the limits of timbrel’s effectiveness.
58AAC56D	The behavior of reinforcement learning (RL) algorithms is best understood in completely observable, discrete-time controlled Markov chains with finite state and action spaces. In contrast, robot-learning domains are inherently continuous both in time and space, and moreover are partially observable. Here we suggest a systematic approach to solve such problems in which the available qualitative and quantitative knowledge is used to reduce the complexity of learning task. The steps of the design process are to:i) decompose the task into subtasks using the qualitative knowledge at hand; ii) design local controllers to solve the subtasks using the available quantitative knowledge and iii) learn a coordination of these controllers by means of reinforcement learning. It is argued that the approach enables fast, semi-automatic, but still high quality robot-control as no fine-tuning of the local controllers is needed. The approach was verified on a non-trivial real-life robot task. Several RL algorithms were compared by ANOVA and it was found that the model-based approach worked significantly better than the model-free approach. The learnt switching strategy performed comparably to a handcrafted version. Moreover, the learnt strategy seemed to exploit certain properties of the environment which were not foreseen in advance, thus supporting the view that adaptive algorithms are advantageous to non-adaptive ones in complex environments.
7D037EB2	The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.
774A06D0	The activated sludge process is a commonly used method for treating sewage and waste waters. It is characterised by a lack of relevant instrumentation, control goals that are not always clearly stated, the use of qualitative information in decision making and poorly understood basic biological behaviour mechanisms. In this brief paper we examine the behaviour of an experimental fuzzy control algorithm constructed to reflect actual operational practice. We conclude that this algorithm does rather well and that a fuzzy controller would be a useful and practical way of regulating the activated sludge process.
807C7AAF	A fuzzy-logic based multi-input/multi-output roll controller designed for the Advanced Technology Wing (ATW) aircraft model is presented. The ATW integrates active controls with a flexible wing structure to provide optimal wing shapes to meet particular flight performance criteria. The use of a fuzzy controller for roll rate and load alleviation control was investigated. Fuzzy rules were developed to determine the appropriate control surface deflections to achieve the desired roll rate while ensuring that wing loads are within safe bounds. The modulation of the damping factor is according to the distance of the system state from the goal state. This damping modulation technique allows full utilization of the vehicle's acceleration capability and resulted in an improvement of the response time by a factor of two. The resultant fuzzy controller commands six surface deflections to control the roll rate and four torsion moments.
7DC2AC91	Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.
7ACB133A	The contribution described in this paper is an algorithm for learning nonlinear, reference tracking, control policies given no prior knowledge of the dynamical system and limited interaction with the system through the learning process. Concepts from the field of reinforcement learning, Bayesian statistics and classical control have been brought together in the formulation of this algorithm which can be viewed as a form of indirect self tuning regulator. On the task of reference tracking using a simulated inverted pendulum it was shown to yield generally improved performance on the best controller derived from the standard linear quadratic method using only 30 s of total interaction with the system. Finally, the algorithm was shown to work on the simulated double pendulum proving its ability to solve nontrivial control tasks.
805865FC	In the December 2005 issue of this journal, Lam and Leung proposed stability results for fuzzy control systems where the membership functions in the controller were not the same as those from the process one, but some multiplicative bounds were known. The main practical context where that situation arises is the uncertain knowledge of the memberships of a Takagi-Sugeno model. This correspondence presents further extensions of those results, allowing for a richer description of the membership uncertainty, in terms of affine inequalities.
091BDE56	Conventional reinforcement learning has focused on learning in a stable environment. However, an agent may be given another environment which differs from the old environment. Thus, an autonomous agent needs a method to learn efficiently a new policy suited for the new environment. In this paper, we propose a method to adapt to a new environment for an agent which has a task to reach goals. When an agent is provided with a new environment, our method learns a new partial policy using the precondition of agent’s old policy. The precondition of a policy is a condition that says what must be satisfied in order to reach goals by using the policy. Similarly to learning the precondition of an action from the instances of action’s success or failure by using concept learning, our method learns the precondition of a policy from the instances of policy’s success or failure by using concept learning. We describe a method using inductive logic programming (ILP) as a concept learning method. Since ILP provides methods for learning relational knowledge that is not expressible in attribute-value learning, our method can use relational representation for the precondition. We applied our method to a blocks-world problem for evaluation. We have come to conclusion that our method is effective when the cost to carry out the task is high. 
809C4962	Recent research has demonstrated that human-generated reward signals can be effectively used to train agents to perform a range of reinforcement learning tasks. Such tasks are either episodic - i.e., conducted in unconnected episodes of activity that often end in either goal or failure states - or continuing - i.e., indefinitely ongoing. Another point of difference is whether the learning agent highly discounts the value of future reward - a myopic agent - or conversely values future reward appreciably. In recent work, we found that previous approaches to learning from human reward all used myopic valuation [7]. This study additionally provided evidence for the desirability of myopic valuation in task domains that are both goal-based and episodic.In this paper, we conduct three user studies that examine critical assumptions of our previous research: task episodicity, optimal behavior with respect to a Markov Decision Process, and lack of a failure state in the goal-based task. In the first experiment, we show that converting a simple episodic task to non-episodic (i.e., continuing) task resolves some theoretical issues present in episodic tasks with generally positive reward and - relatedly - enables highly successful learning with non-myopic valuation in multiple user studies. The primary learning algorithm in this paper, which we call "VI-TAMER", is it the first algorithm to successfully learn non-myopically from human-generated reward; we also empirically show that such non-myopic valuation facilitates higher-level understanding of the task. Anticipating the complexity of real-world problems, we perform two subsequent user studies - one with a failure state added - that compare (1) learning when states are updated asynchronously with local bias - i.e., states quickly reachable from the agent's current state are updated more often than other states - to (2) learning with the fully synchronous sweeps across each state in the VI-TAMER algorithm. With these locally biased updates, we find that the general positivity of human reward creates problems even for continuing tasks, revealing a distinct research challenge for future work.
7D526539	This paper presents an output feedback robust H&infin; control problem for a class of uncertain fuzzy dynamic systems with time-varying delayed state. The Takagi-Sugeno fuzzy model is employed to represent an uncertain nonlinear systems with time-varying delayed state. Using a single quadratic Lyapunov function, the globally exponential stability and disturbance attenuation of the closed-loop fuzzy control system are discussed. Sufficient conditions for the existence of robust H&infin; controllers are given in terms of matrix inequalities. Constructive algorithm for design of robust H &infin; controller is also developed. The resulting controller is nonlinear and automatically tuned based on fuzzy operation
7B9C9D36	This work presents in detail the fuzzy control design for yaw tracking of an autonomous underwater vehicle. This control has been developed from the mathematical description of the hydrodynamic model of the vehicle, which is studied and discussed from different situations both in surge velocity as in changes in yaw reference. The model is linearized and several linear controls are designed for their actuation at certain situations, in a way that the fuzzy control allows to handle those controls globally.
7C95773B	We present a new subgoal-based method for automatically creating useful skills in reinforcement learning.  Our method identifies subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.
7616B8B5	This paper uses the relation matrix to examine the structure of fuzzy control algorithms. After introducing some basic notations and definitions, it presents a theorem which allows an arbitrary relation matrix to be translated into a set of fuzzy control rules. Using this result it is possible to show that the implementation of the algorithm does not affect its structure and that the most meaningful way of altering its performance is to change the rules themselves.
7FC23B53	The hierarchical structure of real-world problems has resulted in a focus on hierarchical frameworks in the reinforcement learning paradigm. Preparing mechanisms for automatic discovery of macro-actions has mainly concentrated on subgoal discovery methods. Among the proposed algorithms, those based on graph partitioning have achieved precise results. However, few methods have been shown to be successful both in performance and also efficiency in terms of time complexity of the algorithm. In this paper, we present a SCC-based subgoal discovery algorithm; a graph theoretic approach for automatic detection of subgoals in linear time. Meanwhile a parameter tuning method is proposed to find the only parameter of the method.
7F239E95	emporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artificial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. 
63E81DBB	Reinforcement learning has been used as a reasonably successful method for the problem of model-free learning of action policies for some control problems. However, it is usually assumed that the process to be controlled is either open loop stable or of slow dynamics, when frequency of failures before acceptable performance or input-output processing time are not issues of primary importance. We consider the problem of model-free regulation for an unstable plant. As in many cases the need for state quantisation is an algorithmic storage requirement rather than a sensor limitation, we propose a modification of a standard reinforcement learning method that uses as additional information the distance between sampled and represented states, embedded in actions that are a result of a distance-wise local interpolation scheme. We obtained faster learning under minimal disturbance of the original learning scheme, and the modification is computationally modest enough to allow for real-time implementation.
7D4D87E8	In recent years, evolution-based knowledge optimization has gained a great deal of popularity due to its inherent ability in efficient and parallel search of complex and multimodal landscapes. Application of genetic algorithms (GA) to knowledge enhancement involves several aspects. First is how to code a string to represent all the necessary degrees of freedom for search in the fuzzy knowledge domain. The second aspect is how to incorporate existing expert knowledge into the GA-optimising algorithm. And in general, how to take advantage of several experts' opinions in creation of an initial population. Conventional applications of GA-fuzzy suggest using a random initial population. However, it is intuitively clear that any search routine could converge faster if starting points are good solutions. In this paper, a methodology is illustrated which incorporates expert knowledge in creating an initial population while allowing for randomness among members of the population for diversity. Furthermore, the methodology is applied to step response optimization of a flexible-link feedback control system.
7919B344	We contribute Policy Reuse as a technique to improve a reinforcement learning agent with guidance from past learned similar policies. Our method relies on using the past policies as a probabilistic bias where the learning agent faces three choices: the exploitation of the ongoing learned policy, the exploration of random unexplored actions, and the exploitation of past policies. We introduce the algorithm and its major components: an exploration strategy to include the new reuse bias, and a similarity function to estimate the similarity of past policies with respect to a new one. We provide empirical results demonstrating that Policy Reuse improves the learning performance over different strategies that learn without reuse. Interestingly and almost as a side effect, Policy Reuse also identifies classes of similar policies revealing a basis of core policies of the domain. We demonstrate that such a basis can be built incrementally, contributing the learning of the structure of a domain.
7F65C381	This work introduces Human-Agent Transfer (HAT), an algorithm that combines transfer learning, learning from demonstration and reinforcement learning to achieve rapid learning and high performance in complex domains. Using experiments in a simulated robot soccer domain, we show that human demonstrations transferred into a baseline policy for an agent and refined using reinforcement learning significantly improve both learning time and policy performance. Our evaluation compares three algorithmic approaches to incorporating demonstration rule summaries into transfer learning, and studies the impact of demonstration quality and quantity, as well as the effect of combining demonstrations from multiple teachers. Our results show that all three transfer methods lead to statistically significant improvement in performance over learning without demonstration. The best performance was achieved by combining the best demonstrations from two teachers.
7C544A25	This paper investigates the stability of a polynomial-fuzzy-model-based (PFMB) control system formed by a nonlinear plant represented by a polynomial fuzzy model and a polynomial fuzzy controller connected in a closed loop. Three cases of polynomial fuzzy controllers are proposed for the control process with the consideration of a matched/mismatched number of rules and/or premise membership functions, which demonstrate different levels of controller complexity, design flexibility, and stability analysis results. A general polynomial Lyapunov function candidate is proposed to investigate the system stability. Unlike the published work, there is no constraint on the polynomial Lyapunov function candidate, which is independent of the form of the polynomial fuzzy model. Thus, it can be applied to a wider class of PFMB control systems and potentially produces more relaxed stability analysis results. Two-step stability conditions in terms of sum-of-squares (SOS) are obtained to numerically find a feasible solution. To facilitate the stability analysis and relax the stability analysis result, the boundary information of membership functions is taken into account in the stability analysis and incorporated into the SOS-based stability conditions. Simulation examples are given to illustrate the effectiveness of the proposed approach.
7DA1591F	This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning. 
7F68B703	The basic operation of biological and electronic (artificial) neural networks (NNs) is examined. Learning by NNs is discussed, covering supervised learning, particularly back-propagation, and unsupervised and reinforcement learning. The use of VLSI implementation to speed learning is considered briefly. Applications of neural-style learning chips to pattern recognition, data compression, optimization, and expert systems is discussed. Problem areas and issues for further research are addressed.
7769F1C6	In this research, double-command control of a nonlinear chemical system is addressed. The system includes a stirred tank; two flows of liquid with different concentrations are entering the system through two valves and another flow is exiting the tank with a concentration between two input concentrations. The outlet concentration is usually regulated by the control of one of the valves and the flow rate of the second valve is fixed. The authors have already accomplished the single-command control of this system using neuro-predictive technique. Although, this problem is known as a good example for neuro-predicitve control, but this technique was found both ineffective (in terms of offering improper performance) and inefficient (in terms of needing heavy computations) when it is tried for double-command control (the control of both valves). Therefore, a fuzzy controller is designed to double-command control this system in the simulation environment. In order to avoid output chattering and frequent change of control command (leading frequent closing-opening of control valves, in practice) a damper rule is added to the fuzzy control system. A steady state control law is also derived from nonlinear mathematical model of the system to be added to transient control command whose increments are generated by fuzzy controller. The hybrid control system leads a very smooth change of control input which is suitable for real applications. The designed control systems offer lower error integral, control command change and processing time in comparison to single-command neuro-predictive controllers.
7BA2C2AF	The paper describes the application of fuzzy algorithms to the control of dynamic processes. The fuzzy control algorithm is used to implement linguistically expressed heuristic control policies directly, with a view to automating those complex and poorly-defined processes where modelling difficulties and lack of suitable measurements make manual control imperative. The results obtained from two pilot-scale studies are presented and the stability of the fuzzy control system is discussed.
775D0DE4	This study investigates the stability of parameter-dependent polynomial-fuzzy-model-based (PDPFMB) control systems. A parameter-dependent polynomial fuzzy model is proposed to represent a non-linear plant. A parameter-dependent polynomial fuzzy controller is then employed to stabilise the non-linear plant. The stability of PDPFMB control systems is investigated using the sum-of-squares (SOS) technique based on a parameter-dependent Lyapunov function candidate. With the consideration of the information of system parameters, parameter-dependent SOS-based stability conditions are obtained to determine the system stability and polynomial feedback gains. A feasible solution to the stability conditions can be obtained numerically using the third-party Matlab toolbox SOSTOOLS. Unlike the membership functions, as the system parameters are not single signed functions, the traditional analysis approach obtaining the stability conditions cannot be applied. Instead, the membership functions and system parameters are considered as symbolic variables for the construction of the parameter-dependent SOS-based stability conditions. A simulation example is given to demonstrate the merits of the proposed approach.
75345268	This paper reviews the studies on fuzzy control by referring to most of the papers ever written on fuzzy control. As an introduction, the paper picks up key points in applying fuzzy control and shows very recent results in industrial applications. The paper also points out some interesting and important problems to be solved.
7F673B7C	The basic concept of learning control is introduced. The following five learning schemes are briefly reviewed: 1) trainable controllers using pattern classifiers, 2) reinforcement learning control systems, 3) Bayesian estimation, 4) stochastic approximation, and 5) stochastic automata models. Potential applications and problems for further research in learning control are outlined.
76FDBE68	The basic concept of learning control is introduced. The following five learning schemes are briefly reviewed: 1) trainable controllers using pattern classifiers, 2) reinforcement learning control systems, 3) Bayesian estimation, 4) stochastic approximation, and 5) stochastic automata models. Potential applications and problems for further research in learning control are outlined.
03C5C081	Often the most practical way to define a Markov Decision Process (MDP) is as a simulator that, given a state and an action, produces a resulting state and immediate reward sampled from the corresponding distributions. Simulators in natural resource management can be very expensive to execute, so that the time required to solve such MDPs is dominated by the number of calls to the simulator. This paper presents an algorithm, \algname, that combines improved confidence intervals on the Q values (as in interval estimation) with a novel upper bound on the discounted state occupancy probabilities to intelligently choose state-action pairs to explore. We prove that this algorithm terminates with a policy whose value is within $\epsilon$ of the optimal policy (with probability $1-\delta$) after making only polynomially-many calls to the simulator. Experiments on one benchmark MDP and on an MDP for invasive species management show very large reductions in the number of simulator calls required. 
7D003953	Classical Takagi-Sugeno (T-S) fuzzy models are formed by convex combinations of linear consequent local models. Such fuzzy models can be obtained from nonlinear first-principle equations by the well-known sector-nonlinearity modeling technique. This paper extends the sector-nonlinearity approach to the polynomial case. This way, generalized polynomial fuzzy models are obtained. The new class of models is polynomial, both in the membership functions and in the consequent models. Importantly, T-S models become a particular case of the proposed technique. Recent possibilities for stability analysis and controller synthesis are also discussed. A set of examples shows that polynomial modeling is able to reduce conservativeness with respect to standard T-S approaches as the degrees of the involved polynomials increase.
7A6AF114	Stable direct and indirect adaptive controllers are presented, which use Takagi-Sugeno fuzzy systems, conventional fuzzy systems, or a class of neural networks to provide asymptotic tracking of a reference signal for a class of continuous-time nonlinear plants with poorly understood dynamics. The indirect adaptive scheme allows for the inclusion of a priori knowledge about the plant dynamics in terms of exact mathematical equations or linguistics while the direct adaptive scheme allows for the incorporation of such a priori knowledge in specifying the controller. We prove that with or without such knowledge both adaptive schemes can "learn" how to control the plant, provide for bounded internal signals, and achieve asymptotically stable tracking of a reference input. In addition, for the direct adaptive scheme a technique is presented in which linguistic knowledge of the inverse dynamics of the plant may be used to accelerate adaptation. The performance of the indirect and direct adaptive schemes is demonstrated through the longitudinal control of an automobile within an automated lane.
7DD3B0E7	We introduce and discuss the participatory learning paradigm. A formal system implementing this type of learning agent is described. We then extend this system so that it can learn from interval-type observations. We further extend this system to the case when the observation is a more general granular object such as a fuzzy set. In the initial stage, while we allowed our observations to be granular, we restricted the learning to be precise values. In the next part, we allow both the observations and learned object to be granular. An important issue that arises when learning granular values relates to the specificity of the learned value. Learned values that are too unspecific can be useless. We suggest methods for controlling the specificity of the values learned.
7EB479C6	This paper is a presentation of neuronal control systems in the terms of the dynamical systems theory, where (1) the controller and its surrounding environment are seen as two co-dependent controlled dynamical systems (2) the behavioral transitions that take place under adaptation processes are analyzed in terms of phase-transitions. We present in the second section a generic method for the construction of multi-population random recurrent neural networks. The third section gives an overview of the various phase transitions that take place under an external forcing signal, or under internal parametric changes. The section 4 presents some applications in the domain of sequence identification and active perception modeling. The section 5 presents some applications in the domain of closed-loop control systems and reinforcement learning.
7FA59B5D	To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.
76192604	This paper is concerned with the problem of guaranteed cost control for a class of continuous-time networked control systems based on the Takagi-Sugeno fuzzy model approach. In this study, network-induced transmission delays and packet dropouts are taken into simultaneous consideration. By the improved Lyapunov-Krasovskii functionals, a less-conservative delay-dependent condition is proposed for the stability analysis of the resulting closed-loop system. Furthermore, the sufficient condition under which the guaranteed cost controller exists is formulated in terms of linear matrix inequalities, and the suboptimum solution is obtained by a cone complementarity linearization algorithm. Finally, numerical examples are provided to illustrate the effectiveness of the proposed method.	
7D142A8F	Cognitive flexibility reflects both a trait that reliably differs between individuals and a state that can fluctuate moment-to-moment. Whether individuals can undergo persistent changes in cognitive flexibility as a result of reward learning is less understood. Here, we investigated whether reinforcing a periodic shift in an object selection strategy can make an individual more prone to switch strategies in a subsequent unrelated task. Participants completed two different choice tasks in which they selected one of four objects in an attempt to obtain a hidden reward on each trial. During a training phase, objects were defined by color. Participants received either consistent reward contingencies in which one color was more often rewarded, or contingencies in which the color that was more often rewarded changed periodically and without warning. Following the training phase, all participants completed a test phase in which reward contingencies were defined by spatial location and the location that was more often rewarded remained constant across the entire task. Those participants who received inconsistent contingencies during training continued to make more variable selections during the test phase in comparison to those who received the consistent training. Furthermore, a difference in the likelihood to switch selections on a trial-by-trial basis emerged between training groups: participants who received consistent contingencies during training were less likely to switch object selections following an unrewarded trial and more likely to repeat a selection following reward. Our findings provide evidence that the extent to which priority shifting is reinforced modulates the stability of cognitive control settings in a persistent manner, such that individuals become generally more or less prone to shifting priorities in the future.
8066365F	A nonlinear controller based on a fuzzy model of MIMO dynamical systems is described and analyzed. The fuzzy model is based on a set of ARX models that are combined using a fuzzy inference mechanism. The controller is a discrete-time nonlinear decoupler, which is analyzed both for the adaptive and the fixed parameter cases. A detailed stability analysis is carried out, and the main result is that the closed loop is globally stable and robust with respect to unstructured uncertainty, which may include modeling error and disturbances. In addition, bounds on the asymptotic and transient performance are given. The main assumptions on the system and model are that they must not have strong nonminimum-phase effects, except time-delay, and the unstructured uncertainty must not be too large. A simulation example illustrates some of the properties of the modeling method and model based control structure.80A2C1E3
7F0B94A0	This study introduces a fuzzy linear control design method for nonlinear systems with optimal H/sup /spl infin// robustness performance. First, the Takagi and Sugeno fuzzy linear model (1985) is employed to approximate a nonlinear system. Next, based on the fuzzy linear model, a fuzzy controller is developed to stabilize the nonlinear system, and at the same time the effect of external disturbance on control performance is attenuated to a minimum level. Thus based on the fuzzy linear model, H/sup /spl infin// performance design can be achieved in nonlinear control systems. In the proposed fuzzy linear control method, the fuzzy linear model provides rough control to approximate the nonlinear control system, while the H/sup /spl infin// scheme provides precise control to achieve the optimal robustness performance. Linear matrix inequality (LMI) techniques are employed to solve this robust fuzzy control problem. In the case that state variables are unavailable, a fuzzy observer-based H/sup /spl infin// control is also proposed to achieve a robust optimization design for nonlinear systems. A simulation example is given to illustrate the performance of the proposed design method.
5E0782CB	In this paper we present a new method for reinforcement learning in relational domains. A logical language is employed to abstract over states and actions, thereby decreasing the size of the state-action space significantly. A probabilistic transition model of the abstracted Markov-Decision-Process is estimated to to speed-up learning. We present theoretical and experimental analysis of our new representation. Some insights concerning the problems and opportunities of logical representations for reinforcement learning are obtained in the context of a growing interest in the use of abstraction in reinforcement learning contexts.
7A50D4B3	This article considers adaptive control architectures that integrate active sensory-motor systems with decision systems based on reinforcement learning. One unavoidable consequence of active perception is that the agent's internal representation often confounds external world states. We call this phoenomenon perceptual aliasingand show that it destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy. We then describe a new decision system that overcomes these difficulties for a restricted class of decision problems. The system incorporates a perceptual subcycle within the overall decision cycle and uses a modified learning algorithm to suppress the effects of perceptual aliasing. The result is a control architecture that learns not only how to solve a task but also where to focus its visual attention in order to collect necessary sensory information.
7AE4E1F1	Many complex industrial processes cannot be satisfactorily controlled using the results of modern control theory, mainly because their precise structure is unknown. However, this is often balanced by a considerable amount of ‘engineering feel’ for the process which is difficult to quantify and utilise. Fuzzy set theory is a relatively new concept which allows this qualitativeness to be expressed rigorously and in this paper its usefulness for control is assessed. The state of the art is reviewed and reveals a surprising number of practical successes.