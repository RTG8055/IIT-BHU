7E42B2FA	Face detection and facial feature extraction plays an important role in video surveillance, human computer interaction and face recognition. Color is a useful piece of information in computer vision especially for skin detection. In this paper, we propose a novel approach for skin segmentation and facial feature extraction. The proposed skin segmentation is a method for integrating the chrominance components of nonlinear YCrCb color model. The goal of skin detection is to group pixels to form possible face candidate regions and then use connected components analysis for pixels grouping. In order to detect the facial feature in scale invariant, the possible face candidate regions will be normalized, and then texture information in these regions will be segmented by means of mean and variance of face region. Edge will be detected using the method based on multi-scale morphological. Eye will be located by the PCA edge direction. The others feature, such as nose and mouth, also located using the geometrical shape information. As all the above-mentioned techniques are simple and efficient, the proposed method is computational effective and suitable for practical applications. In our experiments, the proposed method has been successfully evaluated using two different test datasets. The detection accuracy is around 98%, the average run time ranged from 0.1-0.3 sec per frame.
5B5A6EB8	The recognition of human gestures and facial expressions in image sequences is an important and challenging problem that enables a host of human-computer interaction applications. This paper describes a framework for incremental recognition of human motion that extends the “Condensation” algorithm proposed by Isard and Blake (ECCV'96). Human motions are modeled as temporal trajectories of some estimated parameters over time. The Condensation algorithm uses random sampling techniques to incrementally match the trajectory models to the multi-variate input data. The recognition framework is demonstrated with two examples. The first example involves an augmented office whiteboard with which a user can make simple hand gestures to grab regions of the board, print them, save them, etc. The second example illustrates the recognition of human facial expressions using the estimated parameters of a learned model of mouth motion.
5F3038AE	Automatic analysis of facial gestures is rapidly becoming an area of intense interest in computer science and human-computer interaction design communities. However, the basic goal of this area of research translating detected facial changes into a human-like description of shown, facial expression is yet to be achieved. One of the main impediments to achieving this aim is the fad that human interpretations of a facial expression differ depending upon whether the observed person is speaking or not. A first step in tackling this problem is to achieve automatic detection of facial gestures that are typical for speech articulation. This paper presents our approach to seizing this step in the research on automatic facial expression analysis. It presents: a robust and flexible method for recognition of 22 facial muscle actions from face image sequences, a method for automatic determination of whether the observed subject is speaking or not, and an experimental study on facial muscle actions typical for speech articulation.
75F43DFE	Improving Human-Computer Interaction (HCI) necessitates building an efficient human emotion recognition approach that involves various modalities such as facial expressions, hand gestures, acoustic data, and biophysiological data. In this paper, we address the perception of the universal human emotions (happy, surprise, anger, disgust, fear, and sadness) from facial expressions. In our companion-based assistant system, facial expression is considered as complementary aspect to the hand gestures. Unlike many other approaches, we do not rely on prior knowledge of the neutral state to infer the emotion because annotating the neutral state usually involves human intervention. We use features extracted from just eight fiducial facial points. Our results are in a good agreement with those of a state-of-the-art approach that exploits features derived from 68 facial points and requires prior knowledge of the neutral state. Then, we evaluate our approach on two databases. Finally, we investigate the influence of the facial points detection error on our emotion recognition approach.
76E2F827	We describe a facial feature tracker based on the combined range and amplitude data provided by a 3D Time-Of-Flight camera. We use this tracker to implement a head mouse, an alternative input device for people who have limited use of their hands. The facial feature tracker is based on geometric features that are related to the intrinsic dimensionality of multidimensional signals. We show how the position of the nose in the image can be determined robustly using a very simple bounding-box classifier, trained on a set of labelled sample images. Despite its simplicity, the classifier generalises well to subjects that it was not trained on. An important result is that the combination of range and amplitude data dramatically improves robustness compared to a single type of data. The tracker runs in real time at around 30 frames per second. We demonstrate its potential as an input device by using it to control Dasher, an alternative text input tool.
81206858	In the recent few decades, automatic detection and tracking of face and facial features such as eyes and mouth, in image and video sequences has become an active research area in machine vision applications such as Human-Computer Interaction (HCI). In this paper, a new algorithm for detection of face and facial features is proposed that can localize eyes and mouth very accurately in images. In this method, a combination of luminance, color and edge properties of image is used. This method is compared to the method introduced by Rein Lien Hsu, in which color and luminance information is used, and it is shown that the new algorithm is more robust and accurate in locating eyes and mouth in facial images with maximum 30 degrees of lateral rotation. Both methods are implemented and tested on a database containing 103 different images of face, and it is shown that the proposed method increases the accuracy by 4 percent and reaches to 91.26% of accuracy.
7DE3EAB8	Due to the development of technology, electronics have become intertwined with our daily lives. Because of our reliance on such products, they need to be user-friendly, thus, improving current technology products is critical. One example of an electronic product with an opportunity for improvement is the projector. In many places, projectors have become an indispensable instrument for presentations or teaching. In this paper, we will improve the traditional projector, whose harsh light may hurt or cause discomfort for human eyes. Our system has been successfully applied to projected PowerPoint presentations and the experimental results speak to its performance.
7EC2D89D	The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient "purposive" approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of human-computer interaction.
8167E95F	Predefined sequences of eye movements, or ‘gaze gestures’, can be consciously performed by humans and monitored non-invasively using remote video oculography. Gaze gestures hold great potential in human–computer interaction, HCI, as long as they can be easily assimilated by potential users, monitored using low cost gaze tracking equipment and machine learning algorithms are able to distinguish the spatio-temporal structure of intentional gaze gestures from typical gaze activity performed during standard HCI. In this work, an evaluation of the performance of a bioinspired Bayesian pattern recognition algorithm known as Hierarchical Temporal Memory (HTM) on the real time recognition of gaze gestures is carried out through a user study. To improve the performance of traditional HTM during real time recognition, an extension of the algorithm is proposed in order to adapt HTM to the temporal structure of gaze gestures. The extension consists of an additional top node in the HTM topology that stores and compares sequences of input data by sequence alignment using dynamic programming. The spatio-temporal codification of a gesture in a sequence serves the purpose of handling the temporal evolution of gaze gestures instances. The extended HTM allows for reliable discrimination of intentional gaze gestures from otherwise standard human–machine gaze interaction reaching up to 98% recognition accuracy for a data set of 10 categories of gaze gestures, acceptable completion speeds and a low rate of false positives during standard gaze–computer interaction. These positive results despite the low cost hardware employed supports the notion of using gaze gestures as a new HCI paradigm for the fields of accessibility and interaction with smartphones, tablets, projected displays and traditional desktop computers.
790FB066	Document recognition and retrieval technologies complement one another, providing improved access to increasingly large document collections. While recognition and retrieval of textual information is fairly mature, with wide-spread availability of optical character recognition and text-based search engines, recognition and retrieval of graphics such as images, figures, tables, diagrams, and mathematical expressions are in comparatively early stages of research. This paper surveys the state of the art in recognition and retrieval of mathematical expressions, organized around four key problems in math retrieval (query construction, normalization, indexing, and relevance feedback), and four key problems in math recognition (detecting expressions, detecting and classifying symbols, analyzing symbol layout, and constructing a representation of meaning). Of special interest is the machine learning problem of jointly optimizing the component algorithms in a math recognition system, and developing effective indexing, retrieval and relevance feedback algorithms for math retrieval. Another important open problem is developing user interfaces that seamlessly integrate recognition and retrieval. Activity in these important research areas is increasing, in part because math notation provides an excellent domain for studying problems common to many document and graphics recognition and retrieval applications, and also because mature applications will likely provide substantial benefits for education, research, and mathematical literacy.
7FB0C49F	Automatic facial expression recognition is an important technique for interaction between humans and machines such as robots or computers. In particular, pose invariant facial expression recognition is needed in an automatic facial expression system because frontal faces are not always visible in real situations. The present paper introduces a multi-view method for recognizing facial expressions using a parametric kernel eigenspace method based on class features (pKEMC). We first describe pKEMC that finds the manifold of data patterns in each class on a non-linear discriminant subspace for separating multiple classes. Then, we apply pKEMC for pose-invariant facial expression recognition. We also utilize facial-component-based representation to improve the robustness to pose variation. We carried out the validation of our method on a Multi-PIE database. The results show that our method has high discrimination accuracy and provides an effective means to recognize multi-view facial expressions.
0FD7E6B2	
813F0A9B	Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted
7D124D47	This paper proposes a new hand gesture recognition method using a 3D prediction model used for developing human-computer interfaces. It focuses on real-time hand-pose estimation, which is indispensable for virtual object manipulation. In the first stage, simple but effective parallel pipelined algorithms are employed to extract hand features from binocular images. In the second stage, the hand pose is estimated using the hand's features. Even if occlusion occurs, a ghost correspondence eliminating table, used with the 3D prediction model, makes the estimation possible on the assumption that finger tips are occluded by other finger tips. The hand-feature extraction method, the pose estimation method, the constructed human-computer interfaces, and a hand-pose estimation experiment are discussed.
77F6FFE7	For human beings, facial expression is one of the most powerful and natural way to communicate their emotions and intensions. A human being can detect facial expressions without effort, but for a machine it is very difficult. Automatic facial expression recognition is an interesting and challenging problem. Automatic facial expression recognition systems can be mainly used for human computer interaction and data driven animation. There are three sub problems while designing automatic facial expression recognition system, face detection, extraction of the facial expression information, and classification of the expression. A system that performs these operations more accurately and in real time would be crucial to achieve a human-like interaction between man and machine. This paper reviews the past work done in solving these problems for image sequences.
802F34E9	We propose a fast and robust method for tracking a user's hand and multiple fingertips; we then demonstrate gesture recognition based on measured fingertip trajectories for augmented desk interface systems. Our tracking method is capable of tracking multiple fingertips in a reliable manner even in a complex background under a dynamically changing lighting condition without any markers. First, based on its geometrical features, the location of each fingertip is located in each input infrared image frame. Then, correspondences of detected fingertips between successive image frames are determined based on a prediction technique. Our gesture recognition system is particularly advantageous for human-computer interaction (HCI) in that users can achieve interactions based on symbolic gestures at the same time that they perform direct manipulation with their own hands and fingers. The effectiveness of our proposed method has been successfully demonstrated via a number of experiments. 
7D6D6948	People suffering from autism have difficulty with recognizing other people's emotions and are therefore unable to react to it. Although there have been attempts aimed at developing a system for analyzing facial expressions for persons suffering from autism, very little has been explored for capturing one or more expressions from mixed expressions which are a mixture of two closely related expressions. This is essential for psychotherapeutic tool for analysis during counseling. This paper presents the idea of improving the recognition accuracy of one or more of the six prototypic expressions namely happiness, surprise, fear, disgust, sadness and anger from the mixture of two facial expressions. For this purpose a motion gradient based optical flow for muscle movement is computed between frames of a given video sequence. The computed optical flow is further used to generate feature vector as the signature of six basic prototypic expressions. Decision Tree generated rule base is used for clustering the feature vectors obtained in the video sequence and the result of clustering is used for recognition of expressions. The relative intensity of expressions for a given face present in a frame is measured. With the introduction of Component Based Analysis which is basically computing the feature vectors on the proposed regions of interest on a face, considerable improvement has been noticed regarding recognition of one or more expressions. The results have been validated against human judgement.
7BB0EAD2	Facial expression is a basic way to express emotions, which plays an important role in daily communications. Existing expression recognition algorithms contain problem such as. high dimension of the feature dictionary, low calculating rate, and lack of robustness in recognition of the occlusion and in noise expression. To refine the issues mentioned above, a novel method for facial expression recognition is proposed on PHOG Feature and Sparse representation. In this method, the salient parts of the PHOG feature, which is relevant to the facial expression changes, were extracted. In addition, a facial expression dictionary was built up by online learning method. Then, the facial expressions could be classified through sparse representation classification algorithm. The results prove that the proposed algorithm is valid, and perform high recognition rate and robustness against occlusion and noise.
7FD994B5	All sections of the society need to benefit from the strides in Information Technology, more so the differently enabled. This paper presents a Novel solution for a text read out OCR system adapted for the visually challenged. A paper of text from Malayalam magazines, newspapers, books or journals placed on a flatbed scanner would be recognized and read out as text. Selected text could also be printed with a Braille Embosser. The paper addresses all issues related to the integration of such a system. This novel system helps to bring reading rooms and libraries to the specially enabled populace. It finds application in all areas where documents need to be accessed. This system designed has an inherent advantage in the sense that it can also be of functional utility to the literate and semi-literate citizen, besides to visually affected persons. Such systems are not presently available for Indian languages. This system has been tested and evaluated on Malayalam scripts.
7F8C9C65	In this paper we introduce our method for enabling dynamic gesture recognition for hand gestures. Like a number of other research work focusing on gesture recognition we use a camera to track the motions and interpret these in terms of actual meaningful gestures; however we emphasise the tracking of fingers as well as the hand in order to cover a much wider range of gestures. The recognition is processed as part of three key stages, with a fourth in development. The first stage processes the visual information from the camera, and identifies the key regions and elements (such as the hand and fingers), this classified information is passed to a 2D to 3D module that transforms the 2D classified information into a full 3D space applying it to a calibrated hand model using inverse projection matrices and inverse kinematics. Simplifying this model into posture curvature information we apply this to a hidden Markov model (HMM). This model is used to identify and differentiate between different gestures, even ones using the same finger combinations. We briefly discuss our current development in the application of context awareness to this scenario, which is used in combination with the HMM in order to apply a different semantic to each gesture. This is especially useful due to the huge overlap in semantics specifically appropriated to hand gestures
7D36A809	How to build virtual agents that establish rapport with human? According to Tickle-Degnen and Rosenthal [4], the three essential components of rapport are mutual attentiveness, positivity and coordination. In our previous work, we designed an embodied virtual agent to establish rapport with a human speaker by providing rapid and contingent nonverbal feedback [13] [22]. How do we know that a human speaker is feeling a sense of rapport? In this paper, we focus on the positivity component of rapport by investigating the relationship of human speakers' facial expressions on the establishment of rapport. We used an automatic facial expression coding tool called CERT to analyze the human dyad interactions and human-virtual human interactions. Results show that recognizing positive facial displays alone may be insufficient and that recognized negative facial displays was more diagnostic in assessing the level of rapport between participants.
7B46D7DA	Sign language recognition (SLR) not only facilitates the communication between the deaf and hearing society, but also serves as a good basis for the development of gesture-based human-computer interaction (HCI). In this paper, the portable input devices based on accelerometers and surface electromyography (EMG) sensors worn on the forearm are presented, and an effective fusion strategy for combination of multi-sensor and multi-channel information is proposed to automatically recognize sign language at the subword classification level. Experimental results on the recognition of 121 frequently used Chinese sign language subwords demonstrate the feasibility of developing SLR system based on the presented portable input devices and that our proposed information fusion method is effective for automatic SLR. Our study will promote the realization of practical sign language recognizer and multimodal human-computer interfaces.
80AFFD0F	Humans detect and interpret faces and facial expressions in a scene with little or no effort. Still, development of an automated system that accomplishes this task is rather difficult. There are several related problems: detection of an image segment as a face, extraction of the facial expression information, and classification of the expression (e.g., in emotion categories). A system that performs these operations accurately and in real time would form a big step in achieving a human-like interaction between man and machine. The paper surveys the past work in solving these problems. The capability of the human visual system with respect to these problems is discussed, too. It is meant to serve as an ultimate goal and a guide for determining recommendations for development of an automatic facial expression analyzer.
7D8BBA9A	Human face detection plays an important role in applications such as video surveillance, human computer interface, face recognition, and face image database management. We propose a face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds. Based on a novel lighting compensation technique and a nonlinear color transformation, our method detects skin regions over the entire image and then generates face candidates based on the spatial arrangement of these skin patches. The algorithm constructs eye, mouth, and boundary maps for verifying each face candidate. Experimental results demonstrate successful face detection over a wide range of facial variations in color, position, scale, orientation, 3D pose, and expression in images from several photo collections (both indoors and outdoors).
7D3A313E	In this paper, we propose a novel method for facial expression recognition. The facial expression is extracted from human faces by an expression classifier that is learned from boosting Haar feature based look-up-table type weak classifiers. The expression recognition system consists of three modules, face detection, facial feature landmark extraction and facial expression recognition. The implemented system can automatically recognize seven expressions in real time that include anger, disgust, fear, happiness, neutral, sadness and surprise. Experimental results are reported to show its potential applications in human computer interaction.
752B1D60	This paper presents the state-of-the art dynamic sign language recognition (DSLR) system for smart home interactive applications. Our novel DSLR system comprises two main subsystems: an image processing (IP) module and a stochastic linear formal grammar (SLFG) module. Our IP module enables us to recognize the individual words of the sign language (i.e., a single gesture). In this module, we used the bag-of-features (BOFs) and a local part model approach for bare hand dynamic gesture recognition from a video. We used dense sampling to extract local 3-D multiscale whole-part features. We adopted 3-D histograms of a gradient orientation descriptor to represent features. The k-means++ method was applied to cluster the visual words. Dynamic hand gesture classification was conducted using the BOFs and nonlinear support vector machine methods. We used a multiscale local part model to preserve temporal context. The SLFG module analyzes the sentences of the sign language (i.e., sequences of gestures) and determines whether or not they are syntactically valid. Therefore, the DSLR system is not only able to rule out ungrammatical sentences, but it can also make predictions about missing gestures, which, in turn, increases the accuracy of our recognition task. Our IP module alone seals the accuracy of 97% and outperforms any existing bare hand dynamic gesture recognition system. However, by exploiting syntactic pattern recognition, the SLFG module raises this accuracy by 1.65%. This makes the aggregate performance of the DSLR system as accurate as 98.65%.
81355EE7	Facial expression recognition (FER) is an important task in human- computer interaction systems to include emotion processing. In this work we present the recognition of facial expressions in OFDM systems. The original image is converted to binary code then it is transmitted via OFDM over a multi-path fading channel. The features are then extracted from the reconstructed image at the receiver based on the log Gabor filter. Finally, the features are classified using the naive Bayesian classifier. Simulation results on the Cohn-Kanade database show that the system obtains good results in the presence of noise in the received images
7EC18FB1	This paper presents a system for the development of new human-machine interfaces focused on static gestures recognition of human hands. The proposal is to aid access to certain objects to the occupant of an intelligent wheelchair in order to facilitate their daily life. The proposed methodology relies on the use of simple computational processes and low-cost hardware. Its development involves a comprehensive approach to the problems of computer vision, based on the steps of the video image capture, image segmentation, feature extraction, pattern recognition and classification. The importance of this work relates to the need to build new models of interaction that allow, in a natural and intuitive way, to simplify the daily life of a disable person.
7B4BEE05	As one of the key techniques for futuristic human computer interaction, facial expression recognition has received much attention in recent years. A method of facial expression recognition based on selective feature extraction is presented in this paper. In this method we classify expressions roughly into three kinds according to the deformation of mouth firstly. Then we select some expression feature areas which contribute much to each kind expression according to the rough classification results and extract features for them. Lastly we classify expressions finely using method based on rule. Experiments show that facial expression recognition based on selective feature extraction can get high recognition rate and has strong robustness
7F1D3472	In daily interactions, humans convey their emotions through facial expression and other means. There are several facial expressions that reflect distinctive psychological activities such as happiness, surprise or anger. Accurate recognition of these activities via facial image analysis will play a vital role in natural human-computer interfaces, robotics and mimetic games. This paper focuses on the extraction and selection of salient features for facial expression recognition. We introduce a cascade of fixed filters and trainable non-linear 2-D filters, which are based on the biological mechanism of shunting inhibition. The fixed filters are used to extract primitive features, whereas the adaptive filters are trained to extract more complex facial features for classification by SVMs. This paper investigates a feature selection approach that is based on the reduction of mutual information among the selected features. The proposed approach is evaluated on the JAFFE database with seven types of facial expressions: anger, disgust, fear, happiness, neutral, sadness and surprise. Using only two-thirds of the total features, our approach achieves a classification rate (CR) of 96.7%, which is higher than the CR obtained using all features. Our system also outperforms several existing methods, evaluated on the same JAFFE database.
7BFCA253	Human-Computer Interaction (HCI) exists ubiquitously in our daily lives. It is usually achieved by using a physical controller such as a mouse, keyboard or touch screen. It hinders Natural User Interface (NUI) as there is a strong barrier between the user and computer. There are various hand tracking systems available on the market, but they are complex and expensive. In this paper, we present the design and development of a robust marker-less hand/finger tracking and gesture recognition system using low-cost hardware. We propose a simple but efficient method that allows robust and fast hand tracking despite complex background and motion blur. Our system is able to translate the detected hands or gestures into different functional inputs and interfaces with other applications via several methods. It enables intuitive HCI and interactive motion gaming. We also developed sample applications that can utilize the inputs from the hand tracking system. Our results show that an intuitive HCI and motion gaming system can be achieved with minimum hardware requirements.
814C78DF	In this paper, a real-time technique for segmenting a hand, determining hand's locations is described. The algorithm consists of two steps; the first is to location the hand area in noise condition and segment the part of hand. Then seek for angles of the fingers and location of fingertip with a novel factor determination in special OPS (Optical Parameter Sequence). It keeps the advantages of optical flow and overcome the complex computation cost and recognizes the fingerspell correctly. According to the traditional technique, the algorithm has a good performance.
7DA855CE	Recent work in muscle sensing has demonstrated the poten-tial of human-computer interfaces based on finger gestures sensed from electrodes on the upper forearm. While this approach holds much potential, previous work has given little attention to sensing finger gestures in the context of three important real-world requirements: sensing hardware suitable for mobile and off-desktop environments, elec-trodes that can be put on quickly without adhesives or gel, and gesture recognition techniques that require no new training or calibration after re-donning a muscle-sensing armband. In this note, we describe our approach to over-coming these challenges, and we demonstrate average clas-sification accuracies as high as 86% for pinching with one of three fingers in a two-session, eight-person experiment.
5D39EE53	This paper presents an approach for simultaneous tracking and recognition of hierarchical object representations in terms of multiscale image features. A scale-invariant dissimilarity measure is proposed for comparing scale-space features at different positions and scales. Based on this measure, the likelihood of hierarchical, parameterized models can be evaluated in such a way that maximization of the measure over different models and their parameters allows for both model selection and parameter estimation. Then, within the framework of particle filtering, we consider the area of hand gesture analysis, and present a method for simultaneous tracking and recognition of hand models under variations in the position, orientation, size and posture of the hand. In this way, qualitative hand states and quantitative hand motions can be captured, and be used for controlling different types of computerised equipment.
7F1416FB	This paper proposes a new approach to solve the problem of real-time vision-based hand gesture recognition with the combination of statistical and syntactic analyses. The fundamental idea is to divide the recognition problem into two levels according to the hierarchical property of hand gestures. The lower level of the approach implements the posture detection with a statistical method based on Haar-like features and the AdaBoost learning algorithm. With this method, a group of hand postures can be detected in real time with high recognition accuracy. The higher level of the approach implements the hand gesture recognition using the syntactic analysis based on a stochastic context-free grammar. The postures that are detected by the lower level are converted into a sequence of terminal strings according to the grammar. Based on the probability that is associated with each production rule, given an input string, the corresponding gesture can be identified by looking for the production rule that has the highest probability of generating the input string.
7B0AD55A	
7D56B5C0	This paper presents a new classification algorithm for real-time inference of affect from nonverbal features of speech and applies it to assessing public speaking skills. The classifier identifies simultaneously occurring affective states by recognizing correlations between emotions and over 6,000 functional-feature combinations. Pairwise classifiers are constructed for nine classes from the Mind Reading emotion corpus, yielding an average cross-validation accuracy of 89 percent for the pairwise machines and 86 percent for the fused machine. The paper also shows a novel application of the classifier for assessing public speaking skills, achieving an average cross-validation accuracy of 81 percent and a leave-one-speaker-out classification accuracy of 61 percent. Optimizing support vector machine coefficients using grid parameter search is shown to improve the accuracy by up to 25 percent. The emotion classifier outperforms previous research on the same emotion corpus and is successfully applied to analyze public speaking skills.
7705FB2F	Vision is a natural tool for human-computer interaction, since it provides visual feedback to the user and mimics some human behaviors. It requires however the fast and robust computation of motion primitives, which remains a difficult problem. In this work, we propose to apply some recent mathematical results about convex optimization to the TV-L1 optical flow problem. At the cost of a small smoothing of the Total Variation (TV), the convergence speed of the numerical scheme is improved, leading to earlier termination. Furthermore, we successfully implement our algorithm on GPU for realtime performance using the OpenCL framework. We demonstrate the potential of our optical flow by using it as primary sensor in a remotely controlled image browsing software.
7ED6DC30	This paper presents algorithms and a prototype system for hand tracking and hand posture recognition. Hand postures are represented in terms of hierarchies of multi-scale colour image features at different scales, with qualitative inter-relations in terms of scale, position and orientation. In each image, detection of multi-scale colour features is performed. Hand states are then simultaneously detected and tracked using particle filtering, with an extension of layered sampling referred to as hierarchical layered sampling. Experiments are presented showing that the performance of the system is substantially improved by performing feature detection in colour space and including a prior with respect to skin colour. These components have been integrated into a real-time prototype system, applied to a test problem of controlling consumer electronics using hand gestures. In a simplified demo scenario, this system has been successfully tested by participants at two fairs during 2001.
787FC895	Hand gestures are a powerful way for human communication, with lots of potential applications in the area of human computer interaction. Vision-based hand gesture recognition techniques have many advantages compared with traditional devices, giving users a simpler and more natural way to communicate with electronic devices. This work proposes a generic system architecture based in computer vision and machine learning, able to be used with any interface for real-time human-machine interaction. Its novelty is the integration of different tools for gesture spotting and the proposed solution is mainly composed of three modules: a pre-processing and hand segmentation module, a static gesture interface module and a dynamic gesture interface module. The experiments showed that the core of vision-based interaction systems could be the same for all applications and thus facilitate the implementation. For hand posture recognition, a SVM (Support Vector Machine) model was trained with a centroid distance dataset composed of 2170 records, able to achieve a final accuracy of 99.4 %. For dynamic gestures, an HMM (Hidden Markov Model) model was trained for each one of the defined gestures that the system should recognize with a final average accuracy of 93.7 %. The datasets were built from four different users with a total of 25 gestures per user, totalling 1100 records for model construction. The proposed solution has the advantage of being generic enough with the trained models able to work in real-time, allowing its application in a wide range of human-machine applications. To validate the proposed framework two applications were implemented. The first one is a real-time system able to interpret the Portuguese Sign Language. The second one is an online system able to help a robotic soccer game referee judge a game in real-time.
3D3070E9	Facial Expression Recognition is rapidly becoming area of interest in computer science and human computer interaction because the most expressive way of displaying the emotions by human is through the facial expressions. In this paper, recognition of facial expression is studied with the help of several properties associated with the face itself. As facial expression changes, the curvatures on the face and properties of the objects such as, eyebrows, nose, lips and mouth area changes. We have used Affine Moment Invariants to compute these changes and computed results (changes) are recorded as feature vectors. We have introduced a method for facial expression recognition using Affine Moment Invariants as features. We have used Artificial Neural Network as a classification tool and we developed associated scheme. The Generalized Feed-forward Neural Network recognizes six universal expressions i.e. anger, disgust, fear, happy, sad, and surprise as well as seventh one neutral. The Neural Network trained and tested by using Scaled Conjugate Gradient Backpropogation Algorithm. As a result we got 93.8% classification rate.
5FD34F9C	In recent years, facial expression recognition has become an active research area that finds potential applications in the fields such as images processing and pattern recognition, and it plays a very important role in the applications of human-computer interfaces and human emotion analysis. This paper proposes an algorithm called BoostingTree, which is based on the conventional Adaboost and uses tree-structure to convert seven facial expressions to six binary problems, and also presents a novel method to compute projection matrix based on Principal Component Analysis (PCA). In this novel method, a block-merger combination is designed to solve the “data disaster” problem due to the combination of eigenvectors. In the experiment, we construct the weak classifiers set based on this novel method. The weak classifiers selected from the above set by Adaboost are combined into strong classifier to be as node classifier of one level of the tree structure. N-level tree structure built by BoostingTree can effectively solve multiclass problem such as facial expression recognition
75FCFB03	This work proposes a new approach for facial expression recognition in color image sequences, based on integrated evaluation of geometric and dynamic features. For this purpose a series of methods is introduced that on the one hand achieve high recognition rates for expressive facial behavior and on the other hand address a couple of common problems in this area of research. In particular we apply physiologically motivated image regions for the detection of dynamic features by using an optical flow method. In this way dynamic features capture the variations caused by facial expression changes. Opposed, geometric features do not contain temporal information but describe spatial feature parameters. These correspond to 3-D based Euclidean distances and angles. Particularly, the hypothesis of this work is that through integrated evaluation of geometric and dynamic features, improved recognition rates can be achieved. Based on comprehensive experimental investigations we show the advantage of the suggested approach.
7938E9FF	Over the last decade, automatic facial expression analysis has become an active research area that finds potential applications in areas such as more engaging human–computer interfaces, talking heads, image retrieval and human emotion analysis. Facial expressions reflect not only emotions, but other mental activities, social interaction and physiological signals. In this survey, we introduce the most prominent automatic facial expression analysis methods and systems presented in the literature. Facial motion and deformation extraction approaches as well as classification methods are discussed with respect to issues such as face normalization, facial expression dynamics and facial expression intensity, but also with regard to their robustness towards environmental changes.
7740A0D5	Facial expression recognition utilizes collection of information from characteristic actions to analyze emotions and mental states of a person. It has emerged as the pivotal research topics in areas such as human computer interaction, sentimental analysis and synthetic face animation over the last years. This paper proposes an approach for facial expression by discovering associations between visual feature and Local Binary Pattern (LBP). Unlike many previous studies, the proposed approach automatically tracks the facial area and segments face into meaningful areas based on description of Local Binary Pattern. And then it accumulates the probabilities throughout the frames from video data to capture the temporal characteristics of facial expressions by analyzing facial expressions. Through the proposed approach, the temporal variation of facial expression can be quantified in individual areas. Thus, the recognition process of facial expression tends to be more comprehensible without sacrificing results of recognition. The empirical evaluation results of the approach are realized using video data which is collected from 10 volunteers. The results demonstrated that the proposed approach can effectively segment face into specific area and recognize facial expression.
7E57644A	This paper presents a novel and real-time system for interaction with an application or video game via hand gestures. Our system includes detecting and tracking bare hand in cluttered background using skin detection and hand posture contour comparison algorithm after face subtraction, recognizing hand gestures via bag-of-features and multiclass support vector machine (SVM) and building a grammar that generates gesture commands to control an application. In the training stage, after extracting the keypoints for every training image using the scale invariance feature transform (SIFT), a vector quantization technique will map keypoints from every training image into a unified dimensional histogram vector (bag-of-words) after K-means clustering. This histogram is treated as an input vector for a multiclass SVM to build the training classifier. In the testing stage, for every frame captured from a webcam, the hand is detected using our algorithm, then, the keypoints are extracted for every small image that contains the detected hand gesture only and fed into the cluster model to map them into a bag-of-words vector, which is finally fed into the multiclass SVM training classifier to recognize the hand gesture.
7E39E4B4	Recognition of human intention is an important issue in human-robot interaction research and allows a robot to respond adequately according to human's wish. In this paper, we discuss how robots can infer human intention by learning affordance, a concept used to represent the relation between an agent and its environment. Learning of the robot, to understand human and its interaction with environment, is achieved within the framework of action-perception cycle. The action-perception cycle explains how an intelligent agent learns and enhances its ability continuously by interacting with its surrounding. The proposed intention recognition and recommendation system includes several key functions such as joint attention, object recognition, affordance model, motion understanding module and so on. The experimental results show high successful recognition performance and the plausibility of the proposed system.
7809F49C	Affective reasoning holds significant potential for intelligent tutoring systems. Incorporating affective reasoning into pedagogical decision-making capabilities could enable learning environments to create customised experiences that are dynamically tailored to individual students' ever-changing levels of engagement, interest, motivation and self-efficacy. Because physiological responses are directly triggered by changes in affect, biofeedback data such as heart rate and galvanic skin response can be used to infer affective changes in conjunction with the situational context. This article explores an approach to inducing affect models for a learning environment. The inductive approach is examined for the task of modelling students' self-efficacy and empathy for companion agents. Together, these studies on affect in a narrative learning environment suggest that it is possible to build models of affective constructs from observations of the situational context and students' physiological response.
7823847F	In this paper a facial feature point tracker that is motivated by applications such as human–computer interfaces and facial expression analysis systems is proposed. The proposed tracker is based on a graphical model framework. The facial features are tracked through video streams by incorporating statistical relations in time as well as spatial relations between feature points. By exploiting the spatial relationships between feature points, the proposed method provides robustness in real-world conditions such as arbitrary head movements and occlusions. A Gabor feature-based occlusion detector is developed and used to handle occlusions. The performance of the proposed tracker has been evaluated on real video data under various conditions including occluded facial gestures and head movements. It is also compared to two popular methods, one based on Kalman filtering exploiting temporal relations, and the other based on active appearance models (AAM). Improvements provided by the proposed approach are demonstrated through both visual displays and quantitative analysis.
81F534C6	In this paper, a real-time experimental of Hand Gesture sEMG signal using artificial neural networks for Wheel Vehicle Control is proposed. The raw SEMG signals been captured from SEMG amplifier, up to 8 channels of NI-DAQ card responses data will be combined and a fine tuning step by using pattern classification. The database then been build and use for real-time experimental control classification. Captured data will send through serial port and Wheel Machine will receive and move accordingly. The detail of the experiment and simulation conducted described here to verify the differentiation and effectiveness of combined channels sEMG pattern classification of hand gesture for real-time control.
5A2C693D	We present a new scheme for the recognition of facial expressions from a set of facial features using fuzzy enhanced Case-based Reasoning (CBR). Facial expression recognition has become the cornerstone of human-computer interaction (HCI) systems. It has wide range applications information systems and e-commerce such as intelligent desktops and intelligent web agents. Our system is the integration of two fundamental paradigms of AI, i.e., Fuzzy logic and Case Based Reasoning. Fuzzy logic is embedded into our CBR system for improved case retrieval. A dual, fuzzy similarity determination module is the core of our system and works within the Case-based Reasoning system. The advantages from Fuzzy Rule Based Systems (FRBS) like linguistic modeling and fault tolerance when combined with the reasoning capability of CBR greatly improved the sophistication and utility of our system. A reinforcing combination of two approaches might lead to exciting applications that are yet to be envisaged. 
747B8A26	The goal of sensor-bridging is to map sensory information to a modality that humans are able to process effectively. As our vision has the highest bandwidth to our brain from all our senses, data visualization has an important role in presenting such information. We review research in data visualization and human visual perception, show several examples for images that are easy to process, and provide some recommendations for effective visual sensor-bridging. At the end, we pose some open research questions for the area of cognitive infocommunications and human-computer interaction.
5EDFDBDD	In this paper we present a fast and robust nose detection and tracking application which runs on a consumer-grade computer with video input from an inexpensive Universal Serial Bus camera. Nose detection is based on the AdaBoost algorithm with Haar-like features. A detailed study was developed to select the positive and negative training samples and the parameters of the detector. Pyramidal Lucas-Kanade optical flow tracking algorithm is applied to the nostrils from a previous nose detection in a frame of a video sequence. Tracking takes 2 ms and is robust to different face positions, backgrounds and illumination. The nose detection and tracking application can be used alone or integrated in a hand-free vision-based Human-Computer Interface.
76C912FC	This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7% in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.
797B0B76	Face and facial feature detection plays an important role in various applications such as human computer interaction, video surveillance, face tracking, and face recognition. Efficient face and facial feature detection algorithms are required for applying to those tasks. This paper presents the algorithms for all types of face images in the presence of several image conditions. There are two main stages. In the first stage, the faces are detected from an original image by using Canny edge detection and our proposed average face templates. Second, a proposed neural visual model (NVM) is used to recognize all possibilities of facial feature positions. Input parameters are obtained from the positions of facial features and the face characteristics that are low sensitive to intensity change. Finally, to improve the results, image dilation is applied for removing some irrelevant regions. Additionally, the algorithms can be extended to rotational invariance problem by using Radon transformation to extract the main angle of the face. With more than 1000 images, the algorithms are successfully tested with various types of faces affected by intensity, occlusion, structural components, facial expression, illumination, noise, and orientation.
5CFA90DD	Acknowledgments are relatively rare in human computer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowledgments, about half of our subjects used acknowledgments at least once and nearly 30% used them extensively during the interaction.
7FDA4BDA	Automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human-computer interaction (HCI). We developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using hidden Markov models (HMMs). Our approach to facial expression recognition is based an the Facial Action Coding System (FACS), which separates expressions into upper and lower face action. We use three approaches to extract facial expression information: (1) facial feature point tracking; (2) dense flow tracking with principal component analysis (PCA); and (3) high gradient component detection (i.e. furrow detection). The recognition results of the upper face expressions using feature point tracking, dense flow tracking, and high gradient component detection are 85%, 93% and 85%, respectively.
7F4C6BB3	Navigating virtual environments usually requires a wired interface, game console, or keyboard. The advent of perceptual interface techniques allows a new option: the passive and untethered sensing of users' pose and gesture to allow them maneuver through and manipulate virtual worlds. We describe new algorithms for interacting with 3-D environments using real-time articulated body tracking with standard cameras and personal computers. Our method is based on rigid stereo-motion estimation algorithms and uses a linear technique for enforcing articulation constraints. With our tracking system users can navigate virtual environments using 3-D gesture and body poses. We analyze the space of possible perceptual interface abstractions for full-body navigation, and present a prototype system based on these results. We finally describe an initial evaluation of our prototype system with users guiding avatars through a series of 3-D virtual game worlds.
7DA3BA41	In this paper an interactive and realistic facial animation oriented to human-computer interaction and social robotics is proposed. An initial model is obtained from a real face using a range scanner, and is then animated using a hierarchical skeleton oriented procedure. The rig structure is close to that of real facial muscular anatomy and the behavior follows the Facial Action Coding System. The resulting animated face is suitable for both real-time interfacing in human-machine interaction such as a social robot, and for helping in design and parametrization of android heads.
7A726687	Human Computer Interaction techniques have become a bottleneck in the effective utilization of the available information flow. The development of user interfaces influences the changes in the Human-Computer Interaction (HCI). Human hand gestures have been a mode of non verbal interaction widely used. Naturalistic and intuitiveness of the hand gesture has been a great motivating factor for the researchers in the area of HCI to put their efforts to research and develop the more promising means of interaction between human and computers. This paper designs a system for gestural interaction between a user and a computer in dynamic environment. The gesture recognition system uses image processing techniques for detection, segmentation, tracking and recognition of hand gestures for converting it to a meaningful command. The interface being proposed here can be substantially applied towards different applications like image browser, games etc.
8087B8EA	Facial expression recognition is of prime importance in human-computer interaction systems (HCI). We present a novel scheme for facial expression recognition from facial features using Mamdani-type fuzzy system. We present a novel algorithm for facial region extraction from static image. These extracted facial regions are used for facial feature extraction. Facial features are fed to a Mamdani-type fuzzy rule based system for facial expression recognition. Another distinct feature of our system is the membership function model of expression output which is based on different psychological studies and surveys. The validation of the model is further supported by the high expression recognition percentage.