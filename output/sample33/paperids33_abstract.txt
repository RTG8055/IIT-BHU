7F29961A	Decision trees and diagrams (also known as sequential evaluation procedures) have widespread applications in databases, decision table programming, concrete complexity theory, switching theory, pattern recognmon, and taxonomy--in short, wherever discrete functions must be evaluated sequentially. In this tutorial survey a common framework of defimtmns and notation is established, the contributions from the main fields of apphcatmn are reviewed, recent results and extensions are presented, and areas of ongoing and future research are discussed.
7EE9E0F7	We address the issue of compiling ML pattern matching to compact and efficient decisions trees. Traditionally, compilation to decision trees is optimized by (1) implementing decision trees as dags with maximal sharing; (2) guiding a simple compiler with heuristics. We first design new heuristics that are inspired by necessity, a concept from lazy pattern matching that we rephrase in terms of decision tree semantics. Thereby, we simplify previous semantic frameworks and demonstrate a straightforward connection between necessity and decision tree runtime efficiency. We complete our study by experiments, showing that optimizing compilation to decision trees is competitive with the optimizing match compiler of Le Fessant and Maranget (2001).
5BA42D2D	 We present an optimal hyperplane searching method for decision tables using Genetic Algorithms. This method can be used to construct a decision tree for a given decision table. We also present some properties of the set of hyperplanes determined by our methods and evaluate an upper bound on the depth of the constructed decision tree. 
753DB472	The construction of sequential testing procedures from functions of discrete arguments is a common problem in switching theory, software engineering, pattern recognition, and management. The concept of the activity of an argument is introduced, and a theorem is proved which relates it to the expected testing cost of the most general type of decision trees. This result is then extended to trees constructed from relations on finite sets and to decision procedures with cycles. These results are used, in turn, as the basis for a fast heuristic selection rule for constructing testing procedures. Finally, some bounds on the performance of the selection rule are developed.
7B468C32	Nowadays the applications in multimedia domain require that the Speech/Music classifier has many other merits in addition to the accuracy, such as short-time delay and low complexity. Here, we endeavor to form a Speech/Music classifier by using different data mining methods. The main contributions of this paper are to obtain a system by analyzing the inherent validity of diverse features extracted from the audio, building a hierarchical structure of oblique decision trees (HODT) to maintain optimal performances, and applying a novel context-based state transform (ST) strategy to refine the classification results. The proposed algorithm is evaluated by a set of 5–11 min 702 audio files, which are made from 54 speech or music files according to different Signal-to-Noise Ratio (SNR) levels and diverse noise types. The experiment results show that our proposed classifier outperforms AMR-WB+ by achieving 97.9% and 95.9% in classification rate at the 10 ms frame level in pure and high SNR (> = 20 dB) environment, respectively. The post-processing ST strategy further enhances the system performance, particularly at low SNR circumstances (10 dB), with 5.6% up in the accuracy rate. In addition, the complexity of the proposed system is lower than 1WMOPS which make it easily adaptable to many scenarios.
7CA4D089	The 2008 World Health Organization Classification of Tumors of the Haematopoietic and Lymphoid Tissues defines current standards of practice for the diagnosis and classification of malignant lymphomas and related entities. More than 50 different types of lymphomas are described. Faced with such a broad range of different lymphomas, some encountered only rarely, and a rapidly growing armamentarium of 80 or more pertinent immunohistochemical (IHC) "stains," the challenge to the pathologist is to use IHC in an efficient manner to arrive at an assured and timely diagnosis. This review uses deductive reasoning following a decision tree or dendrogram model, combining basic morphologic patterns and common IHC markers to classify node-based malignancies by the World Health Organization schema. The review is divided into 2 parts, the first addressing those lymphomas that produce a follicular or nodular pattern of lymph nodal involvement appeared in the previous issue of AIMM. The second part addresses diffuse proliferations in lymph nodes. Emphasis is given to the more common lymphomas and the more commonly available IHC "stains" for a pragmatic and practical approach that is both broadly feasible and cost-effective. By this method, an assured diagnosis may be reached in the majority of nodal lymphomas, at the same time developing a sufficiency of data to recognize those rare or atypical cases that require referral to a specialized center.
7DB968DA	This paper describes a novel and practical Japanese parser that uses decision trees. First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another. Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation. The constructed parsers are evaluated using the EDR Japanese annotated corpus. The single-tree method significantly outperforms the conventional Japanese stochastic methods. Moreover, the boosted version of the parser is shown to have great advantages; (1) a better parsing accuracy than its single-tree counterpart for any amount of training data and (2) no over-fitting to data for various iterations. The presented parser, the first non-English stochastic parser with practical performance, should tighten the coupling between natural language processing and machine learning.
0B89C91B	We consider the problem of optimally partitioning an n-dimensional lattice, L = L, X ... X LN, where Lj is a one-dimensional lattice with kj elements, by means of a binary tree into specified (labeled) subsets of L. Such lattices arise from problems in pattern classification, in nonlinear regression, in defining logical equations, and a number of related areas. When viewed as the partitioning of a vector space, each point in the lattice corresponds to a subregion of the space which is relatively homogeneous with respect to classification or range of a dependent variable. Optimality is defined in terms of a general cost function which includes the following: 1) min-max path length (i. e., minimize the maximum number of nodes traversed in making a decision); 2) minimum number of nodes in the tree; and 3) expected path length. It is shown that an optimal tree can be recursively constructed through the application of invariant imbedding (dynamic programming). An algorithm is detailed which embodies this recursive approach. The algorithm allows the assignment of a "don't care" label to elements of L.
7CBC26A4	All novel proteins must be assessed for their potential allergenicity before they are introduced into the food market. One method to achieve this is the 2001 FAO/WHO Decision Tree recommended for evaluation of proteins from genetically modified organisms (GMOs). It was the aim of this study to investigate the allergenicity of microbial transglutaminase (m-TG) from Streptoverticillium mobaraense. Amino acid sequence similarity to known allergens, pepsin resistance, and detection of protein binding to specific serum immunoglobulin E (IgE) (RAST) have been evaluated as recommended by the decision tree. Allergenicity in the source material was thought unlikely, since no IgE-mediated allergy to any bacteria has been reported. m-TG is fully degraded after 5 min of pepsin treatment. A database search showed that the enzyme has no homology with known allergens, down to a match of six contiguous amino acids, which meets the requirements of the decision tree. However, there is a match at the five contiguous amino acid level to the major codfish allergen Gad c1. The potential cross reactivity between m-TG and Gad c1 was investigated in RAST using sera from 25 documented cod-allergic patients and an extract of raw codfish. No binding between patient IgE and m-TG was observed. It can be concluded that no safety concerns with regard to the allergenic potential of m-TG were identified.
5C09EBCE	Predicting the quality of software modules prior to testing or system operations allows a focused software quality improvement endeavor. Decision trees are very attractive for classification problems, because of their comprehensibility and white box modeling features. However, optimizing the classification accuracy and the tree size is a difficult problem, and to our knowledge very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (GP) based decision tree modeling technique for calibrating software quality classification models. The proposed technique is based on multi-objective optimization using strongly typed GP. Two fitness functions are used to optimize the classification accuracy and tree size of the classification models calibrated for a real-world high-assurance software system. The performances of the classification models are compared with those obtained by standard GP. It is shown that the gp-based decision tree technique yielded better classification models.
8079A985	Learning classifiers has been studied extensively the last two decades. Recently, various approaches based on patterns (e.g.. association rules) that hold within labeled data have been considered. In this paper, we propose a novel associative classification algorithm that combines rules and a decision tree structure. In a so-called delta-PDT (delta-pattern decision tree), nodes are made of selected disjunctive delta- strong classification rules. Such rules are generated from collections of delta-free patterns that can be computed efficiently. These rules have a minimal body, they are non- redundant and they avoid classification conflicts under a sensible condition on delta. We show that they also capture the discriminative power of emerging patterns. Our approach is empirically evaluated by means of a comparison to state-of-the-art proposals
7CB41D25	This paper suggests a method to recognize the various defect patterns of a cold mill strip using a binary decision tree. In classifying complex patterns with high similarity like the defect patterns of a cold mill strip, the selection of an optimal feature set and an appropriate recognizer is important to achieve high recognition rate. In this paper the GA (genetic algorithm) and K-means algorithm were used to select a subset of the suitable features at each node in the binary decision tree. The feature subset with maximum fitness is chosen and the patterns are classified into two classes using a linear decision function. This process is repeated at each node until all the patterns are classified into individual classes. In this way, the classifier using the binary decision tree is constructed automatically. After constructing the binary decision tree, the final recognizer is accomplished by having a neural network learning sets of standard patterns at each node. In this paper, the classifier using the binary decision tree was applied to the recognition of defect patterns of a cold mill strip, and the experimental results are given to demonstrate the usefulness of the proposed scheme.
03089401	This paper presents a preliminary study on the application of data mining to the problem of student retention in a post-secondary institution. Specifically, we apply the classification technique of decision tree induction. We focus on both the accuracy of classification and the identification of specific factors that reveal students who are at risk of dropping out before the completion of their program. We present our approach to applying a decision tree to solve this problem, including some necessary modifications to the training and testing algorithms. Then, we present the results of a preliminary performance evaluation of the correctness, accuracy and running time of the strategy, and conclude with future directions of work. In this paper, we present our preliminary study into the application of classification to the problem of detecting factors affecting student retention at a post-secondary institution. Specifically, we use a classification strategy called decision tree induction. We provide an overview of the decision tree induction and our modifications to the existing algorithm in Section 2. We present and discuss the results of our preliminary experiments in Section 3. Finally, we conclude and provide research directions in Section 4 
80C07367	Classification is an important issue both in Machine Learning and Data Mining. Decision tree is one of the famous classification models. In the reality case, the dimension of data is high and the data size is huge. Building a decision in large data base cost much time in computation. It is a computationally expensive problem.GPU is a special design processor of graphic. The highly parallel features of graphic processing made today's GPU architecture. GPGPU means use GPU to solve non-graphic problems which need amounts of computation power. Since the high performance and capacity/price ratio, many researches use GPU to process lots computation. Compute Unified Device Architecture (CUDA) is a GPGPU solution provided by NVIDIA.This paper provides a new parallel decision tree algorithm base on CUDA. The algorithm parallel computes building phase of decision tree.
801CB0AF	While decision tree techniques have been widely used in classification applications, a shortcoming of many decision tree inducers is that they do not learn intermediate concepts, i.e., at each node, only one of the original features is involved in the branching decision. Combining other classification methods, which learn intermediate concepts, with decision tree inducers can produce more flexible decision boundaries that separate different classes, potentially improving classification accuracy. We propose a generic algorithm for cascade generalization of decision tree inducers with the maximum cascading depth as a parameter to constrain the degree of cascading. Cascading methods proposed in the past, i.e., loose coupling and tight coupling, are strictly special cases of this new algorithm. We have empirically evaluated the proposed algorithm using logistic regression and C4.5 as base inducers on 32 UCI data sets and found that neither loose coupling nor tight coupling is always the best cascading strategy and that the maximum cascading depth in the proposed algorithm can be tuned for better classification accuracy. We have also empirically compared the proposed algorithm and ensemble methods such as bagging and boosting and found that the proposed algorithm performs marginally better than bagging and boosting on the average.
78EDF66D	This paper proposes a method to improve ID5R, an incremental TDIDT algorithm. The new method evaluates the quality of attributes selected at the nodes of a decision tree and estimates a minimum number of steps for which these attributes are guaranteed such a selection. This results in reducing overheads during incremental learning. The method is supported by theoretical analysis and experimental results.
7DFA28C0	Decision trees that are limited to testing a single variable at a node are potentially much larger than trees that allow testing multiple variables at a node. This limitation reduces the ability to express concepts succinctly, which renders many classes of concepts difficult or impossible to express. This paper presents the PT2 algorithm, which searches for a multivariate split at each node. Because a univariate test is a special case of a multivariate test, the expressive power of such decision trees is strictly increased. The algorithm is incremental, handles ordered and unordered variables, and estimates missing values.
7B676FBF	Classification is a frequently encountered data mining problem. Decision tree techniques have been widely used to build classification models as such models closely resemble human reasoning and are easy to understand. Many real-world classification problems are cost-sensitive, meaning that different types of misclassification errors are not equally costly. Since different decision trees may excel under different cost settings, a set of non-dominated decision trees should be developed and presented to the decision maker for consideration, if the costs of different types of misclassification errors are not precisely determined. This paper proposes a multi-objective genetic programming approach to developing such alternative Pareto optimal decision trees. It also allows the decision maker to specify partial preferences on the conflicting objectives, such as false negative vs. false positive, sensitivity vs. specificity, and recall vs. precision, to further reduce the number of alternative solutions. A diabetes prediction problem and a credit card application approval problem are used to illustrate the application of the proposed approach.
7AED9476	Model trees are a particular case of decision trees employed to solve regression problems. They have the advantage of presenting an interpretable output, helping the end-user to get more confidence in the prediction and providing the basis for the end-user to have new insight about the data, confirming or rejecting hypotheses previously formed. Moreover, model trees present an acceptable level of predictive performance in comparison to most techniques used for solving regression problems. Since generating the optimal model tree is an NP-Complete problem, traditional model tree induction algorithms make use of a greedy top-down divide-and-conquer strategy, which may not converge to the global optimal solution. In this paper, we propose a novel algorithm based on the use of the evolutionary algorithms paradigm as an alternate heuristic to generate model trees in order to improve the convergence to globally near-optimal solutions. We call our new approach evolutionary model tree induction (E-Motion). We test its predictive performance using public UCI data sets, and we compare the results to traditional greedy regression/model trees induction algorithms, as well as to other evolutionary approaches. Results show that our method presents a good trade-off between predictive performance and model comprehensibility, which may be crucial in many machine learning applications.
7D9256B9	In this paper a new method is suggested for designing patterns in data-mining. These patterns are designed using probability rules in decision trees and are cared to be valid, novel, useful and understandable. By using the suggested patterns in data-mining, the system gets efficient information about the data stored in its data-bases and uses them in the best planning for special objectives.
03FE9B06	We reformulate the notion of quantum query complexity in terms of inequalities and equations for a set of positive matrices, which we view as a quantum analogue of a decision tree. Using the new formulation we show that: 1. every quantum query algorithm needs to use at most n quantum bits in addition to the query register. 2. For any function f there is an algorithm that runs in polynomial time in terms the truth table of f and (for {var_epsilon} > 0) computes the {var_epsilon}-error quantum decision tree complexity of f. 3. Using the dual of our system we can treat lower bound methods on a uniform platform, which paves the way to their future comparison. In particular we describe Ambainis's bound in our framework. 4. The output condition on quantum algorithms used by Ambainis and others is not sufficient for an algorithm to compute a function with {var_epsilon}-bounded error: we show the existence of algorithms whose final entanglement matrix satisfies the condition, but for which the value of f cannot be determined from a quantum measurement on the accessible part of the computer. 
76E9B0F8	This paper proposes that online HCI studies (such as web-surveys and remotely monitored usability tests) can benefit from statistical data analysis using modern statistical learning methods such as classification and regression trees (CARTs). Applying CARTs to the often large amount of data yielded by online studies can easily provide clarity concerning the most important effects underlying experimental data in situations where myriad possible factors are under consideration. The feedback provided by such an analysis can also provide valuable reflection on the experimental methodology. We discuss these matters with reference to a study of 1300 participants in a structured experiment concerned with head-interaction techniques for first-person-shooter games.
81620FC4	Decision tree (DT) induction is regarded as a representative of traditional approaches to classification rule mining which is an important technique for many data mining applications. Using a heuristic-based local search, DT induction appends attribute at a time to rules in the order of goodness. This method may eliminate some typical structures that several attributes collectively determine the class. Recently, there has been growing interest in the problem of discovering functional dependencies (FDs) from existing databases.Some efficient and scalable algorithms have been proposed. In this paper, we present a new method to build a DT classifier using approximate FDs. The new method is different from the traditional ways of building DTs in that it searches composite attributes for individual node of a DT which leads to substantially smaller and more understandable DTs without adversely affecting the accuracy gains. Experiments showed that the new method not only builds more accurate classifiers, but also does this with more compact structures.
5F64476A	Although Genetic Programming (GP) is a very general technique, it is also quite powerful. As a matter of fact, GP has often been shown to outperform more specialized techniques on a variety of tasks. In data mining, GP has successfully been applied to most major tasks; e.g. classification, regression and clustering. In this chapter, we introduce, describe and evaluate a straightforward novel algorithm for post-processing genetically evolved decision trees. The algorithm works by iteratively, one node at a time, search for possible modifications that will result in higher accuracy. More specifically, the algorithm, for each interior test, evaluates every possible split for the current attribute and chooses the best. With this design, the post-processing algorithm can only increase training accuracy, never decrease it. In the experiments, the suggested algorithm is applied to GP decision trees, either induced directly from datasets, or extracted from neural network ensembles. The experimentation, using 22 UCI datasets, shows that the suggested post-processing technique results in higher test set accuracies on a large majority of the datasets. As a matter of fact, the increase in test accuracy is statistically significant for one of the four evaluated setups, and substantial on two out of the other three
7776D503	Among the several tasks that evolutionary algorithms have successfully employed, the induction of classification rules and decision trees has been shown to be a relevant approach for several application domains. Decision tree induction algorithms represent one of the most popular techniques for dealing with classification problems. However, conventionally used decision trees induction algorithms present limitations due to the strategy they usually implement: recursive top-down data partitioning through a greedy split evaluation. The main problem with this strategy is quality loss during the partitioning process, which can lead to statistically insignificant rules. In this paper, we propose a new GA-based algorithm for decision tree induction. The proposed algorithm aims to prevent the greedy strategy and to avoid converging to local optima. For such, it is based on a lexicographic multi-objective approach. In order to evaluate the proposed algorithm, it is compared with a well-known and frequently used decision tree induction algorithm using different public datasets. According to the experimental results, the proposed algorithm is able to avoid the previously described problems, reporting accuracy gains. Even more important, the proposed algorithm induced models with a significantly reduction in the complexity considering tree sizes.
800F0CB7	A shortcoming of univariate decision tree learners is that they do not learn intermediate concepts and select only one of the input features in the branching decision at each intermediate tree node. It has been empirically demonstrated that cascading other classification methods, which learn intermediate concepts, with decision tree learners can alleviate such representational bias of decision trees and potentially improve classification performance. However, a more complex model that fits training data better may not necessarily perform better on unseen data, commonly referred to as the overfitting problem. To find the most appropriate degree of such cascade generalization, a decision forest (i.e., a set of decision trees with other classification models cascaded to different degrees) needs to be generated, from which the best decision tree can then be identified. In this paper, the authors propose an efficient algorithm for generating such decision forests. The algorithm uses an extended decision tree data structure and constructs any node that is common to multiple decision trees only once. The authors have empirically evaluated the algorithm using 32 data sets for classification problems from the University of California, Irvine (UCI) machine learning repository and report on results demonstrating the efficiency of the algorithm in this paper.
77FF541F	Recommender systems perform much better on users for which they have more information. This gives rise to a problem of satisfying users new to a system. The problem is even more acute considering that some of these hard to profile new users judge the unfamiliar system by its ability to immediately provide them with satisfying recommendations, and may quickly abandon the system when disappointed. Rapid profiling of new users by a recommender system is often achieved through a bootstrapping process - a kind of an initial interview - that elicits users to provide their opinions on certain carefully chosen items or categories. The elicitation process becomes particularly effective when adapted to users' responses, making best use of users' time by dynamically modifying the questions to improve the evolving profile. In particular, we advocate a specialized version of decision trees as the most appropriate tool for this task. We detail an efficient tree learning algorithm, specifically tailored to the unique properties of the problem. Several extensions to the tree construction are also introduced, which enhance the efficiency and utility of the method. We implemented our methods within a movie recommendation service. The experimental study delivered encouraging results, with the tree-based bootstrapping process significantly outperforming previous approaches.
772EAA54	Software effort prediction has been a challenge for researchers throughout the years. Several approaches for producing predictive models from collected data have been proposed, although none has become standard given the specificities of different software projects. The most commonly employed strategy for estimating software effort, the multivariate linear regression technique has numerous shortcomings though, which motivated the exploration of many machine learning techniques. Among the researched strategies, decision trees and evolutionary algorithms have been increasingly employed for software effort prediction, though independently. In this paper, we propose employing an evolutionary algorithm to generate a decision tree tailored to a software effort data set provided by a large worldwide IT company. Our findings show that evolutionarily-induced decision trees statistically outperform greedily-induced ones, as well as traditional logistic regression. Moreover, an evolutionary algorithm with a bias towards comprehensibility can generate trees which are easier to be interpreted by the project stakeholders, and that is crucial in order to improve the stakeholder's confidence in the final prediction.
6A823642	Let R^d -> A be a query problem over R^d for which there exists a data structure S that can compute P(q) in O(log n) time for any query point q in R^d. Let D be a probability measure over R^d representing a distribution of queries. We describe a data structure called the odds-on tree, of size O(n^\epsilon) that can be used as a filter that quickly computes P(q) for some query values q in R^d and relies on S for the remaining queries. With an odds-on tree, the expected query time for a point drawn according to D is O(H*+1), where H* is a lower-bound on the expected cost of any linear decision tree that solves P.Odds-on trees have a number of applications, including distribution-sensitive data structures for point location in 2-d, point-in-polytope testing in d dimensions, ray shooting in simple polygons, ray shooting in polytopes, nearest-neighbour queries in R^d, point-location in arrangements of hyperplanes in R^d, and many other geometric searching problems that can be solved in the linear-decision tree model. A standard lifting technique extends these results to algebraic decision trees of constant degree. A slightly different version of odds-on trees yields similar results for orthogonal searching problems that can be solved in the comparison tree model. 
5C799391	Rapid development of information technologies, in particular, progress in methods of collection, storage and processing of data has allowed to collect huge data arrays with the purpose of their analysis in many organizations. Opportunities of experts are not enough because amount of these data are too much. This generates demand for methods of automatic data analysis number of which annually increase. Their application in biomechanical studies for the purpose of determining of stability of fixation of a fracture based on a physiological condition of patients is not exception. Most of algorithms for building of decision trees are time-consuming and poorly parallelized. Matrix method of building of nonbinary trees is presented in this article. Its advantages are a large reserve of an internal parallelism and lack of an inherent disadvantage of existing algorithms for building of decision trees - dependence on a choice of a variable of an initial split. Result of the method will be a complete set of rules on which can build a few trees with selection of the most significant of them in the future.
7A269D68	Decision trees are widely disseminated as an effective solution for classification tasks. Decision tree induction algorithms have some limitations though, due to the typical strategy they implement:recursive top-down partitioning through a greedy split evaluation. This strategy is limiting in the sense that there is quality loss while the partitioning process occurs, creating statistically insignificant rules. In order to prevent the greedy strategy and to avoid converging to local optima, we present a novel Genetic Algorithm for decision tree induction based on a lexicographic multi-objective approach, and we compare it with the most well-known algorithm for decision tree induction, J48, over distinct public datasets. The results show the feasibility of using this technique as a means to avoid the previously described problems, reporting not only a comparable accuracy but also, importantly, a significantly simpler classification model in the employed datasets. 
2DD3568F	This paper presents a method to recognize the various defect patterns of a cold mill strip using a binary decision tree constructed by genetic algorithm(GA). In this paper, GA was used to select a subset of the suitable features at each node in the binary decision tree. The feature subset with maximum fitness is chosen and the patterns are divided into two classes using a linear decision function. In this way, the classifier using the binary decision tree can be constructed automatically, and the final recognizer is implemented by a neural network trained by standard patterns at each node. Experimental results are given to demonstrate the usefulness of the proposed scheme.
80E8E893	Continuous attributes are hard to handle and require special treatment in decision tree induction algorithms. In this paper, we present a multisplitting algorithm, RCAT, for continuous attributes based on statistical information. When calculating information gain for a continuous attribute, it first splits the value range of the attribute into some initial intervals, computes the probability estimation of every class at each interval and finds the best threshold in the probability space, uses this threshold to separate the initial intervals into two sets, combines adjacent intervals in the same set, optimizes the boundary of every combined interval, and finally obtains the information gain of the continuous attribute. We also provide a pruning method to simplify the decision trees. Empirical results show that the RCAT algorithm can realise decision trees with much higher intelligibility than C4.5 while retaining their accuracy.
5F87D343	This paper reports on the results of some pronoun resolution experiments performed by applying a decision tree and a rule-based algorithm on an annotated Turkish text. The text has been compiled mostly from various popular child stories in a semi-automatic way. A knowledge-lean learning model has been devised using only nine most commonly employed features. An evaluation and comparison of the performances achieved with the two different algorithms is offered in terms of the recall, precision and f-measure metrics.
5C165CA1	A common problem in datamining is to find accurate classifiers for a dataset. For this purpose, genetic programming (GP) is applied to a set of benchmark classification problems. Using GP we are able to induce decision trees with a linear combination of variables in each function node. A new representation of decision trees using strong typing in GP is introduced. With this representation it is possible to let the GP classify into any number of classes. Results indicate that GP can be applied successfully to classification problems. Comparisons with current state-of-the-art algorithms in machine learning are presented and areas of future research are identified.
7920FF9A	Two variable selection criteria are proposed for converting a decision table to a near-optimum decision tree in the sense of minimal average cost of testing. A criterion, Q, is introduced that is based on the potential of a decision table. The previously known criterion 'loss' and Q are combined into a third criterion O. The performance of the three criteria is examined both theoretically and experimentally. Of most importance is that Q and O do not select a nonessential variable, while 'loss' may do so. It is also shown that the performance of the three criteria is not worse than that of any other known heuristics, at least for a particular example. The algorithm requires at most O(L/sup 2/2/sup L/) operations, where L is the arity of an input table.
762A9646	The National Patient Safety Agency has developed the Incident Decision Tree to help National Health Service (NHS) managers in the United Kingdom determine a fair and consistent course of action toward staff involved in patient safety incidents. Research shows that systems failures are the root cause of the majority of safety incidents. Despite this, when an adverse incident occurs, the most common response is to suspend the clinician(s) involved, pending investigation, in the belief that this serves the interests of patient safety. The Incident Decision Tree supports the aim of creating an open culture, where employees feel able to report patient safety incidents without undue fear of the consequences. The tool comprises an algorithm with accompanying guidelines and poses a series of structured questions to help managers decide whether suspension is essential or whether alternatives might be feasible. The approach does not seek to diminish health care professionals’ individual accountability, but encourages key decisionmakers to consider systems and organizational issues in the management of error. Initial findings show the Incident Decision Tree to be robust and adaptable for use in a range of health care environments and across all professional groups. It is hoped that applying the tool throughout the NHS will encourage open reporting of actual and prevented patient safety incidents and promote a uniformly fair and consistent approach toward the staff involved. 
7A725891	Decision forest is an ensemble classification method that combines multiple decision trees to in a manner that results in more accurate classifications. By combining multiple heterogeneous decision trees, decision forest is effective in mitigating noise that is often prevalent in real-world classification tasks. This paper presents a new genetic algorithm for constructing a decision forest. Each decision tree classifier is trained using a disjoint set of attributes. Moreover, we examine the effectiveness of using a Vapnik–Chervonenkis dimension bound for evaluating the fitness function of decision forest. The new algorithm was tested on various datasets. The obtained results have been compared to other methods, indicating the superiority of the proposed algorithm
72BE2358	Accurate text categorization is needed for efficient and effective text retrieval, search and filtering. Finding appropriate categories and manually assigning them to existing documents is very laborious. The paper shows a simple procedure for automatic extraction of atomic sense types (semantic categories) from documents based on rough sets. The atomic sense types are nodes of a sense type decision tree, which represents a taxonomy.
78C47905	The paper presents a system using Decision tree, Particle swarm optimization, and Support vector regression to design a Median-type filter with a 2-level impulse detector for image enhancement, called DPSM filter. First, it employs a varying 2-level hybrid impulse noise detector (IND) to determine whether a pixel is contaminated by impulse noises or not. The 2-level IND is constructed by a decision tree (DT) which is built via combining 10 impulse noise detectors. Also, the particle swarm optimization (PSO) algorithm is exploited to optimize the DT. Subsequently, the DPSM filter utilizes the median-type filter with the support vector regression (MTSVR) to restore the corrupted pixels. Experimental results demonstrate that the DPSM filter achieves high performance for detecting and restoring impulse noises, and also outperforms the existing well-known methods under consideration in the paper.
7D5B4500	Scene interpretation systems need to match (often ambiguous) lowlevel input data to concepts from a high-level ontology. In many domains, these decisions are uncertain and benefit greatly from proper context. In this paper, we demonstrate the use of decision trees for estimating class probabilities for regions described by feature vectors,and show how context can be introduced in order to improve the matching performance
7A3C8E5F	We propose the use of Vapnik's vicinal risk minimization (VRM) for training decision trees to approximately maximize decision margins. We implement VRM by propagating uncertainties in the input attributes into the labeling decisions. In this way, we perform a global regularization over the decision tree structure. During a training phase, a decision tree is constructed to minimize the total probability of misclassifying the labeled training examples, a process which approximately maximizes the margins of the resulting classifier. We perform the necessary minimization using an appropriate meta-heuristic (genetic programming) and present results over a range of synthetic and benchmark real datasets. We demonstrate the statistical superiority of VRM training over conventional empirical risk minimization (ERM) and the well-known C4.5 algorithm, for a range of synthetic and real datasets. We also conclude that there is no statistical difference between trees trained by ERM and using C4.5. Training with VRM is shown to be more stable and repeatable than by ERM.
707DCFB4	In cardiology, determining whether an electrocardiogram (ECG) is normal or not is sometimes referred to as ECG classification. ECG is the most frequently-used means of cardiac diagnosis. It is the cheapest and the most widely-available; it is also crucial for detecting rhythmic problems. In this paper we derive fuzzy rules for ECG classification from ID3-induced decision trees. The system of fuzzy rules is designed based on 106 ECGs, and it is evaluated using a validation set of 48 ECGs carefully selected by cardiologists. Using the same 106 ECGs for design and the same 48 ECGs for validation, an ID3-generated decision tree yields 73% correct classifications, and a neural network trained with the feedforward cascade-correlation algorithm produces 85.4% correct classifications. On the other hand, the derived fuzzy rules, combined with an optimized defuzzification using the cascade correlation neural network, produces 100% correct classifications.
7A8E7E5E	Decision tree induction algorithms represent one of the most popular techniques for dealing with classification problems. However, traditional decision-tree induction algorithms implement a greedy approach for node splitting that is inherently susceptible to local optima convergence. Evolutionary algorithms can avoid the problems associated with a greedy search and have been successfully employed to the induction of decision trees. Previously, we proposed a lexicographic multi-objective genetic algorithm for decision-tree induction, named LEGAL-Tree. In this work, we propose extending this approach substantially, particularly w.r.t. two important evolutionary aspects: the initialization of the population and the fitness function. We carry out a comprehensive set of experiments to validate our extended algorithm. The experimental results suggest that it is able to outperform both traditional algorithms for decision-tree induction and another evolutionary algorithm in a variety of application domains.
7EE2F400	Decision trees have the characteristics of quick learning while its ability of representing complex concept is not very strong. This paper proposes a new algorithm, which can improve the ability of concept representation of decision trees, by utilizing a neural network to compound composite attributes. The composite attribute is compounded only when needed. The computational complexity is not increased much. Experiments show that both training accuracy and test accuracy have remarkable improvements by using this algorithm.
7F288280	We present an automatic method for constructing high-performance preclassification decision trees for OCR. Good preclassifiers prune the set of alternative classes to many fewer without erroneously pruning the correct class. We build the decision tree using greedy entropy minimization, using pseudo-randomly generated training samples derived from a model of imaging defects, and then `populate' the tree with many more samples to drive down the error rate. In [BM94] we presented a statistically rigorous stopping rule for population that enforces a user-specified upper bound on error: this works in practice, but is too conservative, driving the error far below the bound. Here, we describe a refinement that achieves the user- specified accuracy more closely and thus improves the pruning rate of the resulting tree. The method exploits the structure of the tree: the essential technical device is a leaf-selection rule based on Good's Theorem [Good53]. We illustrate its effectiveness through experiments on a pan-European polyfont classifier.
812CDE0E	In this paper a new method is suggested for risk management by the designed patterns in data-mining. These patterns are designed using probability rules in decision trees and are cared to be valid, novel, useful and understandable. Considering a set of functions, the system reaches to a good pattern or better objectives. By using the suggested method the direction of the functionality route in the system can be controlled and systems use them in the best planning for special objectives.
7DF1E102	Most satellites for environmental monitoring point ground targets from a perpendicular position to the terrain, also known as nadir. The CHRIS/PROBA platform provides four extra angles of observation, plus nadir pointing: -55°, -36°, 0°, +36°, and +55°. The purpose of this study is to investigate the best season (wet or dry season) and angle of observation to map tropical dry forest succession in Brazil. Nonparametric decision trees were used to build up classification maps based on principal component analysis (PCA) inputs. Our results indicate that the use of off-nadir data improves the map accuracy of successional stages of tropical dry forests and riparian forests. Particularly, extreme and negative angles of observation generate higher map accuracies, suggesting that tree shadows are enhancing spectral differences between the studied vegetation classes. Images from the dry season provide better total and classes' map accuracies for late and intermediate stages of tropical dry forests. On the other hand, some classes, such as riparian forests and early stage of tropical forests need the use of off-nadir angles of observation to reach a minimum accuracy and best scores are reached using wet season's images.
7612C724	Decision tree is an important method for both induction research and data mining, which is mainly used for model classification and prediction. WEKA is software which is capable of doing work on various decision tree algorithms and support vector machine. In this paper, the comparative study of all Decision Tree algorithms is done. The training time, Accuracy and size of tree are the parameters used as performance measures. It is concluded that J48 Graft algorithm performs better than other algorithms. The dataset used is KDD cup'99 dataset. This dataset contains normal as well as abnormal packets. The dataset is highly uneven. We worked out on some 1000 selected packets. The support vector machine algorithms have the ability to be trained and `learn' in a given environment. This feature can be used in connection with an intrusion detection system, where the support vector machine algorithm can be trained to detect intrusions by recognizing patterns of an intrusion. This paper outlines an investigation on the support vector machine models and choice of one of them for evaluation and implementation. The work also includes works on computer networks, providing a description and analysis of the structure of the computer network in order to generate network features.
756FDCD7	A state space has been assumed as a primitive for modeling uncertainty, which presumes that the analyst knows all the uncertainties a decision maker (DM) perceives. This is problematic because states are private information of the DM, and hence are not directly observable to the analyst. Dekel et al. [Representing preferences with a unique subjective state space, Econometrica 69 (2001) 891–934] derive, rather than assume, the subjective state space from preference over suitable choice objects.In a dynamic setting, a decision tree, that is, a pair consisting of a state space and a filtration, has been taken as a primitive. This assumption is also problematic—a decision tree should be derived rather than assumed as a primitive. We formulate a three-stage extension of the above literature in order to model a DM who anticipates subjective uncertainty to be resolved gradually over time. We identify also subjective beliefs on the subjective state space.
835CB3D9	Sentiment mining is a field of text mining to determine the attitude of people about a particular product, topic, politician in newsgroup posts, review sites, comments on facebook posts twitter, etc. There are many issues involved in opinion mining. One important issue is that opinions could be in different languages (English, Urdu, Arabic, etc.). To tackle each language according to its orientation is a challenging task. Most of the research work in sentiment mining has been done in English language. Currently, limited research is being carried out on sentiment classification of other languages like Arabic, Italian, Urdu and Hindi. In this paper, three classification models are used for text classification using Waikato Environment for Knowledge Analysis (WEKA). Opinions written in Roman-Urdu and English are extracted from a blog. These extracted opinions are documented in text files to prepare a training dataset containing 150 positive and 150 negative opinions, as labeled examples. Testing data set is supplied to three different models and the results in each case are analyzed. The results show that Naïve Bayesian outperformed Decision Tree and KNN in terms of more accuracy, precision, recall and F-measure.
7F2CA464	The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive. Two such approaches are described here, one being incremental tree induction (ITI), and the other being non-incremental tree induction using a measure of tree quality instead of test quality (DMTI). These approaches and several variants offer new computational and classifier characteristics that lend themselves to particular applications.
7B8672EB	The decision tree methodology is an important nonparametric technique for building classifiers from a set of training examples. Most of the existing top-down decision tree design methods make use of single feature splits at successive stages of the tree design. While computationally attractive, single feature splits generally lead to large trees and inferior performance. This paper presents a new top-down decision tree design method that generates compact trees of superior performance by using multifeature splits in place of single feature splits at successive stages of the tree development. The multifeature splits in the proposed method are obtained by combining the concept of information measure of a partition with perceptron learning. Several decision tree induction results for a broad range of classification problems are presented to demonstrate the strengths of the proposed decision tree design methods.
6A1B7A68	Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.
7D22C14D	The authors present a statistical-heuristic feature selection criterion for constructing multibranching decision trees in noisy real-world domains. Real world problems often have multivalued features. To these problems, multibranching decision trees provide a more efficient and more comprehensible solution that binary decision trees. The authors propose a statistical-heuristic criterion, the symmetrical tau and then discuss its consistency with a Bayesian classifier and its built-in statistical test. The combination of a measure of proportional-reduction-in-error and cost-of-complexity heuristic enables the symmetrical tau to be a powerful criterion with many merits, including robustness to noise, fairness to multivalued features, and ability to handle a Boolean combination of logical features, and middle-cut preference. The tau criterion also provides a natural basis for prepruning and dynamic error estimation. Illustrative examples are also presented.
5F4B9926	Bankruptcy is one of the most important considered issues in financial management and investing. Investors always try to predict probability of bankruptcy in order to prevent wasting their capital. Therefore they are search for methods to forecast a potential crisis in the bankruptcy prediction domain. Data mining is an approach which produces prediction models in different grounds by using statistical techniques and artificial intelligence. In this study, the Clementine software and the method of classification and regression tree are used for mining financial variables. The predictor variables are collected from financial statements of firms accepted in Tehran Stock Exchange during the 1996 to 2009. The 94.5% accuracy of the model and extracted rules indicate that the suggested method has an acceptable efficiency to predict bankruptcy. Also the most important financial ratios are determined in decision making.
80438687	The application of simulated annealing to the optimization of decision trees is investigated. An efficient perturbation procedure is described and used as the basis of the Simulated Annealing Classifier System or SACS algorithm. We show that the algorithm is asymptotically convergent for any choice of global cost function. The algorithm is then illustrated, using the Minimum Description Length Principle as cost function, by applying it to several problems involving both noisy and noise-free data.