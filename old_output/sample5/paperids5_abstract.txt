7F3F0AE2	Ecommerce is developing into a fast-growing channel for new business, so a strong presence in this domain could prove essential to the success of numerous commercial organizations. However, there is little research examining ecommerce at the individual customer level, particularly on the success of everyday ecommerce searches. This is critical for the continued success of online commerce. The purpose of this research is to evaluate the effectiveness of search engines in the retrieval of relevant ecommerce links. The study examines the effectiveness of five different types of search engines in response to ecommerce queries by comparing the engines  quality of ecommerce links using topical relevancy ratings. This research employs 100 ecommerce queries, five major search engines, and more than 3540 Web links. The findings indicate that links retrieved using an ecommerce search engine are significantly better than those obtained from most other engines types but do not significantly differ from links obtained from a Web directory service. We discuss the implications for Web system design and ecommerce marketing campaigns.
7510D861	Search engines are essential for finding information on the World Wide Web. We conducted a study to see how effective eight search engines are. Expert searchers sought information on the Web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. We calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. We also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.
5F4B20E5	The effectiveness of twenty public search engines is evaluated using TREC-inspired methods and a set of 54 queries taken from real Web search logs. The World Wide Web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. The engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. Statistical testing reveals a significant difference between engines and high intercorrelations between measures. Surprisingly, given the dynamic nature of the Web and the time elapsed, there is also a high correlation between results of this study and a previous study by Gordon and Pathak. For nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. Performance of the engines as a group is found to be inferior to the group of participants in the TREC-8 Large Web task, although the best engines approach the median of those systems. Shortcomings of current Web search evaluation methodology are identified and recommendations are made for future improvements. In particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. By contrast, real Web search reflects a range of other information need types which require different judging and different measures.
754D28B2	Most online travelers in the United States use search engines to seek out travel information. Thus, Destination Marketing Organizations (DMOs) need to attract clicks through returned results on search engines. We modeled clickthrough rates (CTRs) of several published clickthrough reports and investigate the CTRs of a DMO's webpages on different ranks of different properties (web, image, and mobile searches) on a search engine. The results validated the power-law distribution of CTRs on different ranks: the top results attract high CTRs but the rates decrease precipitously when the ranks go down. However, top ranks are a necessary condition but not a sufficient one: many top ranked results have low CTRs. Image search and mobile search have different CTR curves, providing different opportunities for tourism destinations and businesses.
78A4DC3B	The phenomenon of sponsored search advertising â where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results â is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets
7FF3B599	With the rapid growth of search advertising, there has been an increased interest amongst both practitioners and academics in enhancing our understanding of how consumers respond to contextual and sponsored search advertising on the Internet. An emerging stream of work has begun to explore these issues. In this paper, we focus on a previously unexplored question: How does sponsored search advertising compare to organic listings with respect to predicting conversion rates, order values and profits from a keyword ad? We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that on an average while the conversion rates, order values and profits from paid search advertisements were much higher than those from natural search, most of the keyword-level characteristics have a statistically significant and stronger impact on these three performance metrics for organic search than paid search. This could shed light on understanding what the most "attractive" keywords are from advertisers' perspective, and how advertisers should invest in search engine advertising campaigns relative to search engine optimization.
75FC39F7	A set of measurements is proposed for evaluating Web search engine performance. Some measurements are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. Others are newly developed to evaluate search engine stability, an issue unique to Web information retrieval systems. An experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: Google, AltaVista, and Teoma. Twenty-four subjects ranked four sets of Web pages and their rankings were used as benchmarks against which to compare search engine performance. Results show that the proposed measurements are able to distinguish search engine performance very well
75CC9391	With much information available from open sources on the internet, computer generated databases have become commonplace. The question whether computer generated databases can be protected under the sui generis database right has hitherto received little attention. This article investigates this question and finds that the substantiality of investment, the definition of the rights holder and the interpretation of exclusive rights raise fundamental issues.
7F2A565B	The phenomenon of sponsored search advertising—where advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) Web search results—is gaining ground as the largest source of revenues for search engines. Using a unique six-month panel data set of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different sponsored search metrics such as click-through rates, conversion rates, cost per click, and ranking of advertisements. Our paper proposes a novel framework to better understand the factors that drive differences in these metrics. We use a hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo methods. Using a simultaneous equations model, we quantify the relationship between various keyword characteristics, position of the advertisement, and the landing page quality score on consumer search and purchase behavior as well as on advertiser's cost per click and the search engine's ranking decision. Specifically, we find that the monetary value of a click is not uniform across all positions because conversion rates are highest at the top and decrease with rank as one goes down the search engine results page. Though search engines take into account the current period's bid as well as prior click-through rates before deciding the final rank of an advertisement in the current period, the current bid has a larger effect than prior click-through rates. We also find that an increase in landing page quality scores is associated with an increase in conversion rates and a decrease in advertiser's cost per click. Furthermore, our analysis shows that keywords that have more prominent positions on the search engine results page, and thus experience higher click-through or conversion rates, are not necessarily the most profitable ones—profits are often higher at the middle positions than at the top or the bottom ones. Besides providing managerial insights into search engine advertising, these results shed light on some key assumptions made in the theoretical modeling literature in sponsored search.
75F71181	One of the enabling technologies of the World Wide Web, along with browsers, domain name servers, and hypertext markup language, is the search engine. Although the Web contains over 100 million pages of information, those millions of pages are useless if you cannot find the pages you need. All major Web search engines operate the same way: a gathering program explores the hyperlinked documents of the Web, foraging for Web pages to index. These pages are stockpiled by storing them in some kind of database or repository. Finally, a retrieval program takes a user query and creates a list of links to Web documents matching the words, phrases, or concepts in the query. Although the retrieval program itself is correctly called a search engine, by popular usage the term now means a database combined with a retrieval program. For example, the Lycos search engine comprises the Lycos Catalog of the Internet and the Pursuit retrieval program. This paper describes the Lycos system for collecting, storing, and retrieving information about pages on the Web. After outlining the history and precursors of the Lycos system, the paper discusses some of the design choices made in building this Web indexer and touches briefly on the economic issues involved in working with very large retrieval systems.
7FBD8242	The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.
7DA63EC1	The phenomenon of sponsored search advertising is gaining ground as the largest source of revenues for search engines. Firms across different industries have are beginning to adopt this as the primary form of online advertising. This process works on an auction mechanism in which advertisers bid for different keywords, and final rank for a given keyword is allocated by the search engine. But how different are firm's actual bids from their optimal bids? Moreover, what are other ways in which firms can potentially benefit from sponsored search advertising? Based on the model and estimates from prior work [10], we conduct a number of policy simulations in order to investigate to what extent an advertiser can benefit from bidding optimally for its keywords. Further, we build a Hierarchical Bayesian modeling framework to explore the potential for cross-selling or spillovers effects from a given keyword advertisement across multiple product categories, and estimate the model using Markov Chain Monte Carlo (MCMC) methods. Our analysis suggests that advertisers are not bidding optimally with respect to maximizing profits. We conduct a detailed analysis with product level variables to explore the extent of cross-selling opportunities across different categories from a given keyword advertisement. We find that there exists significant potential for cross-selling through search keyword advertisements in that consumers often end up buying products from other categories in addition to the product they were searching for. Latency (the time it takes for consumer to place a purchase order after clicking on the advertisement) and the presence of a brand name in the keyword are associated with consumer spending on product categories that are different from the one they were originally searching for on the Internet.
7F561450	The market for Internet search is not only economically and socially important, it is also highly concentrated. Is this a problem? We study the question whether "competition is only a free click away". We argue that the market for Internet search is characterized by indirect network externalities and construct a simple model of search engine competition, which produces a market share development that fits the empirically observed development since 2003 well. We find that there is a strong tendency towards market tipping and, subsequently, monopolization, with negative consequences on economic welfare. Therefore, we propose to require search engines to share their data on previous searches. We compare the resulting "competitive oligopoly" market structure with the less competitive current situation and show that our proposal would spur innovation, search quality, consumer surplus, and total welfare. We also discuss the practical feasibility of our policy proposal and sketch the legal issues involved.
5ADB3A52	Describes a study of the retrieval results of World Wide Web search engines. Research quantified accurate matches versus matches of arguable quality for 200 subjects relevant to undergraduate curricula. Both "evaluative" engines (Magellan, Point Communications) and "nonevaluative" engines (Lycos, InfoSeek, AltaVista) were examined. Taking into account indexing depth and searching vagaries, AltaVista and InfoSeek performed the best. (BEW)
8091AFDD	Most major Web search engines typically present sponsored and non-sponsored results in separate listing on the search engine results page. In this research, we investigate the effect of integrating both sponsored and non-sponsored results into a single listing. The premise underlying this research is that searchers are primarily interested in relevant results to their queries. Given the reported negative bias that searchers have concerning sponsored results, separate listings may be a disservice to Web searchers by not directly them to relevant results. Some meta-search engines do combine sponsored and non-sponsored results into a single listing. Using a Web search engine log of more than 7 million interactions from hundreds of thousand of users from a major Web meta-search engine, we analyze the click through patterns of both sponsored and non-sponsored listings from various perspectives. We also classify queries as informational, navigational, and transactional based on the expected type of content destination desired and analyze click through patterns of each. Our findings show that about 80 % of Web queries are informational in nature, approximately 10 % each being transactional, and navigational. Combining sponsored and nonsponsored links does not appear to increase clicks on sponsored listings. In fact, it may decrease such clicks. We discuss how one could use these research results to enhance future sponsored search platforms and search engine results pages.
79110C9C	Commercial search engines are now playing an increasingly important role in Web information dissemination and access. Of particular interest to business and national governments is whether the big engines have coverage biased towards the US or other countries. In our study we tested for national biases in three major search engines and found significant differences in their coverage of commercial Web sites. The US sites were much better covered than the others in the study: sites from China, Taiwan and Singapore. We then examined the possible technical causes of the differences and found that the language of a site does not affect its coverage by search engines. However, the visibility of a site, measured by the number of links to it, affects its chance to be covered by search engines. We conclude that the coverage bias does exist but this is due not to deliberate choices of the search engines but occurs as a natural result of cumulative advantage effects of US sites on the Web. Nevertheless, the bias remains a cause for international concern.
7FDDCCD4	We explore substitution patterns across advertising platforms. Using data on the advertising prices paid by lawyers for 139 Google search terms in 195 locations, we exploit a natural experiment in “ambulance-chaser” regulations across states. When lawyers cannot contact clients by mail, advertising prices per click for search engine advertisements are 5%–7% higher. Therefore, online advertising substitutes for offline advertising. This substitution toward online advertising is strongest in markets with fewer customers, suggesting that the relationship between the online and offline media is mediated by the marketers' need to target their communications.
7F48E77D	With the proliferation of social media, consumers  cognitive costs during information-seeking can become non-trivial during an online shopping session. We propose a dynamic structural model of limited consumer search thatcombines an optimal stopping framework with an individual-level choice model. We estimate the parameters of the model using a dataset of approximately 1 million online search sessions resulting in bookings in 2117 U.S. hotels. The model allows us to estimate the monetary value of the the search costs incurred by users of product search engines in a social media context. On average, searching an extra page on a search engine costs consumers $39.15 and examining an additional offer within the same page has a cost of $6.24, respectively. A good recommendation saves consumers, on average, $9.38, whereas a bad one costs $18.54. Our policy experiment strongly supports this finding by showing that the quality of ranking can have significant impact on consumers  search efforts, and customized ranking recommendations tend to polarize the distribution of consumer search intensity. Our model-fit comparison demonstrates that the dynamic search model provides the highest overall predictive power compared to the baseline static models. Our dynamic model indicates that consumers have lower price sensitivity than a static model would have predicted, implying that consumers pay a lot of attention to non-price factors during an online hotel search 
5D259979	Searching for information in large rather unstructured real-world data sets is a difficult task, because the user expects immediate responses as well as high-quality search results. Today, existing search engines, like Google, apply a keyword-based search, which is handled by indexed-based lookup and subsequent ranking algorithms. This kind of search is able to deliver many search results in a short time, but fails to guarantee that only relevant data is presented. The main reason for the low search precision is the lack of understanding of the system for the original user intention of the search. In the system presented in this paper, the search problem is tackled within a closed domain, which allows semantic technologies to be used. Concretely, a multi-agent system architecture is presented, which is capable of interpreting a key-words based search for the car component domain. Based on domain specific ontologies the search is analyzed and directed towards the interpreted intentions. Consequently, the search precision is increased leading to a substantial improvement of the user search experience. The system is currently in beta state and it is planned to roll out the functionality in near future at the car component online market-place motoso.de.
79ADFC87	The number of health-related websites is increasing day-by-day; however, their quality is variable and difficult to assess. Various "trust marks" and filtering portals have been created in order to assist consumers in retrieving quality medical information. Consumers are using search engines as the main tool to get health information; however, the major problem is that the meaning of the web content is not machine-readable in the sense that computers cannot understand words and sentences as humans can. In addition, trust marks are invisible to search engines, thus limiting their usefulness in practice. During the last five years there have been different attempts to use Semantic Web tools to label health-related web resources to help internet users identify trustworthy resources. This paper discusses how Semantic Web technologies can be applied in practice to generate machine-readable labels and display their content, as well as to empower end-users by providing them with the infrastructure for expressing and sharing their opinions on the quality of health-related web resources.
79A89D40	We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they reflect the interests of a community of like-minded searchers. To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.
787F20C4	The semantic Web browsing offers several benefits for the users. The researchers have done lots of work in this area. The proposals specified by them are not used much effectively for accessing the information. The search engines built today serve all users, independent of the special needs of any individual user. Personalization of Web search for each user incorporating his/her interests would give effective information retrieval. A user may associate one or more categories to their query manually. We have improved the existing, Rocchio based algorithm to construct the general profile and user profile for personalization. In our proposed method, we have constructed a Web browser; the information is retrieved through the browser with the aid of category hierarchy. The category hierarchy information will be frequently updated and ranked as per the user's interest during his/her Web search dynamically, the information retrieved is also cached on the client side using semantic cache mechanism which improves the response time. We have experimentally proved that our technique personalizes the Web search and reduces the hits made by the search engine providing appropriate results and improves the retrieval efficiency
63913EDD	The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming. 
77B7328F	By combinating agent theory and meta search engine, design a reflecting individual query meta search engine model which has a reasonable structure, excellent functionality, and providing integration technology related areas. It is better to help users quickly and accurately search the information to their needs.
77C4F21A	In  this  work,  we  investigate  how  to  propagate  annotated labels for a given single image from the image-level to their corresponding semantic regions, namely Label-to-Region  (L2R),  by  utilizing  the  auxiliary  knowledge  from Internet image search with the annotated image labels as queries.  A nonparametric solution is proposed to perform L2R for single image with complete labels.  First, each la- bel of the image is used as query for online image search engines to obtain a set of semantically related and visually similar images, which along with the input image are encoded as Bags-of-Hierarchical-Patches.   Then,  an efficient  two-stage feature  mining procedure is  presented  to discover  those  input-image  specific,  salient  and  descriptive features for each label from the proposed Interpolation SIFT (iSIFT) feature pool. These features consequently constitute a patch-level representation, and the continuity biased sparse coding is proposed to select few patches from the online images with preference to larger patches to reconstruct a candidate region, which randomly merges the spatially connected patches of the input image.  Such candidate regions are further ranked according to the reconstruction errors, and the top regions are used to derive the label confidence vector for each patch of the input image. Finally, a patch clustering procedure is performed as postprocessing to finalize L2R for the input image.  Extensive experiments on three public databases demonstrate the encouraging performance of the proposed nonparametric L2R solution.
80BE578A	Structured documents (predominantly encoded in XML) utilize markup dialects for several purposes, such as conveying logical structure, or providing rendering instructions. XML structure can also help users to navigate within documents to satisfy their information needs. However, including the user's structural preferences in the ranking of retrieved elements remains a key challenge in XML retrieval. In this paper, we propose an approach for including structural preferences in the ranking of XML elements by improving the structural relevance (SR) of results. SR is an evaluation measure which relies on graphical navigation models to capture the structural preferences of users. We propose several algorithms to post-process search engine output to improve the SR of the output. Experimental results (using data, assessments, and search engines from INEX 2007 and 2008) demonstrate the effect of different combinations of post-processing algorithms and navigation models on the effectiveness of systems.
00B17298	The search engines accessible through the search page of the Comprehensive TEX Archive Network (CTAN) allow you to search in three places:a) the CTAN directory structure and its filenames;b) Google; or c) the Graham Williams catalogue. While each has its advantages, they have a tendency to provide too much information. A new interface to (a) is being tested, which only shows direct matches, and categorises the output into different types of file.
6E411207	This study examines existent and new methods for evaluating the success of information retrieval systems. The theory underlying current methods is not robust enough to allow testing retrieval using different meta-tagging schemas. Traditional measures rely on judgments of whether a document is relevant to a particular question. A good system returns all the relevant documents and no extraneous documents. There is a rich literature questioning the efficacy of relevance judgments. Such questions as, Relevant to whom? When? and To what purpose? are not well-answered in traditional theory. In this study, two new measures (Spink's Information Need and Cooper's Utility) are used in evaluating two search tools (tag-based and text-based), comparing these new measures with traditional measures and each other. The open-source Swish text-based search engine and a self-constructed tag-based search tool were used. Thirty-four educators searched for information using both search engines and evaluated the information retrieved by each. Construct measures, derived by multiplying each of the three measures (traditional, information need, and utility) by a rating of satisfaction were compared using two way analysis of variance. This study specifically analyzes small information systems. The design concepts would be untenable for large systems. Results indicated that there was a significant correlation between the three measures, indicating that the new measures provide an equivalent method of evaluating systems and have some significant advantages, which include not requiring relevance judgments and the ability to use the measures in situ
77AEC8E8	This Study presents a smart information retrieval methodology/smart retrieval query technique that depends on the power of search engine, clawers, full text indexing, and descriptions points for documents contents or websites as known as “An integration framework for search engine architecture to improve information retrieval quality” or smart information retrieval. The new idea for search engine architecture able to make search statement or document print that used in searching operations which depend on Boolean retrieval that uses Boolean algebra and truth table comparative technique. Search engine indexer makes indexing for documents and web sites contents which depend on the performance and quality of search engine, indexer and web clawer to produce precision, recall through crawling and indexing operations to identify folding and stemming words according to smart web query engine which has accurate crawler architecture, truth table comparative technique and search statement or document print.
595640C9	In order to retrieve an item of information on the Web, many search engines have been proposed. They are rarely efficient at the first attempt: the display of results “forces” the user to navigate. In parallel, Web query languages have been developed to avoid these two sequential phases: research then navigation. In this context, the QIRI@D experimental platform, based on the functional programming language SgmlQL, enables both information retrieval and manipulation of distributed semi-structured documents published on a sub-network made up of the sites where QIRI@D is running. It is possible in an unique query to specify criteria to find a document, to filter it, to extract parts of it for building the result. An automatical enrichment of any published document is used to improve the search efficiency.
7928820F	Search reranking is regarded as a common way to boost retrieval precision. The problem nevertheless is not trivial especially when there are multiple features or modalities to be considered for search, which often happens in image and video retrieval. This paper proposes a new reranking algorithm, named circular reranking, that reinforces the mutual exchange of information across multiple modalities for improving search performance, following the philosophy that strong performing modality could learn from weaker ones, while weak modality does benefit from interacting with stronger ones. Technically, circular reranking conducts multiple runs of random walks through exchanging the ranking scores among different features in a cyclic manner. Unlike the existing techniques, the reranking procedure encourages interaction among modalities to seek a consensus that are useful for reranking. In this paper, we study several properties of circular reranking, including how and which order of information propagation should be configured to fully exploit the potential of modalities for reranking. Encouraging results are reported for both image and video retrieval on Microsoft Research Asia Multimedia image dataset and TREC Video Retrieval Evaluation 2007-2008 datasets, respectively
7F2BF256	Today, Web browsers can interpret an enormous amount of different file types, including time-continuous data. By consuming an audio or video, however, the hyperlinking functionality of the Web is "left behind" since these files are typically unsearchable, thus not indexed by common text-based search engines. Our XML-based CMML annotation format and the Annodex file format presented in this paper are designed to solve this problem of "dark matter" on the Internet: Continuous media files are annotated and indexed (i.e., Annodexed), enabling hyperlinks to and from the media. Furthermore, the hyperlinks do not typically point to an entire media file, but to and from arbitrary fragments or intervals. The standards proposed in to create the Continuous Media Web have been submitted to the IETF for review.
790B5A53	We consider fast two-sided error-tolerant search that is robust against errors both on the query side(type alogrithm,find documents with algorithm) as well as on the documentside (type algorithm, find documents with alogrithm). We show how to realize this feature with an index blow-up of 10% to 20% and an increase in query time by a factor of at most 2. We have integrated our two-sided error-tolerant search into a fully-functional search engine, and we provide experiments on three data sets of different kinds and sizes.
7EC88017	General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms.
759C853B	XCD Search- An XML Context-driven Search Engine answers both Keyword-based and Context-driven queries using stack-based sort merge algorithm. It performs well with all criteria of queries against XML trees, except queries submitted against a document, whose XML tree contains a parent node and child interior node, both having same Taxonomic Label and both have child/children data node(s) and/or attribute(s). In this paper we propose An Improved XML Context-driven Search Engine. It uses all the techniques used in XCD Search, in addition to new techniques that handle the type of XML trees mentioned above, which XCD Search does not handle well. We evaluated this system experimentally and compared with original version of XCD Search. The results showed remarkable improvement.
80ED8842	Take a peek at future plans for search engines such as Ask Jeeves and Google, and you ll see a vision for Web search s future that's more sophisticated, individual, and portable.