7DAD351F	Parallel algorithms on SIMD (single-instruction stream multiple-data stream) machines for hierarchical clustering and cluster validity computation are proposed. The machine model uses a parallel memory system and an alignment network to facilitate parallel access to both pattern matrix and proximity matrix. For a problem with N patterns, the number of memory accesses is reduced from O(N/sup 3/) on a sequential machine to O(N/sup 2/) on an SIMD machine with N PEs.
58CF4DA7	Clustering of data has numerous applications and has been studied extensively. Though most of the algorithms in the literature are sequential, many parallel algorithms have also been designed. In this paper, we present parallel algorithms with better performance than known algorithms. We consider algorithms that work well in the worst case as well as algorithms with good expected performance.
6FA7B132	Clustering of data has numerous applications and has been studied extensively. It is very important in Bioinformatics and data mining. Though many parallel algorithms have been designed, most of algorithms use the CRCW-PRAM or CREW-PRAM models of computing. This paper proposed a parallel EREW deterministic algorithm for hierarchical clustering. Based on algorithms of complete graph and Euclidean minimum spanning tree, the proposed algorithms can cluster n objects with O(p) processors in O(n2/p) time where 1≤ p ≤ nlogn. Performance comparisons show that our algorithm is the first algorithm that is both without memory conflicts and adaptive.
7CE77E1C	Hierarchical agglomerative clustering (HAC) is a clustering method widely used in various disciplines from astronomy to zoology. HAC is useful for discovering hierarchical structure embedded in input data. The cost of executing HAC on large data is typically high, due to the need for maintaining global inter-cluster distance information throughout the execution. To address this issue, we propose a new parallelization scheme for multi-threaded shared-memory machines based on the concept of nearest-neighbor (NN) chains. The proposed multi-threaded algorithm allocates available threads into two groups, one for managing NN chains and the other for updating distance information. In-depth analysis of our approach gives insight into the ideal configuration of threads and theoretical performance bounds. We evaluate our proposed method by testing it with multiple public datasets and comparing its performance with that of several alternatives. In our test, the proposed method completes hierarchical clustering 3.09-51.79 times faster than the alternatives. Our test results also reveal the effects of performance-limiting factors such as starvation in chain growing, overhead incurred from using synchronization locks, and hardware aspects including memory-bandwidth saturation. According to our evaluation, the proposed scheme is effective in improving the HAC algorithm, achieving significant gains over the alternatives in terms of runtime and scalability.
79AFD2A7	We introduce information retrieval strategies which are based on automatic hierarchic clustering of documents. We discuss the evaluation of retrieval strategies and show, using a subset of the Cranfield Aeronautics document collection, that cluster-based retrieval strategies can be devised which are as effective as linear associative retrieval strategies and much more efficient. Finally, we outline how cluster-based retrieval may be extended to large growing document collections and indicate some ways in which the effectiveness of cluster-based retrieval strategies may be improved.
7CB6B315	A monotone invariant method of hierarchical clustering based on the Mann-Whitney U-statistic is presented. The effectiveness of the complete-link, single-link, and U-statistic methods in recovering tree structures from error perturbed data are evaluated. The U-statistic method is found to be consistently more effective in recovering the original tree structures than either the single-link or complete-link methods.
756A50D3	Johnson has shown that the single linkage and the complete linkage hierarchical clustering algorithms induce a metric on the data known as the ultrametric. Through the use of the Lance and Williams recurrence formula, Johnson's proof is extended to four other common clustering algorithms. It is also noted that two additional methods produce hierarchical structures which can violate the ultrametric inequality.
77245C93	Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally “connected,” while the other forms clusters that are optimally “compact.”
778095CE	Clustering techniques are usually used in pattern recognition, image segmentation and object detection. Let N be the number of patterns and M be the number of features of each pattern and . In this paper, we first design two O(1) time basic operations for concentrating all nonempty data of size N and computing the proximity matrix using N × N and N × N × M processors, respectively. Then, based on these two operations, a constant time parallel hierarchical clustering algorithm is proposed on a 3-D processor array with reconfigurable bus system using N4 processors. Then, by reducing the number of processors by a factor of N, an O(log2 N) time algorithm for this problem is also derived. Note that no one had ever obtained a constant time algorithm for this problem on the existing parallel computation models
7C9D9D1B	Milligan presented the conditions that are required for a hierarchical clustering strategy to be monotonic, based on a formula by Lance and Williams. In the present paper, the statement of the conditions is improved and shown to provide necessary and sufficient conditions.
76035A9B	Hierarchical agglomerative clustering (HAC) is very useful but due to high CPU time and memory complexity its practical use is limited. Earlier, we proposed an efficient partitioning – partially overlapping partitioning (POP) – based on the fact that in HAC small and closely placed clusters are agglomerated initially, and only towards the end larger and distant clusters are agglomerated. Here, we present the parallel version of POP, pPOP. Theoretical analysis shows that, compared to the existing algorithms, pPOP achieves CPU time speed-up and memory scale-down of O(c) without compromising accuracy where c is the number of cells in the partition. A shared memory implementation shows that pPOP outperforms existing algorithms significantly.
7FA96432	It has often been asserted that since hierarchical clustering algorithms require pairwise interobject proximities, the complexity of these clustering procedures is at least O(N2). Recent work has disproved this by incorporating efficient nearest neighbour searching algorithms into the clustering algorithms. A general framework for hierarchical, agglomerative clustering algorithms is discussed here, which opens up the prospect of much improvement on current, widely-used algorithms. This ‘progress report’ details new algorithmic approaches in this area, and reviews recent results
7F1704E8	Clustering is a basic operation in image processing and computer vision, and it plays an important role in unsupervised pattern recognition and image segmentation. While there are many methods for clustering, the single-link hierarchical clustering is one of the most popular techniques. In this paper, with the advantages of both optical transmission and electronic computation, we design efficient parallel hierarchical clustering algorithms on the arrays with reconfigurable optical buses (AROB). We first design three efficient basic operations which include the matrix multiplication of two N×N matrices, finding the minimum spanning tree of a graph with N vertices, and identifying the connected component containing a specified vertex. Based on these three data operations, an O(log N) time parallel hierarchical clustering algorithm is proposed using N3 processors. Furthermore, if the connectivity of the AROB with four-port connection is allowed, two constant time clustering algorithms can be also derived using N4 and N3 processors, respectively. These results improve on previously known algorithms developed on various parallel computational models.
85C8C653	This paper proposes a hierarchical clustering algorithm based on Saturated Neighbor Graph -- hi-CLUBS and a new concept, natural nearest neighbor, which adopts a parameter-less algorithm of searching the natural neighbors for each point in a dataset. In the work, the Saturated Neighbor Graph is constructed by the natural nearest neighbor firstly. Then modularity is introduced into graph partitioning algorithm, with which the generated graph is partitioned into small sub-clusters without any parameters. Finally, these initial sub-clusters are repeatedly merged with another cluster according to similarity measurement based on connectivity and closeness, until the desired cluster number is reached. The results show that hi-CLUBS produces a set of final clusters achieves better quality than the traditional clustering algorithms.
805F1CE5	HMM has been largely applied in many fields with great success. To achieve a better performance, an easy way is using more states or more free parameters for a better signal modelling. Thus, state sharing and state clipping methods have been proposed to reduce parameter redundancy and to limit the explosive consummation of system resources. We focus on a simple state sharing method for a hybrid neuro-Markovian on-line handwriting recognition system. At first, a likelihood-based distance is proposed for measuring the similarity between two HMM state models. Afterwards, a minimum quantification error aimed hierarchical clustering algorithm is also proposed to select the most representative models. Here, models are shared to the most under the constraint of the minimum system performance loss. As the result, we maintain about 98% of the system performance while about 60% of the parameters are reduced.
59658E71	When performing hierarchical clustering, some metric must be used to determine the similarity between pairs of clusters. Traditional similarity metrics either can only deal with simple shapes or are very sensitive to outliers. We propose two potential-based similarity metrics, APES and AMAPES, inspired by the concept of electric potential in physics. The main features of these metrics are: they have strong anti-jamming capability; and they are capable of finding clusters of complex irregular shapes.
7E6AAED5	Among the microarray data analysis clustering methods, K-means and hierarchical clustering are researchers' favorable tools today. However, each of these traditional clustering methods has its limitations. In this study, we introduce a new method, hierarchical K-means regulating divisive or agglomerative approach. The hierarchical K-means firstly employs K-means' algorithm in each cluster to determine K cluster while operating and then employs it on hierarchical clustering technique to shorten merging clusters time while generating a tree-like dendrogram. We apply this method in two original microarray datasets. The result indicates divisive hierarchical K-means is superior to hierarchical clustering on cluster quality and is superior to K-means clustering on computational speed. Our conclusion is that divisive hierarchical K-means establishes a better clustering algorithm satisfying researchers' demand. 	
71CD6C9A	In this paper we introduce the novel class hierarchy construction algorithm (CHCA) in order to create hierarchical clusterings of Web documents. Unlike most clustering methods, CHCA operates on nominal data (the words occurring in each document) and it differs from other hierarchical clustering techniques in that it uses the object-oriented concept of inheritance to create the parent/child relationship between clusters. A prototype system has been developed using CHCA to create cluster hierarchies from web search results returned by conventional search engines. CHCA, without any guidance, creates term-based clusters from the contents of the retrieved pages and assigns each page to a cluster; the clusters correspond to topics and sub-topics in the investigated domain. The performance of our system is compared with a similar web search clustering system (Vivisimo).
76DDB4CD	Non-hierarchical clustering methods are frequently based on the idea of forming groups around 'objects'. The main exponent of this class of methods is the "k"-means method, where these objects are points. However, clusters in a data set may often be due to certain relationships between the measured variables. For instance, we can find linear structures such as straight lines and planes, around which the observations are grouped in a natural way. These structures are not well represented by points. We present a method that searches for linear groups in the presence of outliers. The method is based on the idea of impartial trimming. We search for the 'best' subsample containing a proportion 1 - "&agr;" of the data and the best "k" affine subspaces fitting to those non-discarded observations by measuring discrepancies through orthogonal distances. The population version of the sample problem is also considered. We prove the existence of solutions for the sample and population problems together with their consistency. A feasible algorithm for solving the sample problem is described as well. Finally, some examples showing how the method proposed works in practice are provided. Copyright (c) 2009 Royal Statistical Society. 
06E566AC	this paper provides a good representation of the clustering structure, thus it can be used as a tool to get insight into the distribution of a data set. In addition, the visualization can also reveal hierarchical cluster with different sizes, densities and shapes. 
79249275	Tens, hundreds and even thousands of cores are to be integrated into a single chip. Network on chip appears to offer efficient communication between cores. However, the increased requirement for larger communication bandwidths and lower power consumption challenges the traditional electrical interconnects. Advances in silicon nanophotonics make optical interconnect a promising candidate for network on chip architectures in future. In this paper, we develop a hierarchical cluster-based optical NoC (HCONoC). It is a hybrid electrical/optical on chip network architecture. It connects the lowest level cluster of IP cores with electrical interconnects while optical interconnects are used for inter-cluster communication to improve the efficiency of the network. Three types of routers including electrical and optical router architectures are carefully designed together with an efficient routing algorithm. Packet switching is employed which helps to avoid the overhead of path setup, alleviating the contention. Finally, we simulate the HCONoC for the 64-core architecture and show the network performance including end-to-end delay and network throughput.
7528D3D9	In this paper, we propose and evaluate a novel solution for delay sensitive one-to-many content distribution in P2P networks based on  hierarchical clustering. Our delivery scheme involves cooperation among the participating peers.The source splits the content into variable size blocks and sends the blocks to a subset of peers. All peers redistribute parts of the content to other peers. Our approach adopts a tree  structure as the global structure to achieve scalability, while fully connected small clusters based on proximity are built in each hierarchical  level of the tree, taking advantage of the higher transmission capacities among neighboring peers. Every peer contributes its upload capacity to the system by being a forwarding peer within a cluster. Our evaluation made on PlanetLab shows that our proposal achieves a good performance, short delivery time and scalability.    
7A989C58	In this paper we propose to combine two clustering approaches, namely fuzzy and possibilistic c-means. While fuzzy c-means algorithm finds suitable clusters for groups of data points, obtained memberships of data, however, encounters a major deficiency caused by misinterpretation of membership values of data points. Therefore, membership values cannot correctly interpret compatibility or degree to which data points belong to clusters. As a result, noisy data will be misinterpreted by incorrect memberships assigned, as sum of memberships of each noisy data to all clusters is constrained to be equal to 1. To overcome this, a possibilistic approach has been proposed which removes this constraint. It has, however, caused another shortcoming as cluster centers converge to an identical point. Therefore, possibilities cannot correctly interpret the degrees of compatibilities. To correct this problem, a number of works have been carried out which all try to change possibilistic objective function proposed by Krishnapuram and James M. Keller. In this work, a hierarchical approach has been proposed based on properties of both fuzzy and possibilistic approaches to overcome this deficiency. Sensitivities of both methods have been studied together with analyzing results obtained by both methods. Superiority of the proposed method as opposed to conventional possibilistic c-means is shown to be conspicuous. 
04208BBF	 Clustering analysis is an important technique in many applications, such as in biology, medicine, psychology, pattern recognition, image processing, marketing, and data engineering. The large number of existing clustering algorithms can be broadly classified into two types: (1) hierarchical and (2) partitional. Depending on the algorithmic approach taken, a hierarchical structure begins with N clusters, one per pattern, and grows a sequence of clusterings until all N patterns are in a single cluster (the agglomerative approach), or begins with one cluster containing all N patterns and successively divides clusters until N clusters are achieved (the divisive approach). Hierarchical clustering is a sequence of nested partitions in the form of a tree diagram or a dendrogram, whereas a partitional clustering is a single partition. Some fuzzy partitional clustering algorithms and their convergence properties have been given in the literature, but no fuzzy hierarchical clustering algorithm has yet been presented. In this study, fuzzy hierarchical clustering algorithms for the agglomerative approach and the divisive approach, respectively, are proposed. A performance comparison among the two proposed algorithms with different parameter values is included. The two proposed algorithms are compared with two existing algorithms. Additionally, to teduce computational time, the corresponding parallel versions of the two proposed algorithms are developed. Some experimental results show the feasibility of the proposed approaches. 
7731DE76	The processing power of parallel coprocessors like the Graphics Processing Unit (GPU) is dramatically increasing. However, until now only a few approaches have been presented to utilize this kind of hardware for mesh clustering purposes. In this paper, we introduce a Multilevel clustering technique designed as a parallel algorithm and solely implemented on the GPU. Our formulation uses the spatial coherence present in the cluster optimization and hierarchical cluster merging to significantly reduce the number of comparisons in both parts. Our approach provides a fast, high-quality, and complete clustering analysis. Furthermore, based on the original concept, we present a generalization of the method to data clustering. All advantages of the mesh-based techniques smoothly carry over to the generalized clustering approach. Additionally, this approach solves the problem of the missing topological information inherent to general data clustering and leads to a Local Neighbors k-means algorithm. We evaluate both techniques by applying them to Centroidal Voronoi Diagram (CVD)-based clustering. Compared to classical approaches, our techniques generate results with at least the same clustering quality. Our technique proves to scale very well, currently being limited only by the available amount of graphics memory.
77560214	Attribute discretization is one of the basis pre-treatment methods for data stream mining. For the reason that the attribute discretization algorithm cannot be applied with the highspeed data stream directly, we firstly build a synopsis structure for data stream, then proposed an attribute discretization algorithm based on a synopsis structure, finally, the simulation experiment results show that the method has achieved the problem of data stream attribute discretization.
000069BF	A few years ago, Vanecek (1994) suggested to apply a variant of back-face culling to speed-up collision detection between polyhedral objects. However, Vanecek's method is linear in the number of faces in the object, which is unpractical for large models. This paper suggests to add some geometrical information to hierarchies of bounding volumes, typically used in collision detection, and perform conservative back-face culling at the bounding-volume level in constant time. The method described in this paper can be applied to complement any kind of bounding-volumes hierarchy and allows a trade-off between memory and speed. Preliminary experimental results suggest that the method allows a significant speed-up, especially in close proximity situations.
590717D5	Chapter Eight begins with a review of the clustering task, and the concept of distance. Good clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation. First, hierarchical clustering methods are examined. In hierarchical clustering, a treelike cluster structure (dendrogram) is created through recursive partitioning (divisive methods) or combining (agglomerative) of existing clusters. Single-linkage, complete-linkage, and average-linkage methods are discussed. The single-linkage and complete-linkage clustering algorithms are walked-through, using a small univariate data set. Differences in the resulting dendrogram structure are discussed. The average-linkage algorithm is shown to produce the same dendrogram as the complete-linkage algorithm, for this data set, though not necessarily in general. Next, we turn to the k-means clustering algorithm, beginning with the definition of the steps involved in the algorithm. Cluster centroids are defined. The k-means algorithm is walked-through, using a tiny bivariate data set, showing graphically how the cluster centers are updated. An application of k-means clustering to the large churn data set is undertaken, using SAS Enterprise Miner. The resulting clusters are profiled. Finally, the methodology of using cluster membership for further analysis downstream is illustrated, with the clusters identified by SAS Enterprise Miner helping to predict churn. The exercises include challenges to readers to construct single-linkage, complete-linkage, and k-means clustering solutions for small univariate and bivariate data sets. The hands-on analysis problems include generating k-means clusters using the cereals data set, and applying these clusters to help predict nutrition rating.
7DA4FF3A	This paper focuses on document clustering algorithms that build hierarchical solutions. In this paper is evaluate the performance of different criterion functions for the problem of clustering documents.
798FC174	Hierarchical clustering (HC) is a widely used approach both in pattern recognition and data mining and has rich solutions in the literature. But all these existing solutions have some restrictions when the clustered dataset has complex structure. Spectral clustering is a graph-based, simple and outperforming method with the ability to find complex structure in dataset using spectral properties of the dataset-associated affinity matrix. In this paper, we propose a novel effective HC algorithm called SHC base on the techniques of spectral method. The experiment results both on artificial and real data sets show that our algorithm can hierarchically cluster complex data effectively and naturally.